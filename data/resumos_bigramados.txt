 Este trabalho apresenta uma ferramenta de apoio ao desenvolvimento de aplicações distribuídas , baseadas em RPCs , para o ambiente Windows 95 . Discute , ainda , algumas aplicações construídas para validar o sistema , as quais seguem o paradigma cliente-servidor . É feita uma revisão bibliográfica dos assuntos mais relevantes relacionados ao tema e apresentados os detalhes de implementação da ferramenta . Esta foi implementada com técnicas de orientação a objetos , sendo composta por um Gerador Automático de Stubs e uma Biblioteca RPC , além de possibilitar a utilização de um Serviço de Binding . As aplicações construídas procuram explorar as potencialidades da ferramenta , sendo fornecidas também orientações gerais para o desenvolvimento de aplicações distribuídas no ambiente Windows 95 
 O crescimento do mercado de software a cada dia acarreta o aumento do uso de técnicas de desenvolvimento , muitas_vezes  informais . A manutenção de tais softwares torna-se problemática , uma vez que a documentação associada ao software , na maioria das vezes , não está de acordo com o código implementado . Dessa forma , quando diante da manutenção do produto , o engenheiro de software encontra uma documentação informal e incompleta , que não reflete o software existente . Nesse contexto é que se encontra a Engenharia Reversa de Software , com o propósito de recuperar as informações de projeto perdidas durante a fase de desenvolvimento , e de documentar o real estado do software . O principal objetivo deste trabalho de mestrado foi a investigação de uma estrutura adequada de hiperdocumento para apoiar a documentação requerida durante o processo de engenharia_reversa  de software . A partir de um levantamento dos requisitos desejáveis em um hiperdocumento , para que possua as habilidades de suporte à documentação de engenharia de reversa de software , um conjunto de links e estrutura de nós foi definido . Os requisitos , para a composição de tal hiperdocumento , foram investigados por meio de uma experiência : a autodocumentação do sistema hipermídia denominado SASHE ( Sistema de Autoria e Suporte Hipermídia para Ensino ) , que já possui suporte ao tratamento de nós aninhados e outras características de aplicativo para ensino . A engenharia_reversa  foi desenvolvida baseando-se no método de engenharia_reversa  Fusion-RE/I , e os produtos obtidos foram inseridos em uma hiperbase no SASHE 
 Nossa dissertação é investigado o uso de métodos probabilísticos para a navegação de robôs_móveis  autônomos . ( ) objetivo principal é o desenvolvimento de um software para navegação de um robô_móvel  que seja capaz de transportar documentos de forma autônoma entre locais distantes , como as diversas salas do prédio de laboratórios do Instituto de Ciências_Matemáticas  e de Computação ( ICMC ) da Universidade de São Paulo ( USP ) . Isso deve ser feito sem que sejam necessárias modificações de qualquer tipo no ambiente . Para isso , o software deve ser capaz de : ( l ) estimar a posição do robô sem o uso de marcos artificiais , ( 2 ) criar e manter mapas do ambiente , ( 3 ) desviar de obstáculos estáticos e dinâmicos , e ( 4 ) calcular a trajetória entre os pontos em que o robô deverá receber e entregar documentos . Um sistema de navegação , utilizando os métodos probabilísticos de mapeamento Occupancy Grui e de localização Monte_Carlo  , foi implementado e diversos experimentos foram realizados para demonstrar sua adequação à tarefa de entrega de documentos . Esses métodos foram testados em um ambiente real do ICMC-USP , utilizando-se o robô_móvel  Pioneer 1 
 A capacidade mais importante de um sistema de navegação para um robô_móvel  , além de manter-se operacional e evitar colisões , é a que estabelece a sua própria posição em relação ao ambiente . A esta capacidade dá-se o nome de localização , que consiste em atualizar a posição do robô no ambiente , tendo como base , as leituras provenientes dos sensores . Talvez o fato mais importante obtido a partir do vasto conjunto de pesquisas sobre o tema localização de robôs_móveis  , seja que , até hoje , apesar de existirem muitas aplicações bem sucedidas , ainda busca-se uma solução elegante e robusta para o problema de localização . As técnicas existentes para localização de robôs_móveis  podem ser classificadas nos seguintes grupos : técnicas baseadas em posicionamento relativo e as que se baseiam em posicionamento absoluto . Dentre as pertencentes ao primeiro grupo , destacam-se os métodos de localização odométricos e aos pertencentes ao segundo , os baseados em landmarks . Este trabalho_investiga  o potencial de alguns algoritmos que implementam métodos odométricos e métodos baseados em landmarks , visando uma implementação híbrida dos mesmos . O desempenho dos métodos implementados foi avaliado no simulador Saphira e no robô_móvel  Pioneer 1 , existente no Laboratório de Inteligência Computacional ( LABIC ) , através de aplicações práticas 
 Nesta dissertação foi desenvolvido um sistema para controle remoto de robôs_móveis  via Internet . Este sistema é composto por um módulo de controle do robô e uma interface Web . O módulo de controle do robô é responsável pela navegação do robô que envolve as seguintes tarefas : mapeamento do ambiente , localização automática do robô e planejamento de trajetórias . A interface Web foi desenvolvida em Java e é responsável por estabelecer a comunicação entre o usuário e o robô . Através desta interface , o usuário pode controlar o robô e visualizar as imagens capturadas durante a execução de uma determinada tarefa . Este sistema foi testado tanto em ambientes simulados quanto em um ambiente real . Para realização dos testes , foram utilizados vários usuários enviando requisições ao sistema simultaneamente . Nestes testes foram utilizados vários ambientes diferentes . Em todos os experimentos , o sistema proposto neste trabalho apresentou resultados_satisfatórios  tanto em relação à comunicação entre os vários módulos que compõem o sistema quanto na realização das tarefas por parte do robô . Este sistema integra recursos de hardware e software e disponibiliza esses recursos a qualquer usuário que esteja conectado na Internet . Usuários de qualquer parte do mundo poderão , graças ao sistema desenvolvido , controlar um robô remotamente 
 Os cartões , sejam de crédito ou débito , são meios de pagamento altamente utilizados . Esse fato desperta o interesse de fraudadores . O mercado de cartões enxerga as fraudes como custos operacionais , que são repassados para os consumidores e para a sociedade em geral . Ainda , o alto volume de transações e a necessidade de combater as fraudes abrem espaço para a aplicação de técnicas de Aprendizagem de Máquina ; entre elas , os classificadores . Um tipo de classificador largamente utilizado nesse domínio é o classificador baseado em regras . Entretanto , um ponto de atenção dessa categoria de classificadores é que , na prática , eles são altamente dependentes dos especialistas no domínio , ou seja , profissionais que detectam os padrões das transações fraudulentas , os transformam em regras e implementam essas regras nos sistemas de classificação . Ao reconhecer esse cenário , o objetivo desse trabalho é propor a uma arquitetura baseada em regras de associação e regressão logística - técnicas estudadas em Aprendizagem de Máquina - para minerar regras nos dados e produzir , como resultado , conjuntos de regras de detecção de transações fraudulentas e disponibilizá-los para os especialistas no domínio . Com isso , esses profissionais terão o auxílio dos computadores para descobrir e gerar as regras que embasam o classificador , diminuindo , então , a chance de haver padrões fraudulentos ainda não reconhecidos e tornando as atividades de gerar e manter as regras mais eficientes . Com a finalidade de testar a proposta , a parte experimental do trabalho contou com cerca de 7,7 milhões de transações reais de cartões fornecidas por uma empresa participante do mercado de cartões . A partir daí , dado que o classificador pode cometer erros ( falso-positivo e falso-negativo ) , a técnica de análise sensível ao custo foi aplicada para que a maior_parte  desses erros tenha um menor_custo  . Além_disso  , após um longo trabalho de análise do banco de dados , 141 características foram combinadas para , com o uso do algoritmo FP-Growth , gerar 38.003 regras que , após um processo de filtragem e seleção , foram agrupadas em cinco conjuntos de regras , sendo que o maior deles tem 1.285 regras . Cada um desses cinco conjuntos foi submetido a uma modelagem de regressão logística para que suas regras fossem validadas e ponderadas por critérios estatísticos . Ao final do processo , as métricas de ajuste estatístico dos modelos revelaram conjuntos bem ajustados e os indicadores de desempenho dos classificadores também indicaram , num geral , poderes de classificação muito bons ( AROC entre 0,788 e 0,820 ) . Como conclusão , a aplicação combinada das técnicas estatísticas - análise sensível ao custo , regras de associação e regressão logística - se mostrou conceitual e teoricamente coesa e coerente . Por fim , o experimento e seus resultados demonstraram a viabilidade técnica e prática da proposta 
 A classificação é uma tarefa do aprendizado de máquina e mineração de dados , na qual um classificador é treinado sobre um conjunto de dados rotulados de forma que as classes de novos itens de dados possam ser preditas . Tradicionalmente , técnicas de classificação trabalham por definir fronteiras de decisão no espaço de dados considerando os atributos físicos do conjunto de treinamento e uma nova instância é classificada verificando sua posição relativa a tais fronteiras . Essa maneira de realizar a classificação , essencialmente baseada nos atributos físicos dos dados , impossibilita que as técnicas tradicionais sejam capazes de capturar relações semânticas existentes entre os dados , como , por exemplo , a formação de padrão . Por outro_lado  , o uso de redes complexas tem se apresentado como um caminho promissor para capturar relações espaciais , topológicas e funcionais dos dados , uma vez que a abstração da rede unifica a estrutura , a dinâmica e as funções do sistema representado . Dessa forma , o principal objetivo desta tese é o desenvolvimento de métodos e heurísticas baseadas em teorias de redes complexas para a classificação de dados . As principais_contribuições  envolvem os conceitos de conformidade de padrão , caracterização de importância e otimização estrutural de redes . Para a conformidade de padrão , onde medidas de redes complexas são usadas para estimar a concordância de um item de teste com a formação de padrão dos dados , é apresentada uma técnica híbrida simples pela qual associações físicas e topológicas são produzidas a partir da mesma rede . Para a caracterização de importância , é apresentada uma técnica que considera a importância individual dos itens de dado para determinar o rótulo de um item de teste . O conceito de importância aqui é definido em termos do PageRank , algoritmo usado na engine de busca do Google para definir a importância de páginas da web . Para a otimização estrutural de redes , é apresentado um framework bioinspirado capaz de construir a rede enquanto otimiza uma função de qualidade orientada à tarefa , como , por exemplo , classificação , redução de dimensionalidade , etc . A última investigação apresentada no documento explora a representação baseada em grafo e sua habilidade para detectar classes de distribuições arbitrárias na tarefa de difusão de papéis semânticos . Vários experimentos em bases de dados artificiais e reais , além de comparações com técnicas bastante usadas na literatura , são fornecidos em todas as investigações . Em suma , os resultados obtidos demonstram que as vantagens e novos conceitos propiciados pelo uso de redes se configuram em contribuições relevantes para as áreas de classificação , sistemas de aprendizado e redes complexas 
 Modelos de arquitetura têm sido utilizados para permitir o desenvolvimento mais adequado e estruturado de sistemas , desde os mais simples até os mais complexos . A utilização desses modelos em sistemas embarcados , principalmente quando se trata de sistemas embarcados_críticos  , como é o caso de veículos aéreos não tripulados ( VANT ) , visam a permitir conformidades de padrões , redução no tempo de produção , redução e facilidade no processo de manutenção e desenvolvimento . Sistemas embarcados_críticos  possuem requisitos específicos , tais como alta confiabilidade e resposta em tempo real , segurança e desempenho . A definição de um modelo arquitetural que permita que esses quesitos sejam levados em consideração , propicie o atendimento aos padrões , além de permitir o desenvolvimento correto e acelerado é inovador , permitindo que não só a comunidade_científica  venha a ter benefícios com a sua concepção , mas também a indústria brasileira possa ganhar . Nesse sentido , este trabalho desenvolveu um modelo de arquitetura para a interconexão de sistemas aéreos não tripulados ( SANTs ) em Unified Modeling Language ( UML ) /System Modelling Language ( SysML ) denominado LARISSA ( Layered ARchitecture model for Interconnection of SystemS in uAs ) . Como resultado deste trabalho foi possível a modelagem e especificação completa de um SANT fazendo_uso  desse modelo e a realização de diversos experimentos que permitiram validar o LARISSA . Os experimentos , focados na parte de comunicação , permitiram a concepção de um simulador de redes de VANTs . Os resultados obtidos demonstraram a eficiência e a eficácia do modelo de arquitetura LARISSA , além da sua flexibilidade em permitir que diferentes experimentos possam ser realizados , o que auxilia na obtenção de dados que facilitam o processo de certificação desses VANTs 
 Esta dissertação aborda o problema de redução do consumo de combustível para veículos . Com esse objetivo , realiza-se o levantamento de um modelo estocástico e de seus parâmetros , o desenvolvimento de um controlador para o veículo , e análise dos resultados . O problema considera a interação com o trânsito de outros veículos , que limita a aplicação de resultados antes disponíveis . Para isto , propõe-se modelar a dinâmica do problema de maneira aproximada , usando sistemas com saltos markovianos , e levantar as probabilidades de transição dos estados da cadeia através de um modelo mais completo para o trânsito no percurso 
 O mercado global dos sistemas educacionais Web ( WES , do inglês , Web Educational Systems ) continua a mudar , crescer e evoluir em todo o mundo , impulsionado por diversos fatores , entre eles , pelas novas tendências em tecnologias emergentes e ferramentas que dão suporte aos WES e , ainda , ao papel crescente da aprendizagem social como prioridade para o processo de desenvolvimento de tais sistemas . Os Sistemas Educacionais baseados em Web Semântica ( SWBES , do inglês Semantic Web-based Educational Systems ) são plataformas educacionais Web desenvolvidas para resolver diversos problemas enfrentados pelos alunos e outros usuários , como dificuldades relacionadas à busca , compartilhamento e reutilização de recursos educacionais . SWBES têm sido frequentemente usados como motores de busca para plataformas MOOC e Sistemas Tutores Inteligentes ( STI ) . Portanto , é necessário garantir a qualidade desses sistemas para possibilitar melhores experiências de ensino e aprendizagem . No entanto , a avaliação da qualidade dos SWBES é uma tarefa complexa , uma vez que requer um amplo conhecimento sobre as tecnologias da Web Semântica , Educação a Distância , Engenharia de Software , além das normas e padrões utilizados para a Avaliação da Qualidade de Software . O problema identificado foi a ausência de uma abordagem de avaliação de qualidade desses sistemas a partir dos elementos da arquitetura do sistema , ou seja , a atuação e estruturação dos agentes inteligentes , ontologias , objetos de aprendizagem , repositórios , metadados etc . Nesse contexto , o objetivo dessa tese foi desenvolver uma abordagem para a avaliação de qualidade de SWBES . Para atingir o objetivo proposto , foram necessários identificar os critérios de qualidade utilizados na avaliação de qualidade de software , de WES ( incluindo recursos educacionais , tais como os objetos de aprendizagem ) , das tecnologias da Web Semântica ( tais como ontologias ) . Esses critérios foram classificados , analisados e ajustados , com base nos requisitos de qualidade dos SWBES apontados na literatura . Em seguida , um conjunto de fatores de qualidade foi estabelecido , com subfatores e critérios que serão utilizados como diretriz geral para avaliar e comparar a qualidade de SWBES . Foram definidos os avaliadores responsáveis de acordo com os papéis que representam no sistema . Estabeleceu-se também quais os artefatos do SWBES que devem ser avaliados pela abordagem , de modo que o processo de avaliação seja simplificado . A abordagem foi automatizada em uma ferramenta Web e validada por meio de estudos de caso , por meio de especialistas no domínio da Web Semântica , Informática e Educação . Espera-se que a abordagem desenvolvida possa contribuir com profissionais , desenvolvedores e outros usuários ( professores , educadores , alunos , mediadores , tutores e gestores ) que desejam ou necessitam adquirir e utilizar SWBES , de forma que possam efetuar a aquisição adequada às suas necessidades e objetivos . Além_disso  , espera-se poder colaborar no processo de desenvolvimento de SWBES , a partir dos fatores de qualidade estabelecidos para a abordagem , e promover a garantia de qualidade desde o início do processo do desenvolvimento . Como trabalhos futuros , pretende-se ampliar os fatores de qualidade e estender a abordagem para permitir a avaliação de outros Sistemas baseados em Web Semântica , em outros domínios do conhecimento 
 Séries temporais são conjuntos de dados ordenados no tempo . Devido à ubiquidade desses dados , seu estudo é interessante para muitos campos da ciência . A mineração de dados temporais é uma área de pesquisa que tem como objetivo extrair_informações  desses dados relacionados no tempo . Para isso , modelos são usados para descrever as séries e buscar por padrões . Uma forma de modelar séries_temporais  é por meio de redes complexas . Nessa modelagem , um mapeamento é feito do espaço temporal para o espaço topológico , o que permite avaliar dados temporais usando técnicas de redes . Nesta tese , apresentamos soluções para tarefas de mineração de dados de séries_temporais  usando redes complexas . O objetivo principal foi avaliar os benefícios do uso da teoria de redes para extrair_informações  de dados temporais . Concentramo-nos em três tarefas de mineração . ( 1 ) Na tarefa de agrupamento , cada série_temporal  é representada por um vértice e as arestas são criadas entre as séries de acordo com sua similaridade . Os algoritmos de detecção de comunidades podem ser usados para agrupar séries semelhantes . Os resultados mostram que esta abordagem apresenta melhores_resultados  do que os resultados de agrupamento tradicional . ( 2 ) Na tarefa de classificação , cada série_temporal  rotulada em um banco de dados é mapeada para um gráfico de visibilidade . A classificação é realizada transformando uma série_temporal  não marcada em um gráfico de visibilidade e comparando-a com os gráficos rotulados usando uma função de distância . O novo rótulo é dado pelo rótulo mais frequente nos k grafos mais próximos . ( 3 ) Na tarefa de detecção de periodicidade , uma série_temporal  é primeiramente transformada em um gráfico de visibilidade . Máximos locais em uma série_temporal  geralmente são mapeados para vértices altamente conectados que ligam duas comunidades . O método proposto utiliza a estrutura de comunidades para realizar a detecção de períodos em séries_temporais  . Este método é robusto para dados ruidosos e não requer parâmetros . Com os métodos e resultados apresentados nesta tese , concluímos que a teoria da redes complexas é benéfica para a mineração de dados em séries_temporais  . Além_disso  , esta abordagem pode proporcionar melhores_resultados  do que os métodos tradicionais e é uma nova forma de extrair_informações  de séries_temporais  que pode ser facilmente estendida para outras tarefas 
 Esta dissertação descreve a aplicação do KIII , um modelo de rede_neural  biologicamente mais plausível , para a previsão de séries_temporais  econômicas . Os conjuntos K são modelos conexionistas baseados em populações de neurônios e foram usados em muitas aplicações de aprendizado de máquina , incluindo previsões de séries_temporais  . Nesta dissertação , este método foi aplicado ao IPCA , um índice de preços ao consumidor brasileiro pesquisado pelo IBGE em 13 regiões metropolitanas . Os valores abrangem o período de agosto de 1994 a junho de 2017 . Os experimentos foram realizados utilizando quatro modelos não-paramétricos ( KIII , kNN contínuo , RNAs clássicas e SVM ) e seis métodos paramétricos : ARIMA , SARIMA , Médias Móveis , SES , Holt , Holt-Winters Aditivo e Holt-Winters Multiplicativo . A médida estatística RMSE foi utilizada para comparar o desempenho dos métodos . Os conjuntos KIII de Freeman funcionaram bem como um filtro , melhorando o desempenho do método , mas não foram um bom método de previsão , sendo superado , na maior_parte  dos experimentos , por outros métodos de previsão de séries_temporais  . Esta dissertação contribui com o uso de modelos não paramétricos para prever a inflação em um país em desenvolvimento 
 Neste trabalho , estudamos o problema de corte bidimensional multiperíodo com sobras aproveitáveis , que consiste em cortar objetos grandes visando a produção de um conjunto de itens menores . Supomos um horizonte de planejamento finito com uma quantidade finita de períodos entre os tempos inicial e final . Primeiramente consideramos uma versão determinística em que conhecemos , à priori , os itens solicitados em uma ordem de trabalho e o custo dos objetos a cada_período  . Algumas das sobras geradas durante o processo de corte dos itens solicitados em um período podem ser utilizadas como objetos no futuro . As sobras que podem ser usadas no futuro são denominadas sobras aproveitáveis . De forma geral , uma sobra é considerada aproveitável se possui dimensões iguais ou superiores as de algum item de uma lista pré-definida para o período . O objetivo é minimizar o custo total dos objetos utilizados para satisfazer a ordem de trabalho dos itens solicitados de todo o horizonte considerado . Havendo soluções com o mesmo custo , desejamos encontrar aquela que , no fim do horizonte de tempo considerado , maximize o valor das sobras aproveitáveis remanescentes . Apresentamos uma modelagem matemática do problema usando uma formulação em dois_níveis  , que é transformada em um modelo de programação linear_inteira  mista , devido às características do problema . Considerando a dificuldade em resolver o modelo desenvolvido , apresentamos uma proposta de uma abordagem heurística baseada em Programação Dinâmica Aproximada ( PDA ) para lidar com o problema proposto . Outras opções baseadas em estratégias do tipo horizonte rolante e relax-and-fix também são consideradas . Consideramos também o cenário onde não conhecemos de antemão os itens da ordem de trabalho e o custo dos objetos , mas temos informações das distribuições de probabilidade de ambos . Nesse caso , apresentamos uma abordagem baseada em programação_dinâmica  aproximada para estimar a melhor estratégia a ser seguida em cada_período  . Comparamos os resultados obtidos pela PDA com os resultados encontrados por um método guloso . Em cenários adequados , os resultados mostram que a PDA consegue soluções superiores ao método guloso 
 Aplicações sensíveis a contexto utilizam informações de contexto para fornecer serviços adaptados a usuários na realização de suas tarefas . Informação de contexto é qualquer informação considerada relevante para caracterizar entidades de uma interação usuário-computador , como a identidade e a localização de usuários . Esta tese trata a carência de uma abordagem que considere , em termos de processo de software , a complexidade de desenvolvimento de software sensível a contexto . O problema em questão é tratado por meio de três linhas de investigação : modelagem de informação contextual , serviços para tratamento de informação contextual e processo de software para computação sensível a contexto . As contribuições desta tese incluem : ( i ) o processo de software POCAp ( Process for Ontological Context-aware Applications ) para apoiar a construção de aplicações sensíveis a contexto baseadas em ontologias ; ( ii ) o modelo de informações de contexto SeCoM ( Semantic Context Model ) baseado em ontologias e em padrões da Web Semântica ; ( iii ) a infra-estrutura de serviços configuráveis SCK ( Semantic Context Kernel ) para interpretar informações de contexto apoiadas por modelos ontológicos de informação contextual , como o modelo SeCoM ; ( iv ) uma instanciação do processo POCAp correspondente à extensão de uma aplicação com informações de contexto apoiadas pelo modelo SeCoM , e sua integração com serviços da infra-estrutura SCK ; e ( v ) a identificação de questões de projeto relacionadas à inferência sobre informação contextual 
 Em um mundo computacional em constante evolução , a Web se apresenta como um ambiente caracterizado por um desenvolvimento acelerado de suas informações . Além das informações na Web sofrerem muitas mudanças e com extrema freqüência , os Webmasters enfrentam dificuldades nas suas atividades quando envolvem muitas pessoas trabalhando em paralelo no desenvolvimento de uma página ou de um conjunto de páginas . Diante desses problemas , este trabalho apresenta a ferramenta Version Web que foi desenvolvida . A idéia básica foi a de proporcionar que os usuários ( internautas ) obtivessem as versões das páginas durante a navegação . O objetivo principal da Version Web foi o de fomecer um modo fácil de controle de versões de páginas da Web ao Webmaster , através da própria Web 
 A utilização de arquiteturas paralelas MIMD na resolução de problemas que exigem alta demanda computacional tem sido cada vez mais frequente . A máquina_paralela  SPP2 surge neste cenário como uma arquitetura alternativa , oferecendo alto poder_computacional  a baixo custo através da utilização de componentes de hardware amplamente disponíveis . Visando facilitar a interação dos usuários com o SPP2 , novos mecanismos foram implementados , resultando na máquina_paralela  SPP3 . Para realizar o monitoramento e gerenciamento da máquina_paralela  SPP2 desenvolveu- se a ferramenta Mirador . Execução remota , gerenciamento de usuários , monitoramento de carga de CPU e memória , monitoramento e gerenciamento da execução de processos , são algumas de suas características . A ferramenta Mirador , quando comparada com outras ferramentas do gênero , apresenta peculiaridades que a colocam em posição de destaque , motivando a continuidade de seu desenvolvimento . O presente_trabalho  leva adiante o desenvolvimento da Mirador , suprimindo certas deficiências e especificando novas_funcionalidades  que vão de encontro às necessidades de monitoramento e gerenciamento da máquina_paralela  SPP3 . A ferramenta Mirador II , como resultado deste trabalho , apresenta melhor desempenho e maiores facilidades de interação com o usuário , em relação à sua antecessora . A Mirador II disponibiliza também mecanismos de monitoramento das redes de conexão de alta velocidade Myrinet , mecanismos de gerenciamento autônomo dos nós da máquina_paralela  e dispositivos de monitoramento via hardware , dentre outras características 
 A partir dos anos 90 , a qualidade tornou-se uma necessidade básica na luta pelo mercado competitivo e acabou afetando a indústria de software . Os desenvolvedores de software são obrigados a aperfeiçoarem o seu produto_final  para continuarem competindo no mercado . Após alguns anos de experiência no desenvolvimento de software , percebeu-se que alguns fatores de qualidade considerados pelos clientes , estão mais relacionados ao processo de software do que ao produto_final  . A melhoria do processo de software aumenta as chances de se obter um produto que atenda as expectativas dos clientes . No entanto , a realização de uma melhoria de processo de software não é uma tarefa simples e envolve vários fatores . Para auxiliar a tarefa de melhorar o processo de software , existem hoje na literatura vários modelos de melhoria como por exemplo o SW-CMM ( Software-Capability Maturity Model ) . Porém , a maioria dos modelos de melhoria existem são voltados para as empresas de grande porte , possuindo uma estrutura complexa e `` fechada '' que dificilmente se ajustam às necessidades das empresas desenvolvedoras de software brasileiras , as quais , são na sua maioria , empresas de pequeno_porte  . Dentro deste contexto , este trabalho apresenta diretrizes para orientar clara e objetivamente a melhoria de processo de software . As diretrizes seguem os passos da fase de estabelecimento da Abordagem IDEAL- Estabelecimento de Prioridades , Desenvolvimento da Abordagem e Planejamento de Ações - e considera algumas idéias obtidas_através  do estudo das abordagens S : PLAN , Bootstrap e a parte 7 do Modelo SPICE 
 objetivo deste trabalho é o desenvolvimento de um software : o Gerador Automático de Arquivo Html de Ajuda ( GAAHA ) para a geração de arquivos de indexação e ajuda em HTML baseado na análise automática de documentos . Esses arquivos de indexação e ajuda têm uma dinâmica muito rápida de atualizações de seu conteúdo . Este trabalho focaliza principalmente : a manutenção de arquivos de ajuda para documentos de cursos para Educação a Distância ( via WWW ) e a criação de um formato de arquivos de ajuda ( Help files ) eficiente , para facilitar e agilizar a pesquisa de tópicos ou palavras nesses cursos . O aluno , que participa de um treinamento ou curso de educação a distância , poderá aproveitar o sistema de ajuda para pesquisa sobre tópicos relacionados ao curso ou treinamento em questão . O programa GAAHA é uma solução para a análise do conteúdo de cursos residentes em Web Sites e posterior criação de um mapa gráfico preciso , intuitivo e interativo . O GAAHA tem um robô de `` parseamento '' responsável pela geração de uma vista hierárquica de todos os Units de um curso e uma base de dados indexada que é utilizada para pesquisas de palavras-chave entradas pelo usuário . Este sistema é composto por três subsistemas : um agente crawler para a indexação das informações , um agente mentor que é responsável pela pesquisa no banco de dados , e um cliente WWW para interface com os usuários . Nessa tese também é apresentado um breve histórico das ferramentas e métodos que foram utilizados para o desenvolvimento desse software e uma breve explanação do conceito de educação a distância . Esta última com o objetivo de esclarecer o quão importante é , para um aluno que participa deste tipo de educação , ter uma ferramenta de pesquisa rápida e eficiente 
 Dificuldades e empecilhos encontrados na melhoria de processo rin rnaware são em grande parte fruto das barreiras colocadas pelas pessoas envotv. , ,.. , no processo . Neste trabalho o P-CMM ( People Capability Maturity Model ) é interpretado com a intenção de minimizar essas barreiras . Também são apresentados como duas áreas chave de processo do P-CMM , consideradas como as mais importantes para facilitar a melhoria de processo de software , Comunicação e Mentoring , podem ser implantadas em empresas de pequeno e médio porte 
 Os veículos aéreos não tripulados ( UAVs Unmanned Aerial Vehicles ) , têm desempenhado um papel_importante  em diversas aplicações . Os UAVs podem chegar a lugares de difícil acesso , como encostas de montanhas e vales . Com a miniaturização dos componentes eletrônicos e o com aumento do poder de processamento dos computadores , ampliou-se o campo de aplicações dos UAVs . Estes sistemas de baixo custo podem realizar tarefas normalmente executadas por aviões tripulados . O Projeto ARARA Aeronaves de Reconhecimento Assistidas por Rádio e Autônomas ( Autonomous and Radio-Assisted Reconnaissance Aircraft ) , está centrado no uso de aeromodelos rádio-controlados para aquisição de dados . Seu principal objetivo está na substituição das aeronaves convencionais usadas na obtenção das fotografias aéreas . A aeronave do ARARA possui baixo custo de manutenção e não requer uma pessoa especializada para sua operação . Embora algumas aeronaves do ARARA foram especificamente projetadas para aquisição de dados , todos os componentes estão disponíveis comercialmente , reduzindo o seu custo e facilitando a sua manutenção . Este trabalho apresenta o SIT ( Sistema de Telemetria e Telecomando ) , como parte do Projeto ARARA . O SIT permite operar a aeronave do ARARA além do alcance visual do piloto . As imagens de vídeo e os dados dos instrumentos são transmitidos da aeronave para a estação de base em tempo real . A interface_gráfica  do SIT apresenta o vídeo sobreposto por um painel de instrumentos , similar aos simuladores de vôo tomando a sua operação muito intuitiva . Os resultados obtidos da simulação do uso do SIT confirmaram sua aplicabilidade para o Projeto ARARA . Os teste operacionais de campo dependem do hardware do sistema que está em desenvolvimento 
 Este projeto é uma extensão do trabalho SIGEM ( Sistema Iteraivo de Geração de Malha ) , desenvolvido por [ Teodoro 98 ) , no qual foi implementado um sistema de geração de malhas estruturadas em domínios simplesmente conexos bidimensionais [ Hoffmann 89 ) . O SIGEM possui o controle do posicionamento dos pontos no domínio físico , o qual é exercido pelo usuário . Isso permite que os pontos do domínio físico possam ser distribuídos de maneira conveniente para o problema tratado . O objetivo deste projeto foi o desenvolvimento de uma ferramenta gráficanumérica ( visualizador ) que , sob o controle do usuário , mapeia o domínio físico no domínio computacional . Essa ferramenta permite a visualização dos domínios físico e computacional , com suas respectivas malhas , verificando se a mesma precisa de ajustes ( aglutinação/afastamento de pontos ) . A essa ferramenta foi dado o nome de X-SIGEM . O X-SIGEM tornou mais prático o processo de geração de malha facilitando a entrada de dados , a mudança do domínio físico ( se necessário ) e a alteração dos parâmetros que definem a malha no domínio físico 
 Este trabalho propõe uma interface para a integração do ambiente de escalonamento de processos AMIGO ( dynAMical flexIble schedulinG envirOnment ) ao MPI ( Message Passing Interface ) , permitindo que aplicações construídas nesse ambiente de passagem de mensagens sejam escalonadas de acordo com as políticas adotadas pelo AMIGO . Essas políticas são independentes do ambiente de passagem de mensagens , o que caracteriza a flexibilidade do ambiente AMIGO . Assim , foi proposta e construída uma interface de comunicação entre o ambiente paralelo virtual adotado ( MPI ) e o ambiente AMIGO , responsável por interagir com a ( s ) política ( s ) de escalonamento implementada ( s ) . Para essa interface , a implementação do padrão MPI escolhida foi o LAM ( Local Área Multicomputer ) por preencher os requisitos desejados a priori ( implementação de domínio público , destinada à plataforma Unix e por utilizar processos daemons ) . As análises e os resultados obtidos demonstram as vantagens em se integrar o LA/vI a um ambiente de escalonamento com os objetivos aos quais o AMIGO se propõe . A interface proposta foi implementada com sucesso e aplicações tradicionais MPI podem ser escalonadas com ganhos de desempenho quando seus processos são escalonados através das políticas do AMIGO 
 O planejamento de projeto de software uma prática gerencial vital para o sucesso da gestão de um projeto . A ausência de práticas gerenciais no desenvolvimento de software é a principal causa de sérios problemas enfrentados pelas organizações : atraso em cronogramas , custo maior do que o esperado e presença de defeitos . Tais problemas ocasionam inconveniências aos usuários e perda de tempo e de recursos dos desenvolvedores . Segundo os modelos e padrões de qualidade ISO , SPICE e CMM , planejamento de projeto é um dos itens básicos para que uma empresa inicie a melhoria de processo de desenvolvimento de software . Este trabalho apresenta um modelo de processo de planejamento que define , relaciona e organiza as principais atividades que devem ser realizadas para planejar um projeto de software . O trabalho também apresenta um estudo de caso que mostra a aplicação do modelo de processo em um centro de desenvolvimento de sistemas de uma empresa privada 
 Data_Mining  refere-se ao processo de análise de dados e à aplicação de algoritmos que , mediante limitações de eficiência computacional aceitáveis , são capazes de produzir uma relação particular de padrões a partir de grandes massas de dados [ Fayyad , 19964 A utilização desse processo em problemas do mundo_real  consiste na classificação dos dados , sejam eles categóricos ou contínuos . Problemas envolvendo dados categóricos são comumente denominados de problemas de classificação , enquanto que os dados contínuos são denominados de/problemas de regressão . Problemas do mundo_real  consistem geralmente de problemas de regressão . Dessa forma , cresce o interesse em utilizar o processo Data_Mining  para extrair padrões de problemas de regressão . Além da extração , esses padrões devem ser posteriormente analisados segundo algumas medidas de avaliação de conhecimento para determinar se o padrão é preciso , compreensível ou de interesse ao usuário . Para explorar esse processo de avaliação do conhecimento em problemas de regressão , são realizados , neste trabalho , experimentos com conjuntos de diferentes domínios e características utilizando o ambiente RREvaluation O RREvaluation tem a finalidade de apoiar os usuários do processo Data_Mining  na análise do conhecimento_extraído  de problemas de regressão . O ambiente RREvaluation aqui proposto permite a utilização de diversas formas de avaliação da precisão utilizando as medidas MSE , MAD e NMSE . A compreensibilidade através da identificação do número de condições da regra e da função matemática envolvida , assim como algumas medidas de interessabilidade como GanhoMAD , LC e Q 
 Filas paralelas são de crucial importância em supermercados , computação_paralela  , tráfego , etc . Usualmente , em filas paralelas , o critério de decisão para a questão `` qual fila juntar-se '' é juntar-se à fila mais curta , embora possa-se mostrar que em determinadas circunstâncias este não é o melhor critério . Com esta abordagem , são apresentados resultados conhecidos na literatura que são analisados conjuntamente . Primeiramente apresentamos um sistema de fila M / ( M / 1 ) 2 , duas filas paralelas com o critério de juntar-se à fila mais curta . São apresentados a distribuição de equilíbrio para o processo conjunto { N1 , N2 } e o tempo_médio  de espera na fila . Apresentamos também um sistema M / ( G / 1 ) 2 , onde o serviço geral é especial , implicando que juntar-se à fila mais longa é mais benéfico . Finalmente são apresentados os sistemas de filas G / ( ( M1 + M2 / 1 ) 2 e M / ( ( M + G ) / 1 ) 2 , onde o cliente esperto utiliza o critério de esperar e juntar-se à fila que produz o primeiro fim de serviço e seu tempo_médio  de espera é ainda menor . Resultados_numéricos  comparativos são apresentados 
 Neste trabalho é apresentada a biblioteca AIM , uma biblioteca de objetos Pd que combina ferramentas de análise de áudio e síntese de imagens para a geração de acompanhamentos visuais para uma entrada musical . O usuário estabelece conexões que determinam como os parâmetros musicais afetam a síntese de objetos gráficos , e controla estas conexões em tempo-real durante a performance . A biblioteca combina um protocolo de comunicação para intercâmbio de parâmetros musicais e visuais com uma interface fácil de usar , tornando-a acessível a usuários sem experiência em programação de computadores . Suas áreas possíveis de aplicação incluem a musicalização infantil e a indústria de entretenimento 
 Os operadores convencionais para comparação de dados por igualdade e por relação de ordem total não são adequados para o gerenciamento de dados complexos como , por exemplo , os dados multimí ? dia ( imagens , áudio , textos longos ) , séries_temporais  e seqüências genéticas . Para comparar dados desses tipos , o grau de similaridade entre suas instâncias é , em geral , o fator mais importante sendo , portanto , indicado que as operações de consulta sejam realizadas utilizando os chamados operadores por similaridade . Existem operadores de busca por similaridade tanto unários quanto binários . Os operadores unários são utilizados para implementar operações de seleção , enquanto os operadores binários destinam-se a operações de junção . A álgebra relacional , usada nos Sistemas de Gerenciamento de Bases de Dados Relacionais , não provê suporte para expressar critérios de busca por similaridade . Para suprir esse suporte , está em desenvolvimento no Grupo de Bases de Dados e Imagens ( GBdI-ICMC-USP ) uma extensão à álgebra relacional que permite representar as consultas por similaridade em expressões algébricas . Esta dissertação incorpora-se nesse empreendimento , abordando o tratamento aos operadores unários por similaridade na álgebra , bem como a implementação do otimizador de consultas por similaridade no SIREN ( Similarity Retrieval Engine ) para que as consultas por similaridade possam ser respondidas pelos Sistemas de Gerenciamento de Bases de Dados 
 As atividades de controle de versões são consideradas essenciais para a manutenção de sistemas de computador . Elas começaram a ser realizadas na década de 1950 de forma_manual  . As primeiras ferramentas de controle de versões , que surgiram nos anos setenta , não evoluíram significativamente desde sua criação e , até hoje , o controle de versões de arquivos é geralmente realizado em arquivos ou mesmo módulos completos , utilizando os conceitos que foram lançados há mais de três décadas . Com a popularização da utilização de sistemas computacionais , ocorreu um sensível aumento no número de sistemas existentes e , também , na complexidade dos mesmos . Além_disso  , muitas alterações ocorreram nos ambientes de desenvolvimento de software , e existe demanda por sistemas que permitam aos desenvolvedores ter cada vez mais controle automatizado sobre o que está sendo desenvolvido . Para isso , algumas abordagens de controle de versões refinados para artefatos de software foram propostas , mas , muitas_vezes  , não oferecem a exibilidade de utilização exigida pelos ambientes de desenvolvimento de software . Neste trabalho , é apresentado um sistema que visa a fornecer suporte ao controle de versões refinado e flexível para artefatos de software , tendo por base um modelo bem definido para representação das informações da estrutura dos arquivos que compõem determinado projeto de software , sejam eles código-fonte dos programas de computador , documentação criada em Latex , arquivos XML , entre outros . O sistema apresentado foi projetado para ser integrado com outras soluções utilizadas em ambientes de desenvolvimento de 
 A Clusterização de dados em grupos oferece uma maneira de entender e extrair_informações  relevantes de grandes_conjuntos  de dados . A abordagem em relação a aspectos como a representação dos dados e medida de similaridade entre clusters , e a necessidade de ajuste de parâmetros iniciais são as principais diferenças entre os algoritmos de clusterização , influenciando na qualidade da divisão dos clusters . O uso cada vez mais comum de grandes_conjuntos  de dados aliado à possibilidade de melhoria das técnicas já existentes tornam a clusterização de dados uma área de pesquisa que permite inovações em diferentes campos . Nesse trabalho é feita uma revisão dos métodos de clusterização já existentes , e é descrito um novo método de clusterização de dados baseado na identificação de comunidades em redes complexas e modelos computacionais inspirados biologicamente . A técnica de clusterização proposta é composta por duas etapas : formação da rede usando os dados de entrada ; e particionamento dessa rede para obtenção dos clusters . Nessa última etapa , a técnica de otimização por nuvens de partículas é utilizada a fim de identificar os clusters na rede , resultando em um algoritmo de clusterização hierárquico divisivo . Resultados experimentais revelaram como características do método proposto a capacidade de detecção de clusters de formas arbitrárias e a representação de clusters com diferentes níveis de refinamento 
 Este projeto apresenta uma metodologia aplicada à análise da viabilidade de se otimizar código fonte C para o processador embarcado Nios_II  . Esta metodologia utiliza ferramentas de análise de código que traçam o perfil da aplicação , identificando suas partes críticas em relação ao tempo de execução , as quais são o gprof e o performance counter . Para otimizar o código para o processador Nios_II  , são utilizadas tanto instruções customizadas quanto uma ferramenta automática de aceleração de código , o compilador C2H . Como casos de estudo , foram escolhidos três algoritmos devido à sua importância no campo da robótica_móvel  , sendo eles o gaxpy , o EKF e o SIFT . A partir da aplicação da metodologia para se otimizar cada um dos casos , foi comparada a eficiência tanto das ferramentas de análise de código , quanto das ferramentas de otimização , bem como a validade da metodologia_proposta 
 O projeto VoIPFix surgiu da necessidade de uma ferramenta que complementasse as demais existentes no ramo de análise de redes de computadores para telefonia IP . Ele foi construído para ser uma ferramenta de gerenciamento eficiente e exclusiva para VoIP , com funcionalidades necessárias para dar suporte ao profissional de rede de computadores e telefonia IP a observar e diagnosticar problemas de VoIP 
 Atualmente a Integração de Aplicações Empresariais ( EAI ) desempenha um papel fundamental no cenário de integração de sistemas corporativos . Isso pode ser feito de diferentes formas , como por exemplo , por meio do compartilhamento de acesso às bases de dados ou trabalhando-se com Web_Services  , em que um serviço é disponibilizado por um sistema e pode ser chamado por outro sistema a ser integrado . Essas duas soluções estão sendo empregadas com sucesso , mas ambas apresentam vantagens e desvantagens que devem ser analisadas . Assim , este trabalho tem como objetivo primeiramente efetuar uma comparação entre essas duas_abordagens  de integração ( tomando como base a norma ISO-IEC 9126-1 ) por meio de uma revisão bibliográfica complementada por uma revisão_sistemática  e relatos da experiência profissional da autora deste trabalho e da sua orientadora . Com o intuito de validar esta comparação , foi feito um estudo_experimental  , cujo_objetivo  do experimento foi avaliar a melhor abordagem para se realizar uma integração de aplicações empresariais : EAI-Dados e EAI-WS no que diz_respeito  ao esforço necessário para a implantação de cada uma . Assim , a avaliação foi realizada para responder à seguinte questão : Em termos de tempo de desenvolvimento e código produzido , é mais fácil realizar EAI-WS ou EAI-Dados ? Finalmente , foram propostos cinco padrões para EAI , com o objetivo de auxiliar desenvolvedores com problemas similares de integração de aplicações . Esses padrões podem ser reusados em diversos contextos de integração , obedecendo às regras de negócios específicas a serem consideradas no momento da integração , e agilizando a modelagem da solução por meio da instanciação do padrão mais adequado a cada 
 A criação do projeto conceitual de um bancos de dados que represente adequadamente um determinado domínio de aplicação continua sendo um dos principais desafios da área de banco de dados . Por outro_lado  , a discussão_sobre  métodos ágeis de desenvolvimento de software alcançou , recentemente , a comunidade de banco de dados . Este trabalho apresenta o projeto conceitual de bancos de dados sob a luz de métodos ágeis de desenvolvimento . Desenvolvemos uma extensão do arcabouço Naked Objects que permite uma validação ágil e precisa do projeto conceitual junto ao especialista do domínio . Em nossa abordagem , o projeto conceitual de bancos de dados é descrito por meio de anotações que representam as abstrações de dados em um ambiente dinâmico de validação 
 Vários métodos tradicionais de segmentação de imagens , como a transformada de watershed de marcado- res e métodos de conexidade fuzzy ( Relative Fuzzy Connectedness- RFC , Iterative Relative Fuzzy Connected- ness - IRFC ) , podem ser implementados de modo eficiente utilizando o método em grafos da Transformada Imagem-Floresta ( Image Foresting Transform - IFT ) . No entanto , a carência de termos de regularização de fronteira em sua formulação fazem com que a borda do objeto segmentado possa ser altamente irregular . Um modo de contornar isto é por meio do uso de restrições de forma do objeto , que favoreçam formas mais regulares , como na recente restrição de convexidade geodésica em estrela ( Geodesic Star Convexity - GSC ) . Neste trabalho , apresentamos uma nova restrição de forma , chamada de Faixa de Restrição Geodésica ( Geodesic Band Constraint - GBC ) , que pode ser incorporada eficientemente em uma sub-classe do fra- mework de corte em grafos generalizado ( Generalized Graph Cut - GGC ) , que inclui métodos pela IFT . É apresentada uma prova da otimalidade do novo algoritmo em termos de um mínimo global de uma função de energia sujeita às novas restrições de borda . A faixa de restrição geodésica nos ajuda a regularizar a borda dos objetos , consequentemente melhorando a segmentação de objetos com formas mais regulares , mantendo o baixo custo_computacional  da IFT . A GBC pode também ser usada conjuntamente com um mapa de custos pré estabelecido , baseado em um modelo de forma , de modo a direcionar a segmentação a seguir uma dada forma desejada , com grau de liberdade de escala e demais deformações controladas por um parâmetro único . Essa nova restrição também pode ser combinada com a GSC e com as restrições de polaridade de borda sem custo adicional . O método é demonstrado em imagens naturais , sintéticas e médicas , sendo estas provenientes de tomografias computadorizadas e de ressonância_magnética  
 Este trabalho mostra o desenvolvimento de uma Interface de Programação de Aplicativos ( Application Program Interface - API ) para um gerenciador de dados orientado a objetos 
 A API é composta por um conjunto de primitivas que integram a definição e 
 manipulação de objetos em uma representação compatível com uma linguagem 
 programação orientada a objetos 
 A definição da API explora os recursos básicos de modelos de dados orientados a objetos 
 baseia-se nas extensões de um metamodelo baseado em quatro abstrações : classificação 
 generalização , agregação e composição . O suporte à abstração de classificação com hierarquias em múltiplos níveis é tratado com especial destaque , pois resulta em uma 
 características predominantes da API : o tratamento homogêneo de tipos e instâncias 
 tempo de execução , unificando comandos usualmente separados em DDL ( Data 
 Language ) e DML ( Data Manipulation Language ) 
 A implementação da API sobre um gerenciador relacional emula um gerenciador de 
 orientado a objetos . Os conceitos envolvidos no trabalho de emulação foram aplicados 
 desenvolvimento de uma versão com núcleo relacional do Gerenciador de Objetos SIRIUS 
 criando em ambiente experimental , precursor à versão com núcleo nativo 
 gerenciador . A API definida neste trabalho é compatível com ambas as versões 
 Gerenciador SIRIUS , permitindo que uma aplicação utilize qualquer uma das versões 
 alterações em seu código fonte 
 Para exemplificar a utilização prática da API , foi implementado um utilitário de bases 
 dados destinado à representação de modelagens baseadas no modelo de dados 
 usando a versão relacional do Gerenciador de Objetos SIRIUS . Esse utilitário , além 
 demonstrar a utilização da API , demonstra também como as operações típicas da DDL e 
 DML são integradas em um único conjunto de comandos que não faz diferença entre 
 definição de tipos e de instâncias 
 O estudo de redes complexas tem alavancado um tremendo interesse em anos recentes . Uma das características salientes de redes complexas é a presença de comunidades , ou grupos de nós densamente conectados . A detecção de comunidades pode não apenas ajudar a entender as estruturas topológicas de redes complexas , mas também pode fornecer novas técnicas para aplicações reais , como mineração de dados . Neste trabalho , propomos um novo modelo para detecção de comunidades em redes complexas , no qual várias partículas caminham na rede e competem umas com as outras para marcar seu próprio território e rejeitar partículas intrusas . O processo atinge o equilíbrio dinâmico quando cada comunidade tem apenas uma partícula . Nossa abordagem não apenas pode obter bons_resultados  na detecção de comunidades , como também apresenta diversas características interessantes : 1 ) O processo de competição de partículas é similar a muitos processos naturais e sociais , tais como competição de animais por recursos , exploração territorial por humanos ( animais ) , campanhas eleitorais , etc.. Portanto , o modelo proposto neste trabalho pode ser útil para simular a dinâmica evolutiva de tais processos . 2 ) Neste modelo , nós introduzimos uma regra para controlar o nível de aleatoriedade do passeio da partícula . Descobrimos que uma pequena porção de aleatoriedade pode aumentar bastante a taxa de detecção de comunidades . Nossa descoberta é análoga ao notável fenômeno chamado ressonância estocástica onde o desempenho de um sistema determinístico não-linear pode ser bastante melhorado através da introdução de um certo nível de ruído . É interessante notar que tal fenômeno é observado em uma situação diferente aos sistemas clássicos de ressonância estocástica . 3 ) Nossa descoberta indica que a aleatoriedade tem um papel_importante  em sistemas evolutivos . Ela serve para automaticamente escapar de armadilhas não desejáveis e explorar novos espaços , isto é , ela é um descobridor de novidades . 4 ) Uma análise quantitativa para processo de competição entre duas particulas e duas comunidades foi conduzida , a qual é um passo de avanço para desenvolvimento de teoria fundamental de aprendizado 
 Para o desenvolvimento de aplicações Web de qualidade , o uso de uma abordagem sistemática e disciplinada é essencial , dado o crescente aumento do uso e da complexidade de tais aplicações . Nesse cenário , a Engenharia de Aplicações Web , ou simplesmente Engenharia Web , torna-se uma atividade mandatória . A Engenharia Web não é uma transcrição exata da Engenharia de Software , pois considera caracterí ? sticas inerentes às aplicações Web como , por exemplo , multiplicidade do perfil de usuários e uso de multimídia . A Engenharia Web é apoiada por ferramentas , técnicas e métodos . A escolha desses elementos para um domínio de aplicação pode tornar-se uma tarefa_difícil  para desenvolvedores e demais interessados e é fundamental para o desenvolvimento efetivo de aplicações Web . Neste trabalho , é apresentado uma proposta de avaliação de métodos de desenvolvimento de aplicações Web . Essa proposta partiu de um conjunto de métodos selecionados por meio de trabalhos comparativos anteriores disponíveis na literatura e do levantamento de suas principais características , após estudo de caso que consistiu em modelar uma mesma aplicação ( um portal Web ) com os diferentes métodos selecionados . A cada característica associaram-se pesos , refletindo sua relevância a domínios especifícos , e valores refletindo seu grau de apoio a um método . Como resultado , obteve-se um arcabouço - conjunto de guidelines - para avaliar e/ou especificar métodos de desenvolvimento de aplicações Web . Esse arcabouço foi aplicado ao conjunto de métodos selecionados do estudo de caso e foram obtidas evidências da efetividade do arcabouço 
 O processo de escolha de portfólios é um problema clássico da área financeira . Neste problema , o investidor busca aplicar seu dinheiro em um mercado de ações de forma a obter um bom compromisso entre o retorno esperado e o risco . Em geral , quanto maior o retorno esperado da carteira , maior o risco a ela associado . Neste trabalho foram estudadas modelagens para o problema de escolha de portfólio ótimo e suas aplicações ao mercado brasileiro . Do ponto de vista de modelagem foi proposta a inclusão do risco diversificável e não-diversificável ao modelo linear estudado . O risco diversificável foi incluído através de uma restrição que impõe um número mínimo de ativos na composição do portfólio ótimo , enquanto o risco não-diversificável foi adicionado considerando o beta da carteira . Do ponto de vista de aplicação , foi considerada a atribuição de valores de probabilidade para os retornos históricos dos ativos utilizados na análise do problema , visando incorporar informações do comportamento apresentado pelo mercado nos resultados . Na geração dos resultados , foram desenvolvidos em CPLEX um método ótimo de solução para o problema e um método para geração de uma curva de soluções Pareto 
 A Morfologia Matemática ( MM ) é um arcabouço geral para o estudo de mapeamentos entre imagens binárias . Estes estudos são de especial interesse na área de Processamento de Imagens . Tais mapeamentos entre imagens binárias são conhecidos como operadores de conjunto . Um aspecto importante da MM é a representação destes operadores em termos de dilatações , erosões e outras operações usuais de conjunto ( interseção , união , complemento e diferença ) . Por este motivo , a dilatação e a erosão são ditos operadores morfológicos elementares . Este trabalho visa propor novos métodos para calcular a erosão e a dilatação morfológica binária rapidamente . Tais métodos se fundamentam em conceitos e técnicas de pré-processamento ( em tempo linear ) introduzidas por este trabalho , como a Transformada da Densidade , ou ainda , um Conjunto de Cascas . O resultado destes pré-processamentos é traduzido em ganho de velocidade dos algoritmos de erosão e dilatação , além de apresentar uma representação compacta dos conjuntos operandos . O consumo de tempo dos métodos propostos é no pior caso quadrático , porém , num estudo_experimental  preliminar , o algoritmo se comporta eficientemente , chegando a ser até mesmo linear em alguns_casos  . Além_disso  , um levantamento sucinto de outros métodos de erosão e dilatação morfológica binária conhecidos pela literatura atual é apresentado . Algumas simulações e uma breve análise de complexidade mostram que os métodos propostos são boas alternativas para implementação de erosão e dilatação morfológica eficiente 
 Agentes inteligentes é um termo guarda-chuva que agrega diversas pesquisas no desenvolvimento de softwares autônomos que utilizam técnicas de Inteligência_Artificial  a fim de satisfazer metas estabelecidas por seus usuários . A construção de sistemas baseados em agentes inteligentes é uma tarefa complexa que envolve aspectos como comunicação entre agentes , planejamento , divisão de tarefas , coordenação , representação e manipulação de conhecimento e comportamentos , entre outras tarefas . Plataformas para agentes prevêem alguns serviços que permitem a desenvolvedores construir soluções sem a necessidade de se preocupar com todos detalhes da implementação 
 Um novo modelo para criação de agentes chamado 'agentes atômicos ' é proposto com o objetivo de oferecer flexibilidade para o gerenciamento de conhecimento e implementação de comportamentos . A arquitetura Agentes Semânticos provê um framework para a implementação de tal modelo , oferecendo um conjunto de ferramentas para a criação de agentes inteligentes . Um protótipo de plataforma para agentes , baseado em tal arquitetura , foi desenvolvido em Java e permite a criação de aplicações capazes de processar linguagem_natural  restrita , manipular conhecimento e executar ações úteis 
 Atualmente , a segurança computacional vem se tornando cada vez mais necessária devido ao grande crescimento das estatísticas que relatam os crimes computacionais . Uma das ferramentas utilizadas para aumentar o nível de segurança é conhecida como Sistemas de Detecção de Intrusão ( SDI ) . A flexibilidade e usabilidade destes sistemas têm contribuído , consideravelmente , para o aumento da proteção dos ambientes computacionais . Como grande parte das intrusões seguem padrões bem definidos de comportamento em uma rede de computadores , as técnicas de classificação e clusterização de dados tendem a ser muito apropriadas para a obtenção de uma forma eficaz de resolver este tipo de problema . Neste trabalho será apresentado um modelo dinâmico de clusterização baseado em um mecanismo de movimentação dos dados . Apesar de ser uma técnica de clusterização de dados aplicável a qualquer tipo de dados , neste trabalho , este modelo será utilizado para a detecção de intrusão . A técnica apresentada neste trabalho obteve resultados de clusterização comparáveis com técnicas tradicionais . Além_disso  , a técnica proposta possui algumas vantagens sobre as técnicas tradicionais investigadas , como realização de clusterizações multi-escala e não necessidade de determinação do número inicial de 
 Para analisar características de interesse a respeito de um conjunto de dados reais da área 
 Odontologia apresentado em Hadgu & Koch ( 1999 ) , ajustaremos um modelo de regressão 
 multivariado com erros nas variáveis com intercepto nulo . Este conjunto de dados é 
 por medições de placa bacteriana em três grupos de voluntários , antes e após 
 dois líquidos de bochecho experimentais e um líquido de bochecho controle , com medições ( sujeitas a erros de medição ) no início do estudo , após três e seis meses de utilização dos líquidos 
 Neste caso , uma possível estrutura de dependência entre as medições feitas em um mesmo indivíduo deve ser incorporada ao modelo e , além disto , temos duas variáveis resposta para 
 indivíduo . Após a apresentação do modelo estatístico , iremos obter estimativas de 
 verossimilhança dos parâmetros utilizando o algoritmo iterativo EM e testaremos as 
 de interesse utilizando testes assintóticos de Wald , razão de verossimilhanças e score . 
 neste caso não existe um teste ótimo , faremos um estudo de simulação para verificar o 
 das três estatísticas de teste em relação a diferentes tamanhos amostrais e 
 valores de parâmetros . Finalmente , faremos um estudo de diagnóstico buscando 
 possíveis pontos influentes no modelo , considerando o enfoque de influência local proposto por Cook ( 1986 ) e a medida de curvatura normal conformal desenvolvida por Poon & Poon ( 1999 ) 
 Com o aumento no uso de aglomerados e grades de computadores , cresce o interesse no estudo de comunicações entre processadores . Em um computador paralelo dedicado , ou em uma rede local homogênea , o tempo de comunicação é geralmente modelado de forma similar , independente de quais processadores estão se comunicando . Em uma rede onde os links entre os computadores são heterogêneos , computadores mais próximos tendem a apresentar menor latência e maior largura de banda do que computadores distantes . Além_disso  , a largura de banda agregada é diferente dependendo do número de conexões simultâneas existentes entre dois aglomerados distantes . Neste trabalho estudaremos a troca completa de mensagens de tamanhos diferentes entre aglomerados interligados por backbones . Proporemos um novo algoritmo de comunicação baseado em algoritmos conhecidos , apresentaremos simulações de escalonamentos dos algoritmos estudados para esta rede multi-aglomerado e analisaremos os resultados destas simulações 
 Durante o processo de desenvolvimento de software a atividade de teste pode contribuir com a melhoria de qualidade , produtividade e redução de custos , por meio da aplicação de técnicas e critérios de teste . Esses instrumentos viabilizam a detecção de defeitos , minimizando a quantidade de falhas no produto_final  . Durante o desenvolvimento de sistemas robóticos móveis , técnicas VST ( Virtual Simulation Tools/Technology ) são enormemente utilizadas . Apesar dos testes usando simulação serem eficientes em alguns_casos  , eles não permitem uma sistematização da atividade de teste com relação à cobertura de código . Este projeto de doutorado contribui nessa direção , explorando essa lacuna e propondo uma abordagem de teste de integração híbrida , composta por teste funcional e estrutural , a qual explora defeitos relacionados à comunicação entre os processos do sistema robótico . Este estudo foi conduzido com base em conceitos explorados no teste de programas concorrentes , dado que os sistemas robóticos podem apresentar problemas similares aos encontrados nos programas concorrentes , como não determinismo , diferentes possibilidades de comunicação , entre outros . Durante o trabalho foi definida uma abordagem de teste de integração e critérios de cobertura aplicados a sistemas robóticos . Os estudos exploratórios conduzidos indicaram que a abordagem proposta é aplicável e pode contribuir para revelar defeitos não explorados pela simulação e técnicas de teste utilizadas anteriormente 
 Provedores de serviços de nuvem disponibilizam uma interface através da qual seus clientes podem solicitar , usar e liberar estes recursos . Muitos serviços implantados em nuvens incluem um componente para gerenciamento automatizado de recursos , encarregado de requisitar e librar recursos sem intervenção humana , à medida que a demanda varia . A técnica padrão para o gerenciamento de recursos se baseia em regras sobre utilização de recursos . Quando ocorre um aumento significativo na carga em um curto espaço de tempo , o sistema pode_levar  vários ciclos de monitoramento e ação até alcançar uma configuração adequada . Neste período , o sistema permanece sobrecarregado . Nesta pesquisa , investigamos como compreender adequadamente os efeitos da variação na disponibilidade de recursos sobre a capacidade de um sistema e como aplicar este conhecimento para melhorar sua elasticidade . Propomos uma estratégia que abrange avaliação da escalabilidade do sistema , visando sua modelagem , e a aplicação deste modelo nas estimativas de necessidade por recursos com base na carga de trabalho . Introduzimos um arcabouço para automatizar a avaliação de escalabilidade de sistemas distribuídos e efetuamos uma validação experimental da estratégia proposta . Comparamos a alocação de recursos e o desempenho obtido usando nossa estratégia e estratégia baseada em regras , fazendo a reprodução de carga real e usando cargas sintéticas . De forma geral , nossa proposta foi capaz de prover melhor desempenho , ao ponto que o uso de recursos cresceu , e consequentemente o custo de utilização . No entanto , a melhora de desempenho foi mais significativa que o aumento dos custos 
 O volume de informação manipulada em sistemas apoiados por computador tem crescido tanto no número de objetos que compõem os conjuntos de dados quanto na quantidade e na complexidade dos atributos . Em conjuntos de dados do mundo_real  , a uniformidade na distribuição de valores e a independência entre atributos são propriedades bastante incomuns . De fato , dados reais são em geral caracterizados pela ampla presença de correlações entre seus atributos . Além_disso  , num mesmo conjunto podem existir correlações de naturezas diversas , como correlações lineares , não-lineares e não-polinomiais . Todo esse cenário pode degradar a performance dos algoritmos que manipulam e , principalmente , dos que realizam análises dos dados . Além da grande quantidade de objetos a serem tratados e do número elevado de atributos , as correlações nem sempre são conhecidas , o que pode comprometer a eficácia de tais algoritmos . Nesse contexto , as técnicas de redução de dimensionalidade permitem diminuir o número de atributos de um conjunto de dados , minimizando assim os problemas decorrentes da alta dimensionalidade . Algumas delas são baseadas na análise de correlações e , com o objetivo de reduzir a perda de informação relevante causada pela remoção de atributos , procuram eliminar apenas aqueles que sejam correlacionados aos restantes . No entanto , essas técnicas geralmente analisam como cada atributo está correlacionado a todos os demais , tratando o conjunto de atributos como um todo e usando ferramentas de análise estatística . Esta tese propõe uma abordagem diferente , baseada na Teoria dos Fractais , para detectar a existência de correlações e identificar subconjuntos de atributos correlacionados . Para cada correlação encontrada é possível ainda identificar quais são os atributos que melhor a descrevem . Conseqüentemente , um subconjunto de atributos relevantes para representar as características fundamentais dos dados é determinado , não apenas com base em correlações globais entre todos os atributos , mas também levando em consideração especificidades de correlações que envolvem subconjuntos reduzidos . A técnica apresentada é uma ferramenta a ser utilizada em etapas de pré-processamento de atividades de descoberta de conhecimento , principalmente em operações de seleção de atributos para redução de dimensionalidade . A proposta para a identificação de correlações e os conceitos que a fundamentam são validados por meio de estudos_experimentais  usando tanto dados sintéticos quanto reais . Finalmente , os conceitos básicos da Teoria dos Fractais são aplicados na análise de comportamento de data streams , também constituindo uma contribuição relevante desta tese de doutorado 
 Algoritmos de aprendizado de máquina são frequentemente os mais indicados em uma grande variedade de aplicações de mineração dados . Entretanto , a maioria das pesquisas em aprendizado de máquina refere-se ao problema bem definido de encontrar um modelo ( geralmente de classificação ) de um conjunto de dados pequeno , relativamente bem preparado para o aprendizado , no formato_atributo-valor  , no qual os atributos foram previamente selecionados para facilitar o aprendizado . Além_disso  , o objetivo a ser alcançado é simples e bem definido ( modelos de classificação precisos , no caso de problemas de classificação ) . Mineração de dados propicia novas direções para pesquisas em aprendizado de máquina e impõe novas necessidades para outras . Com a mineração de dados , algoritmos de aprendizado estão quebrando as restrições descritas anteriormente . Dessa maneira , a grande contribuição da área de aprendizado de máquina para a mineração de dados é retribuída pelo efeito inovador que a mineração de dados provoca em aprendizado de máquina . Nesta tese , exploramos alguns desses problemas que surgiram ( ou reaparecem ) com o uso de algoritmos de aprendizado de máquina para mineração de dados . Mais especificamente , nos concentramos seguintes problemas 

 Novas abordagens para a geração de regras . Dentro dessa categoria , propomos dois novos métodos para o aprendizado de regras . No primeiro , propomos um novo método para gerar regras de exceção a partir de regras gerais . No segundo , propomos um algoritmo para a seleção de regras denominado Roccer . Esse algoritmo é baseado na análise ROC . Regras provêm de um grande conjunto externo de regras e o algoritmo proposto seleciona regras baseado na região convexa do gráfico ROC 

 Proporção de exemplos entre as classes . Investigamos vários aspectos_relacionados  a esse tópico . Primeiramente , realizamos uma série de experimentos em conjuntos de dados artificiais com o objetivo de testar nossa hipótese de que o grau de sobreposição entre as classes é um fator complicante em conjuntos de dados muito desbalanceados . Também executamos uma extensa análise experimental com vários métodos ( alguns deles propostos neste trabalho ) para balancear artificialmente conjuntos de dados desbalanceados . Finalmente , investigamos o relacionamento entre classes desbalanceadas e pequenos disjuntos , e a influência da proporção de classes no processo de rotulação de exemplos no algoritmo de aprendizado de máquina semi-supervisionado Co-training 

 Novo método para a combinação de rankings . Propomos um novo método , chamado BordaRank , para construir ensembles de rankings baseado no método de votação borda count . BordaRank pode ser aplicado em qualquer problema de ordenação binária no qual vários rankings estejam disponíveis . Resultados experimentais_mostram  uma melhora no desempenho com relação aos rankings individuais , alem de um desempenho comparável com algoritmos mais sofisticados que utilizam a predição numérica , e não rankings , para a criação de ensembles para o problema de ordenação binária 

 Métodos do tipo Lagrangiano Aumentado são muito utilizados para minimização de funções sujeitas a restrições gerais . Nestes métodos , podemos separar o conjunto de restrições em dois grupos : restrições fáceis e restrições difíceis . Dizemos que uma restrição é fácil se existe um algoritmo disponível e eficiente para resolver problemas restritos a este tipo de restrição . Caso contrário , dizemos que a restrição é difícil . Métodos do tipo Lagrangiano aumentado resolvem , a cada iteração , problemas sujeitos às restrições fáceis , penalizando as restrições difíceis . Problemas de minimização com restrições_lineares  aparecem com freqüência , muitas_vezes  como resultados da aproximação de problemas com restrições gerais . Este tipo de problema surge também como subproblema de métodos do tipo Lagrangiano aumentado . Assim , uma implementação eficiente para resolver problemas com restrições_lineares  é relevante para a implementação eficiente de métodos para resolução de problemas de programação não-linear . Neste trabalho , começamos considerando fáceis as restrições de caixa . Introduzimos BETRA-ESPARSO , uma versão de BETRA para problemas de grande porte . BETRA é um método de restrições ativas que utiliza regiões de confiança para minimização em cada face e gradiente espectral projetado para sair das faces . Utilizamos BETRA ( denso ou esparso ) na resolução dos subproblemas que surgem a cada iteração de ALGENCAN ( um método de lagrangiano aumentado ) . Para decidir qual algoritmo utilizar para resolver cada subproblema , desenvolvemos regras que escolhem um método para resolver o subproblema de acordo com suas características . Em seguida , introduzimos dois algoritmos de restrições ativas desenvolvidos para resolver problemas com restrições_lineares  ( BETRALIN e GENLIN ) . Estes algoritmos utilizam , a cada iteração , o método do Gradiente Espectral Projetado Parcial quando decidem mudar o conjunto de restrições ativas . O método do gradiente Espectral Projetado Parcial foi desenvolvido especialmente para este propósito . Neste método , as projeções são computadas apenas em um subconjunto das restrições , com o intuito de torná-las mais eficientes . Por fim , tendo introduzido um método para minimização com restrições_lineares  , consideramos como fáceis as restrições_lineares  . Incorporamos BETRALIN e GENLIN ao arcabouço de Lagrangianos aumentados e verificamos experimentalmente a eficiência e eficácia destes métodos que trabalham explicitamente com restrições_lineares  e penalizam as demais 
 Programas concorrentes possuem características que os diferenciam de programas sequenciais , tornando a atividade de teste mais complexa . Questões como definição e uso de variáveis por diferentes processos , comunicação , sincronização e comportamento não-determinístico precisam ser consideradas . Apesar dos avanços obtidos nesse contexto , um problema que ainda persiste é o custo da atividade de teste , relacionado principalmente ao número excessivo de elementos requeridos a serem testados . Neste contexto , este trabalho apresenta duas propostas : a primeira estática , focando na detecção automática de elementos requeridos não-executáveis , para os critérios relacionados a sincronizações ; e a segunda dinâmica , aplicando-se o teste de alcançabilidade para apoiar a avaliação da cobertura de sequências de sincronizações , desse modo , usando informações dinâmicas para apoiar o teste de cobertura . Estas propostas foram implementadas na ferramenta ValiMPI e um experimento foi realizado a fim de verificar a eficácia da proposta . Os resultados indicam que as duas propostas implementadas neste trabalho são promissoras e auxiliam a reduzir o custo da atividade de 
 O paradigma orientado a objetos não dispõe de abstrações adequadas à modularização de interesses_transversais  - interesses cuja implementação encontra-se entrelaçada e espalhada por todos os módulos de um sistema . Com o surgimento da Programação Orientada a Aspectos e a disponibilidade de abstrações adequadas à modularização de interesses_transversais  , o reúso desses interesses foi facilitado . Nesta tese são apresentados alguns resultados de esforços de pesquisa que visam a tornar o reúso de interesses_transversais  mais efetivo . Definições e classificações para frameworks desenvolvidos no contexto da POA - frameworks transversais - são apresentadas com o objetivo de facilitar a comunicação entre usuários e pesquisadores . Uma arquitetura de referência para o projeto e implementação de frameworks transversais , que torna sua estrutura mais clara e facilita a integração de vários frameworks quando a intenção é criar um repositório de aspectos reusáveis , é proposta . Três famílias de frameworks transversais : de persistência , de segurança e de regras de negócio foram desenvolvidas . Cada família de frameworks transversais constitui uma linha de produtos de software , em que membros podem ser configurados com características ( features ) distintas . Um processo de desenvolvimento , denominado ProFT/PU , baseado no Processo Unificado ( PU ) , que considera as famílias de frameworks ao longo de todo o processo é apresentado , juntamente com um exemplo de sua utilização para uma aplicação típica . Um estudo de caso para comparar o tempo requerido para reusar um framework transversal de persistência e um framework orientado a objetos de persistência é também apresentado e discutido 
 Nas ciências de computação , o estudo de emoções tem sido impulsionado pela construção de ambientes interativos , especialmente no contexto dos dispositivos_móveis  . Pesquisas envolvendo interação humano-robô têm explorado emoções para propiciar experiências naturais de interação com robôs sociais . Um dos aspectos a serem investigados é o das abordagens práticas que exploram mudanças na personalidade de um sistema artificial propiciadas por alterações em um estado_emocional  inferido do usuário . Neste trabalho , é proposto um ambiente para interação humano-robô baseado em emoções , reconhecidas por meio de análise de expressões faciais , para plataforma Android . Esse sistema consistiu em um agente virtual agregado a um aplicativo , o qual usou informação proveniente de um reconhecedor de emoções para adaptar sua estratégia de interação , alternando entre dois paradigmas discretos pré-definidos . Nos experimentos_realizados  , verificou-se que a abordagem proposta tende a produzir mais empatia do que uma condição controle , entretanto esse resultado foi observado somente em interações suficientemente longas 
 O controle de plantas daninhas é uma importante preocupação para a agricultura tendo em vista as perdas de produtividade que estas causam ao competir com a cultura por água , luz e nutrientes . O uso de herbicida é a forma de manejo mais empregada em todo o mundo para o controle destas plantas . Entretanto , o uso frequente de um dado herbicida , além de causar diversos impactos ambientais , pode_levar  à diminuição da eficiência do próprio herbicida ao promover a seleção de plantas que são resistentes a este herbicida . Com o crescente número de novos casos de biótipos resistentes aos herbicidas , conter a evolução da resistência tornou-se uma necessidade para a agricultura convencional . Assim , grande esforço tem sido despendido para compreender este fenômeno e tentar contornar este problema . Neste sentido , os modelos computacionais se apresentam como importantes ferramentas para investigar os efeitos dos diversos fatores , em particular das estratégias de aplicação dos herbicidas , que influenciam na dinâmica da evolução da resistência . Com esta motivação , este trabalho tem como objetivo propor e estudar algumas estratégias de aplicação de herbicidas , ou ditos simplesmente controladores , que sejam implementáveis e que diminuam os impactos ambientais considerando a evolução da resistência . Para isto , assumimos que existe um herbicida , denominado neste trabalho por herbicida recomendado , que é o preferível dentre os disponíveis por produzir uma boa relação entre os benefícios produtivos e os malefícios aos ecossistemas . Para projetar os controladores , assumimos que é possível_obter  informações sobre a identificação visual da resistência em campo , feitas por um agente quando o número de indivíduos resistentes ultrapassa um certo limiar , assim como informações sobre a quantidade de plantas daninhas na área , feita possivelmente empregando técnicas de sensoriamento remoto . Então , para definir os controladores , empregamos diretamente a identificação visual da resistência e estimativas para o banco de sementes e para a fração dos genótipos do banco , geradas por um filtro de Kalman a partir de informações sobre a quantidade de plantas na área . Os controladores foram avaliados em relação à preservação da eficiência do herbicida recomendado , produtividade , impacto ambiental e propagação da resistência . Concluímos destes estudos que o controlador sugerido pode apresentar melhores_resultados  que os obtidos por controladores ditos convencionais , que se baseiam apenas na informação de identificação da resistência em campo 
 Este trabalho é dedicado à análise e implementação de esquemas `` upwind '' de alta ordem modernos e o modelo de turbulência k-epsilon padrão no Freeflow-2D ; um ambiente integrado para simulação_numérica  em diferenças_finitas  de problemas de escoamentos_incompressíveis  com superfícies_livres  . O propósito do estudo é a simulação de escoamentos de fluidos newtonianos incompressíveis , bidimensionais , confinados e/ou com superfícies_livres  e a altos valores do número de Reynolds . O desempenho do código Freeflow-2D atual é avaliada na simulação do escoamento numa expansão brusca e de um jato livre incidindo perpendicularmente sobre uma superfície rígida impermeável . O código é então aplicado na simulação de um jato planar turbulento em uma porção de fluido com superfície_livre  e estacionário . Os resultados numéricos obtidos são comparados com dados experimentais , soluções analíticas e soluções numéricas de outros trabalhos 
 Os modelos do volatilidade estocástica ( MVE ) são bastante utilizados pela sua semelhança com os modelos habitualmente usados na Teoria Financeira . Nos MVE a volatilidade independe dos retornos passados e é modelada como uma variável latente não observada , através de uma componente preditível e outra aleatória . A função de verossimilhança desses modelos é difícil de ser obtida e maximizada . Neste trabalho descrevemos as suposições em que os modelos do difusão para séries de retornos se baseiam , assim como as suposições tomadas pela modelagem discreta . Apresentamos os MVE e alguns de seus métodos de estimação . Tratamos de dois modelos contínuos , do algumas do suas propriedades e também do dois MVE discretos que convergem para tais contínuos . Trabalhamos com uma aproximação linear de um deles , apresentando o filtro de Kalman , e sua verossimilhança obtida depois da filtragem . O algoritmo de Metropolis-Hastings foi empregado na abordagem da verossimilhança , assim como na bayesiana do caso linear . Utilizamos o filtro estendido do Kalman combinado com a aproximação do Laplace na construção da função do verossimilhança dos dois MVE abordados neste trabalho 
 O crescimento em quantidade e complexidade dos dados processados e armazenados torna a busca por similaridade uma tarefa fundamental para tratar esses dados . No entanto , atributos faltantes ocorrem freqüentemente , inviabilizando os métodos de acesso métricos ( MAMs ) projetados para apoiar a busca por similaridade . Assim , técnicas de tratamento de dados faltantes precisam ser desenvolvidas . A abordagem mais comum para executar as técnicas de indexação existentes sobre conjuntos de dados com valores faltantes é usar um indicador de valores faltantes e usar as técnicas de indexação tradicionais . Embora , esta técnica seja útil para os métodos de indexação multidimensionais , é impraticável para os métodos de acesso métricos . Esta dissertação apresenta os resultados da pesquisa realizada para identificar e lidar com os problemas de indexação e recuperação de dados em espaços_métricos  com valores faltantes . Uma análise experimental dos MAMs aplicados a conjuntos de dados incompletos identificou dois problemas principais : distorção na estrutura interna do índice quando a falta é aleatória e busca tendenciosa na estrutura do índice quando o processo de falta não é aleatório . Uma variante do MAM Slim-tree , chamada Hollow-tree foi proposta com base nestes resultados . A Hollow-tree usa novas técnicas de indexação e de recuperação de dados com valores faltantes quando o processo de falta é aleatório . A técnica de indexação inclui um conjunto de políticas de indexação que visam a evitar distorções na estrutura interna dos índices . A técnica de recuperação de dados melhora o desempenho das consultas por similaridade sobre bases de dados incompletas . Essas técnicas utilizam o conceito de dimensão_fractal  do conjunto de dados e a densidade local da região de busca para estimar um raio de busca ideal para obter uma resposta mais correta , considerando os dados com valores faltantes como uma resposta potencial . As técnicas propostas foram avaliadas sobre diversos conjuntos de dados reais e sintéticos . Os resultados mostram que a Hollow-tree atinge quase 100 % de precisão e revocação para consultas por abrangência e mais de 90 % para k vizinhos mais próximos , enquanto a Slim-tree rapidamente deteriora com o aumento da quantidade de valores faltantes . Tais resultados indicam que a técnica de indexação proposta ajuda a estabelecer a consistência na estrutura do índice e a técnica de busca pode ser realizada com um desempenho notável . As técnicas propostas são independentes do MAM básico usado e podem ser aplicadas em uma grande variedade deles , permitindo estender a classe dos MAMs em geral para tratar dados faltantes 
 A presente pesquisa explora areas de Processamento de Linguagem Natural ( PLN ) , tais como , analisadores , gramaticas e ontologias no desenvolvimento de um modelo para o mapeamento de consulta em lingua portuguesa controlada para consultas SPARQL . O SPARQL e uma linguagem de consulta capaz de recuperar e manipular dados armazenados em RDF , que e a base para a construcao de Ontologias . Este projeto pretende investigar utilizacao das tecnicas supracitadas na mitigacao do problema de consulta a Ontologias utilizando linguagem_natural  controlada . A principal motivacao para o desenvolvimento deste trabalho e pesquisar tecnicas e modelos que possam proporcionar uma melhor interacao do homem com o computador . Facilidade na interacao homem-computador e convergida em produtividade , eficiencia , comodidade dentre outros beneficios implicitos . Nos nos concentramos em medir a eficiencia do modelo proposto e procurar uma boa combinacao entre todas as tecnicas em questao 
 Neste trabalho , é apresentada uma análise_Bayesiana  de dados de confiabilidade ou dados médicos cuja população está sujeita a duas causas de falha e só é possível observar o mínimo entre os dois tempos de falha . Estas falhas podem ser falha precoce e falha por envelhecimento e geralmente , nestas situações as funções de risco apresentam forma de U , exigindo uma análise mais complexa com uso de modelos mais elaborados como Weibull-Exponenciada ( Mudholkar , 1995 ) , Gama . generalizada ( Stacy , 1982 ) , ou modelos de mistura de distribuições para , métricas . Na maioria , dos casos existe dependência entre os dois tempos de falha e o uso de um modelo que ( incorpore esta dependência se faz necessária . Desta forma , propomos a utilização da distribuição bivariada de Ryu ( 1993 ) que assume dependência , entre os tempos . Um dos objetivos principais deste trabalho está relacionado á análise_Bayesiana  de dados com observações censuradas , na presença ou não de covariáveis sob as suposições citadas . No contexto Bayesiano foram utilizadas técnicas de simulação via . Monte_Carlo  em Cadeias de Markov ( MCMC ) 
 A alta competitividade e os avanços tecnológicos propiciaram o surgimento de um novo modelo de terceirização de Tecnologia de Informação ( TI ) , os Provedores de Serviços de Aplicativos ( ASP - Application Service Providers ) . A proposta do modelo ASP é a comercialização de aplicativos de software como serviço . A satisfação dos clientes está diretamente relacionada à qualidade do serviço prestado . Para garantir que sejam fornecidos serviços com qualidade , é fundamental a adoção dos Acordos dos Níveis de Serviços ( SLA - Service Levei Agreements ) como mecanismo de controle . No entanto , são escassas as informações formalizadas que venham a contribuir com a especificação desses acordos . Propondo-se a contribuir com a compreensão de SLA para ASP , apresenta-se uma pesquisa exploratória do conceito de SLA para ASP , de modelos de qualidade e dos fatores de qualidade a serem considerados nos SLA para garantir a qualidade dos serviços prestados 
 Esse trabalho apresenta uni método_numérico  para simular_escoamentos  visco-elásticos com superfícies_livres  de fluidos de Segunda Ordem . As equações governantes são resolvidas através de uma técnica de diferenças_finitas  em uma malha diferenciada baseada em um método similar ao SM AC ( Si/mplified-Marker-And-Celi ) . Partículas marcadoras são utilizadas para representar a superfície_livre  do fluido . Detalhes completos para a aproximação das tensões na superfície_livre  são dados . São apresentados resultados numéricos demonstrando a capacidade desta nova_técnica  para resolver escoamentos visco-elásticos com superfícies_livres  de fluidos de Segunda Ordem para vários problemas . Fornecemos resultados numéricos sobre a simulação do inchamento do extrudado e sobre a contração 4 : 1 . Além_disso  , , são apresentados resultados referentes à validação e convergência do método_numérico  desenvolvido nesse trabalho 
 Em ambientes virtualizados , como nuvens computacionais , a capacidade efetiva de transmissão de dados via rede tende a ser inferior à de ambientes não virtualizados quando aplicações que fazem_uso  intensivo da rede são executadas . Uma das principais causas para essa diferença na capacidade de transmissão é a arquitetura da virtualização de rede , que adiciona passos para o sistema_operacional  transmitir e receber um pacote . Esses passos adicionais acarretam em maior utilização de memória e de processamento . Em ambientes virtualizados com o sistema_operacional  GNU/Linux , a New Application Programming Interface ( NAPI ) é utilizada para reduzir os impactos negativos da virtualização por meio de agregação de interrupções . Nesta dissertação de mestrado , são estudados mecanismos que modificam a configuração da NAPI . Experimentos mostram que esses mecanismos afetam o desempenho de máquinas virtuais e tem consequências diretas nas aplicações que fazem_uso  intensivo de rede e que são executadas em ambientes com os softwares de virtualização Xen , VMware e VirtualBox 
 O registro de imagens tem um papel_importante  em várias aplicações , tais como reconstrução de objetos 3D , reconhecimento de padrões , imagens microscópicas , entre outras . Este registro é composto por três passos principais : ( 1 ) seleção de pontos de interesse ; ( 2 ) extração de características dos pontos de interesse ; ( 3 ) correspondência entre os pontos de interesse de uma imagem para a outra . Para os passos 1 e 2 , algoritmos como SIFT e SURF têm apresentado resultados_satisfatórios  . Entretanto , para o passo 3 ocorre a presença de outliers , ou seja , pontos de interesse que foram incorretamente correspondidos . Uma única correspondência incorreta leva a um resultado final indesejável . Os algoritmos para remoção de outliers ( consenso ) possuem um alto custo_computacional  , que cresce à medida que a quantidade de outliers aumenta . Com o objetivo de reduzir o tempo de processamento necessário por esses algoritmos , o algoritmo FOMP ( do inglês , Filtering out Outliers from Matched Points ) , foi proposto e desenvolvido neste trabalho para realizar a filtragem de outliers no conjunto de pontos inicialmente correspondidos . O método FOMP considera cada conjunto de pontos como um grafo completo , no qual os pesos são as distâncias entre os pontos . Por meio da soma de diferenças entre os pesos das arestas , o vértice que apresentar maior valor é removido . Para validar o método FOMP , foram realizados_experimentos  utilizando quatro bases de imagens . Cada base apresenta características intrínsecas : ( a ) diferenças de rotação zoom da câmera ; ( b ) padrões repetitivos , os quais geram duplicidade nos vetores de características ; ( c ) objetos de formados , tais como plásticos , papéis ou tecido ; ( d ) transformações afins ( diferentes pontos de vista ) . Os experimentos_realizados  mostraram que o filtro FOMP remove mais de 65 % dos outliers , enquanto mantém cerca de 98 % dos inliers . A abordagem proposta mantém a precisão dos métodos de consenso , enquanto reduz o tempo de processamento pela metade para os métodos baseados em grafos 
 Dados de alta dimensão são tipicamente tratados como pertencentes a um único subespaço do espaço onde estão imersos . Entretanto , dados utilizados em aplicações reais estão usualmente distribuídos entre subespaços independentes e com dimensões distintas . Um objeto de estudo surge a partir dessa afirmação : como essa distribuição em subespaços independentes pode auxiliar tarefas de visualização ? Por outro_lado  , se o dado parece estar embaralhado nesse espaço de alta dimensão , como visualizar seus padrões e realizar tarefas como classificação ? Podemos , por exemplo , mapear esse dado num outro espaço utilizando uma função capaz de o desembaralhar , de modo que os padrões intrínsecos fiquem mais claros e , assim , facilitando nossa tarefa de visualização ou classificação . Essa Tese apresenta dois estudos que abordam ambos os problemas . Para o primeiro , utilizamos técnicas de subspace clustering para definir , quando existente , a estrutura de subespaços do dado e estudamos como essa informação pode auxiliar em visualizações utilizando projeções_multidimensionais  . Para o segundo problema , métodos de kernel , bastante conhecidos na literatura , são as ferramentas a nos auxiliar . Utilizamos a medida de similaridade do kernel para desenvolver uma nova_técnica  de projeção_multidimensional  capaz de lidar com dados imersos no espaço de características induzido implicitamente pelo kernel 
 O manejo de proteção com uso de produtos fitofarmacêuticos possibilita o controle de pragas em ambientes agrícolas , tornando-o menos nocivo para o desenvolvimento da cultura e com produção em grande escala . Porém , apenas uma pequena parte do produto pulverizado realmente é depositado na área alvo enquanto a maior_parte  do produto sofre deriva para regiões vizinhas . A literatura científica possui trabalhos com o uso de técnicas matemáticas para calcular a transformação física e movimento para estimar a deposição do produto . Com base nessa predição é possível configurar o sistema de pulverização para realizar a pulverização sob uma condição meteorológica comum na região para um desempenho satisfatório , mas as condições meteorológicas podem sofrer alterações e tornar qualquer configuração estática ineficiente . Uma alternativa para esse problema é realizar a adaptação da atuação do elemento pulverizador às condições meteorológicas durante a execução do manejo de proteção . Contudo , as técnicas existentes são computacionalmente custosas para serem executadas , tornando-as inadequadas para situações em que é requerido baixo tempo de execução . Esta tese se concentra no contexto descrito com objetivo de permitir a predição da deposição de forma rápida e precisa . Assim , espera-se que as novas abordagens sejam capazes de possibilitar a adaptação do elemento pulverizador às condições meteorológicas durante a realização do manejo de proteção . Este trabalho inicia com o processo de redução do custo de execução de um modelo computacional do ambiente , tornando sua execução mais rápida . Posteriormente , utiliza-se este modelo computacional para predição da deposição como função Fitness em algoritmos de meta-heurística para adaptar o comportamento do elemento pulverizador às condições meteorológicas durante a realização do manejo . Os resultados desta abordagem demonstram que é possível utilizá-la para realizar a adaptação em ambientes com baixa variabilidade . Por outro_lado  , pode apresentar baixo desempenho em ambientes com alta variabilidade nas condições meteorológicas . Uma segunda abordagem é investigada e analisada para este cenário , onde o processo de adaptação requer um tempo de execução reduzido . Nesta segunda abordagem é utilizado uma técnica de Aprendizado de Máquina treinada com os resultados gerados pela primeira abordagem em diferentes cenários . Os resultados obtidos demonstram que essa abordagem possibilita realizar a adaptação do elemento pulverizador compatível com a proporcionada pela abordagem anterior em um menor espaço de tempo 
 Este trabalho discute o processo de mudança de domínio de software , ou seja , a adaptação do software para um domínio de informações diferente do original . A investigação é fundamentada em um exercício prático : o sistema SASHE ( Sistema de Autoria e Suporte Hipermidia para Ensino ) foi adaptado para o domínio da Engenharia de Software . A partir desse exercício , um novo sistema , o SASHDoe , foi projetado com base no SASHE com o objetivo de tornar-se um sistema hipermídia de apoio à documentação de software 
 Nesta dissertação , desenvolvemos uma análise_Bayesiana  de modelos de regressão para dados binários correlacionados com covariáveis , podendo ocorrer réplicas . Assumimos os modelos de regressão logístico e probito para dados binários correlacionados considerando efeitos aleatórios com uma mistura de distribuições normais , pois este modelo tem uma grande flexibilidade para ser ajustado aos dados binários correlacionados em muitas aplicações . Também fazemos algumas considerações aos casos onde podem ocorrer repetições das observações ou réplicas . Assumimos distribuições a priori informativas para os parâmetros do modelo e consideramos os algoritmos Gibbs sampling e Metropolis- Hastings , para obter as estimativas de Monte_Carlo  para as quantidades a posteriori de interesse . Apresentamos também algumas considerações na seleção de modelos utilizando uma medida da discrepância entre o modelo ajustado e os dados ( resíduo de Pearson ) e utilizando as densidades preditivas ( fator de Bayes ) estimadas por MCMC ( Monte_Carlo  em Cadeias de Markov ) . Apresentamos um exemplo númerico para ilustrar os métodos propostos 
 A necessidade de poder_computacional  é crescente nas mais diversas_áreas  de atuação humana , tanto na indústria como no ambiente acadêmico . A Computação em Grade permite a interligação de recursos_computacionais  dispersos de maneira a permitir sua utilização mais efetiva , provendo aos usuários acesso simplificado ao poder_computacional  de diversas máquinas . Entretanto , os primeiros projetos de Computação em Grade objetivavam a interligação de máquinas_paralelas  ou aglomerados de alto_desempenho  e alto custo , acessível apenas a poucas instituições 
 Em contraponto ao alto custo das máquinas_paralelas  , os computadores_pessoais  se difundiram de maneira extraordinária nos últimos quinze anos . A expansão da base instalada foi acompanhada de uma crescente evolução na capacidade de tais máquinas . Os aglomerados dedicados de computadores_pessoais  representam a primeira tentativa de utilização de tais recursos para computação_paralela  e , apesar de serem amplamente_utilizados  , ainda requerem um investimento significativo . Entretanto , as mesmas instituições que adquirem tais aglomerados dedicados normalmente possuem centenas ou até milhares de computadores_pessoais  , os quais têm sua capacidade utilizada apenas parcialmente , resultando em grande ociosidade dos recursos 
 Os sistemas de Computação em Grade Oportunistas fornecem a possibilidade de se utilizar a base instalada de computadores_pessoais  de maneira a realizar computação utilizando a parte dos recursos que , de outra forma , estariam ociosos . Diversos sistemas dessa categoria foram desenvolvidos e utilizados com êxito para realizar tarefas de computação em diversas_áreas  como astronomia , biologia e matemática 
 O InteGrade , sistema de Computação em Grade Oportunista aqui apresentado , pretende oferecer características inovadoras para sistemas oportunistas , como suporte a aplicações paralelas que demandam comunicação entre nós e a utilização de coleta e análise de padrões de uso das máquinas da Grade , de maneira a permitir que se realize previsões sobre a disponibilidade das máquinas , permitindo uma utilização mais eficiente das mesmas . Além_disso  , o InteGrade emprega amplamente o paradigma de Orientação a Objetos , tanto na definição da arquitetura do sistema quanto na sua implementação 
 O trabalho aqui apresentado consistiu no estudo de outros projetos de Computação em Grade , na definição de uma arquitetura inicial para o InteGrade , passando pela descrição de seus principais módulos assim como sua implementação . Além_disso  , também descrevemos o projeto e a implementação de uma biblioteca para programação paralela no InteGrade utilizando o modelo BSP 
 O problema abordado nesta pesquisa consiste na distribuição de água em redes urbanas para o atendimento de demandas conhecidas , com o objetivo de minimizar o custo da energia elétrica necessária para o funcionamento de bombas hidráulicas . As bombas hidráulicas são utilizadas para captar água de poços artesianos ou estações de tratamento de água para abastecer reservatários distribuídos por bairros de uma cidade , de onde a população será atendida por força gravitacional . Como o custo da energia elétrica varia ao longo do dia , se faz necessário um planejamento do funcionamento das bombas para que não sejam ligadas nos horários em que a energia elétrica é mais cara . O problema de planejamento de estoque de água em reservatórios ( PPEAR ) consiste em decidir em quais períodos ou frações dos períodos do horizonte de planejamento as bombas hidráulicas que abastecem os reservatórios devem permanecer ligadas e em quais períodos ou frações dos períodos deve haver transporte de água entre os reservatórios para que a demanda de cada reservatório seja atendida em cada_período  e sejam respeitados os níveis mínimos e máximos de água nos reservatórios . Uma solução heurística para resolver o PPEAR é proposta e analisada por comparação com as soluções obtidas pelo método de enumeração implícita . Resultados computacionais comprovam a eficiência da abordagem , tanto pela qualidade das soluções como pelo baixo tempo de 
 Neste trabalho a detecção de novidade é tratada como o problema de identificação de conceitos emergentes em dados que podem ser apresentados em um fluxo contínuo . Considerando a relação intrínseca entre tempo e novidade e os desafios impostos por fluxos de dados , uma nova abordagem é proposta . OLINDDA ( OnLIne Novelty and Drift Detection Algorithm ) vai além da classficação com uma classe e concentra-se no aprendizado contínuo não-supervisionado de novos conceitos . Tendo aprendido uma descrição inicial de um conceito normal , prossegue à análise de novos dados , tratando-os como um fluxo contínuo em que novos conceitos podem aparecer a qualquer momento . Com o uso de técnicas de agrupamento , OLINDDA pode empregar diversos critérios de validação para avaliar grupos em termos de sua coesão e representatividade . Grupos considerados válidos produzem conceitos que podem sofrer fusão , e cujo conhecimento é continuamente incorporado . A técnica é avaliada experimentalmente com dados artificiais e reais . O módulo de classificação com uma classe é comparado a outras técnicas de detecção de novidade , e a abordagem como um todo é analisada sob vários aspectos por meio da evolução_temporal  de diversas métricas . Os resultados reforçam a importância da detecção contínua de novos conceitos , assim como as dificuldades e desafios do aprendizado não-supervisionado de novos conceitos em fluxos de 
 O problema reconstrução filogenética têm como objetivo determinar as relações evolutivas das espécies , usualmente representadas em estruturas de árvores . No entanto , esse problema tem se mostrado muito difícil uma vez que o espaço de busca das possíveis árvores é muito grande . Diversos métodos de reconstrução filogenética têm sido_propostos  . Vários desses métodos definem um critério de otimalidade para avaliar as possíveis soluções do problema . Porém , a aplicação de diferentes critérios resulta em árvores diferentes , inconsistentes entre sim . Nesse contexto , uma abordagem multi-objetivo para a reconstrução filogenética pode ser útil produzindo um conjunto de árvores consideradas adequadas por mais de um critério . Nesta tese é proposto um algoritmo_evolutivo  multi-objetivo , denominado PhyloMOEA , para o problema de reconstrução filogenética . O PhyloMOEA emprega os critérios de parcimônia e verossimilhança que são dois dos métodos de reconstru ção filogenética mais empregados . Nos experimentos , o PhyloMOEA foi testado utilizando quatro bancos de seqüências freqüentemente empregados na literatura . Para cada banco de teste , o PhyloMOEA encontrou as soluções da fronteira de Pareto que representam um compromisso entre os critérios considerados . As árvores da fronteira de Pareto foram validadas estatisticamente utilizando o teste SH . Os resultados mostraram que o PhyloMOEA encontrou um número de soluções intermediárias que são consistentes com as soluções obtidas por análises de máxima parcimônia e máxima_verossimilhança  realizados separadamente . Além_disso  , os graus de suporte dos clados pertencentes às árvores encontradas pelo PhyloMOEA foram comparadas com a probabilidade posterior dos clados calculados pelo programa Mr.Bayes aplicados aos quatro bancos de teste . Os resultados indicaram que há uma relação entre ambos os valores para vários grupos de clados . Em resumo , o PhyloMOEA é capaz de encontrar uma diversidade de soluções intermediárias que são estatisticamente tão boas quanto as melhores soluções de máxima parcimônia e máxima_verossimilhança  . Tais soluções apresentam um compromisso entre os dois 
 A qualidade dos sistemas de software é uma preocupação de todo bom projeto e muito tem se estudado para melhorar tanto a qualidade do produto_final  quanto do processo de desenvolvimento . Teste de Software é uma área de estudo que tem crescido significativamente nos últimos tempos , em especial a automação de testes que está cada vez mais em evidência devido à agilidade e qualidade que pode trazer para o desenvolvimento de sistemas de software . Os testes automatizados podem ser eficazes e de baixo custo de implementação e manutenção e funcionam como um bom mecanismo para controlar a qualidade de sistemas . No entanto , pouco conhecimento sobre a área e erros comuns na escrita e manutenção dos testes podem trazer dificuldades adicionais aos projetos de software . Testes automatizados de baixa qualidade não contribuem efetivamente com o controle de qualidade dos sistemas e ainda demandam muito tempo do desenvolvimento . Para evitar esses problemas , esta dissertação apresenta de forma crítica e sistemática as principais práticas , padrões e técnicas para guiar o processo da criação , manutenção e gerenciamento dos casos de testes automatizados . Inicialmente , são feitas comparações entre a automação de testes e outras práticas de controle e garantia de qualidade . Em seguida , são apresentados os problemas e soluções mais comuns durante a automação de testes , tais como questões relacionadas a tipos específicos de algoritmos , sistemas com persistência de dados , testes de interfaces de usuário e técnicas de desenvolvimento de software com testes automatizados . Para finalizar , a dissertação traz uma reflexão sobre o gerenciamento e a abordagem da automação de testes para tornar o processo mais produtivo e eficaz 
 O desafio de encontrar corretamente genes eucarioticos codificadores de proteinas nas sequencias genomicas e um problema em aberto . Neste trabalho , implementamos uma plata- forma , com o objetivo de melhorar a forma com que preditores de genes sao implementados e avaliados . Tres novas ferramentas foram implementadas : ToPS ( Toolkit of Probabilistic Models of Sequences ) foi o primeiro arcabouco orientado a objetos que fornece ferramentas para implementacao , manipulacao , e combinacao de modelos probabilisticos para representar sequencias de simbolos ; MYOP ( Make Your Own Predictor ) e um sistema que tem como objetivo facilitar a construcao de preditores de genes ; e SGEval utiliza grafos de splicing para comparar diferente anotacoes com eventos de splicing alternativos . Utilizamos nossas ferramentas para o desenvolvimentos de preditores de genes em onze genomas distintos : A. thaliana , C. elegans , Z. mays , P. falciparum , D. melanogaster , D. rerio , M. musculus , R. norvegicus , O. sativa , G. max e H. sapiens . Com esse desenvolvimento , estabelecemos um protocolo para implementacao de novos preditores . Alem disso , utilizando a nossa plata- forma , desenvolvemos um fluxo de trabalho para predicao de genes no projeto do genoma da cana de acucar , que ja foi utilizado em 109 sequencias de BAC geradas pelo BIOEN ( FAPESP Bioenergy Program ) 
 O processo automático de classificação de dados em geral , e em particular de classificação de imagens , é uma tarefa computacionalmente intensiva e variável em termos de precisão , sendo consideravelmente dependente da configuração do classificador e da representação dos dados utilizada . Muitos dos fatores que afetam uma adequada aplicação dos métodos de classificação ou categorização para imagens apontam para a necessidade de uma maior interferência do usuário no processo . Para isso são necessárias mais ferramentas de apoio às várias etapas do processo de classificação , tais como , mas não limitadas , a extração de características , a parametrização dos algoritmos de classificação e a escolha de instâncias de treinamento adequadas . Este doutorado apresenta uma metodologia para Classificação Visual de Imagens , baseada na inserção do usuário no processo de classificação automática através do uso de técnicas de visualização . A ideia é permitir que o usuário participe de todos os passos da classificação de determinada coleção , realizando ajustes e consequentemente melhorando os resultados de acordo com suas necessidades . Um estudo de diversas técnicas de visualização candidatas para a tarefa é apresentado , com destaque para as árvores de similaridade , sendo apresentadas melhorias do algoritmo de construção em termos de escalabilidade visual e de tempo de processamento . Adicionalmente , uma metodologia de redução de dimensionalidade visual semi-supervisionada é apresentada para apoiar , pela utilização de ferramentas visuais , a criação de espaços reduzidos que melhorem as características de segregação do conjunto original de características . A principal_contribuição  do trabalho é um sistema de classificação visual incremental que incorpora todos os passos da metodologia_proposta  , oferecendo ferramentas interativas e visuais que permitem a interferência do usuário na classificação de coleções incrementais com configuração de classes variável . Isso possibilita a utilização do conhecimento do ser humano na construção de classificadores que se adequem a diferentes necessidades dos usuários em diferentes cenários , produzindo resultados_satisfatórios  para coleções de dados diversas . O foco desta tese é em categorização de coleções de imagens , com exemplos também para conjuntos de dados 
 O recente crescimento da utilização de Unidades de Processamento Gráfico ( GPUs ) em aplicações científicas , que são voltadas ao desempenho , gerou a necessidade de otimizar os programas que nelas rodam . Uma ferramenta adequada para essa tarefa é o modelo de desempenho que , por sua vez , se beneficia da existência de uma ferramenta de extração de informações de desempenho para GPUs . Este trabalho cobre a criação de um gerador de microbenchmark para instruções PTX que também obtém informações sobre as características do hardware da GPU . Os resultados obtidos com o microbenchmark foram validados através de um modelo simplificado que obteve erros entre 6,11 % e 16,32 % em cinco kernels de teste . Também foram levantados os fatores de imprecisão nos resultados do microbenchmark . Utilizamos a ferramenta para analisar o perfil de desempenho das instruções e identificar grupos de comportamentos semelhantes . Também testamos a dependência do desempenho do pipeline da GPU em função da sequência de instruções executada e verificamos a otimização do compilador para esse caso . Ao fim deste trabalho concluímos que a utilização de microbenchmarks com instruções PTX é factível e se mostrou eficaz para a construção de modelos e análise detalhada do comportamento das instruções 
 Este trabalho apresenta o Super Spider : um sistema de exploração visual baseado no Spider Cursor , que abrange várias técnicas interativas da área de Visualização Computacional e conta com novos recursos de auxílio à investigação visual , além de ser uma ferramenta portável e flexível 
 O CoGrOO é um corretor gramatical de código_aberto  em uso por milhares de usuários de uma popular suíte de escritório de código_aberto  . Ele é capaz de identificar erros como : colocação pronominal , concordância nominal , concordância sujeito-verbo , uso da crase , concordância nominal e verbal e outros erros comuns de escrita em Português do Brasil . Para tal , o CoGrOO realiza uma análise híbrida : inicialmente o texto é anotado usando técnicas estatísticas de Processamento de Linguagens Naturais e , em seguida , um sistema baseado em regras é responsável por identificar os possíveis erros gramaticais . O objetivo deste trabalho é reduzir a quantidade de omissões e intervenções indevidas e , ao mesmo tempo , aumentar a quantidade de verdadeiros positivos sem , entretanto , adicionar novas regras de detecção de erros . A última avaliação científica do corretor gramatical foi realizada em 2006 e , desde então , não foram realizados estudos detalhados quanto ao seu desempenho , apesar de o código do sistema ter passado por substancial evolução . Este trabalho contribuirá com uma detalhada avaliação dos anotadores estatísticos e os resultados serão comparados com o estado da arte . Uma vez que os anotadores do CoGrOO estão disponíveis como software_livre  , melhorias nesses módulos gerarão boas alternativas a sistemas proprietários 
 A indústria de software é muito dinâmica e novas ideias surgem a todo instante em todas as partes do mundo . Nem sempre é fácil fazer com que essas ideias sejam adotadas , pois , para isso , é preciso fazer as pessoas mudarem sua forma de pensar . Deve-se sempre considerar o fato de que o ser humano , diferente do computador , é inusitado e imprevisível . Apesar disso , podemos encontrar determinados padrões de comportamento , que não resolvem todas as questões , mas ajudam a lidar com situações e continuar caminhando para atingir um determinado objetivo . Trazemos nesta dissertação uma pequena introdução sobre o conceito de padrões e , em seguida , apresentamos 48 Padrões para Introduzir Novas Ideias , propostos por Linda Rising e Mary Lynn Manns . Esses Padrões têm o objetivo de ajudar na difícil tarefa de introduzir uma nova ideia dentro de alguma organização , pois se essa ideia pressupõe mudanças culturais , o trabalho é ainda mais complicado . Propomos também quatro novos padrões , que podem ser incorporados ao catálogo original . Num desses novos padrões , mostramos a importância de se usar atividades artísticas no dia-a-dia de pessoas que trabalham com desenvolvimento de software ; mostramos também como a Arte pode nos ajudar a introduzir novas ideias . Pesquisamos algumas práticas como teatro , pintura , poesia , música e meditação . Pudemos encontrar elementos de ligação entre o lado puramente matemático e bem definido do ser humano e o seu lado abstrato , analógico e artístico . Desenvolver software deve ser encarado como uma atividade humana , acima da questão técnica e puramente lógica . Existem pessoas envolvidas no processo : as que usam e as que criam o software . Existe uma barreira que separa os programadores das pessoas que usam o software . Essa barreira pode ser quebrada se pessoas da Computação começarem a desenvolver , além das habilidades lógicas que já lhes são óbvias , habilidades artísticas e de relações humanas 
 A complexidade é uma característica de grande_importância  em processos de reconhecimento de padrões , especialmente naqueles que envolvem imagens biológicas . Este trabalho tem como objetivo estudar métodos que realizam a análise de imagens por meio da análise de sua complexidade . Os métodos a serem estudados foram selecionados com base na similaridade de seus algoritmos e metodologia : dimensão_fractal  , Caminhada Determinística do Turista e Redes Complexas . Estes métodos permitem realizar a análise e segmentação de formas ou texturas contidas em uma imagem com base na sua variação de complexidade . Dos três métodos considerados , dois deles fazem parte do estado da arte em análise de complexidade , enquanto que a dimensão_fractal  já é aplicada a mais tempo na análise de formas e texturas . Os trabalhos aqui desenvolvidos visam comparar e analisar os métodos selecionados por meio de experimentos com imagens de forma e texturas , sendo utilizadas texturas naturais e de Brodatz , freqüentemente utilizadas na literatura como benchmark para texturas . Com base no conhecimento adquirido , novas técnicas voltadas para a análise e segmentação de formas e texturas foram desenvolvidas , assim como foram analisadas as deficiências e propostas melhorias às técnicas estudadas . Além_disso  , diversos experimentos com estas metodologias foram realizados em aplicações de 
 Algoritmos de aprendizado semissupervisionado aprendem a partir de uma combinação de dados rotulados e não rotulados . Assim , eles podem ser aplicados em domínios em que poucos exemplos_rotulados  e uma vasta quantidade de exemplos não rotulados estão disponíveis . Além_disso  , os algoritmos semissupervisionados podem atingir um desempenho_superior  aos algoritmos supervisionados treinados nos mesmos poucos exemplos_rotulados  . Uma poderosa abordagem ao aprendizado semissupervisionado , denominada aprendizado multidescrição , pode ser usada sempre que os exemplos de treinamento são descritos por dois ou mais conjuntos de atributos disjuntos . A classificação de textos é um domínio de aplicação no qual algoritmos semissupervisionados vêm obtendo sucesso . No entanto , o aprendizado semissupervisionado multidescrição ainda não foi bem explorado nesse domínio dadas as diversas maneiras possíveis de se descrever bases de textos . O objetivo neste trabalho é analisar o desempenho de algoritmos semissupervisionados multidescrição na classificação de textos , usando unigramas e bigramas para compor duas descrições distintas de documentos textuais . Assim , é considerado inicialmente o difundido algoritmo multidescrição CO-TRAINING , para o qual são propostas modificações a fim de se tratar o problema dos pontos de contenção . É também proposto o algoritmo COAL , o qual pode melhorar ainda mais o algoritmo CO-TRAINING pela incorporação de aprendizado ativo como uma maneira de tratar pontos de contenção . Uma ampla avaliação_experimental  desses algoritmos foi conduzida em bases de textos reais . Os resultados mostram que o algoritmo COAL , usando unigramas como uma descrição das bases textuais e bigramas como uma outra descrição , atinge um desempenho significativamente melhor que um algoritmo semissupervisionado monodescrição . Levando em consideração os bons_resultados  obtidos por COAL , conclui-se que o uso de unigramas e bigramas como duas descrições distintas de bases de textos pode ser bastante 
 Este trabalho estuda o problema da Satisfazibilidade Probabilística ( PSAT ) , revendo a sua solução via programação linear , além de propor novos algoritmos para resolvê-lo através da redução ao SAT . Construímos uma redução polinomial do PSAT para o SAT , chamada de Redução Canônica , codificando operações da aritmética racional em bits , como variáveis lógicas . Analisamos a complexidade computacional dessa redução e propomos uma Redução Canônica de Precisão Limitada para contornar tal complexidade . Apresentamos uma Redução de Turing do PSAT ao SAT , baseada no algoritmo Simplex e na Forma Normal Atômica que introduzimos . Sugerimos modificações em tal redução em busca de eficiência computacional . Por fim , implementamos essas reduções a m de investigar o perl de complexidade do PSAT , observamos o fenômeno de transição de fase e discutimos as condições para sua detecção 
 A automação das atividades em diversas_áreas  , tais como negócios , engenharia , medicina , ciência e governo , vêm aumentando a cada dia , e com isso , cresce o volume de dados armazenados nas bases de dados . É interessante usar esses dados além dos objetivos originais das operações de pura armazenagem e busca do empreendimento inicial . Em outras_palavras  , é desejável extrair_informações  úteis , não previstas inicialmente , agregando valor ao empreendimento . Embora os gerenciadores de bases de dados forneçam ferramentas básicas para recuperação de dados em transações padrão em grandes quantidades de dados , analisar grandes volumes de dados em formato numérico ou textual , especialmente em espaços de altas dimensões , não é fácil para o ser humano . Por outro_lado  , o ser humano tem uma capacidade de absorver e entender informação representada de forma gráfica muito rapidamente . Dessa forma , este trabalho visa o desenvolvimento de uma nova_técnica  para construção visual de classificadores de dados em atividades de Data_Mining  , utilizando essa reconhecidamente elevada capacidade humana de análise de dados representados em formato gráfico como fator que auxilia o treinamento do classificador . Este trabalho estende a ferramenta FastMapDB , que em sua versão original permitia apenas a visualização de dados e não possibilitava ao usuário interferir no processo de visualização , para permitir , não apenas essa visualização de dados , mas também , a seleção visual dos objetos mapeados e a delimitação de regiões de objetos na visualização . Com isso , a ferramenta passa a permitir a recuperação dos dados presentes na base de dados que o usuário considerar interessantes a partir da visualização , e provê à ferramenta recursos de classificação de novos objetos segundo regras visuais que o usuário possa definir no espaço mapeado de visualização . A proposta deste trabalho é inédita no sentido de aplicar técnicas de visualização como fator de treinamento do classificador . As ferramentas atuais apenas mostram resultados já elaborados pelo sistema , e a continuação do processo é sempre solicitada , ou decidida manualmente , pelo usuário . Nosso sistema , ao contrário , realimenta a interpretação que o usuário pode fazer a respeito dos dados , e permite que os passos de análise seguintes utilizem automaticamente esse resultado 
 Esse trabalho apresenta um estudo das atuais alternativas para reconhecimento de padrões aplicados à segurança computacional , mais precisamente aos Sistemas de Detecção de Intrusão . São estudadas as abordagens propostas por cinco trabalhos publicados em simpósios internacionais e os resultados experimentais apresentados por cada um . Esses trabalhos fazem_uso  de variados métodos para reconhecimento de padrões , como Redes_Neurais  Artificiais , Algoritmos Evolucionários e Mineração de Dados entre outras . Uma avaliação crítica de cada uma das abordagens . é apresentada , e são feitas propostas de implementação de algum dos métodos avaliados , objetivando suportar um Sistema de Detecção de Intrusões baseado em agentes_móveis  . Esse sistema foi modelado em estudos realizados pelo Grupo de Segurança Computacional do Instituto de Ciências_Matemáticas  e de Computação 
 Qualidade de Serviço ( Qualiíy of Service - QoS ) é um tema bastante explorado na literatura atualmente . Contudo , as potenciais vantagens da tecnologia de redes ativas , como forma de facilitar a obtenção e o gerenciamento de parâmetro de QoS , é pouco estudada . Então , auxiliando o preenchimento dessa lacuna , este trabalho visa a especificação e o projeto de um serviço ativo para o provimento de parâmetros de QoS . O serviço permite ao administrador da rede criar e refinar o cálculo de parâmetros de QoS por meio de uma interface Web amigável . Os parâmetros são disponibilizados em locais e formatos pré-definidos , possibilitando assim que outros serviços possam acessar os dados . O Serviço projetado neste trabalho deve servir como base para outros serviços , por exemplo , o serviço de reserva de recursos , criando-se subsídios para o desenvolvimento de estratégias , baseadas nos parâmetros , de controle da rede e de negociação com aplicações 
 Um melhor escalonamento em simulação distribuída é fundamental para uma execução mais rápida e eficiente . O projeto desenvolvido tem como objetivo a avaliação de desempenho de políticas de escalonamento convencionais e específicas para Simulação Distribuída ( SD ) , apresentando uma comparação do desempenho destas duas_abordagens  . Análises das pesquisas feitas na área mostram que não existe avaliação semelhante . Assim , este trabalho tem a importante contribuição de demonstrar as vantagens e desvantagens do uso de políticas tradicionais em relação às específicas em SD . Para execução das simulações foi utilizada a ferramenta Warped , que está descrita nesta dissertação . Foram desenvolvidas e implementadas novas técnicas de escalonamento que utilizam os resultados da simulação em execução , assim executam um melhor balanceamento de carga . Para o desenvolvimento deste projeto foi necessária uma revisão bibliográfica envolvendo conceitos de simulação distribuída com seus respectivos protocolos de sincronização , escalonamento de processos específicos para programas de SD e políticas tradicionais . Com este estudo soma-se como contribuição deste trabalho uma nova classificação das políticas específicas para SD que utilizam protocolo 
 Esta dissertação de mestrado apresenta a implementação e validação de um modelo de servidor web que divide o funcionamento de o servidor web em quatro módulos onde cada um desses módulos é responsável por uma etapa que a requisição percorre ao longo de seu processamento . Esses módulos são : atendimento da requisição ( módulo 1 ) , transferência do arquivo para a memória principal ( módulo 2 ) , processamento de requisições dinâmicas ( módulo 3 ) e envio do arquivo ao cliente ( módulo 4 ) . Esses quatro módulos são interligados e são alimentados primeiramente por uma carga inicial gerada pelo gerador de cargas W4Gen e passa obrigatoriamente , nessa ordem , pelo módulo 1 , módulo 2 e módulo 4 . O módulo 3 só é utilizado quando se trata de uma requisição dinâmica . Ao ser atendido por um dos módulos , é atribuído um tempo de execução ( leia-se tempo que a requisição toma para ser processada por esse módulo ) . Esses tempos foram baseados em trabalhos que fizeram benchmarks em servidores_web  reais . Os resultados alcançados com o desenvolvimento deste trabalho visam principalmente integrar-se aos trabalhos de simulação envolvendo servidores_web  do grupo de Sistemas_Distribuídos  e Programação Concorrente ( LaSDPC ) e com isso alcançar resultados próximos a resultados aplicados em servidores_web  
 O presente texto descreve a tese de doutorado intitulada Análise de Formas usando Wavelets em Grafos . O tema está relacionado à área de Visão Computacional , particularmente aos tópicos de Caracterização , Descrição e Classificação de Formas . Dentre os métodos da extensa literatura em Análise de Formas 2D , percebe-se uma presença menor daqueles baseados em grafos com topologia arbitrária e irregular . As contribuições desta tese procuram preencher esta lacuna . É proposta uma metodologia baseada no seguinte pipeline : ( i ) Amostragem da forma , ( ii ) Estruturação das amostras em grafos , ( iii ) Função-base definida nos vértices , ( iv ) Análise multiescala de grafos por meio da Transformada Wavelet Espectral em grafos , ( v ) Extração de Características da Transformada Wavelet e ( vi ) Discriminação . Para cada uma das etapas ( i ) , ( ii ) , ( iii ) , ( v ) e ( vi ) , são inúmeras as abordagens possíveis . Um dos desafios é encontrar uma combinação de abordagens , dentre as muitas alternativas , que resulte em um pipeline eficaz para nossos propósitos . Em particular , para a etapa ( iii ) , dado um grafo que representa uma forma , o desafio é identificar uma característica associada às amostras que possa ser definida sobre os vértices do grafo . Esta característica deve capturar a influência subjacente da estrutura combinatória de toda a rede sobre cada vértice , em diversas escalas . A Transformada Wavelet Espectral sobre os Grafos revelará esta influência subjacente em cada vértice . São apresentados resultados obtidos de experimentos usando formas 2D de benchmarks conhecidos na literatura , bem como de experimentos de aplicações em astronomia para análise de formas de galáxias do Sloan Digital Sky Survey não-rotuladas e rotuladas pelo projeto Galaxy Zoo 2 , demonstrando o sucesso da técnica proposta , comparada a abordagens clássicas como Transformada de Fourier e Transformada Wavelet Contínua 2D 
 Neste trabalho , aplicamos um modelo de 
 binária com erros de medição na variável explicativa para analisar sistemas de medição do tipo atributo . Para isto , utilizamos o 
 logístico com erros na variável , para o qual obtemos as estimativas de máxima_verossimilhança  via o algoritmo EM e a matriz de informação de Fisher observada . Além_disso  , fizemos um estudo de simulação para compararmos o método analítico e os modelos logístico sem erros na variável ( ingênuo ) e logístico com erros na variável 
 Finalmente , aplicamos nossa metodologia para avaliarmos um sistema de medição passa/não passa da maior montadora de motores Diesel ( 
 International ) 
 A criptografia de chave_pública  sem certificado ( certificateless ) é uma alternativa ao modelo convencional de criptografia assimétrica , pois a autenticação da chave_pública  ocorre implicitamente durante a execução dos protocolos , sem a necessidade de gerenciamento e distribuição de certificados digitais . Potencialmente reduz custos computacionais e o nível de segurança alcançado é maior quando comparado ao modelo baseado em identidade . Nesta tese de doutorado , modelos formais de segurança para acordo de chave com autenticação sem certificado são aprimorados visando dois objetivos paralelos : ( 1 ) aumentar o nível de confiança que usuários podem depositar na autoridade geradora de chaves secretas parciais e ( 2 ) viabilizar protocolos que sejam eficientes computacionalmente e com propriedades de segurança relevantes , dentre as quais se inclui resistência a ataques de adversários que têm total controle do canal de comunicação e que podem substituir chaves públicas de usuários por valores arbitrários . Para atestar que as melhorias efetuadas são praticáveis e possibilitam que os objetivos sejam alcançados , novos protocolos são propostos para o caso que envolve dois participantes na comunicação . Os protocolos são provados seguros , usando-se técnica de redução de problemas computacionais 
 O crescente uso de ambientes virtuais de aprendizagem faz com que os professores e desenvolvedores precisem avaliar qual o melhor formato de visualização a ser utilizado . Infográfico é tipo de visualização de informação que vem ganhando popularidade no contexto educacional . Ele utiliza figuras , gráficos e curtas explicações textuais associados à elementos de design , para transformar informações complexas em visualizações simples . No entanto , poucos trabalhos empíricos investigam a interferência do formato infográfico no processo de aprendizagem e consideram a influência dos estados afetivos ( em particular a satisfação ) e a preferência ( i.e . estilo de aprendizagem ) do aluno na sua capacidade de reter informação quando o material apresentado é o infográfico . Esse trabalho de mestrado tem como objetivo investigar os benefícios educacionais do uso de infográficos como material de aprendizagem comparando-os com materiais tradicionais , texto puro e gráfico+texto . Também visa analisar a influência de outras variáveis experimentais no processo de aprendizagem como complexidade do infográfico , satisfação e estilo de aprendizagem . Para isso , foi executado um experimento com 74 alunos de graduação distribuídos entre três formatos de materiais de aprendizagem ( i.e . infográficos , gráficos+texto e texto puro ) em uma sessão de aprendizagem individual suportada por computador . Os sujeitos foram avaliados quanto à aprendizagem imediata , retenção e perda de conhecimento , satisfação , estilos de aprendizagem e tempo . Os resultados obtidos sugerem que os infográficos são tão bons para a aprendizagem e retenção de conhecimento quanto os materiais tradicionais . Não foram encontradas evidências de que os estilos de aprendizagem visual ou verbal têm impacto na aprendizagem ou na satisfação dos alunos . Além_disso  , foram encontrados indícios de que satisfações positivas podem resultar em uma maior retenção de conhecimento . Para analisar a variável complexidade , foi proposto um framework para classificação dos infográficos . Pode-se concluir que , de fato , infográficos classificados como de baixa complexidade proporcionam maior aprendizagem . No entanto , o aprendizado por meio dos infográficos de complexidade alta não foi significativamente menor . É possível atribuir essa queda não significativa à natureza explicativa dos infográficos , capaz de estabilizar o aprendizado do aluno a partir de um determinado nível de complexidade , mesmo que a complexidade aumente 
 O Sistema ALIS ( Atmospheric Limb Imaging System ) , do Instituto Nacional de Pesquisas Espaciais ( INPE ) em São José dos Campos , a bordo do satélite EQUARS ( Equatorial Atmosphere Research Satellite ) , tem como objetivo observar três fenômenos ópticos da atmosfera na direção horizontal ( Limb ) do satélite em órbita : a aeroluminescência , 
 relâmpagos e os sprites 
 Os sprites são fenômenos luminosos verticais que ocorrem na mesosfera e ionosfera inferior de coloração vermelho-alaranjada e o seu estudo é 
 grande_importância  devido à influência que exerce sobre o clima terrestre , juntamente com demais fenômenos 
 O objetivo deste projeto é a criação de um software capaz de identificar relâmpagos e sprites capturados por meio de um CCD e , através desta identificação , decidir quais imagens devem ser guardadas para posterior envio e quais devem ser descartadas 
 Estudos preliminares demonstram que por meio de técnicas de segmentação de imagem utilizando-se contorno ativo ( que poderiam ser empregados em uma posterior análise de formas ) e compactação por meio de wavelets é possível reconhecer , localizar e guardar aqueles fenômenos que forem de interesse 
 O presente texto descreve métodos e apresenta resultados do projeto de pesquisa de mestrado intitulado `` Dois Problemas em Análise de Formas de Estruturas de Ramificação '' . Ambos os problemas abordados estão relacionados às sub-áreas da Análise de Formas denominadas Caracterização e Descrição de Formas . O primeiro problema consiste na investigação de um conjunto de características propostas para distingüir , primeiramente , entre estruturas de ramificação de vasos sangüíneos em imagens de retina segmentadas manualmente e automaticamente . A seguir , as mesmas características são aplicadas para discernir entre estruturas de ramificação de vasos sangüíneos em imagens de retina com e sem retinopatia diabética proliferativa ( Proliferative Diabetic Retinopathy - PDR ) . A PDR é uma das patologias associadas à diabetes , que pode culminar na cegueira do indivíduo . Diagnósticos são possíveis por meio de imagens de fundo de olho e , quando efetuados precocemente , viabilizam intervenções oportunas evitando a perda da visão . Neste trabalho , 27 imagens_digitais  de fundo de olho foram segmentadas por dois processos distintos , isto é , segmentação manual por um especialista e a segmentação automática , mediante a transformada contínua Wavelet - CWT e classificadores estatísticos . Visando à caracterização destas formas , um conjunto de 08 características foi proposto . Este conjunto foi formado por três grupos , a saber : descritores tradicionais geométricos ( Área , Perímetro e Circularidade ) , descritores associados à transformada wavelet ( 2o momento estatístico da distribuição de módulos da CWT , Entropia de Orientação da distribuição de fases da CWT e Curvatura ) e um descritor fractal ( Dimensão de Correlação - Global e Mediana ) . Uma Análise Discriminante Linear LDA revelou que as características geométricas tradicionais não detectam o início da retinopatia diabética proliferativa . A maior capacidade discriminante individual foi exibida pela Curvatura , com Área sob a curva ROC de 0.76 . Um subconjunto com 6 características apresentou grande capacidade discriminante com Área sob a curva ROC de 0.90 . O segundo problema diz_respeito  à extração de contorno de estruturas de ramificação bidimensionais de neurônios tridimensionais . Este trabalho contribui originalmente com uma solução para este problema , propondo dois algoritmos desenvolvidos para Rastreamento de Ramos e Extração do Contorno Paramétrico de estruturas de ramificação , capazes de transpor regiões críticas formadas por cruzamentos ocasionados pela projeção de estruturas 3D no plano das imagens 2D . Grande parte dos métodos baseados em contorno para análise de formas de estruturas de ramificação de células neuronais não produz representações corretas destas formas , devido à presença de sobreposições entre processos neuronais , levando os algoritmos tradicionais de extração de contorno a ignorar as regiões mais internas destas estruturas , gerando representações incompletas . O sistema proposto neste trabalho foi desenvolvido objetivando a solução do problema de extração de contorno , mesmo na presença de múltiplas sobreposições . Inicialmente , a imagem de entrada é pré-processada , gerando um esqueleto 8-conexo com ramos de um pixel de largura , um conjunto de sementes de sub-árvores dendríticas e um conjunto de regiões críticas ( bifurcações e cruzamentos ) . Para cada sub-árvore , o algoritmo de rastreamento rotula todos os pixels válidos de um ramo , até chegar em uma região crítica , onde o algoritmo decide a direção em que deve continuar o rastreamento . Nosso algoritmo mostrou-se robusto , mesmo quando aplicado a imagens com segmentos paralelos muito próximos . Resultados obtidos com imagens reais ( neurônios ) são apresentados 
 A reutilização de software busca aumentar a qualidade e produtividade no desenvolvimento de software , evitando a duplicação do esforço e reaproveitando o máximo possível das experiências de projetos passados . Apesar de simples , esta idéia não é facilmente colocada em prática , principalmente de maneira sistemática e controlada . Técnicas de engenharia de domínio e linhas de produtos de software buscam facilitar esta tarefa , porém ainda existem outros fatores que dificultam a adoção da prática da reutilização . Entre estes , destacam-se os problemas inerentes ao desenvolvimento de software da maneira como é conduzido atualmente , baseado em código-fonte . Estes problemas têm suas origens na crescente demanda por software cada vez mais complexo e afetam negativamente a capacidade de reutilizar software . O desenvolvimento orientado a modelos surge como uma alternativa atraente neste cenário , elevando a importância de modelos dentro do ciclo de vida do software , incorporando-os como parte integrante do produto_final  por meio de técnicas de modelagem e geração de código . Com isto , parte da complexidade do software fica escondida dentro dos geradores , protegendo os desenvolvedores , reduzindo a incidência de erros , aumentando a produtividade , qualidade , interoperabilidade e manutenibilidade dos artefatos produzidos . Nesta dissertação defende-se a tese de que o desenvolvimento orientado a modelos pode efetivamente aumentar e/ou melhorar a reutilização de software , e que para isso ela deve ser tratada de forma consistente dentro de um processo de engenharia de domínio . Para demonstrar esta tese , é apresentada uma abordagem orientada a modelos para reutilização de software , com atividades que guiam o desenvolvedor durante a análise , projeto e implementação do domínio . São também apresentados os resultados de uma avaliação envolvendo três estudos empíricos , realizados em ambiente acadêmico e industrial , que buscou determinar a viabilidade da abordagem e os benefícios que podem ser alcançados com a combinação de técnicas do desenvolvimento orientado a modelos e da reutilização de software . Os resultados mostram que a abordagem pode trazer diferentes benefícios para organizações de software , incluindo aumento da quantidade e qualidade da reutilização , e reduzindo a complexidade de desenvolvimento e configuração de 
 A vasta quantidade de informações disponível sobre qualquer área de conhecimento torna cada vez mais difícil selecionar e analisar informações específicas e relevantes sobre determinado assunto . Com isso , faz-se necessário o aprimoramento de técnicas automáticas para recuperação , análise e extração de conhecimento em conjuntos de dados , destacando-se dessa forma as pesquisas em Aprendizado de Máquina e em Mineração de Dados . Em aprendizado de máquina e em mineração , a grande maioria das técnicas utiliza-se de uma representação proposicional dos dados , que considera apenas caracter características individuais dos objetos descritos em uma tabela atributo-valor . Porém , existem aplicações nas quais além da descrição dos objetos também estão disponíveis informações sobre relações existentes entre eles . Esses domínios podem ser representados via grafos , nos quais vértices representam objetos e arestas relações entre objetos , possibilitando a aplicação de técnicas relacionais aos dados . Conceitos de Redes Complexas ( RC ) podem ser utilizados neste contexto . RC é um campo de pesquisa recente e ativo , que estuda o comportamento de diversos sistemas reais , modelados via grafos . Entretanto , ainda há poucos trabalhos que utilizam Redes Complexas em aprendizado de máquina ou mineração de dados . Este projeto apresenta uma proposta de utilização do formalismo de redes complexas e grafos para descoberta de padrões no contexto de aprendizado_supervisionado  . O formalismo de grafos permite representar as relações entre objetos e características particulares do domínio , permitindo agregar informações estruturais das relações à descoberta de conhecimento . Especificamente , neste trabalho desenvolve-se uma representação relacional baseada em grafos construídos a partir de relações de similaridade entre objetos . Baseado nesta representação são propostas abordagens de classificação relacional . Também é proposto um modelo de rede denominado K-Associados . Propriedades da rede K-Associados foram investigadas . Os resultados experimentais demonstram um grande potencial para classificação utilizando os algoritmos de classificação e de formação de redes 
 No Brasil , cerca de 68 % da população é classificada como leitores com baixos níveis de alfabetização , isto é , possuem o nível de alfabetização rudimentar ( 21 % ) ou básico ( 47 % ) , segundo dados do INAF ( 2009 ) . O projeto PorSimples utilizou as duas_abordagens  de Adaptação Textual , a Simplificação e a Elaboração , para ajudar leitores com baixo_nível  de alfabetização a compreender documentos disponíveis na Web em português do Brasil , principalmente textos jornalísticos . Esta pesquisa de mestrado também se dedicou às duas_abordagens  acima , mas o foco foi o gênero de textos instrucionais . Em tarefas que exigem o uso de documentação técnica , a qualidade da documentação é um ponto crítico , pois caso a documentação seja imprecisa , incompleta ou muito complexa , o custo da tarefa ou até mesmo o risco de acidentes aumenta muito . Manuais de instrução possuem duas relações procedimentais básicas : a relação gera generation ( quando uma ação gera automaticamente uma ação ) , e a relação habilita enablement ( quando a realização de uma ação permite a realização da ação , mas o agente precisa fazer algo a mais para garantir que irá ocorrer ) . O projeto aqui descrito , intitulado NorMan , estudou como as relações procedimentais gera e habilita são realizadas em manuais de instruções , dando base para a criação do sistema NorMan Extractor , que implementa um método de extração de termos dedicado ao gênero de textos instrucionais , especificamente aos manuais técnicos . Também foi proposta a adaptação do sistema de autoria de textos simplificados criado no projeto PorSimples o SIMPLIFICA para atender o gênero de textos instrucional . O SIMPLIFICA adaptado usa a lista de candidatos a termo , gerada pelo sistema NorMan Extractor , com duas funções : ( a ) para auxiliar na identificação de palavras que não devem ser simplificadas pelo método de simplificação léxica baseado em sinônimos , e ( b ) para gerar uma elaboração léxica para facilitar o entendimento do 
 Aprendizado em fluxo de dados é uma área de pesquisa importante e que vem crescendo nos últimos tempos . Em muitas aplicações reais os dados são gerados em uma sequência temporal potencialmente infinita . O processamento em fluxo possui como principal característica a necessidade por respostas que atendam restrições severas de tempo e memória . Por exemplo , um classificador aplicado a um fluxo de dados deve prover uma resposta a um determinado evento antes que o próximo evento ocorra . Caso isso não ocorra , alguns eventos do fluxo podem ficar sem classificação . Muitos fluxos geram eventos em uma taxa de chegada com grande variabilidade , ou seja , o intervalo de tempo de ocorrência entre dois eventos sucessivos pode variar muito . Para que um sistema de aprendizado obtenha sucesso na aquisição de conhecimento é preciso que ele apresente duas características principais : ( i ) ser capaz de prover uma classificação para um novo exemplo em tempo hábil e ( ii ) ser capaz de adaptar o modelo de classificação de maneira a tratar mudanças de conceito , uma vez que os dados podem não apresentar uma distribuição estacionária . Algoritmos de aprendizado de máquina em lote não possuem essas propriedades , pois assumem que as distribuições são estacionárias e não estão preparados para atender restrições de memória e processamento . Para atender essas necessidades , esses algoritmos devem ser adaptados ao contexto de fluxo de dados . Uma possível adaptação é tornar o algoritmo de classificação anytime . Algoritmos anytime são capazes de serem interrompidos e prover uma resposta ( classificação ) aproximada a qualquer instante . Outra adaptação é tornar o algoritmo incremental , de maneira que seu modelo possa ser atualizado para novos exemplos do fluxo de dados . Neste trabalho é realizada a investigação de dois métodos capazes de realizar o aprendizado em um fluxo de dados . O primeiro é baseado no algoritmo k-vizinhos mais próximo anytime estado-da-arte , onde foi proposto um novo método de desempate para ser utilizado neste algoritmo . Os experimentos mostraram uma melhora consistente no desempenho deste algoritmo em várias bases de dados de benchmark . O segundo método proposto possui as características dos algoritmos anytime e é capaz de tratar a mudança de conceito nos dados . Este método foi chamado de Algoritmo Anytime Incremental e possui duas versões , uma baseado no algoritmo Space Saving e outra em uma Janela Deslizante . Os experimentos mostraram que em cada fluxo cada versão deste método proposto possui suas vantagens e desvantagens . Mas no geral , comparado com outros métodos baselines , ambas as versões apresentaram melhor desempenho 
 Como a quantidade e a complexidade de informações disponíveis sobre incidentes de segurança é crescente , as tarefas de manipular e gerenciar essas informações tornaram-se bastante custosas . Diversas ferramentas de gerenciamento de segurança estão disponíveis para auxiliar os administradores . Essas ferramentas podem monitorar tudo que entra e saí de uma intranet , como os firewalls ; podem monitorar o tráfego interno da rede para saber o que está acontecendo e detectar possíveis ataques , como os sistemas de detecção de intrusão ( SDIs ) ; podem varrer arquivos em busca de códigos maliciosos , como os antivírus ; podem criar filtros de emails para evitar spams , vírus ou worms ; ou podem varrer uma rede em busca de vulnerabilidades nos sistemas , como os scanners e os agentes_móveis  inteligentes . Essas ferramentas geram uma grande quantidade de logs com informações que são coletadas e armazenadas em formatos próprios e diferentes . Essa falta de um formato único para armazenar as informações de incidentes de segurança , faz com que o trabalho dos administradores fique ainda mais difí ? cil , pois eles/elas devem ser capazes de entender todos esses formatos para identificar e correlacionar informações quando , por exemplo , há um ataque ou uma invasãoo em andamento . Esta tese descreve o projeto e o desenvolvimento de ontologias para representar em uma estrutura padronizada informações sobre incidentes de segurança . A ontologia desenvolvida é denominada OntoSec - Security Incident Ontology . Este trabalho cobre : ( i ) como utilizar ontologias para compartilhar e reusar informações sobre incidentes ; ( ii ) como correlacionar incidentes por meio de ontologias ; ( iii ) como facilitar a interoperabilidade entre diferentes ferramentas de segurança ; ( iv ) a modelagem de um sistema de gerenciamento de incidentes com base na ontologia ; e ( v ) o processo de avaliação da ontologia desenvolvida . Além_disso  , a OntoSec pretende apoiar as decisões gerenciais realizadas pelos administradores quando problemas de segurança acontecem , possibilitando que essas decisões sejam tomadas de maneira mais eficiente e 
 O objetivo deste trabalho é desenvolver um método_numérico  capaz de simular_escoamentos  viscoelásticos com superfícies_livres  governados pela equação_constitutiva  não-linear PTT ( Phan-Thien-Tanner ) . Neste trabalho foram apresentados três métodos_numéricos  para simular_escoamentos  viscoelásticos modelados pela equação PTT . Dois desses métodos foram desenvolvidos para simular_escoamentos  viscoelásticos bidimensionais enquanto o terceiro método foi desenvolvido para simular_escoamentos  viscoelásticos tridimensionais . Estes métodos_numéricos  foram incorporados aos ambientes de simulação FreeDow2D e FreeDow3D , extendendo estes ambientes para escoamentos viscoelásticos descritos por uma equação_constitutiva  não-linear . Inicialmente , uma descrição de FreeDow2D e FreeDow3D é apresentada . As equações governantes para escoamentos descritos pelo modelo PTT são dadas na forma de tensorial e as formulações_matemáticas  para obtenção dos métodos_numéricos  são apresentadas . As equações que descrevem os métodos_numéricos  são resolvidas pela técnica de diferenças_finitas  numa malha_deslocada  e o fluido é representado por partículas arcadoras usando o método Marker-and-Cell . As condições de contorno para cada tipo de contorno são descritas em detalhes e o cálculo do tensor extra-tensão no contorno rígido é obtido utilizando as idéias de Tomé et_al  . [ 84 ] para o modelo Oldroyd-B . Seguindo a metodologia de Alves et_al  . [ 2 ] , a solução analítica do modelo PTT para escoamentos totalmente desenvolvidos em um canal bidimensional é apresentada em detalhes . Esta solução analítica é então usada para validar o método_numérico  desenvolvido neste trabalho . Os métodos_numéricos  desenvolvidos nesse trabalho foram aplicados para simular os seguintes problemas : um jato de fluido viscoelástico incidindo numa placa rígida ; o inchamento do extrudado e o problema de uma gota esférica de fluido viscoelástico incidindo perpendicularmente contra uma superfície rígida 
 A teoria de Compressive Sensing proporciona uma nova estratégia de aquisição e recuperação de dados com bons_resultados  na área de processamento de imagens . Esta teoria garante recuperar um sinal com alta probabilidade a partir de uma taxa reduzida de amostragem por debaixo do limite de Nyquist-Shanon . O problema de recuperar o sinal original a partir das amostras consiste em resolver um problema de otimização . O método de Gradiente Espectral Projetado é um método para minimizar funções suaves em conjuntos convexos que tem sido aplicado com frequência ao problema de recuperar o sinal original a partir do sinal amostrado . Este trabalho dedica-se ao estudo da aplicação do Método do Gradiente Espectral Projetado ao problema de Compressive Sensing 
 Em sistemas de transporte inteligentes , as redes veiculares têm um papel fundamental . Por meio da comunicação sem fio , veículos irão disseminar conteúdo nessas redes para melhorar a segurança e eficiência no transporte , prover aplicações de entretenimento etc . Beaconing , proposto originalmente para aplicações de segurança , é usado neste estudo como uma das maneiras de disseminação de conteúdo , onde o nó emissor insere uma informação em um quadro de beacon , que é propagado em broadcast . A maioria dos estudos da literatura focam na otimização de desempenho de beaconing e utilizam o método de simulação para validação e avaliação . Esta tese explora estratégias de beaconing com validação e avaliação usando método experimental em ambientes reais para resolver problemas relacionados a aplicações cooperativas de localização e posicionamento de veículos . Para tanto , foi implantado um testbed veicular para realização de testes tradicionais , como o de desempenho sobre os parâmetros de comunicação , mas principalmente de novos protocolos que transmitem informações adicionais nos beacons . Os principais resultados são : i ) uma aplicação para inferência da distância entre os veículos por meio do sinal recebido de rádio frequência , ii ) localização de pontos de interesse para motoristas e passageiros e por fim , iii ) verificação da localização do veículo e disseminação de beacons anonimamente . Desta maneira , este estudo demonstrou , por meio de experimentos em ambientes reais , que estratégias de beacons podem ser aplicadas com sucesso para aplicações que usam cooperação para localização e posicionamento em redes 
 Como lidar com o excesso de informação ao qual usuários são submetidos em suas buscas na Web ? São muitas as páginas sobre um mesmo assunto , por isso uma solução pode ser separá-las segundo os objetivos dos escritores . Melhor ainda seria separá-las segundo os objetivos dos leitores , tão diversos como buscar um programa , aprender sobre uma matéria ou saber as últimas notícias sobre um dado assunto . Esse é o objetivo desta tese . Ir além do conteúdo dos textos para minimizar o esforço do usuário em encontrar os documentos que são relevantes para sua consulta em um dado instante de busca . Investigou-se pela primeira vez a hipótese de que é tecnicamente possível e de fácil compreensão a classificação resultados de busca segundo os seus objetivos . Para isso estudou-se a classificação automática dos resultados de buscas na Web em português segundo a intenção da busca . Foram aplicados algoritmos de aprendizado de máquina sobre características linguísticas relacionadas com o estilo de documentos em português , e desenvolvidos estudos com usuários para avaliar na prática os classificadores criados . Foi também investigada a possibilidade de desenvolver classificadores personalizados que.dentro de um determinado assunto , separassem páginas interessantes de outras irrelevantes , com base em pequenos corpora de treinamento . Para a avaliação , foram utilizadas tanto as avaliações de sistema como as centradas no usuário . Os resultados mostram que ( i ) a classificação em necessidades é um conceito compreendido pelos usuários , ( li ) o uso de marcadores estilísticos é um caminho barato e eficiente a ser investigado para obter classificadores confiáveis , ( iii ) o treinamento com pequenos corpora da Web é capaz de gerar classificadores confiáveis , e ( iv ) a busca pode ser facilitada por resultados classificados segundo necessidades de busca 
 Uma importante etapa do desenvolvimento de software é o de levantamento e análise dos requisitos . Porém , durante esta etapa podem ocorrer inconsistências que prejudicarão o andamento do projeto . Além_disso  , após finalizada a especificação , o cliente pode querer acrescentar ou modificar as funcionalidades do sistema . Tudo isso requer que a especificação do software seja revista , mas isso é altamente custoso , tornando necessário um processo automatizado para simplificar tal revisão . Para lidar com este problema , uma das abordagens utilizadas tem sido o processo de Revisão de Crenças , juntamente com o processo de Verificação de Modelos . O objetivo deste trabalho é utilizar o processo de revisão de crenças e verificação de modelos para avaliar especificações de um projeto procurando inconsistências , utilizando o fragmento universal da Computation Tree Logic ( CTL ) , conhecido como ACTL , e revisá-las gerando sugestões de mudanças na especificação . A nossa proposta é traduzir para lógica_clássica  tanto o modelo ( especificação do software ) quanto a propriedade a ser revisada , e então aplicar um resolvedor SAT para verificar a satisfazibilidade da fórmula gerada . A partir da resposta do resolvedor SAT , iremos gerar sugestões válidas de mudanças para a especificação , fazendo o processo de tradução reversa da lógica_clássica  para o modelo original 
 Especificações formais são úteis para descrever o que um sistema deve fazer sem definir como , e , em virtude da sua natureza formal e da possibilidade de abstração , é possível analisá-las sistematicamente . No entanto , o uso de especificações formais como parte do desenvolvimento de software não constitui prática comum . Isso se dá , em parte , pelo fato de existirem apenas um pequeno número de metodologias e ferramentas adequadas que dêem suporte a esse desenvolvimento . O primeiro objetivo deste trabalho é propor uma metodologia de desenvolvimento que possibilite , a partir de uma especificação formal em notação Z , produzir uma implementação dessa especificação em Java . Essa metodologia centra-se na geração do esqueleto da aplicação Java e na instrumentação desse esqueleto com mecanismos de verificação de condições ( invariantes , pré e pós-condições ) e rastreamento de violações dessas condições . Através desses mecanismos , possibilita-se intercalar desenvolvimento formal e informal no processo global de desenvolvimento de software . O segundo objetivo é desenvolver uma ferramenta que implemente parte dessa metodologia , produzindo uma implementação parcial que deverá ser complementada pelo usuário 
 O objetivo principal do presente_trabalho  é apresentar uma proposta que permita a combinação entre uma solução de captura de pacotes já existente e não muito flexível ( sniffer ) e o conceito de agentes_móveis  para aplicação em redes segmentadas . Essa pesquisa possui como foco_principal  a aplicação da técnica captura de pacotes em SDIs network based , utilizando para isso o modelo desenvolvido no ICMC ( Cansian , 1997 ) e posteriormente adequado ao ambiente de agentes_móveis  ( Bernardes , 1999 ) . Assim sendo , foi especificada a camada base do ambiente desenvolvido em ( Bernardes , 1999 ) visando as interações entre seus agentes e o agente de captura de pacotes 
 Os grandes centros médicos e hospitais de todo o mundo têm procurado integrar as informações de seus pacientes incluindo os exames de imagens efetuados ( tomografia computadorizada , tomografia por ressonância_magnética  , ultrasson , medicina nuclear , etc. ) . Um sistema que integra as imagens junto às informações tradicionais é chamado de Sistema de Arquivamento e Comunicação de Imagens ( Picture Archive and Communication System - PACS ) . Os sistemas PACS comerciais associam as imagens de exames às informações de pacientes através de chaves de consultas textuais e numéricas , não suportando consultas baseadas no conteúdo pictórico das imagens . Entretanto , muitas_vezes  o médico gostaria de recuperar as imagens armazenadas que fossem semelhantes ( similares ) a uma determinada imagem de consulta . Por exemplo , seja a consulta : `` encontre as 10 imagens mais semelhantes à imagem Raio-X-tórax do Jõao da Silva '' . Ao responder a consultas desse tipo , o sistema permite que o médico relembre casos ocorridos anteriormente . Além_disso  , o conhecimento já gerado de exames e tratamentos anteriores pode ser recuperado mais rapidamente do que utilizando apenas a memória humana ou um sistema não automático de recuperação de informações . Um sistema com a capacidade de recuperar imagens utilizando o seu conteúdo pictórico é uma ferramenta valiosa para o auxílio ao diagnóstico médico 
 Esta tese apresenta a arquitetura de um PACS atualmente em desenvolvimento . Este sistema está sendo denominado mini-PACS . Tal sistema necessita da integração de três sistemas , a saber 
 - Um Sistema de Processamento de Imagens ( SPI ) , o qual é responsável pela leitura e pré-processamento das imagens que são recebidas de diferentes dispositivos e possuem diferentes formatos . O SPI extrai as características relevantes das imagens , as quais serão utilizadas para a sua indexação e recuperação por conteúdo 
 - Um Sistema de Gerenciamento de Bases de Dados e Imagens ( SGBDI ) , que permite a armazenagem e a recuperação de imagens baseada em seu conteúdo . O SGBDI utiliza uma estrutura métrica , a Slim-tree , que indexa as imagens através das características_extraídas  pelo SPI e possibilita responder consultas por similaridade 
 - Um Servidor de Web ( SW ) , que disponibiliza o acesso às informações através da internet . A construção do Servidor de Web encontra-se fora do escopo do desenvolvimento desta tese . Porém , testes iniciais sobre a transferência e comunicação de imagens utilizando um servidor e aplicativos Java foram desenvolvidos para avaliar o comportamento do sistema 
 Entre as principais_contribuições  deste trabalho encontra-se um novo método de extração de características de imagens chamado histograma métrico . Os histogramas métricos permitem comparar imagens de diferentes tamanhos e mapeadas em faixas de quantização diferentes ( se a alteração de brilho for linear ) . O tempo de resposta às consultas por similaridade utilizando histogramas métricos é , em média , 5 vezes menor do que o tempo de resposta utilizando histogramas tradicionais . Para permitir a indexação das imagens utilizando a Slim-tree foi necessário desenvolver uma nova função de distância métrica . Tal função de distância utiliza a diferença entre as áreas das curvas do histograma métrico . A construção da árvore de indexação utilizando os histogramas métricos chega a ser 10 vezes mais rápida do que com os histogramas tradicionais 
 As inovações e aperfeiçoamentos oriundos deste trabalho estão sendo integrados ao mini-PACS . Este sistema vem_sendo  desenvolvido de forma conjunta entre o Grupo de Bases de Dados e Imagens ( GDBI ) do Instituto de Ciências_Matemáticas  e de Computação da USP e o Centro de Ciências de Imagens e Física Médica ( CCIFM ) da Faculdade de Medicina de Ribeirão Preto da USP 
 Esta tese descreve um modelo que permitirá ao conselho administrativo a incorporação da Governança da Segurança da Informação como parte do seu processo de Governança Organizacional . A partir da utilização desse modelo , pretende-se que o conhecimento sobre os riscos relacionados à infra-estrutura de Tecnologia da Informação e Comunicação ( TIC ) seja apresentado de forma objetiva ao conselho administrativo ao longo do planejamento estratégico . O modelo apresentando foi elaborado tendo como base a estrutura de tomada de decisão utilizada pelos Sistemas de Informações Gerenciais . A seguir , foi proposta uma nova dimensão para contemplar : controles ( o que ) , processos ( como ) , pessoas ( quem ) e tecnologia ( ferramentas automatizadas ) Uma revisão de literatura sobre Governança de TIC foi realizada para a identificação de modelos que pudessem oferecer alguma contribuição ao trabalho . O modelos COBIT e a norma ISO foram utilizadas para definição dos objetivos de controle a serem implementados em cada nível e o modelo ITIL foi utilizado para definir os processos responsáveis pela implementação . Algumas adaptações foram propostas para alguns processos do modelo ITIL para que o mesmo pudesse contemplar todos os objetivos de controle presentes na norma ISO e no modelo COBIT . Por último , é apresentado um estudo de caso com a implementação do modelo proposto no Centro de Computação Eletrônica da Universidade de São Paulo 
 Este trabalho de mestrado está inserido no contexto de pesquisas sobre as características de desenvolvimento e aperfeiçoamento de uma aplicação para uso na Internet . A aplicação consiste de uma agenda eletrônica , chamada No Risk Planning , na forma de um groupware , que oferece um suporte ao trabalho colaborativo em grupo . Por ser uma aplicação desenvolvida com base na utilização da Internet , o objetivo foi o de observar quais as características de evolução neste tipo de software foram solicitadas e como um processo de desenvolvimento foi utilizado para atende-las . Técnicas de Engenharia de Web ( web engineering ) foram adotadas , mas devido à rapidez com que as solicitações deveriam ser atendidas , técnicas de Programação Extrema ( XP ) foram acopladas para que o processo se desenvolvesse satisfatoriamente . Assim , a evolução da agenda para grupos na Web foi realizada como um estudo de caso , no qual a atualização de documentação , e o acompanhamento das etapas de evolução e dos requisitos foram essenciais para que se tivesse um produto com boa aceitação para uso no meio acadêmico 
 Design Rationale ( DR ) consiste em um conjunto de informações relacionadas ao processo de desenvolvimento e de tomada de decisão de um projeto . Nos projetos , em especial , adquirir e disponibilizar tais informações são práticas importantes para a melhoria das atividades de desenvolvimento e consequentemente da qualidade do produto desenvolvido . Por meio da atividade de documentação , os artefatos produzidos durante o projeto constituem a base para que ligações possam ser inseridas e expressem as relações com o DR correspondente . Um documento XML se apresenta como mecanismo apropriado para essa atividade pois além das informações , é possível acrescentar significado a essas informações . No entanto , a utilização efetiva das informações nesse documento XML só é possível se elas forem encontradas e exploradas de maneira a auxiliar os membros das equipes de projeto na realização de suas atividades . A ferramenta DocRationale foi desenvolvida para permitir o armazenamento e recuperação de informações de projeto de software , e respectivo DR. No entanto , para a exploração do DR armazenado , somente a navegação simples foi prevista inicialmente . Assim , a busca por informações de DR torna-se bastante custosa . Neste trabalho é apresentado um mecanismo para busca de DR , com o propósito de melhorar a exploração dessas informações 
 Uma representação estruturada dos documentos em um formato apropriado para a obtenção automática de conhecimento , sem que haja perda de informações relevantes em relação ao formato originalmente não-estruturado , é um dos passos mais importantes da mineração de textos , pois a qualidade dos resultados obtidos com as abordagens automáticas para obtenção de conhecimento de textos estão fortemente relacionados à qualidade dos atributos utilizados para representar a coleção de documentos . O Modelo de Espaço de Vetores ( MEV ) é um modelo tradicional para obter uma representação estruturada dos documentos . Neste modelo , cada documento é representado por um vetor de pesos correspondentes aos atributos do texto . O modelo bag-of-words é a abordagem de MEV mais utilizada devido a sua simplicidade e aplicabilidade . Entretanto , o modelo bag-of-words não trata a dependência entre termos e possui alta dimensionalidade . Diversos modelos para representação dos documentos foram propostos na literatura visando capturar a informação de relação entre termos , destacando-se os modelos baseados em frases ou termos compostos , o Modelo de Espaço de Vetores Generalizado ( MEVG ) e suas extensões , modelos de tópicos não-probabilísticos , como o Latent Semantic Analysis ( LSA ) ou o Non-negative Matrix Factorization ( NMF ) , e modelos de tópicos probabilísticos , como o Latent Dirichlet Allocation ( LDA ) e suas extensões . A representação baseada em modelos de tópicos é uma das abordagens mais interessantes uma vez que elas fornece uma estrutura que descreve a coleção de documentos em uma forma que revela sua estrutura interna e as suas inter-relações . As abordagens de extração de tópicos também fornecem uma estratégia de redução da dimensionalidade visando a construção de novas dimensões que representam os principais tópicos ou assuntos identificados na coleção de documentos . Entretanto , a extração é eficiente de informações sobre as relações entre os termos para construção da representação de documentos ainda é um grande desafio de pesquisa . Os modelos para representação de documentos que exploram a correlação entre termos normalmente enfrentam um grande desafio para manter um bom equilíbrio entre ( i ) a quantidade de dimensões obtidas , ( ii ) o esforço computacional e ( iii ) a interpretabilidade das novas dimensões obtidas . Assim , é proposto neste trabalho o modelo para representação de documentos Latent Association Rule Cluster based Model ( LARCM ) . Este é um modelo de extração de tópicos não-probabilístico que explora o agrupamento de regras de associação para construir uma representação da coleção de documentos com dimensionalidade reduzida tal que as novas dimensões são extraídas a partir das informações sobre as relações entre os termos . No modelo proposto , as regras de associação são extraídas para cada documento para obter termos correlacionados que formam expressões multi-palavras . Essas relações entre os termos formam o contexto local da relação entre termos . Em seguida , aplica-se um processo de agrupamento em todas as regras de associação para formar o contexto geral das relações entre os termos , e cada grupo de regras de associação obtido formará um tópico , ou seja , uma dimensão da representação . Também é proposto neste trabalho uma metodologia de avaliação que permite selecionar modelos que maximizam tanto os resultados na tarefa de classificação de textos quanto os resultados de interpretabilidade dos tópicos obtidos . O modelo LARCM foi comparado com o modelo LDA tradicional e o modelo LDA utilizando uma representação que inclui termos compostos ( bag-of-related-words ) . Os resultados dos experimentos indicam que o modelo LARCM produz uma representação para os documentos que contribui significativamente para a melhora dos resultados na tarefa de classificação de textos , mantendo também uma boa interpretabilidade dos tópicos obtidos . O modelo LARCM também apresentou ótimo desempenho quando utilizado para extração de informação de contexto para aplicação em sistemas de recomendação sensíveis ao contexto 
 Algoritmos de aprendizado semi-supervisionado ativo podem se configurar como ferramentas úteis em cenários práticos em que os dados são numerosamente obtidos , mas atribuir seus respectivos rótulos de classe se configura como uma tarefa custosa/difícil . A literatura em aprendizado ativo destaca diversos algoritmos , este trabalho partiu do tradicional Hierarchical Sampling estabelecido para operar sobre hierarquias de grupos . As características de tal algoritmo o coloca à frente de outros métodos ativos , entretanto o mesmo ainda apresenta algumas dificuldades . A fim de aprimorá-lo e contornar suas principais dificuldades , incluindo sua sensibilidade na escolha particular de uma hierarquia de grupos como entrada , este trabalho propôs estratégias que possibilitaram melhorar o algoritmo na sua forma original e diante de variantes propostas na literatura . Os experimentos em diferentes bases de dados reais mostraram que o algoritmo proposto neste trabalho é capaz de superar e competir em qualidade dentro do cenário de classificação ativa com outros algoritmos ativos da literatura 
 O uso de abordagens de predição de mudanças conjuntas auxilia os desenvolvedores a encontrar artefatos que mudam conjuntamente em uma tarefa . No passado , pesquisadores utilizaram análise estrutural para construir modelos de predição . Mais recentemente , têm sido propostas abordagens que utilizam informações históricas e análise textual do código fonte . Apesar dos avanços obtidos , os desenvolvedores de software ainda não usam essas abordagens amplamente , presumidamente por conta do número de falsos positivos . A hipótese desta tese é que informações contextuais obtidas das tarefas , da comunicação dos desenvolvedores e das mudanças dos artefatos descrevem as circunstâncias e condições em que as mudanças conjuntas ocorrem e podem ser utilizadas para realizar a predição de mudanças conjuntas . O objetivo desta tese consiste em avaliar se o uso de informações contextuais melhora a predição de mudanças conjuntas entre dois arquivos em relação às regras de associação , que é uma estratégia frequentemente usada na literatura . Foram construídos modelos de predição específicos para cada par de arquivos , utilizando as informações contextuais em conjunto com o algoritmo de aprendizagem de máquina random forest . Os modelos de predição foram avaliados em 129 versões de 10 projetos de código_aberto  da Apache Software Foundation . Os resultados obtidos foram comparados com um modelo baseado em regras de associação . Além de avaliar o desempenho dos modelos de predição também foram investigadas a influência do modo de agrupamento dos dados para construção dos conjuntos de treinamento e teste e a relevância das informações contextuais . Os resultados indicam que os modelos baseados em informações contextuais predizem 88 % das mudanças corretamente , contra 19 % do modelo de regras de associação , indicando uma precisão 3 vezes maior . Os modelos criados com informações contextuais coletadas em cada versão do software apresentaram maior precisão que modelos construídos a partir de um conjunto arbitrário de tarefas . As informações contextuais mais relevantes foram : o número de linhas adicionadas ou modificadas , número de linhas removidas , code churn , que representa a soma das linhas adicionadas , modificadas e removidas durante um commit , número de palavras na descrição da tarefa , número de comentários e papel dos desenvolvedores na discussão , medido pelo valor do índice de intermediação ( betweenness ) da rede social de comunicação . Os desenvolvedores dos projetos foram consultados para avaliar a importância dos modelos de predição baseados em informações contextuais . Segundo esses desenvolvedores , os resultados obtidos ajudam desenvolvedores novatos no projeto , pois não têm conhecimento da arquitetura e normalmente não estão familiarizados com as mudanças dos artefatos durante a evolução do projeto . Modelos de predição baseados em informações contextuais a partir de mudanças de software são relativamente precisos e , consequentemente , podem ser usados para apoiar os desenvolvedores durante a realização de atividades de manutenção e evolução de 
 O problema de roteamento de dados em rede de computadores consiste em minimizar o tempo_médio  de atraso na transmissão de mensagens , escolhendo para elas um caminho ótimo , através dos arcos da rede . Em seu trabalho , Luvezute propôs um algoritmo primai de relaxamento para otimizar o problema de roteamento de dados . O algoritmo proposto por Luvezute resolve iterativamente o problema de multifluxo , decompondo-o da forma mais independente possível , em subproblemas de simples fluxo , sendo um subproblema para cada mensagem . Esta independência entre os cálculos permite que a resolução dos subproblemas seja simultânea , admitindo-se assim uma implementação em paralelo . Nesta dissertação apresentamos um algoritmo paralelo , do tipo Fase I para encontrar uma solução inicial factível para o problema de multifluxo . Este algoritmo permite resolver de maneira mais rápida os problemas de grande porte que é o nosso objetivo inicial . O algoritmo de Fase I aqui desenvolvido pode ser utilizado para problemas de Multifluxo em geral , isto é , problemas com função objetivo linear ou não linear . O algoritmo desenvolvido foi escrito em linguagem C e implementado numa rede de microcomputadores , usando o sistema_operacional  UNIX . Além dos testes computacionais , apresentamos uma análise da eficiência do algoritmo e do seu speedup 
 Este trabalho tem o objetivo de paralelizar a transformada de Hough , de modo a diminuir o tempo de execução . A transformada de Hough é um método que realiza o reconhecimento de curvas paramétricas em imagens , sendo sua mais conhecida utilização o reconhecimento de retas . Neste trabalho são consideradas algumas das estratégias de paralelização da transformada de Hough , encontradas na literatura . Estas estratégias são implementadas para as arquiteturas MIMD de memória distribuída e MIMD de memória_compartilhada  , utilizando as ferramentas de desenvolvimento de programas paralelos OpenMP , Pthreads e MPI . Deste modo , são analisados e comparados os algoritmos e estratégias de paralelismo da transformada de Hough sobre às arquiteturas MIMD compartilhada e MIMD distribuída . Através dos resultados e análises obtidos pelo trabalho , é possível , além de uma melhor compreensão da transformada de Hough paralela , definir as combinações entre estratégias , ferramentas e arquiteturas , que apresentam o menor tempo de execução 
 A estimativa de tamanho de software é fundamental para determinar as estimativas de esforço e de tempo de desenvolvimento necessários à construção de um software , e os aplicativos desenvolvidos para web não são uma exceção a essa premissa . Neste trabalho é apresentada uma simplificação da contagem detalhada de pontos de função promovida pelo IFPUG ( International Function Point Users Group ) utilizando como princípio as idéias de simplificação sugeridas pela NESMA ( Netherlands Software Metrics Association ) para estimar sistemas de informação administrativos , além da apresentação dos requisitos de uma ferramenta para apoiar o processo de contagem dos pontos de função e determinar a produtividade . Em um estudo empírico foram analisadas vinte aplicações web , e as estimativas obtidas_através  do método simplificado ficaram bastante próximas das realizadas utilizando-se o método detalhado do IFPUG . Baseado nesses resultados , foi possível estabelecer um método de estimativa de tamanho simplificado para aplicativos web , levando em consideração as características de desenvolvimento da empresa estudada 
 A Transformada de Distância ( TD ) é um operador geral que constitui a base de diversos algoritmos em visão_computacional  e geometria discreta , com grande poder de aplicação prática . No entanto , todos os diversos algoritmos ótimos para o cálculo da TD euclideana ( TDE ) exata surgiram apenas a partir da década de 1990 . Não estava claro quais são os melhores algoritmos de de TDE exata , nem mesmo se realmente são exatos . Além_disso  , a implementação de tais métodos não é trivial e muitas_vezes  difícil de ser realizada eficientemente a partir da descrição nos artigos . Neste trabalho , são comparados experimentalmente e teoricamente os principais algoritmos de TDE , visando-se obter conclusões mais sólidas das diferenças de desempenho e exatidão de cada um . Os algoritmos também são descritos de maneira unificada e inédita nesta dissertação . Tais realizações são essenciais não só na teoria , mas também para viabilizar a aplicação prática dos algoritmos rápidos de TDE 
 Neste trabalho desenvolvemos um algoritmo para compensarmos as medições do diâmetro de uma peça devido à variação de temperatura ambiente para sistemas de medição que medem por comparação . Para isto , aplicamos os modelos de regressão com erros nas variáveis com o objetivo de determinar os coeficientes de correção no diâmetro úteis no algoritmo . Numa primeira etapa , realizamos dois experimentos , um utilizando da estufa para simular uma situação natural de variação de temperatura e assim obtermos informações prévias do comportamento do diâmetro da peça em função da temperatura . Em um outro experimento onde a temperatura das peças foram obtidas em função da variação natural de temperatura do setor de realização do experimento . Numa segunda etapa , apresentamos o modelo de regressão linear sem intercepto com erros nas variáveis para situação onde conhecemos a variância associada ao erro cometido pelo termômetro utilizado para medir a temperatura das peças . Na terceira etapa , propomos duas_abordagens  para tratar o problema , abordagem_Bayesiana  e Clássica . Numa quarta etapa , comparamos as aproximações para as estimativas dos coeficientes do modelo de regressão com erros nas variáveis obtidos via algoritmo de Gibbs e EM 
 Esse trabalho consiste na implementação em hardware ( MA ) de uma Unidade de Processamento Reconfigurável ( RPU ) para Algoritmos Genéticos ( AGs ) , com o intuito de disponibilizar um módulo de AG não processado para projetistas de circuitos integrados . Para o desenvolvimento do projeto , foi utilizada codificação binária dos cromossomos , elitismo como método de seleção qual o melhor indivíduo não somente é preservado , mas também recombinado com todos os outros . Além_disso  , também foram utilizados os operadores normais de crossover e mutação . Para os testes foram , variados as taxas de crossover e mutação , além do tamanho da população . Os resultados foram obtidos em simulação e corresponderam as expectativas . Essa unidade ainda será implantada num robô para que ele possa evoluir a seu controlador de navegação . O robô aprenderá sozinho a navegar em um ambiente desconhecido , procurando explorar sem colidir com os obstáculos 
 Os Ensaios de Proficiência por comparação interlaboratorial têm sido um importante mecanismo para controlar a consistência dos laboratórios . Instituições do governo , como o INMETRO , têm utilizado tais mecanismos para monitorar a qualidade dos serviços prestados pelos laboratórios da Rede Brasileira de Laboratórios ( RBL ) e da Rede Brasileira de Calibração ( RBC ) . Atualmente , os métodos estatísticos utilizados para analisar os resultados dos Ensaios de Proficiência estão escritos em normas técnicas , como o ISO/IEC Guide 43-1 . Recentemente , Leão , Aoki e Silva ( 2002 ) propuseram um método de regressão para testar a competência dos laboratórios , utilizando a distribuição normal multivariada para modelar os dados e estabelecer testes estatísticos . Como em medições a presença de valores extremos é constante , vamos modelar os dados utilizando a distribuição t-student , para acomodar tais valores extremos . Neste modelo , estamos interessados em estimar o grau de liberdade e o parâmetro de tendência da medição do laboratório com respeito ao valor de referência , já que os laboratórios vão utilizar sistemas de medições similares para medir o mesmo item , com incertezas determinadas por um processo de calibração . Encontraremos os estimadores de máxima_verossimilhança  e de momentos para estes dois parâmetros e vamos desenvolver um teste para avaliarmos a consistência das medições dos laboratórios . No final , a título de aplicação , vamos analisar os dados obtidos pela REMESP ( Rede de Metrologia de São Paulo ) na área de eletricidade , onde vamos medir a tensão DC de um multímetro digital 
 Os geradores de aplicação são ferramentas que recebem uma especificação de software , validam essa especificação e geram artefatos automaticamente . Os geradores de aplicação podem trazer benefícios em termos de produtividade por gerarem automaticamente artefatos de baixo_nível  com base em especificações de nível mais alto . Um dos problemas dos geradores de aplicação é o seu alto custo de desenvolvimento . Os geradores de aplicação configuráveis são adaptados para fornecer apoio em domínios específicos , ou seja , são considerados meta-geradores utilizados para obter geradores de aplicação específicos . Este trabalho delineia um processo de desenvolvimento com geradores confi- guráveis , define a arquitetura e as características de um gerador configurável e apresenta a ferramenta Captor , que é um gerador de aplicação configurável desenvolvido para facilitar a construção de geradores específicos . Três estudos de caso nos quais a Captor é configurada para domínios de aplicação específi- cos são apresentados : persistência de dados , gestão de recursos de negócios e bóias 
 O problema da descontinuidade semântica tem sido um dos principais focos de pesquisa no desenvolvimento de sistemas de recuperação de imagens baseada em conteúdo ( CBIR ) . Neste contexto , as pesquisas mais promissoras focam principalmente na inferência de pesos de características contínuos e na seleção de características . Entretanto , os processos tradicionais de inferência de pesos contínuos são computacionalmente caros e a seleção de características equivale a uma ponderação binária . Visando tratar adequadamente o problema de lacuna semântica , este trabalho propõe dois métodos de transformação de espaço de características métricos baseados na inferência de funções de transformação por meio de algoritmo genético . O método WF infere funções de ponderação para ajustar a função de dissimilaridade e o método TF infere funções para transformação das características . Comparados às abordagens de inferência de pesos contínuos da literatura , ambos os métodos propostos proporcionam uma redução drástica do espaço de busca ao limitar a busca à escolha de um conjunto ordenado de funções de transformação . Análises visuais do espaço transformado e de gráficos de precisão vs. revocação confirmam que TF e WF superam a abordagem tradicional de ponderação de características . Adicionalmente , foi verificado que TF supera significativamente WF em termos de precisão dos resultados de consultas por similaridade por permitir transformação não lineares no espaço de característica , conforme constatado por análise visual 
 Nesta dissertacao estudamos problemas de empacotamento de arvores em grafos , com enfase no caso de grafos completos . Denotamos por Ti uma arvore de ordem i. Dizemos que existe um empacotamento de arvores T1 , . . . , Tn num grafo G se e possivel encontrar em G subgrafos H1 , . . . , Hn , dois a dois disjuntos nas arestas , tais que Hi e isomorfo a Ti . Em 1976 , A. Gyarfas e J. Lehel levantaram a seguinte questao , que conjecturaram ter uma resposta positiva : e possivel empaco- tar qualquer sequencia de arvores T1 , . . . , Tn no Kn ? Esta dissertacao tem como tema principal os estudos realizados por diversos pesquisadores na busca de uma resposta para esta pergunta , que permanece ainda em aberto . Tendo em vista a dificuldade para tratar esta questao , surge natural- mente a pergunta sobre a existencia de classes de arvores para as quais a resposta e afirmativa . Nessa linha , existem diversos resultados positivos , como por exemplo quando queremos empacotar estrelas e caminhos , ou estrelas e biestrelas . Por outro_lado  , em vez de restringir a classe das arvores , faz sentido restringir o tamanho da sequencia e reformular a pergunta . Por exemplo , dado s < n , e possivel empacotar qualquer sequencia de arvores T1 , . . . , Ts no Kn ? Em 1983 , Bollobas mostrou ? que a resposta e afirmativa se s < = n / sqrt ( 2 ) . Na primeira parte deste trabalho focamos nosso estudo em questoes desse tipo . Na segunda parte desta dissertacao investigamos algumas conjecturas que foram motivadas pela pergunta levantada por Gyarfas & Lehel . Por exemplo , Hobbs , Bourgeois e Kasiraj formularam a seguinte questao : para n par , e possivel empacotar qualquer sequencia de arvores T1 , . . . , Tn no grafo bipartido Kn/2 , n-1 ? Para essa pergunta apresentamos alguns resultados conhecidos analogos aos obtidos para a conjectura de Gyarfas & Lehel . Mais recentemente , Gerbner , Keszegh e Palmer estudaram a seguinte generalizacao da conjectura original : e possivel empacotar qualquer sequencia de arvores T1 , . . . , Tk num grafo k-cromatico ? Neste trabalho estudamos essas e outras questoes relacionadas e apresentamos os principais resultados que encontramos na literatura 
 Redes_neurais  caracterizadas por cadeias de osciladores acoplados são um dentre vários_tipos  de redes que possuem propriedades peculiares relacionadas com a sua estrutura topológica . A dinâmica que descreve o comportamento dessas redes é modelada por sistemas de equações_diferenciais  , nos quais cada neurônio ( nó ) é considerado como um oscilador . Estudos realizados em redes desse tipo , em tarefas de reconhecimento de padrões estáveis gerados aleatoriamente , têm apresentado resultados computacionais satisfatórios . Esta tese propôs um desenvolvimento teórico e computacional que forneceu um algoritmo , para o estudo do desempenho de redes neurais em forma de osciladores de Ciclo-Limite de Stuart-Landau , no reconhecimento de figuras fractais . Neste trabalho apresentaremos contextos reais em que podemos encontrar características deste tipo de redes e motivações . Em seguida , serão expostos conceitos de redes de Hopfield , reconhecimento de padrões , teorias dos fractais e dos osciladores de Ciclo-Limite de Stuart-Landau ; tais conceitos , por sua vez , serviram como ferramentas principais para o algoritmo construído que será explicado posteriormente . Antes de apresentá-lo , será exposta a maneira como a dinâmica desses osciladores pode se tornar caótica , por meio de simulações computacionais alterando numericamente variáveis intrínsecas , como tempos de disparos entre neurônios , ou quantidades destes no sistema . Estas descobertas serviram como confirmações para elaborar e compor do algoritmo , bem como orientaram as simulações de reconhecimento de figuras fractais . Por fim , será apresentada a conclusão dos resultados encontrados 
 Cada vez mais o rastreamento do olhar tem sido usado para interação humano-computador em diversos cenários , como forma de interação ( usualmente substituindo o mouse , principalmente para pessoas com deficiências físicas ) ou estudo dos padrões de atenção de uma pessoa ( em situações como fazendo compras no mercado , olhando uma página na internet ou dirigindo um carro ) . Ao mesmo tempo , dispositivos vestíveis tais quais pequenas telas montadas na cabeça e sensores para medir dados relativos à saúde e exercício físico realizado por um usuário , também têm avançado muito nos últimos_anos  , finalmente chegando a se tornarem acessíveis aos consumidores . Essa forma de tecnologia se caracteriza por dispositivos que o usuário usa junto de seu corpo , como uma peça de roupa ou acessório . O dispositivo e o usuário estão em constante interação e tais sistemas são feitos para melhorar a execução de uma ação pelo usuário ( por exemplo dando informações sobre a ação em questão ) ou facilitar a execução de várias tarefas concorrentemente . O uso de rastreadores de olhar em computação vestível permite uma nova forma de interação para tais dispositivos , possibilitando que o usuário interaja com eles enquanto usa as mãos para realizar outra ação . Em dispositivos vestíveis , o consumo de energia é um fator importante do sistema que afeta sua utilidade e deve ser considerado em seu design . Infelizmente , rastreadores oculares atuais ignoram seu consumo e focam-se principalmente em precisão e acurácia , seguindo a ideia de que trabalhar com imagens de alta_resolução  e frequência maior implica em melhor desempenho . Porém tratar mais quadros por segundo ou imagens com resolução maior demandam mais poder de processamento do computador , consequentemente aumentando o gasto energético . Um dispositivo que seja mais econômico tem vários benefícios , por exemplo menor geração de calor e maior vida útil de seus componentes eletrônicos . Contudo , o maior impacto é o aumento da duração da bateria para dispositivos vestíveis . Pode-se economizar energia diminuindo resolução e frequência da câmera usada , mas os efeitos desses parâmetros na precisão e acurácia da estimação do olhar não foram investigados até o presente . Neste trabalho propomos criar uma plataforma de testes , que permita a integração de alguns algoritmos de rastreamento de olhar disponíveis , tais como Starburst , ITU Gaze Tracker e Pupil , para estudar e comparar o impacto da variação de resolução e frequência na acurácia e precisão dos algoritmos . Por meio de um experimento com usuários analisamos o desempenho e consumo desses algoritmos sob diversos valores de resolução e frequência . Nossos resultados indicam que apenas a diminuição da resolução de 480 para 240 linhas ( mantendo a proporção da imagem ) já acarreta em ao menos 66 % de economia de energia em alguns rastreadores sem perda significativa de acurácia 
 Repositórios com volumes de dados cada vez maiores foram viabilizados pelo desenvolvimento tecnológico , criando importantes fontes de informação em diversas_áreas  da atividade humana . Esses repositórios freqüentemente incluem informação sobre o comportamento temporal e o posicionamento espacial dos itens neles representados , os quais são extremamente relevantes para a análise dos dados . O processo de descoberta de conhecimento a partir de grandes volumes de dados tem sido objeto de estudo em diversas disciplinas , dentre elas a Visualização de Informação , cujas técnicas podem apoiar diversas etapas desse processo . Esta tese versa sobre o uso da Visualização Exploratória em conjuntos de dados com atributos temporais e espaciais , empregando a estratégia de múltiplas visualizações coordenadas para apoiar o tratamento de dados em estágios iniciais de processos de descoberta de conhecimento . São propostas duas novas representações_visuais  temporais  denominadas Variação Temporal Uni-escala e Variação Temporal Multi-escala  para apoiar a análise exploratória de dados temporais . Adicionalmente , é proposto um modelo de arquitetura de software  AdaptaVis , que permite a integração dessas e outras representações_visuais  em uma plataforma de visualização de informação flexível , extensível e adaptável às necessidades de diferentes usuários , tarefas e domínios de aplicação  a plataforma InfoVis . Sessões de uso realizadas com dados e usuários reais dos domínios de Climatologia e Negócios permitiram validar empiricamente as representações_visuais  e o modelo . O modelo AdaptaVis e a plataforma InfoVis estabelecem bases para a continuidade de diversas pesquisas em Visualização de Informação , particularmente o estudo de aspectos_relacionados  ao uso coordenado de múltiplas visualizações , à modelagem do processo de coordenação , e à integração entre múltiplas técnicas visuais e analíticas 
 A representação de malhas por meio de estrutura de dados e operadores topológicos e um dos focos principais da modelagem geométrica , onde permite uma implementação robusta e eficiente de mecanismos de refinamento adaptativo , alinhamento de células e acesso as relações de incidência e adjacência entre os elementos da malha , o que é de grande_importância  na maioria das aplicações em mecânica dos fluidos . No caso de malhas não estruturadas , a não uniformidade da decomposição celular e melhor representada por uma estrategia mais sofisticada , que são as estruturas de dados topológicas . As estruturas de dados topológicas indexam elementos de uma malha representando relações de incidência e adjacência entre elementos , garantindo acesso rápido às informações . Um dos aspectos mais comuns aos problemas tratados pela mecânica dos fluidos computacional é a complexidade da geometria do domínio onde ocorre o escoamento . O uso de estruturas de dados para manipular malhas computacionais e de grande_importância  pois realiza de modo eficiente as consultas às informações da malha e centraliza todas as operações sobre a malha em um único módulo , possibilitando sua extensão e adaptação em diversas situações . Este trabalho visou explorar o acoplamento de uma estrutura de dados topológica , a Mate Face , em um módulo simulador existente , de modo a gerenciar todos os acessos à malha e dispor operações e iteradores para pesquisas complexas nas vizinhanças de cada elemento na malha . O módulo simulador resolve as equações governantes da mecânica dos fluidos através da técnica de volumes finitos . Foi utilizada uma formulação que atribui os valores das propriedades aos centroides dos volumes de controle , utiliza métodos de alta ordem , os esquemas ENO e WENO , que tem a finalidade de capturar com eficiência descontinuidades presentes em problemas governados por equações_diferenciais  parciais hiperbólicas . As equações de Euler em duas dimensões representam os escoamentos de interesse no presente_trabalho  . O acoplamento da estrutura de dados Mate Face ao simulador foi realizada através da criação de uma biblioteca desenvolvida que atua como uma interface de comunicação entre os dois módulos , a estrutura de dados e o simulador , que foram implementados em diferentes linguagens de programação . Deste modo , todas as funcionalidades existentes na Mate Face tornaram-se acessíveis ao simulador na forma de procedimentos . Um estudo sobre malhas dinâmicas foi realizado envolvendo o método das molas para movimentação de malhas simulando-se operações de arfagem . A idéia foi verificar a aplicabilidade deste método para auxiliar simulações de escoamentos não estacionarios . Uma outra vertente do trabalho foi estender a estrutura Mate Face de forma a representar elementos não suportados a priori , de modo a flexibilizar o seu uso em simulações de escoamentos baseados no método de volumes finitos espectrais . O método dos volumes espectrais e utilizado para se obter alta_resolução  espacial do domínio computacional , que também atribui valores das propriedades aos centroides dos volumes de controle , porém , os volumes de controle são particionados em volumes menores de variadas topologias . Assim , uma extensão da Mate Face foi desenvolvida para representar a nova malha para a aplicação do método , representando-se cada particionamento localmente em cada volume espectral . Para todas as etapas deste trabalho , realizaram-se experimentos que validaram a utilizaação da estrutura de dados Mate Face junto a métodos_numéricos  . Desta forma , a estrutura pode auxiliar as ferramentas de simulações de escoamentos de fluidos no gerenciamento e acesso à malha 
 A necessidade de jovens pesquisadores lerem correta e rapidamente uma grande quantidade de textos escritos em inglês , que é a língua franca da ciência , representa uma barreira considerável para eles . Dada essa necessidade , em 2001 , o programa de mestrado em Ciências da Computação e Matemática Computacional do ICMC-USP passou a avaliar a proficiência em inglês dos alunos quanto à habilidade de reconhecerem o gênero de textos científicos em inglês , com as convenções que lhe são características . O site do Exame de Proficiência em Inglês ( EPI ) disponibiliza Exames Modelos com correção automatizada para os alunos saberem antecipadamente como será o exame formal , também informatizado . Porém , a prática com o Exame Modelo fornece apenas um relatório com o escore do aluno , não oferecendo meios para ele relembrar seus conhecimentos , receber instrução ou ainda rever conceitos errôneos . Uma possível solução para essa lacuna é um ambiente computacional que propicie um contexto favorável à aprendizagem do inglês instrumental . Na perspectiva de favorecer tal aprendizado e auxiliar os alunos a estudarem para o EPI é que essa pesquisa propôs um Ambiente Computacional de Aprendizagem ( ACA ) para o inglês instrumental . O ACA desenvolvido , denominado Computer-Aided Learning of English for Academic Purposes ( CALEAP-Web ) , é fruto da integração de um Teste Adaptativo Informatizado ( TAI ) , denominado de Adaptive English Proficiency Test for the Web ( ADEPT ) , e um Ambiente Computacional de Tarefas ( ACT ) , denominado de Computer-Aided Task Environment for Scientific English ( CATESE ) . Um TAI consiste em itens ( questões ) selecionados de acordo com o nível de habilidade estimado do aluno , gerando um teste individualizado . Já o ACT fornece tarefas pré-determinadas para o aprendizado do aluno . Para o ADEPT foram realizados dois experimentos com alunos de mestrado em Ciências da Computação e Matemática Computacional do ICMC-USP e com o CALEAP-Web , como um todo , foram realizadas simulações para a verificação de sua operacionalidade . Embora o ACA tenha sido desenvolvido para apoiar os alunos do programa de mestrado do ICMC-USP , ele é suficientemente genérico para ser usado em outros programas de mestrado que avaliam o inglês instrumental e por jovens pesquisadores que desejam conhecer as convenções do gênero de textos científicos em inglês 
 A diversidade de espécies presentes no riquíssimo reino vegetal torna o processo de identificação de órgãos foliares uma tarefa muito complexa . A biodiversidade das espécies , associada aos modelos tradicionais de taxonomia , transforma essa tarefa em um verdadeiro desafio para os pesquisadores . Neste trabalho é apresentada uma nova abordagem para identificação de espécies vegetais baseada em características internas dos órgãos foliares . A coleta de informações é realizada através de técnicas de visão_computacional  e análise de imagens , através das quais são extraídas características relativas à complexidade ( dimensão_fractal  ) e biometria dos órgãos foliares . A eficiência da metodologia desenvolvida foi avaliada em casos reais de identificação de espécies , em que foram utilizados dois conjuntos de imagens : espécies da Mata Atlântica e do Cerrado brasileiro , e espécies de maracujás silvestres do gênero Passiflora . Para classificação das espécies foram utilizadas as técnicas de reconhecimento padrões de análise de agrupamentos e redes neurais_artificiais  
 Robôs móveis dependem de dados provenientes de sensores para ter uma representação do seu ambiente . Porém , os sensores geralmente fornecem informações incompletas , inconsistentes ou imprecisas . Técnicas de fusão de sensores têm sido empregadas com sucesso para aumentar a precisão de medidas obtidas com sensores . Este trabalho propõe e investiga o uso de técnicas de inteligência_artificial  para fusão de sensores com o objetivo de melhorar a precisão e acurácia de medidas de distância entre um robô e um objeto no seu ambiente de trabalho , obtidas com diferentes sensores . Vários algoritmos de aprendizado de máquina são investigados para fundir os dados dos sensores . O melhor modelo gerado com cada algoritmo é chamado de estimador . Neste trabalho , é mostrado que a utilização de estimadores pode melhorar significativamente a performance alcançada por cada sensor isoladamente . Mas os vários algoritmos de aprendizado de máquina empregados têm diferentes características , fazendo com que os estimadores tenham diferentes comportamentos em diferentes situações . Objetivando atingir um comportamento mais preciso e confiável , os estimadores são combinados em comitês . Os resultados obtidos sugerem que essa combinação pode melhorar a confiança e precisão das medidas de distâncias dos sensores individuais e estimadores usados para fusão de sensores 
 A maioria dos algoritmos de aprendizado de máquina simbólico utilizam regras de conhecimento if-then como linguagem de descrição para expressar o conhecimento aprendido . O objetivo desses algoritmos é encontrar um conjunto de regras de classificação que possam ser utilizadas na predição da classe de novos casos que não foram vistos a priori pelo algoritmo . Contudo , este tipo de algoritmo considera o problema da interação entre as regras , o qual consiste na avaliação da qualidade do conjunto de regras induzidas ( classificador ) como um todo , ao invés de avaliar a qualidade de cada regra de forma independente . Assim , como os classificadores têm por objetivo uma boa precisão nos casos não vistos , eles tendem a negligenciar outras propriedades desejáveis das regras de conhecimento , como a habilidade de causar surpresa ou trazer conhecimento novo ao especialista do domínio . Neste trabalho , estamos interessados em construir regras de conhecimento com propriedades específicas de forma isolada , i.e . sem considerar o problema da interação entre as regras . Para esse fim , propomos uma abordagem evolutiva na qual cada individuo da população do algoritmo representa uma única regra e as propriedades específicas são codificadas como medidas de qualidade da regra , as quais podem ser escolhidas pelo especialista do domínio para construir regras com as propriedades desejadas . O algoritmo_evolutivo  proposto utiliza uma rica estrutura para representar os indivíduos ( regras ) , a qual possibilita considerar uma grande variedade de operadores evolutivos . O algoritmo utiliza uma função de aptidão multi-objetivo baseada em ranking que considera de forma concomitante mais que uma medida de avaliação de regra , transformando-as numa função simples-objetivo . Como a avaliação_experimental  é fundamental neste tipo de trabalho , para avaliar nossa proposta foi implementada a Evolutionary Computing Learning Environment --_-  ECLE --_-  que é uma biblioteca de classes para executar e avaliar o algoritmo_evolutivo  sob diferentes cenários . Além_disso  , a ECLE foi implementada considerando futuras implementações de novos operadores evolutivos . A ECLE está integrada ao projeto DISCOVER , que é um projeto de pesquisa em desenvolvimento em nosso laboratório para a aquisição automática de conhecimento . Analises experimentais do algoritmo_evolutivo  para construir regras de conhecimento com propriedades específicas , o qual pode ser considerado uma forma de análise inteligente de dados , foram realizadas utilizando a ECLE . Os resultados mostram a adequabilidade da nossa 
 Diversos métodos têm sido desenvolvidos para a organização da crescente quantidade de documentos textuais . Esses métodos frequentemente fazem_uso  de algoritmos de agrupamento para organizar documentos que referem-se a um mesmo assunto em um mesmo grupo , supondo que conteúdos de documentos de um mesmo grupo são similares . Porém , existe a possibilidade de que documentos pertencentes a grupos distintos também apresentem características semelhantes . Considerando esta situação , há a necessidade de desenvolver métodos que possibilitem a organização flexível de documentos , ou seja , métodos que possibilitem que documentos sejam organizados em diferentes grupos com diferentes graus de compatibilidade . O agrupamento_fuzzy  de documentos textuais apresenta-se como uma técnica adequada para este tipo de organização , uma vez que algoritmos de agrupamento_fuzzy  consideram que um mesmo documento pode ser compatível com mais de um grupo . Embora tem-se desenvolvido algoritmos de agrupamento_fuzzy  que possibilitam a organização flexível de documentos , tal organização é avaliada em termos do desempenho do agrupamento de documentos . No entanto , considerando que grupos de documentos devem possuir descritores que identifiquem adequadamente os tópicos representados pelos mesmos , de maneira geral os descritores de grupos tem sido extraídos utilizando alguma heurística sobre um conjunto pequeno de documentos , realizando assim , uma avaliação simples sobre o significado dos grupos extraídos . No entanto , uma apropriada extração e avaliação de descritores de grupos é importante porque os mesmos são termos representantes da coleção que identificam os tópicos abordados nos documentos . Portanto , em aplicações em que o agrupamento_fuzzy  é utilizado para a organização flexível de documentos , uma descrição apropriada dos grupos obtidos é tão importante quanto um bom agrupamento , uma vez que , neste tipo de agrupamento , um mesmo descritor pode indicar o conteúdo de mais de um grupo . Essa necessidade motivou esta tese , cujo_objetivo  foi investigar e desenvolver métodos para a extração de descritores de grupos fuzzy para a organização flexível de documentos . Para cumprir esse objetivo desenvolveu se : i ) o método SoftO-FDCL ( Soft Organization - Fuzzy Description Comes Last ) , pelo qual descritores de grupos fuzzy at são extraídos após o processo de agrupamento_fuzzy  , visando identicar tópicos da organização flexível de documentos independentemente do algoritmo de agrupamento_fuzzy  utilizado ; ii ) o método SoftO-wFDCL ( Soft Organization - weighted Fuzzy Description Comes Last ) , pelo qual descritores de grupos fuzzy at também são extraídos após o processo de agrupamento_fuzzy  utilizando o grau de pertinência dos documentos em cada grupo , obtidos do agrupamento_fuzzy  , como fator de ponderação dos termos candidatos a descritores ; iii ) o método HSoftO-FDCL ( Hierarchical Soft Organization - Fuzzy Description Comes Last ) , pelo qual descritores de grupos fuzzy hierárquicos são extraídos após o processo de agrupamento hierárquico fuzzy , identificando tópicos da organização hierárquica flexível de documentos . Adicionalmente , apresenta-se nesta tese uma aplicação do método SoftO-FDCL no contexto do programa de educação médica continuada canadense , reforçando a utilidade e aplicabilidade da organização flexível de 
 Nesta tese é abordado o problema da mochila compartimentada e o problema de corte de estoque unidimensional acoplado ao problema dimensionamento de lotes . Para o problema da mochila compartimentada é apresentada a versão unidimensional e proposta a versão bidimensional , denominados como problema da mochila compartimentada unidimensional e problema da mochila compartimentada bidimensional , respectivamente . Para o problema de corte de estoque acoplado ao dimensionamento de lotes são apresentadas três variações : uma máquina para produzir um tipo de objeto ; uma máquina para produzir vários_tipos  de objetos ; múltiplas máquinas para produzir vários_tipos  de objetos . Algumas formulações_matemáticas  de programação inteira e inteira-mista , decomposições dos problemas em problema mestre e subproblemas e heurísticas baseadas no método geração de colunas são propostas para os problemas da mochila compartimenta e o problema acoplado . Em específico , para o problema acoplado são aplicadas decomposições Dantzig-Wolfe , que podem ser por período , por máquina ou por período e máquina . Além_disso  , uma heurística baseada em grafo E/OU é proposta para o problema da mochila compartimentada 
 O Objetivo deste trabalho foi implementar um resolvedor distribuído para o problema de satisfabilidade em lógica proposicional ( SAT ) que pudesse ser executado em uma grade de computadores . Foi analisada a influência que o número de máquinas utilizadas pela grade para resolver diversas instâncias do SAT exerce sobre o desempenho do resolvedor 
 O problema de balanceamento de linhas de produção e designação de trabalhadores ( ALWABP , do inglês Assembly Line Worker Assignment and Balancing Problem ) é uma extensão do problema simples de balanceamento de linhas na qual os tempos de execução de tarefas são dependentes dos trabalhadores . Este problema tem sua motivação prática oriunda de linhas de produção com trabalhadores deficientes . Neste trabalho , focamos em uma extensão para o problema que permite lidar com a possibilidade de layouts de linhas com estações em paralelo . Além_disso  , estudamos uma segunda variante do problema que permite que os trabalhadores colaborem uns com os outros em uma mesma estação . Apresentamos formulações_matemáticas  para ambas as versões do problema . Adicionalmente , a partir de adaptações de métodos existentes para o ALWABP serial , desenvolvemos heurísticas construtivas para o ALWABP paralelo e ALWABP colaborativo . Testes computacionais em instâncias da literatura e uma análise detalhada dos resultados são 
 O presente documento tem por objetivo apresentar o desenvolvimento de uma ferramenta computacional para auxiliar profissionais da área de otimização na implementação de métodos e resolução de problemas . O projeto foi desenvolvido como tema de dissertação no Programa de Mestrado em Ciência da Computação e Matemática Computacional do ICMC/USP . A ferramenta pode ser enquadrada como um ambiente de desenvolvimento ( framework ) e será chamada de ProOF - Professional Optimization Framework . O ProOF tem como foco_principal  nortear a implementação computacional de métodos variados para problemas de otimização , utilizando como paradigma a programação orientada a objetos . Esse framework incorpora as principais características encontradas por outras ferramentas propostas na literatura . Além_disso  , procura facilitar a implementação de métodos e resolução de problemas ao permitir alto reuso de códigos , dar suporte a geração de códigos em diferentes linguagens de programação e gerar uma Graphical User Interface ( GUI ) automática para parametrização dos métodos inseridos pelo usuário . Alguns trabalhos publicados recentemente utilizaram versões em desenvolvimento do ProOF e serão citados como estudo de caso para atestar a robustez do framework proposto . Por fim , uma comparação será realizada entre o ProOF e outros frameworks existentes na 
 Novas formas de interação têm surgido a partir da disseminação das tecnologias associadas á multimídia interativa . O uso de objetos multiinídia apresenta-se , num primeiro momento , como um bónus para muitas aplicações , porém , no contexto no qual este trabalho se insere , pretende-se investigar as possibilidades para o usuário interagir com objetos multimídia , durante a apresentação desses objetos , com características que ultrapassem os limites dos tradicionais controles VCR ( Vídeo- Cassette Recorder ) . Adicionalmente , a apresentação de um vídeo interativo pode ser influenciada e/ou modificada por eventos que ocorrem no ambiente no qual a aplicação multimídia estiver inserida . Neste cenário , o ambiente - visto neste trabalho como sendo a infra-estrutura de comunicação , a aplicação , as mídias e os terminais para interação - pode exigir adaptações na interface do usuário e o ajuste nos parâmetros de distribuição e consumo dos streams elementares de mídia . Tecnologias , técnicas e padrões têm sido produzidos para viabilizar o desenvolvimento dessas novas aplicações multimídia ; entretanto , muitas dessas novas formas de interação continuam inexploradas e não formalizadas . O principal objetivo desta tese1 é apresentar uma proposta para a modelagem de ambientes interativos conscientes de contexto , utilizando a linguagem UML ( Unified Modelling Language ) , tendo como infra-estrutura tecnológica os padrões MPEG-4 e MPEG-7 da família de padrões MPEG ( Moving Picture Expert Group ) e considerando as informações contextuais como uma forma para facilitar e incrementar a interação do usuário com essas novas aplicações multimídia interativos . O trabalho descreve um cenário para aplicações interativas com base nos padrões MPEG-4 e MPEG-7 , no qual seja possível aplicar os conceitos de consciência de contexto como forma para oferecer suporte ao desenvolvimento de novos sistemas multimídia interativos . Neste sentido , novos critérios de interação foram estabelecidos a partir da distribuição de conteúdo multimídia ( representado por objetos MPEG-4 ) e informação associada ao conteúdo , descrita por esquemas MPEG-7 . O futuro padrão MPE.G-2Í está sendo investigado como potencial tecnologia para representar esses novos requisitos multimídia como itens digitais universais . Os principais conceitos de consciência de contexto foram estendidos e adicionados à modelagem proposta também para prover personalização do ambiente . O conceito de perfil de aplicação interativa foi formalizado e discutido . Como forma de validação da proposta , uma aplicação de televisão Interativa f/ Y/ ) foi formalizada e apresentada como estudo de caso , possibilitando a discussão das limitações presentes na modelagem proposta . 1 Parto deste trabalho foi realizada nos laboratórios da T e l e com Italia Lab - T I L A B ( que era c o n h e c i d o c o m o CSELT - Centro dl Studi e Laboratorio lelecommunicazioni ) , junto á Advanced Multimedia Division , em Turim , Itália , no período de maio a dezembro de 2001 
 Nos últimos_anos  o interesse por robôs , em especial por robôs_móveis  , tem crescido muito . Grandes avanços foram obtidos nessa área . Contudo a tarefa de programar um robô_móvel  ainda se mostra uma tarefa extrema , mente árdua tanto em termos da complexidade quanto do tempo desprendido . Algumas novas linguagens próprias para a programação de robôs têm sido propostas pelo meio académico , tais como a CES desenvolvida pelo Institute of Robotics da CMU-USA . Outra tarefa essencial para que os robôs_móveis  tenham um futuro ainda mais promissor é a criação de hardware dedicado para acelerar a execução dos algoritmos que controlam o robô de forma que ele possa tomar decisões mais complexas de maneira mais rápida . Essa tarefa envolve o conhecimento profundo dos algoritmos utilizados pelos mesmos além de conhecimentos sólidos no projeto e desenvolvimento de hardware especializado . Na tentativa de amenizar essas dificuldades , será proposta neste trabalho uma ferramenta para o desenvolvimento de robôs_móveis  reconfiguráveis , denominada ARCHITECT-R. O objetivo do ARCHITECT-R é a geração_automática  de hardware especializado para robôs_móveis  através de algoritmos descritos na linguagem CES . A proposta do ARCHITECT-R é bastante arrojada e praticamente impossível de ser desenvolvida por uma só pessoa em um programa de mestrado . Por isso o trabalho que será realizado no decorrer desse mestrado será apenas a criação de uma parte dessa ferramenta . Mais especificamente , serão tratados a criação de blocos arquiteturais ( hardwares reconfiguráveis ) para redes neurais_artificiais  utilizados pela linguagem CES e a especificação do sistema 
 Numa realidade em que a maioria das atividades desenvolvidas por nós são realizadas com apoio de sistemas computacionais , torna-se ainda mais relevante a atividade de validação desses sistemas . A decompilação é uma das atividades , dentro do processo de desenvolvimento desses sistemas , que está sendo muito difundida atualmente . Assim , torna-se necessária a existência de técnicas e estratégias que visem a aumentar a confiança na corretitude tanto das ferramentas que se usam quanto dos produtos que se geram . Este trabalho propõe uma estratégia incremental de aplicação de diversas técnicas e critérios para o teste e validação de ferramentas de decompilação e , consequentemente , dos produtos gerados por um decompilador . Para ilustrar o emprego da estratégia definida foram realizados dois estudos de caso , usando os decompiladores Dcc ( para a linguagem alvo C ) e JAD ( para a linguagem alvo Java ) 
 Seleção de serviços em composições distribuídas considera principalmente a qualidade de serviço que atenda requisitos estabelecidos pelo usuário , como por exemplo , preço . No entanto , problemas relacionados a execução de composições de serviços podem ocorrer quando não se considera aspectos_relacionados  à rede e ao hardware , que afetam diretamente o desempenho da composição . Esse problema se agrava em composições coreografadas , pois a característica descentralizada requer um maior esforço para que essas informações possam ser consideradas em uma perspectiva global . Dessa forma , apesar da descentralização apresentar vantagens , é necessário que requisitos de qualidade de serviço da composição também sejam considerados em coreografias de serviços_web  para que a escolha de serviços para desempenhar um papel leve em consideração parâmetros importantes que podem afetar no desempenho da composição . Este trabalho apresenta um mecanismo , implementado sobre o framework OpenKnowledge , para selecionar serviços_web  em ambientes coreografados considerando primeiramente estimativas de atraso , taxa de perda e por fim considera a utilização de outros parâmetros , como utilização de CPU . Os primeiros experimentos em diferentes cenários de rede confirmaram as vantagens da proposta em relação a um seletor de serviços que ignora aspectos_relacionados  com a rede . Obteve-se ganhos de 20 a 97 % no que diz_respeito  ao tempo total da execução da coreografia . Em seguida , experimentos inserindo utilização de CPU na escolha dos serviços confirmaram as vantagens de utilização de diferentes parâmetros para seleção de serviços em coreografias 
 Redes de Sensores Sem Fio ( RSSF ) são sistemas distribuídos em rede para sensoreamento , compostos de pequenos dispositivos conectados entre si . Esses sistemas são utilizados para construir aplicações que medem e atuam no meio físico . Cada dispositivo da rede , chamado de nó , é equipado com sensores , e algumas vezes , atuadores . Os nós também comumente possuem limitações em termos de suprimento de energia e capacidade de armazenamento e processamento . Em adição à essas limitações , redes de sensores sem fio também estão sujeitas à diversos tipos de falhas , especialmente quando são implantadas em ambientes de condições naturais extremas , como florestas e plantações . Por essas razões , desenvolvedores de aplicações para redes de sensores sem fio necessitam utilizar mecanismos de tolerância a falhas . Alguns dos mecanismos de tolerância a falhas são implementados em hardware , porém são mais comumente deixados para implementação em software . Além_disso  , a maior_parte  do desenvolvimento de aplicações para RSSF é feita em baixo_nível  de abstração , perto do sistema_operacional  . Desse modo , além de terem que concentrar-se na lógica da aplicação em baixo_nível  , os desenvolvedores ainda têm que implementar os mecanismos de tolerância a falhas junto à aplicação , pela falta de bibliotecas ou componentes genéricos para esse fim . Técnicas de programação em alto nível para RSSF já foram propostas na forma de linguagens e arcabouços de macroprogramação . No entanto , uma minoria lida com aspectos de tolerância a falhas . O objetivo desse trabalho é incorporar funcionalidades para tolerância a falhas ao Srijan , um arcabouço de macroprogramação para redes de sensores sem fio . Srijan possui código_aberto  e é baseado em uma linguagem mista declarativa-imperativa chamada Abstract Task Graph ( ATaG ) . Evoluímos o arcabouço para dar suporte à geração_automática  de código lidando com quedas de nós da rede e falhas que resultam em dados incorretos de sensores . Nesta dissertação , apresentamos a nossa implementação de tais funcionalidades , juntamente com a avaliação conduzida sobre a ferramenta . Mostramos que é possível prover um arcabouço de macroprogramação com suporte apropriado ao desenvolvimento de aplicações para RSSF que necessitam tolerância a falhas 
 O ensino das disciplinas de fundamentos de programação e teste de software não é trivial . Várias iniciativas têm sido investigadas a fim de amenizar os problemas associados a essa atividade . Uma dessas iniciativas é o ensino conjunto de conceitos básicos de programação e de teste de software em disciplinas introdutórias dos cursos de Ciências da Computação . Este trabalho insere-se nessa perspectiva , tendo como principal objetivo fornecer subsídios para o ensino integrado de fundamentos de programação OO e teste de software 

 Para atingir esse objetivo , foi desenvolvido um módulo educacional integrado de fundamentos de programação OO e teste de software , utilizando uma abordagem integrada para modelagem de conteúdos educacionais . Ênfase é dada na modelagem dos aspectos conceituais , instrucionais e didáticos dos domínios de conhecimento envolvidos . Além_disso  , também foi desenvolvido um ambiente , denominado ProgTest , baseado na Web e na concepção de software_livre  , para submissão e avaliação de trabalhos práticos dos alunos . O ambiente foi integrado com ferramentas de teste de modo que a avaliação dos trabalhos seja realizada com base em atividades de teste . Os programas e os resultados são avaliados com base em padrões de consistência , eficácia e eficiência uniformes . Os professores também podem ter uma redução na sua carga de trabalho , uma vez que o sistema avalia automaticamente tanto os casos de teste quanto o código do programa 
 Os métodos existentes para a previsão da posição de átomos de hidrogênio em proteínas são todos baseados na simulação computacional de modelos construídos a partir de características físicas e ( ou ) químicas das moléculas . A abordagem proposta neste trabalho faz_uso  de técnicas inteligentes para a predição da posição de átomos de hidrogênio contidos em grupos hidroxilas ( OH ) pertencentes à cadeias laterais dos aminoácidos serina , treonina e tirosina . Estas técnicas inteligentes são utilizadas em duas fases para a solução do problema proposto : o preprocessamento dos dados e a predição da posição do átomo de hidrogênio . Na fase de preprocessamento , informações sobre os padrões de ligações hidrogênio existentes em moléculas de proteínas são extraídas da base PDB ( Protein Data Bank ) e reunidas em agrupamentos . A base de dados PDB é a principal base internacional que disponibiliza publicamente a estrutura espacial de biomoléculas , principalmente proteínas e ácidos nucléicos , cujas estruturas espacias foram determinadas através de métodos experimentais . Os padrões de ligações hidrogênio obtidos da base de dados são agrupados por similaridade através de um novo algoritimo proposto , o algoritmo de agrupamento por fusão . Este novo algoritmo de agrupamento foi criado com o propósito de tratar dados com distribuição não uniforme , isolando padrões de entrada muito diferentes da média em agrupamento separados . Após o agrupamento , os padrões de ligações hidrogênio contidos em um grupo têm suas estruturas espaciais superpostas ( alinhamento das geometrias dos padrões ) através de operações espaciais de translação e rotações , coordenadas pelo uso de um algoritmo genético . Na fase de predição , os padrões já superpostos contidos em cada agrupamento gerado , são utilizados para o treinamento de uma rede_neural  de arquitetura MLP ( multi layer perceptron ) para a predição da posição do átomo de hidrogênio contido no padrão . Uma parte dos padrões contidos no agrupamento não são usados para o treinamento da rede e reservados para o teste da capacidade da rede_neural  inferir a posição do hidrogênio após o treinamento . Para cada agrupamento é treinada uma rede individual , de forma que os parâmetros livres da rede_neural  sejam calibrados para os dados específicos do agrupamento para o qual a rede_neural  foi treinada . Após diversas alterações de metodogia ao longo dos experimentos computacionais realizados , a nova abordagem proposta mostrouse eficaz , com um bom índice de acerto na predição da posição do hidrogênio após o treino da rede_neural  , para padrões de ligações hidrogênio previamente superpostos em 
 Vários tipos de sensores têm sido explorados na construção de mecanismos para capturar , por exemplo , movimentos dos olhos , da cabeça , ou do tórax do usuário . A construção de dispositivos de interação baseados em tais sensores demandam conhecimentos especializados . O trabalho aqui reportado teve como objetivo desenvolver uma arquitetura de componentes de hardware e software que fornece , como dados de entrada para aplicações computacionais , eventos capturados de sensores tais como os que detectam inclinação , rotação e aceleração . A pesquisa envolveu a concepção e a implementação de uma arquitetura em camadas que possui , em sua base , uma camada de hardware composta por um conjunto de sensores e circuitos apropriados que mensuram eventos físicos e , nas demais camadas , componentes de software que permitem associar uma semântica de dados de entrada para aplicações aos dados primitivos provenientes dos sensores e das camadas intermediárias . Como resultado , a arquitetura disponibiliza os dados dos sensores como eventos em alto nível , que podem ser associados a operações de interação usuário-computador típicas de um mouse , de um teclado ou de um joystick , por exemplo . A implementação de um dispositivo de interação baseado em sensores é facilitada , por um lado , pelo fato de o tratamento de erros associado ao uso de sensores poder ter sido realizado nas camadas inferiores e , por outro , pelo fato de os dados serem apresentados em um alto nível de abstração que não exige conhecimento específico sobre o uso do hardware associado . Como estudo de caso , criouse um dispositivo de interação que permite interações típicas de um mouse , e que pode ser usado por usuários tetraplégicos , que possuam controle fino dos movimentos da cabeça e ausência de deficiências 
 Um dos maiores desafios envolvidos no projeto de grades_computacionais  é o escalonamento de processos , o qual consiste no mapeamento de processos sobre os computadores disponíveis , a fim de reduzir o tempo de execução de aplicações ou maximizar a utilização de recursos . A literatura na área de Sistemas_Distribuídos  trata , geralmente , esses dois objetivos separadamente , dando origem às abordagens de escalonamento orientado a aplicações e orientado a recursos , respectivamente . Mais recentemente , uma nova abordagem , denominada escalonamento orientado a sistemas , tem recebido destaque , buscando otimizar ambos objetivos simultaneamente . Seguindo essas abordagens , algoritmos heurísticos e de aproximação têm sido_propostos  . Os heurísticos buscam por soluções de maneira eficiente sem , contudo , apresentar garantias quanto à qualidade das soluções obtidas . Em contrapartida , os algoritmos de aproximação provêm tais garantias , contudo são mais difíceis de serem projetados , o que justifica o fato de haver apenas versões simplificadas desses algoritmos para cenários de escalonamento de processos . A falta de algoritmos de aproximação adequados para abordar o problema de escalonamento de processos e a necessidade de soluções que atendam o escalonamento orientado a sistemas motivaram esta tese de doutorado que apresenta a proposta do Min Heap-based Scheduling Algorithm ( MHSA ) , um algoritmo de aproximação para o problema de escalonamento de processos orientado a sistemas . Esse algoritmo foi baseado em um modelo de otimização matemática proposto no contexto desta tese . Esse modelo considera os comportamentos de processos e recursos a fim de quantificar a qualidade de soluções de escalonamento . O funcionamento do MHSA envolve a construção de uma árvore min-heap , em que os nós representam computadores e as chaves de ordenação correspondem aos tempos de fila , i.e. , ocupação dos computadores . Apesar de esse algoritmo primordialmente reduzir o tempo de execução ( ou makespan ) de aplicações , essa estrutura em árvore permite que qualquer computador que ocupe o nó raiz receba cargas , o que favorece a ocupação de recursos e , portanto , sua orientação a sistemas . Esse algoritmo tem complexidade assintótica de pior caso igual a O ( 'log IND . 2 m ' ) , em que m corresponde ao número de computadores do sistema . Sua razão de aproximação foi estudada para ambientes distribuídos heterogêneos com e sem a presença de comunicação entre processos , o que permite conhecer , a priori , o nível mínimo de qualidade alcançado por suas soluções . Experimentos foram conduzidos para avaliar o algoritmo proposto e compará-lo a outras propostas . Os resultados confirmam que o MHSA reduz o tempo dispendido na obtenção de boas soluções de 
 Muitos links entre páginas na Web podem ser vistos como indicadores de qualidade e importância para as páginas que eles apontam . A partir desta ideia , vários estudos propuseram métricas baseadas na estrutura de links para inferir qualidade de conteúdo em páginas da web . Contudo , até onde sabemos , o único trabalho que examinou a correlação entre tais métricas e qualidade de conteúdo consistiu de um estudo limitado que deixou várias questões em aberto . Embora tais métricas sejam muito bem sucedidas na tarefa de ranquear páginas que foram fornecidas como respostas para consultas submetidas para máquinas de busca , não é possível determinar a contribuição específica de fatores como qualidade , popularidade e importância para os resultados . Esta dificuldade se deve em parte ao fato de que a informação sobre qualidade , popularidade e importância é difícil de obter para páginas da web em geral . Ao contrário de páginas da web , estas informações podem ser obtidas para artigos da Wikipédia , uma vez que qualidade e importância são avaliadas por especialistas humanos , enquanto a popularidade pode ser estimada com base nas visualizações dos artigos . Isso torna possível a verificação da relação existente entre estes fatores e métricas de análise de links , nosso objetivo neste trabalho . Para fazer isto , nós implementamos vários algoritmos de análise de links e comparamos os rankings obtidos com eles com os obtidos considerando a avaliação humana feita na Wikipédia com relação aos fatores qualidade , popularidade e importância . Nós observamos que métricas de análise de links são mais relacionadas com qualidade e popularidade que com importância e a correlação é 
 Planejamento Probabilístico estuda os problemas de tomada de decisão sequencial de um agente , em que as ações possuem efeitos probabilísticos , modelados como um processo de decisão markoviano ( Markov Decision Process - MDP ) . Dadas a função de transição de estados probabilística e os valores de recompensa das ações , é possível determinar uma política de ações ( i.e. , um mapeamento entre estado do ambiente e ações do agente ) que maximiza a recompensa esperada acumulada ( ou minimiza o custo esperado acumulado ) pela execução de uma sequência de ações . Nos casos em que o modelo MDP não é completamente conhecido , a melhor política deve ser aprendida através da interação do agente com o ambiente real . Este processo é chamado de aprendizado por reforço . Porém , nas aplicações em que não é permitido realizar experiências no ambiente real , por exemplo , operações de venda , é possível realizar o aprendizado por reforço sobre uma amostra de experiências passadas , processo chamado de aprendizado por reforço em lote ( Batch Reinforcement Learning ) . Neste trabalho , estudamos técnicas de aprendizado por reforço em lote usando um histórico de interações passadas , armazenadas em um banco de dados de processos , e propomos algumas formas de melhorar os algoritmos existentes . Como um estudo de caso , aplicamos esta técnica no aprendizado de políticas para o processo de venda de impressoras de grande formato , cujo_objetivo  é a construção de um sistema de recomendação de ações para vendedores iniciantes 
 A diversidade biológica é essencial para a sustentabilidade da vida na Terra e motiva numerosos esforços para coleta de dados sobre espécies , dando origem a uma grande quantidade de informação . Esses dados são geralmente armazenados em bancos de dados relacionais . Pesquisadores usam esses bancos de dados para extrair conhecimento e compartilhar novas descobertas . No entanto , atualmente a busca tradicional ( baseada em palavras-chave ) já não é adequada para ser usada em grandes quantidades de dados heterogêneos , como os de biodiversidade . Ela tem baixa precisão e revocação para esse tipo de dado . Este trabalho apresenta uma nova arquitetura para abordar esse problema aplicando técnicas de buscas semânticas em dados sobre biodiversidade e usando formatos e ferramentas da Web Semântica para representar esses dados . A busca semântica tem como objetivo melhorar a acurácia dos resultados de buscas com o uso de ontologias para entender os objetivos dos usuários e o significado contextual dos termos utilizados . Este trabalho também apresenta os resultados de testes usando um conjunto de dados representativos sobre biodiversidade do Instituto Nacional de Pesquisas da Amazônia ( INPA ) e do Museu Paraense Emílio Goeldi ( MPEG ) . Ontologias permitem que conhecimento seja organizado em espaços conceituais de acordo com seu significado . Para a busca semântica funcionar , um ponto chave é a criação de mapeamentos entre os dados ( neste caso , dados sobre biodiversidade do INPA e MPEG ) e termos das ontologias que os descrevem , neste caso : a classificação taxonômica de espécies e a OntoBio , a ontologia de biodiversidade do INPA . Esses mapeamentos foram criados depois que extraímos a classificação taxonômica do site Catalog of Life ( CoL ) e criamos uma nova versão da OntoBio . Um protótipo da arquitetura foi construído e testado usando casos de uso e dados do INPA e MPEG . Os resultados dos testes mostraram que a abordagem da busca semântica tinha uma melhor precisão ( 28 % melhor ) e revocação ( 25 % melhor ) quando comparada com a busca por palavras-chave . Eles também mostraram que é possível conectar facilmente os dados mapeados a outras fontes de dados abertas , como a fonte Amazon Forest Linked Data do Instituto Nacional de Pesquisas Espaciais . ( INPE 
 Neste trabalho , é apresentado o problema de fluxo de potência ótimo DC utilizando os modelos de fluxos em rede e o princípio do mínimo esforço . É apresentada também uma revisão dos métodos de pontos_interiores  e o mesmo é desenvolvido para um problema geral de programação linear . Em seguida , desenvolvemos os métodos de pontos_interiores  primal-dual e preditor-corretor para os modelos de fluxo de potência específicos estudados . Resultados_numéricos  de uma implementação em MATLAB são apresentados para sistemas testes do IEEE e sistemas reais , comparando o desempenho obtido pelas duas formulações . O método de pontos_interiores  se mostra bastante robusto convergindo rapidamente para todos casos testados . O modelo usando o princípio do mínimo esforço converge em menos iterações , mas a modelagem por fluxos em rede se mostra mais rápida , uma vez que suas iterações são mais baratas 
 Diferentes tipos de neurônios possuem formas distintas , um fator importante para a regulação da forma é a expressão_gênica  . Além_disso  , esta característica também está relacionada com a conectividade entre as células nervosas , formando redes . Por meio delas ocorrem as dinâmicas , como por exemplo o aprendizado . Neste trabalho foi desenvolvido um arcabouço de modelagem e simulação neuronal , para analisar a integração das etapas desde a expressão_gênica  , passando pela geração dos neurônios até as dinâmicas , permitindo o estudo do sistema e relacionamento entre as partes . Na etapa de geração , foram utilizados diferentes padrões de expressão_gênica  . Por meio dos neurônios , foram criadas as redes , caracterizadas utilizando medidas de centralidade . Ademais , foram executadas as dinâmicas integra-e-dispara , que simula a comunicação entre os neurônios , e o desenvolvimento hebiano , que é uma dinâmica aplicada para simular o aprendizado . Para quantificar a influência da expressão_gênica  , foram utilizadas as medidas de correlação de Pearson e a informação mútua . Por meio destes testes , foi possível observar que a expressão_gênica  influencia todas as etapas , sendo que nelas , exceto na geração da forma neuronal , os padrões de expressão com que os neurônios foram organizados também são um fator importante . Além_disso  , na medida de centralidade betweenness centrality , foi possível observar a formação de caminhos , denominados caminhos do betweenness . Para descrever os caminhos , foram feitas comparações entre as redes neuronais e outras redes espaciais . Assim , foi possível observar que estes caminhos são uma característica comum em redes geográficas e estão relacionados com as comunidades da rede 
 O teste de programas concorrentes é uma atividade custosa devido principalmente à quantidade de sequências de sincronização que devem ser testadas para validar tais programas . Uma das técnicas mais utilizadas para testar a comunicação e sincronização de programas concorrentes é a geração_automática  de diferentes pares de sincronização ou , em outras_palavras  , a geração de variantes de disputa ( race variant ) . Nesta técnica as variantes de disputa são geradas a partir de arquivos de rastro de uma execução não-determinística e algoritmos de execução determinística são utilizados para forçar que diferentes sincronizações sejam cobertas . Este trabalho aborda de maneira abrangente este problema , cujo_objetivo  principal é reduzir o tempo de resposta da atividade de teste_estrutural  de programas concorrentes quando diferentes variantes de disputa são executadas . Há três principais_contribuições  neste trabalho : ( 1 ) geração de arquivos de rastro e execução determinística total/parcial , ( 2 ) geração_automática  de variantes e ( 3 ) paralelização da execução das variantes . Diferentemente de outros trabalhos disponíveis na literatura , os algoritmos propostos consideram programas concorrentes que interagem simultaneamente com passagem de mensagens e memória_compartilhada  . Foram consideradas seis primitivas com semânticas distintas : ponto-a-ponto bloqueante/não bloqueante , coletivas um-para-todos/todos-para-um/todos-para-todos e semáforos . Os algoritmos foram desenvolvidos no nível de aplicação em Java , são ortogonais à linguagem de programação utilizada e não requerem privilégios de sistema para serem executados . Estas três contribuições são descritas , detalhando seus algoritmos . Também são apresentados os resultados obtidos com os experimentos feitos durante as fases de validação e avaliação de cada contribuição . Os resultados demonstram que os objetivos propostos foram atingidos com sucesso para cada contribuição e , do ponto de vista do testador , o tempo de resposta da atividade de teste_estrutural  de programas concorrentes foi reduzido enquanto a cobertura de programas concorrentes com ambos os paradigmas aumentou com procedimentos automatizados e transparentes . Os experimentos mostram speedups próximos ao linear , quando comparadas as versões sequencial e paralela dos algoritmos 
 As proteínas são moléculas que desempenham funções essenciais para a vida . Para entender a função de uma proteína é preciso conhecer sua estrutura tridimensional . No entanto , encontrar a estrutura da proteína pode ser um processo caro e demorado , exigindo profissionais altamente qualificados . Neste sentido , métodos computacionais têm sido investigados buscando predizer a estrutura de uma proteína a partir de uma sequência de aminoácidos . Em geral , tais métodos computacionais utilizam conhecimentos de estruturas de proteínas já determinadas por métodos experimentais , para tentar predizer proteínas com estrutura desconhecida . Embora métodos computacionais como , por exemplo , o Rosetta , I-Tasser e Quark tenham apresentado sucesso em suas predições , são apenas capazes de produzir estruturas significativamente semelhantes às já determinadas experimentalmente . Com isso , por utilizarem conhecimento a priori de outras estruturas pode haver certa tendência em suas predições . Buscando elaborar um algoritmo eficiente para Predição de Estruturas de Proteínas livre de tendência foi desenvolvido um Algoritmo de Estimação de Distribuição ( EDA ) específico para esse problema , com modelagens full-atom e algoritmos ab_initio  . O fato do algoritmo proposto ser ab_initio  é mais interessante para aplicação envolvendo proteínas com baixa similaridade , com relação às estruturas já conhecidas . Três tipos de modelos_probabilísticos  foram desenvolvidos : univariado , bivariado e hierárquico . O univariado trata o aspecto de multi-modalidade de uma variável , o bivariado trata os ângulos diedrais ( Φ Ψ ) de um mesmo aminoácido como variáveis correlacionadas . O hierárquico divide o problema em subproblemas e tenta tratá-los separadamente . Os resultados desta pesquisa mostraram que é possível_obter  melhores_resultados  quando considerado a relação bivariada ( Φ Ψ ) . O hierárquico também mostrou melhorias nos resultados obtidos , principalmente para proteínas com mais de 50 resíduos . Além_disso  , foi realiza uma comparação com algumas heurísticas da literatura , como : Busca Aleatória , Monte_Carlo  , Algoritmo Genético e Evolução Diferencial . Os resultados mostraram que mesmo uma metaheurística pouco eficiente , como a Busca Aleatória , pode encontrar a solução correta , porém utilizando muito conhecimento a priori ( predição que pode ser tendenciosa ) . Por outro_lado  , o algoritmo proposto neste trabalho foi capaz de obter a estrutura da proteína esperada sem utilizar conhecimento a priori , caracterizando uma predição puramente ab_initio  ( livre de tendência ) 
 Segmentação de imagem consiste no seu particionamento em regiões , tal como para isolar os pixels pertencentes a objetos de interesse em uma imagem , sendo uma etapa importante para visão_computacional  , processamento de imagens médicas e outras aplicações . Muitas vezes a segmentação automática gera resultados com imperfeições . O usuário pode corrigi-las editando-a manualmente , interativamente ou simplesmente descartar o resultado e gerar outro automaticamente . Métodos interativos combinam os benefícios dos métodos manuais e automáticos , reduzindo o esforço do usuário e utilizando seu conhecimento de alto nível . Nos métodos baseados em sementes , para continuar ou reparar uma segmentação prévia ( presegmentação ) , evitando o usuário começar do zero , é necessário resolver o Problema da Segmentação Interativa Reversa ( RISP ) , ou seja , estimar automaticamente as sementes que o gerariam . Para isso , este trabalho particiona o objeto da segmentação em núcleos . Em um núcleo , duas sementes separadamente produzem o mesmo resultado , tornando uma delas redundante . Com isso , apenas uma semente por núcleo é necessária . Núcleos contidos nos resultados de outros núcleos são redundantes e também podem ser descartados , reduzindo ainda mais o conjunto de sementes , um processo denominado Análise de Redundância . Um conjunto mínimo de sementes para a presegmentação é gerado e o problema da reparação interativa pode então ser resolvido através da adição de novas sementes ou remoção . Dentro do arcabouço da Transformada Imagem-Floresta ( IFT ) , novos métodos como Oriented Image-Foresting Transform ( OIFT ) e Oriented Relative Fuzzy Connectedness ( ORFC ) foram desenvolvidos . Todavia , não há algoritmos para calcular o núcleo destes métodos . Este trabalho desenvolve tais algoritmos , com prova de corretude . Os núcleos também nos fornecem uma indicação do grau de robustez dos métodos sobre o posicionamento das sementes . Por isso , um método híbrido do GraphCut com o núcleo do ORFC , bem como um Coeficiente de Robustez ( RC ) , foram desenvolvidos . Neste trabalho também foi desenvolvida outra solução para reparar segmentações , a qual é baseada em IFT-SLIC , originalmente utilizada para gerar supervoxels . Resultados experimentais analisam , comparam e demonstram o potencial destas soluções 
 Atualmente , o meio mais comum de busca de informações é a WEB . Assim , é importante procurar métodos eficientes para recuperar essa informação . As máquinas de busca na WEB usualmente utilizam palavras-chaves para expressar uma busca . Porém , não é trivial caracterizar a informação desejada . Usuários diferentes com necessidades diferentes podem_estar  interessados em informações relacionadas , mas distintas , ao realizar a mesma busca . O processo de realimentação de relevância torna possível a participação ativa do usuário no processo de busca . A idéia geral desse processo consiste em , após o usuário realizar uma busca na WEB permitir que indique , dentre os sites encontrados , quais deles considera relevantes e não relevantes . A opinião do usuário pode então ser considerada para reordenar os dados , de forma que os sites relevantes para o usuário sejam retornados mais facilmente . Nesse contexto , e considerando que , na grande maioria dos casos , uma consulta retorna um número muito grande de sites WEB que a satisfazem , das quais o usuário é responsável por indicar um pequeno número de sites relevantes e não relevantes , tem-se o cenário ideal para utilizar aprendizado parcialmente supervisionado , pois essa classe de algoritmos de aprendizado requer um número pequeno de exemplos_rotulados  e um grande número de exemplos não-rotulados . Assim , partindo da hipótese que a utilização de aprendizado parcialmente supervisionado é apropriada para induzir um classificador que pode ser utilizado como um filtro de realimentação de relevância para buscas na WEB , o objetivo deste trabalho consiste em explorar algoritmos de aprendizado parcialmente supervisionado , mais especificamente , aqueles que utilizam multidescrição de dados , para auxiliar na recuperação de sites na WEB . Para avaliar esta hipótese foi projetada e desenvolvida uma ferramenta denominada C-SEARCH que realiza esta reordenação dos sites a partir da indicação do usuário . Experimentos mostram que , em casos que buscas genéricas , que o resultado possui um bom diferencial entre sites relevantes e irrelevantes , o sistema consegue obter melhores_resultados  para o 
 Um ambiente inteligente é um sistema de computação ubíqua e sensível ao contexto onde os sistemas computacionais embutidos no ambiente , a comunicação entre dispositivos e o ambiente , e a acessibilidade aos serviços do ambiente são transparentes ao usuário . O presente_trabalho  tem como objetivo propor um modelo para ambientes inteligentes baseado em serviços_web  semânticos , em que os serviços disponíveis para os dispositivos do ambiente são proporcionados como serviços_web  e a interação dispositivo - ambiente é feita em um contexto de computação móvel , onde a disponibilidade dos serviços e a informação de contexto do dispositivo mudam freqüentemente . No modelo proposto todas as funcionalidades do ambiente são fornecidas como serviços . Estes serviços são descobertos e executados automaticamente com a finalidade de ajudar o usuário a desenvolver tarefas específicas , permitindo ao usuário se concentrar nas tarefas e não na interação com o ambiente . O modelo se fundamenta na oferta de serviços dirigida pela tarefa a ser desenvolvida , o que é conhecido como Task-driven Computing . Por outro_lado  , para a automação do processo de descoberta e execução dos serviços é necessário ter uma especificação não ambígua da semântica dos serviços . Empregamos para isso a ontologia WSMO ( Web_Services  Modeling Ontology ) que fornece os elementos necessários para a descrição dos serviços disponíveis no ambiente e o contexto do dispositivo . Finalmente , como prova de conceitos do modelo proposto , foi implementado um ambiente inteligente para uma biblioteca . A ativação de um ambiente inteligente baseado no modelo proposto se baseia na definição de ontologias , descrição semântica dos serviços no ambiente e a implementação de serviços_web  tradicionais 
 A simulação computacional de membranas biológicas , em particular membranas formadas por bicamadas lipídicas , é uma área de grande interesse na atualidade . Enquanto simulações moleculares são bastante populares , a simulação na escala de uma célula inteira requer modelos baseados na mecânica dos meios contínuos . Essas membranas apresentam um comportamento de fluido viscoso incompressível bidimensional . Além_disso  , as formas de equilíbrio são bem explicadas pela energia de Canham-Helfrich , que depende da curvatura da membrana . Neste trabalho , um novo método de simulação de membranas viscosas , baseado em elementos finitos , é apresentado . Ele se inspira no conceito de James Clerk Maxwell de elasticidade fugaz , o qual é usado para adaptar técnicas bem estabelecidas de simulação de membranas elásticas . Trata-se do primeiro método a levar em conta , de maneira rigorosa , o aspecto viscoso da membrana , que é dominante na escala de tamanho de uma célula biológica , além da sua característica de fluido 
 A sumarização multidocumento consiste na produção de um sumário/resumo a partir de uma coleção de textos sobre um mesmo assunto . Devido à grande quantidade de informação disponível na Web , esta tarefa é de grande relevância já que pode facilitar a leitura dos usuários . Os aspectos informativos representam as unidades básicas de informação presentes nos textos . Por exemplo , em textos jornalísticos em que se relata um fato/acontecimento , os aspectos podem representar a seguintes informações : o que aconteceu , onde aconteceu , quando aconteceu , como aconteceu , e por que aconteceu . Conhecendo-se esses aspectos e as estratégias de produção e organização de sumários , é possível automatizar a tarefa de sumarização . No entanto , para o Português do Brasil , não há pesquisa feita sobre sumarização com base em aspectos . Portanto , neste trabalho de mestrado , investigaram-se métodos de sumarização multidocumento com base em aspectos informativos , pertencente à abordagem profunda para a sumarização , em que se busca interpretar o texto para se produzir sumários mais informativos . Em particular , implementaram-se duas etapas relacionadas : ( i ) identificação automática de aspectos os aspectos informativos e ( ii ) desenvolvimento e avaliação de dois métodos de sumarização com base em padrões de aspectos ( ou templates ) em sumários . Na etapa ( i ) , criaram-se classificadores de aspectos com base em anotador de papéis semânticos , reconhecedor de entidades mencionadas , regras manuais e técnicas de aprendizado de máquina . Avaliaram-se os classificadores sobre o córpus CSTNews ( Rassi et al. , 2013 ; Felippo et al. , 2014 ) . Os resultados foram satisfatórios , demostrando que alguns aspectos podem ser identificados automaticamente em textos jornalísticos com um desempenho razoável . Já na etapa ( ii ) , elaboraram-se dois métodos inéditos de sumarização multidocumento com base em aspectos . Os resultados obtidos_mostram  que os métodos propostos neste trabalho são competitivos com os métodos da literatura . Salienta-se que esta abordagem para sumarização tem recebido grande destaque ultimamente . Além_disso  , é inédita nos trabalhos desenvolvidos no Brasil , podendo trazer contribuições importantes para a área 
 Funções submodulares aparecem naturalmente em diversas_áreas  , tais como probabilidade , geometria e otimização combinatória . Pode-se dizer que o papel desempenhado por essas funções em otimização discreta é similar ao desempenhado por convexidade em otimização contínua . Com efeito , muitos problemas em otimização combinatória podem ser formulados como um problema de minimizar uma função submodular sobre um conjunto apropriado . Além_disso  , submodularidade está presente em vários teoremas ou problemas combinatórios e freqüentemente desempenha um papel essencial em uma demonstração ou na eficiência de um algoritmo . Nesta dissertação , estudamos aspectos estruturais e algorítmicos de funções submodulares , com ênfase nos recentes avanços em algoritmos combinatórios para minimização dessas funções . Descrevemos com detalhes os primeiros algoritmos combinatórios e fortemente polinomiais para esse propósito , devidos a Schrijver e Iwata , Fleischer e Fujishige , além de algumas outras extensões . Aplicações de submodularidade em otimização combinatória também estão presentes neste trabalho 
 O objetivo desse trabalho é apresentar as características de um Sistema de Apoio ao Diagnóstico em Sistema Hospitalar Suportando Busca por Imagens Similares , a ser desenvolvido e implantado no Hospital das Clínicas de Ribeirão Preto . A recuperação de imagens baseada no conteúdo é uma área de pesquisa que tem evoluído bastante nos últimos_anos  . Assim , um sistema de busca e obtenção de imagens , utilizando tal técnica , deve ser extensível aos novos algoritmos de extração de características e métodos de indexação . A extração de características de imagens , tais como informações de cor , textura , forma e o relacionamento entre elas são utilizadas para descrever o conteúdo das imagens . Essas características são então utilizadas para indexar e possibilitar a comparação de imagens no processo de recuperação 
 O sistema proposto utilizará um método de indexação de dados recém-desenvolvido  a Slim-tree  para indexar as características_extraídas  das imagens . Através desse método o Sistema de Apoio ao Diagnóstico possibilitará a consulta por conteúdo em imagens médicas 
 Sistemas computacionais são constituídos por componentes de hardware e software que podem eventualmente falhar . Por esse motivo , o mecanismo de transação sempre foi imprescindível para a construção de sistemas robustos . O suporte transacional para a tecnologia Web_services  foi definido em agosto de 2005 , num conjunto de três especificações denominadas WS-Coordination , WS-AtomicTransaction e WS-BusinessActivity . Juntas , essas especificações definem um alicerce sobre o qual aplicações robustas baseadas em Web_services  podem ser construídas . Nesta dissertação realizamos um estudo sobre transações atômicas em ambientes Web_services  . Em particular , estendemos o gerenciador de transações presente no servidor de aplicações JBoss , de modo que ele passasse a comportar transações distribuídas envolvendo Web_services  . Além_disso  , avaliamos o desempenho desse gerenciador de transações quando ele emprega cada um dos seguintes mecanismos de chamada remota : Web services/SOAP , CORBA/IIOP e JBoss Remoting . Finalmente , realizamos experimentos de escalabilidade e interoperabilidade 
 O surgimento e a popularização de arquiteturas de software que fornecem suporte à programação distribuída orientada a objetos , como CORBA , .NET e Java EE , gerou uma demanda por infra-estruturas de segurança eficientes , capazes de proteger os recursos dos sistemas de ataques maliciosos . Essa proteção começa pela identificação dos usuários que interagem com os sistemas , processo conhecido como autenticação . Entretanto , a autenticação por si só não é suficiente para garantir a segurança dos recursos , uma vez que a autenticação não determina quais ações os usuários estão autorizados a executar depois de autenticados . Em outras_palavras  , um mecanismo de autorização , que faz valer as políticas de controle de acesso aos recursos definidas pelos administradores de sistemas , se faz necessário . Neste trabalho estudamos mecanismos de controle de acesso baseado em papéis e a aplicabilidade dos certificados de atributos X.509 como estrutura de armazenamento desses papéis em um ambiente Java EE . Em particular , estendemos a infra-estrutura de segurança do servidor de aplicações JBoss , de modo que ela passasse a comportar os certificados de atributos X.509 . Além_disso  , analisamos as vantagens e desvantagens do uso de tais certificados e avaliamos o desempenho da extensão desenvolvida em relação a outras alternativas que são oferecidas pelo JBoss para o armazenamento de papéis dos usuários 
 É notório que a degradação ambiental vem ao longo do tempo posicionando-se como um dos principais problemas do mundo moderno . Dentre as várias questões do interesse ambiental podemos destacar a ascensão da bolha de metano , em reservatórios hidrelétricos , desde o sedimento anóxico no fundo do reservatório até a interface água atmosfera . Neste contexto , a presente tese vêm propor uma nova modelagem matemática para a ascensão da bolha axissimétrica em fluidos newtonianos/nãonewtonianos e mostrar resultados numéricos simulados . Desta forma , o estado da arte estaria elevado a posição de permitir , via Matemática e Simulação Numérica-Computacional , a análise do transporte de metano em reservatórios de hidrelétricas através da 
 A área de visualização volumétrica engloba um conjunto de técnicas utilizadas na representação , manipulação e exibição de dados associados à região de um volume , possibilitando , assim , a exploração e melhor compreensão do interior de objetos de natureza tridimensional . Contudo , algumas limitações ainda são encontradas nessa área , como , por exemplo , a exploração de mais de um valor simultaneamente em conjuntos de dados volumétricos multivariados . Além desse desafio , outro objeto de grande interesse da comunidade_científica  é a exploração de volumes variantes no tempo . A complexidade nesse caso está em tratar ou processar uma quantidade muito grande de dados buscando descobrir propriedades , estruturas ou características que variam com o tempo . O presente_trabalho  propõe técnicas e abordagens , baseadas no conceito de projeções_multidimensionais  , visando dar_apoio  à análise de conjuntos volumétricos multivariados que variam no tempo . A primeira técnica proposta , denominada Fastmap* , possibilitou a projeção de espaços de alta dimensionalidade em fluxo contínuo . A segunda técnica apresentada , denominada RLNP , permitiu a projeção de dados por vizinhança mantendo a coerência temporal nos dados projetados , além de possuir a capacidade de projetar espaços de alta dimensão com um nível de stressbaixo . Também , propomos uma abordagem para a análise baseada em atributos , denominada Scatter Projection , que facilita a exploração focada em um atributo específico junto com a similaridade dos dados entre eles . Finalmente , propõe-se uma abordagem baseada na reprojeção de agrupamentos usando técnicas de seleção de atributos para tentar identificar melhor as estruturas internas do volume . Assim , o presente_trabalho  contribui no sentido de levantar e discutir limitações das técnicas disponíveis , e em seguida , buscar possibilidades de solução para tais questões , propondo técnicas e abordagens que possibilitam a exploração de grandes_conjuntos  de dados volumétricos multivariados , mantendo a coerência 
 O agrupamento visual de dados é uma abordagem que integra conceitos de visualização de informação e de aprendizado de máquina , especificamente por agrupamento , em um único algoritmo . Isso permite aproveitar a capacidade dos seres_humanos  para tomar decisões baseados em conhecimento específico de domínio , bem como a capacidade dos computadores para registrar , armazenar , manipular , e recuperar dados . Este trabalho apresenta a pesquisa realizada nessa linha , a qual tratou problemas específicos presentes nos métodos de agrupamento visual de dados . O principal problema em estudo foi a escalabilidade de algoritmos de agrupamento visual baseados em densidade , propondo meios para acelerar os processos de cálculo de projeções e de estimativa de densidade envolvidos neles . Como resultado é apresentado o algoritmo HC-Enhanced , que possui desempenho bastante superior ao algoritmo HC-Cooperative no qual é baseado , além de um dispositivo de melhoria do agrupamento 
 Este trabalho apresenta os principais conceitos de um modelo de referência , chamado de Consciência Situacional em Voo ( In-Flight Awareness , IFA ) , e sua implementação embarcada IFA2S ( In-Flight Awareness Augmentation System ) . IFA é um conceito novo e realista e voltado à melhoria da segurança de voo de VANTs . IFA2S tem o potencial de alavancar confiabilidade dos VANTs aos níveis encontrados na aviação geral . Ele aumenta a consciência aeronave tanto em relação a si mesma e seu ambiente circundante e , ao mesmo tempo reconhece restrições da plataforma para agir de acordo com algoritmos de decisão pré-definidos . Este trabalho apresenta o IFA como consequência dos requisitos de segurança estabelecidos através da metodologia STPA , faz uma avaliação quantitativa do impacto do IFA2S no risco operacional dos VANTs e apresenta orientações de implementação em hardware . Simulações de validação são realizadas com uso do software Labview e do simulador de voo XPlane 
 Esta tese propõe uma metodologia baseada em métodos de kernel , teoria fuzzy e probabilidade para tratar conjuntos de dados cujas observações são conjuntos de pontos . As medidas de probabilidade e os conjuntos_fuzzy  são usados para modelar essas observações . Posteriormente , graças a kernels definidos sobre medidas de probabilidade , ou em conjuntos_fuzzy  , é feito o mapeamento implícito dessas medidas de probabilidade , ou desses conjuntos_fuzzy  , para espaços de Hilbert com kernel reproduzível , onde a análise pode ser feita com algum método kernel . Usando essa metodologia , é possível fazer frente a uma ampla gamma de problemas de aprendizado para esses conjuntos de dados . Em particular , a tese apresenta o projeto de modelos de descrição de dados para observações modeladas com medidas de probabilidade . Isso é conseguido graças ao mergulho das medidas de probabilidade nos espaços de Hilbert , e a construção de esferas envolventes mínimas nesses espaços de Hilbert . A tese apresenta como esses modelos podem ser usados como classificadores de uma classe , aplicados na tarefa de detecção de anomalias grupais . No caso que as observações sejam modeladas por conjuntos_fuzzy  , a tese propõe mapear esses conjuntos_fuzzy  para os espaços de Hilbert com kernel reproduzível . Isso pode ser feito graças à projeção de novos kernels definidos sobre conjuntos_fuzzy  . A tese apresenta como esses novos kernels podem ser usados em diversos problemas como classificação , regressão e na definição de distâncias entre conjuntos_fuzzy  . Em particular , a tese apresenta a aplicação desses kernels em problemas de classificação supervisionada em dados intervalares e teste kernel de duas amostras para dados contendo atributos imprecisos 
 Seleção de características é um tópico muito importante em aplicações de reconhecimento de padrões , especialmente em bioinformática , cujos problemas são geralmente tratados sobre um conjunto de dados envolvendo muitas variáveis e poucas observações . Este trabalho analisa aspectos de seleção de características no problema de identificação de redes de regulação gênica a partir de sinais de expressão_gênica  . Particularmente , propusemos um modelo de redes gênicas probabilísticas ( PGN ) que devolve uma rede construída a partir da aplicação recorrente de algoritmos de seleção de características orientados por uma função critério baseada em entropia condicional . Tal critério embute a estimação do erro por penalização de amostras raramente observadas . Resultados desse modelo aplicado a dados sintéticos e a conjuntos de dados de microarray de Plasmodium falciparum , um agente causador da malária , demonstram a validade dessa técnica , tendo sido capaz não apenas de reproduzir conhecimentos já produzidos anteriormente , como também de produzir novos resultados . Outro aspecto investigado nesta tese é o fenômeno da predição intrinsecamente multivariada ( IMP ) , ou seja , o fato de um conjunto de características ser um ótimo caracterizador dos objetos em questão , mas qualquer de seus subconjuntos propriamente contidos não conseguirem representá-los de forma satisfatória . Neste trabalho , as condições para o surgimento desse fenômeno foram obtidas de forma analítica para conjuntos de 2 e 3 características em relação a uma variável alvo . No contexto de redes de regulação gênica , foram obtidas evidências de que genes alvo de conjuntos IMP possuem um enorme potencial para exercerem funções vitais em sistemas biológicos . O fenômeno conhecido como canalização é particularmente importante nesse contexto . Em dados de microarray de melanoma , constatamos que o gene DUSP1 , conhecido por exercer função canalizadora , foi aquele que obteve o maior número de conjuntos de genes IMP , sendo que todos eles possuem lógicas de predição canalizadoras . Além_disso  , simulações computacionais para construção de redes com 3 ou mais genes mostram que o tamanho do território de um gene alvo pode ter um impacto positivo em seu teor de IMP com relação a seus preditores . Esta pode ser uma evidência que confirma a hipótese de que genes alvo de conjuntos IMP possuem a tendência de controlar diversas vias metabólicas cruciais para a manutenção das funções vitais de um organismo 
 Dentre as aplicações mais comuns envolvendo microarrays , pode-se destacar a classificação de amostras de tecido , essencial para a identificação correta da ocorrência de câncer . Essa classificação é realizada com a ajuda de algoritmos de Aprendizagem de Máquina . A escolha do algoritmo mais adequado para um dado problema não é trivial . Nesta tese de doutorado , estudou-se a utilização de meta-aprendizagem como uma solução viável . Os resultados experimentais atestaram o sucesso da aplicação utilizando um arcabouço padrão para caracterização dos dados e para a construção da recomendação . A partir de então , buscou-se realizar melhorias nesses dois aspectos . Inicialmente , foi proposto um novo conjunto de meta-atributos baseado em índices de validação de agrupamentos . Em seguida , estendeu-se o método de construção de rankings kNN para ponderar a influência dos vizinhos mais próximos . No contexto de meta-regressão , introduziu-se o uso de SVMs para estimar o desempenho de algoritmos de classificação . Árvores de decisão também foram empregadas para a construção da recomendação de algoritmos . Ante seu desempenho inferior , empregou-se um esquema de comitês de árvores , que melhorou sobremaneira a qualidade dos 
 A crescente capacidade do poder_computacional  e a proliferação de dispositivos ao nosso redor vem permitindo o desenvolvimento de novas e sofisticadas interfaces para interação humano-computador que reagem à presença e ao estado de seus usuários . Como o olhar tem a capacidade de transmitir muitas informações sobre o usuário , rastreadores de olhar , dispositivos que estimam a direção para onde uma pessoa olha , tem papel_importante  no desenvolvimento de tais interfaces . Entre suas aplicações temos o auxílio a pessoas com dificuldades motoras , que podem utilizar um rastreador de olhar como substituto ao mouse , aplicações de diagnóstico , que estudam evidências do comportamento humano , ou ainda o desenvolvimento de interfaces que utilizem a informação sobre o olhar como um canal a mais de comunicação com o usuário para perceber suas intenções . Muitas técnicas para atingir tal objetivo foram desenvolvidas mas as tradicionais ainda oferecem certas dificuldades de uso para seus usuários como a intolerância a movimentos de cabeça e a necessidade de calibração por sessão de uso . Neste trabalho fizemos um levantamento de uma série de técnicas de rastreamento de olhar , indo das mais tradicionais até algumas mais recentes que visam melhorar a facilidade de uso destes sistemas . Uma das técnicas mais promissoras utiliza múltiplas fontes de luz fixadas nos cantos do monitor do computador . Através da análise da posição dos reflexos gerados por essas fontes de luz sobre a córnea , juntamente com a informação da posição da pupila , presentes em imagens capturadas do olho , é possível estimar o ponto observado no monitor . Devido às suas vantagens ela foi escolhida para estudo mais detalhado e implementação . Extensos testes utilizando simulações foram realizados para avaliar seu desempenho . Foi também desenvolvida uma extensão dessa técnica , utilizando um modelo mais preciso do olho , visando melhorar sua precisão . Ao final apresentamos nossa implementação , baseada nessa extensão da técnica original , que é tolerante a movimentação da face e mostramos os resultados obtidos em testes realizados com um grupo de usuários 
 Análise de agrupamento é um problema fundamental de aprendizado de máquina não supervisionado em que se objetiva determinar categorias que descrevam um conjunto de objetos de acordo com suas similaridades ou inter-relacionamentos . Na formulação tradicional do problema , busca-se por partições ou hierarquias de partições contendo grupos cujos objetos são de alguma_forma  similares entre si e dissimilares aos objetos dos demais grupos , segundo alguma medida direta ou indireta de ( dis ) similaridade que leva em conta o conjunto completo de atributos que descrevem os objetos na base de dados sob análise . Entretanto , apesar de décadas de aplicações bem sucedidas , existem situações em que a natureza dos agrupamentos contidos nos dados não pode ser representada segundo este tipo de formulação . Em particular , existem situações em que grupos de objetos se caracterizam como tais apenas segundo um subconjunto dos atributos que os descrevem , sendo que tal subconjunto pode ser distinto para cada grupo . Ao contrário de algoritmos de agrupamento tradicionais , algoritmos de bi-agrupamento são capazes de agrupar simultaneamente linhas e colunas de uma matriz de dados . Tais algoritmos produzem bi-grupos formados por subconjuntos de objetos e subconjuntos de atributos de alguma_forma  fortemente co-relacionados . Esses algoritmos passaram a atrair a atenção da comunidade_científica  quando se evidenciou a relevância da tarefa de bi-agrupamento em problemas de análise de dados de expressão_gênica  em bioinformática . Embora em menor grau , as abordagens de bi-agrupamento também têm ganho atenção em outros domínios de aplicação , tais como mineração de textos ( text mining ) e filtragem colaborativa em sistemas de recomendação . O problema é que uma variedade de algoritmos de bi-agrupamento têm sido_propostos  na literatura baseados em diferentes princípios e suposições sobre os dados , podendo chegar a resultados completamente distintos em uma mesma aplicação . Nesse cenário , torna-se importante a realização de estudos comparativos que possam contrastar o comportamento e desempenho dos diversos algoritmos . Neste trabalho é apresentado um estudo comparativo envolvendo 17 algoritmos de bi-agrupamento ( representativos das principais categorias de algoritmos existentes ) em coleções de bases de dados tanto de natureza real como simulada , com particular ênfase em problemas de análise de dados de expressão_gênica  . Diversos aspectos metodológicos e procedimentos para a avaliação_experimental  foram considerados , a fim de superar as limitações de estudos comparativos anteriores da literatura . Além da comparação em si , todo o arcabouço comparativo pode ser reutilizado para a comparação de outros algoritmos no futuro 
 A Microscopia Eletrônica de Baixa Energia ( LEEM ) é uma recente e poderosa modalidade para o estudo de superfície passível de uma grande quantidade de degradações , como ruídos e borramento . Ainda incipiente na literatura , este trabalho visou a análise e identificação das fontes de degradações presentes em vídeos , além da utilização de um conjunto de técnicas de remoção de ruído e borramento para a restauração de dados LEEM . Além_disso  , foram desenvolvidas duas novas técnicas de filtragem de vídeo como intuito de preservar detalhes pequenos e texturas presentes . Na etapa de análise foi constatado que as imagens LEEM possuem uma grande quantidade e variedade de ruídos , sendo o Gaussiano o mais preponderante . Foi também estimada a Função de Espalhamento de Ponto ( PSF ) do microscópio utilizado , visando o emprego de técnicas de redução de borramento . Este trabalho também analisou a combinação de técnicas de redução de borramento com as técnicas de filtragem do ruído Gaussiano existente . Foi constatado que as técnicas não locais , como Non-Local Means ( NLM ) eBlock-Matching 3-D ( BM3D ) , proveem uma maior capacidade de filtragem das imagens LEEM , preservando descontinuidades . Ainda nesta análise , identificou-se que algumas técnicas de redução de borramento não são efetivas em imagens LEEM , exceto a técnica Richardson-Lucy ( RL ) que suprimiu grande parte do borramento sem adicionar mais degradação . A indesejável remoção de pequenas estruturas e texturas pelas técnicas de filtragem existentes motivou o desenvolvimento de duas novas técnicas de filtragem de ruído Gaussiano ( NLM3D-LBP-MSB eNLM3D-LBP-Adaptive ) que mostraram resultados superiores para filtragem de imagens com grande quantidade de textura . Porém , em imagens com muitas regiões homogêneas o BM3D foi superior . Avaliações quantitativas foram realizadas sobre imagens artificiais . Em imagens LEEM reais , realizou-se um experimento qualitativo em que observadores avaliaram visualmente o resultado de restaurações por diversas técnicas existentes e as propostas neste trabalho . O experimento comprovou que os métodos de filtragem não locais foram superiores , principalmente quando combinados com o método RL . Os métodos propostos produziram bons_resultados  , entretanto , inferiores aos exibidos pelas técnicas NLM eBM3D . Este trabalho demonstrou que as técnicas de filtragem não locais são as mais adequadas para dados LEEM . Além_disso  , a técnica RL mostrou-se eficaz na redução de borramento 
 Acessibilidade na Web tem como objetivo possibilitar que qualquer pessoa possa entender e interagir com o conteúdo de uma página Web , independente de deficiências ou outros fatores . Embora exista uma grande quantidade de trabalhos realizados na área , pode-se ainda perceber que pessoas com alguns tipos específicos de deficiências encontram mais dificuldades ao navegar na Web , como no caso dos usuários cegos . Com intuito de minimizar os problemas enfrentados por esses usuários cegos ao navegar na Web , foi proposta nesta tese uma abordagem que consiste em extrair automaticamente o conhecimento de navegação implícito na apresentação visual de uma página Web e alterar o design dessa navegação de uma maneira otimizada a esses usuários . Com esse objetivo , primeiramente foram estudados os trabalhos_relacionados  aos principais problemas enfrentados pelos usuários cegos ao interagir com a Web , assim como os conjuntos de diretrizes de acessibilidade e usabilidade específicos para minimizar esses problemas , fundamentados em um mapeamento sistemático conduzido sobre o tema . Após a identificação dos problemas e diretrizes , foi desenvolvido um classificador baseado no método Naïve Bayes , considerando-se três características comumente encontradas nos padrões de design de menus de navegação , com intuito de classificar automaticamente os links contidos nesta página em diferentes classes referentes às estruturas de navegação . Após desenvolvido , o método foi testado e avaliado em sites governamentais por meio de uma análise estatística , apresentando um boa taxa de acerto em suas classificações . Com base nas dificuldades encontradas na primeira etapa de pesquisa , foi proposto um modelo de interface próprio aos usuários cegos que utilizam a Web com auxílio de um leitor de tela . Com base no classificador desenvolvido , foi proposta uma abordagem para automaticamente transformar os links de navegação classificados pelo método , em um padrão de navegação otimizado aos leitores de tela . Como prova de conceito , foi realizada uma extração semiautomática de conteúdo das páginas_Web  , com intuito de transformar esse conteúdo para o novo modelo de interface proposto . Nesse contexto , foi desenvolvido um protótipo de mecanismo de apoio , denominado NavAux . Esse protótipo foi avaliado através de um percurso cognitivo . Para isso , foram definidas algumas tarefas e a exploração da interface foi realizada com apoio do leitor de tela JAWS . Os resultados apresentaram indícios de que a abordagem proposta pode ser considerada uma alternativa viável para minimizar os problemas de acessibilidade enfrentados pelos usuários cegos ao navegar na Web 
 A World_Wide  Web tem apresentado um constante e intenso crescimento , causando um grande congestionamento . Muitas pesquisas têm sido feitas para garantir suas funcionalidades . A prática de cache na Web é uma das formas mais utilizadas para permitir escalabilidade dos serviços da WWW e melhorar seu desempenho , diminuindo os pontos de contenção e mantendo os objetos mais próximos dos clientes . Todavia , os benefícios de cache são limitados e têm diminuído por causa da grande quantidade e diversidade de características dos objetos Web . Prébusca surge nesse contexto , como uma prática complementar ao uso de caches , antecipando a busca dos objetos que serão requisitados e deixando-os disponíveis em um cache , permitindo a diminuição da latência percebida pelo usuário . O uso de pré-busca na Web dá-se de formas bastante diversas , variando seus resultados quanto à implementação de estratégias , cenários , modelos preditores e parametrização das técnicas . A estratégia de pré-busca com predição estatística mostra-se como a mais promissora das abordagens . Este trabalho de mestrado objetiva avaliar o impacto da pré-busca com predição estatística , utilizando simulação de um modelo preditor baseado no algoritmo Prediction by Partial Matching ( PPM ) , por meio da análise das consequências dos ajustes de parâmetros do modelo adotado . Os resultados obtidos_mostram  que a pré-busca é uma técnica efetiva e valiosa na melhora do desempenho da Web e no incremento da taxa de acerto do cache 
 O OBJETIVO da Engenharia de Requisitos é definir os requisitos que devem ser atendidos por um sistema , modelando seu comportamento . Esses requisitos devem ser obtidos a partir dos diversos elementos ligados ao ambiente no qual o sistema será inserido . Nesse processo , destaca-se a identificação dos objetivos do sistema , na abordagem utilizada pela engenharia de requisitos orientada a objetivos . No contexto do desenvolvimento de software para uma organização , o novo sistema irá interagir com diversas entidades desta e os objetivos do sistema devem ser os próprios objetivos da organização . O sistema também deve respeitar as regras de negócio estabelecidas no ambiente . Além_disso  , após a implantação do sistema , a estrutura dos processos de negócio pode mudar , pois o sistema altera a forma de trabalho na organização . Para a identificação dessas entidades , uma ferramenta bastante útil é a modelagem organizacional . Nos modelos da organização podemos identificar elementos importantes a serem considerados no processo de engenharia de requisitos , como objetivos e regras de negócio , atores e recursos da organização e processos de negócio . Porém , a ligação entre os modelos da organização e o processo de engenharia de requisitos não é simples e direta . E necessário que se estabeleça uma forma de guiar o processo de engenharia de requisitos tomando como base esses modelos . A fim de suprir essa necessidade , este trabalho propõe diretrizes para auxiliar nesse processo . As diretrizes orientam o usuário a obter , nos modelos da organização , as informações relevantes ao processo de engenharia de requisitos e a estabelecer requisitos a partir dessas informações , enfatizando a orientação aos objetivos da organização . Apresenta-se , também , um estudo de caso realizado para se avaliar a aplicação das diretrizes propostas 
 Problemas reais em áreas como aprendizado de máquina têm chamado atenção pela enorme_quantidade  de variáveis ( > 10^6 ) e volume de dados . Em problemas dessa escala o custo para se obter e trabalhar com informações de segunda_ordem  são proibitivos . Tais problemas apresentam características que podem ser aproveitadas por métodos de busca em coordenada . Essa classe de métodos é caracterizada pela alteração de apenas uma ou poucas variáveis a cada iteração . A variante do método comumente descrita na literatura é a minimização cíclica de variáveis . Porém , resultados recentes sugerem que variantes aleatórias do método possuem melhores garantias de convergência . Nessa variante , a cada iteração , a variável a ser alterada é sorteada com uma probabilidade preestabelecida não necessariamente uniforme . Neste trabalho estudamos algumas variações do método de busca em coordenada . São apresentados aspectos teóricos desses métodos , porém focamos nos aspectos práticos de implementação e na comparação experimental entre variações do método de busca em coordenada aplicados a diferentes problemas com aplicações reais 
 Em decorrência do aumento do número de ataques de origem interna , a utilização de mecanismos de proteção , como o firewall , deve ser ampliada . Visto que este tipo de ataque , ocasionado pelos usuários internos ao sistema , não permite a localização imediata , torna-se necessário o uso integrado de diversas tecnologias para aumentar a capacidade de defesa de um sistema . Desta forma , a introdução de agentes_móveis  em apoio a segurança computacional apresenta-se como uma solução natural , uma vez que permitirá a distribuição de tarefas de monitoramento do sistema e automatização do processo de tomada de decisão , no caso de ausência do administrador humano . Este trabalho apresenta uma avaliação do uso do mecanismo de agentes_móveis  para acrescentar características de mobilidade ao processo de monitoração de intrusão em sistemas computacionais . Uma abordagem modular é proposta , onde agentes pequenos e independentes monitoram o sistema . Esta abordagem apresenta significantes vantagens em termos de overhead , escalabilidade e flexibilidade 
 A evolução das tecnologias de sequenciamento de DNA tem permitido a elucidação da sequência genômica de um número cada vez maior de organismos . Contudo , a obtenção da sequência nucleotídica do genoma é apenas a primeira etapa no estudo dos organismos . O processo de anotação consiste na identicação as diferentes regiões de interesse no genoma e suas funcionalidades . Várias ferramentas computacionais foram desenvolvidas para auxiliar o processo de anotação , porém nenhuma delas permite ao usuário selecionar sequências , processá-las de forma a encontrar evidências a respeito das regiões genômicas , como predição gênica e de domínios protéicos , analisá-las gracamente e adicionar informações a respeito de suas regiões em um mesmo ambiente . Assim , o objetivo desse projeto foi o desenvolvimento de uma plataforma gráca para a anotação genômica que permite ao usuário realizar as tarefas necessárias para o processo de anotação em uma única ferramenta integrada a um banco de dados . A idéia é proporcionar ao usuário liberdade para trabalhar com o seu conjunto de dados , possibilitando a seleção de sequências para análise , construção dos pipelines processamento das mesmas e análise dos resultados encontrados a partir de visualizador que permite ao usuário adicionar in- formações às regiões e fazer a curadoria das sequências . A ferramenta resultante é facilmente extensível , permitindo o acoplamento modular de novas_funcionalidades  de anotação e sua estrutura permite ao usuário trabalhar tanto com projetos de sequências expressas como anotação de genomas 
 A biodiversidade das espécies existentes no riquíssimo reino vegetal , tornam os modelos tradicionais de taxonomia uma tarefa muito complexa e morosa , na qual o processo de classificação é tradicionalmente realizado manualmente . As dificuldades presentes nesse processo implicam na existência de poucas pesquisas de classificação vegetal utilizando métodos matemáticos e computacionais . Desta forma , visando contribuir com as técnicas de taxonomia já desenvolvidas , este estudo objetiva desenvolver e testar uma metodologia computacional de identificação de espécies vegetais por meio da análise da textura foliar . Motivado pelo projeto TreeVis , este trabalho realiza uma revisão dos métodos utilizados para análise de textura em imagens_digitais  ( foco concentrado em extração de características e classificação ) , investigando a aplicabilidade de métodos tradicionais como matrizes de coocorrência , técnicas estado da arte como Gabor wavelets e também de novos e promissoras técnicas de análise de textura , como a dimensão_fractal  volumétrica . No contexto de classificação investiga-se métodos para reconhecimento de padrões lineares com base em análise de dados multivariados , não lineares com base na teoria das Redes_Neurais  Artificiais e métodos simples para combinação de diferentes classificadores ( comitê de máquinas ) . Apesar da alta similaridade entre classes e similaridade intraclasses não adequada , os resultados alcançados mostraram-se excelentes . A melhor estratégia de classificação , utilizando comitê de máquinas com descritores de Gabor wavelets/cor e dimensão_fractal  volumétrica/cor , obteve uma probabilidade de acerto global de 96:32 % nas 40 classes estudadas . Esse resultado demonstra como os métodos computacionais de análise de imagens , em especial análise de textura , podem contribuir facilitando e agilizando a tarefa de identificação de espécies 
 Atualmente diversos produtos são fabricados por meio de injeção de polímeros , num processo denominado moldagem por injeção : material fundido é injetado em um molde no qual resfria e endurece . Contudo , ao contrário de outros processos de produção , a qualidade da peça criada por meio de moldagem por injeção não depende apenas do material e da sua forma geométrica , mas também da maneira na qual o material é processado durante a moldagem . Por esse motivo , o uso de modelagem matemática e simulações numéricas tem aumentado consideravelmente como maneira de auxiliar o processo de produção e tem-se tornado uma ferramenta indispensável . Desta forma , este projeto tem o propósito de simular o escoamento de fluidos durante a fase de preenchimento do processo de moldagem por injeção , utilizando o modelo 21/2-dimensional , composto por uma equação bidimensional para a pressão , conhecida como equação de Hele-Shaw , e uma equação tridimensional para a temperatura do fluido . Um modelo bidimensional para a temperatura é também desenvolvido e apresentado . Este projeto de doutorado propõe duas estratégias numéricas para a solução da equação de Hele-Shaw . A primeira delas é baseada em uma formulação euleriana do método Smoothed Particle Hydrodynamics , onde os pontos utilizados na discretização não se movem , e não há utilização de malhas . A segunda estratégia é baseada na criação de malhas dinamicamente construídas na região do molde que já encontra-se parcialmente cheio de fluido e subseqüente aplicação do método Control Volume Finite Element Method . Uma estratégia dinâmica do método semi lagrangeano é apresentada e aplicada à solução da equação bidimensional da temperatura . O projeto também pretende investigar três novas abordagens para o tratamento da superfície_livre  . Duas delas são baseadas na técnica Volume of Fluid e uma delas é uma adaptação meshless do método Front-Tracking . O comportamento não newtoniano do fluido é caracterizado por uma família de modelos de viscosidade . Testes numéricos indicando a confiabilidade das metodologias propostas são 
 Aprendizado supervisionado tem sido principalmente utilizado para classificação . Neste trabalho são mostrados os benefícios do uso de rankings ao invés de classificação de exemplos isolados . Um rankeador é um algoritmo que ordena um conjunto de exemplos de tal modo que eles são apresentados do exemplo de maior para o exemplo de menor expectativa de ser positivo . Um ranking é o resultado dessa ordenação . Normalmente , um ranking é obtido pela ordenação do valor de confiança de classificação dado por um classificador . Este trabalho tem como objetivo procurar por novas abordagens para promover o uso de rankings . Desse modo , inicialmente são apresentados as diferenças e semelhanças entre ranking e classificação , bem como um novo algoritmo de ranking que os obtém diretamente sem a necessidade de obter os valores de confiança de classificação , esse algoritmo é denominado de LEXRANK . Uma área de pesquisa bastante importante em rankings é a análise ROC . O estudo de árvores de decisão e análise ROC é bastante sugestivo para o desenvolvimento de uma visualização da construção da árvore em gráficos ROC . Para mostrar passo a passo essa visualização foi desenvolvido uma sistema denominado PROGROC . Ainda do estudo de análise ROC , foi observado que a inclinação ( coeficiente angular ) dos segmentos que compõem o fecho convexo de curvas ROC é equivalente a razão de verossimilhança que pode ser convertida para probabilidades . Essa conversão é denominada de calibração por fecho convexo de curvas ROC que coincidentemente é equivalente ao algoritmo PAV que implementa regressão isotônica . Esse método de calibração otimiza Brier Score . Ao explorar essa medida foi encontrada uma relação bastante interessante entre Brier Score e curvas ROC . Finalmente , também foram explorados os rankings construídos durante o método de seleção de exemplos do algoritmo de aprendizado semi-supervisionado multi-descrição 
 As técnicas de projeção ou posicionamento de pontos no plano , que servem para mapear dados multi-dimensionais em espaços visuais , sempre despertaram grande interesse da comunidade de visualização e análise de dados por representarem uma forma útil de exploração baseada em relações de similaridade e correlação . Apesar disso , muitos problemas ainda são encontrados em tais técnicas , limitando suas aplicações . Em especial , as técnicas de projeção multi-dimensional de maior qualidade têm custo_computacional  proibitivo para grandes_conjuntos  de dados . Adicionalmente , problemas referentes à escalabilidade visual , isto é , à capacidade da metáfora visual empregada de representar dados de forma compacta e amigável , são recorrentes . Esta tese trata o problema da projeção multi-dimensional de vários pontos de vista , propondo técnicas que resolvem , até certo ponto , cada um dos problemas verificados . Também é fato que a complexidade e o tamanho dos conjuntos de dados indicam que a visualização deve trabalhar em conjunto com técnicas de mineração , tanto embutidas no processo de mapeamento , como por meio de ferramentas auxiliares de interpretação . Nesta tese incorporamos alguns aspectos de mineração integrados ao processo de visualização multi-dimensional , principalmente na aplicação de projeções para visualização de coleções de documentos , propondo uma estratégia de extração de tópicos . Como suporte ao desenvolvimento e teste dessas técnicas , foram criados diferentes sistemas de software . O principal inclui as técnicas desenvolvidas e muitas das técnicas clássicas de projeção , podendo ser usado para exploração de conjuntos de dados multi-dimensionais em geral , com funcionalidade adicional para mapeamento de coleções de documentos . Como principal_contribuição  desta tese propomos um entendimento mais profundo dos problemas encontrados nas técnicas de projeção vigentes e o desenvolvimento de técnicas de projeção ( ou mapeamento ) que são rápidas , tratam adequadamente a formação visual de grupos de dados altamente similares , separam satisfatoriamente esses grupos no layout , e permitem a exploração dos dados em vários níveis de 
 Uma Linha de Produtos de Software ( LPS ) consiste de um conjunto de sistemas de software que compartilham características comuns e satisfazem às necessidades específicas de um segmento particular . Para tornar o processo de instanciação de produtos mais rápido e menos suscetível a erros , o projeto de uma LPS pode adotar a utilização de geradores de aplicação , que podem gerar os artefatos da LPS utilizando uma especificação das variabilidades de um certo produto . Adicionalmente , notase que determinadas características transversais de uma linha de produtos têm potencial de reúso em diferentes domínios , podendo ser implementadas usando a programação orientada a aspectos ( POA ) . Neste trabalho é proposto um processo para o desenvolvimento de LPS e geração automatizada de produtos levando em consideração os interesses_transversais  existentes em cada domínio de aplicação . Os interesses_transversais  são as características comuns espalhadas pelas divisões ou módulos do programa de diferentes domínios . O processo aqui proposto tem a finalidade de aumentar o reúso de características de linhas de produtos por meio da POA , permitindo que as LPSs sejam projetadas de forma mais coesa e , consequentemente , facilitando sua manutenção e evolução . Visando diminuir o esforço necessário para a instanciação dos produtos provenientes dessas linhas de produtos , neste trabalho também é apresentada uma extensão do gerador Captor , denominada Captor-AO . Esse gerador fornece suporte ao processo proposto , permitindo a criação de produtos formados por características de diferentes domínios . Por fim , é apresentado um estudo de caso em que é realizada a configuração de um domínio transversal para o interesse de persistência , a definição de um domínio-base compatível com esse domínio transversal e a geração de produtos formados pelas características de ambos os domínios utilizando o gerador estendido 
 Robôs sociáveis devem ser capazes de interagir , se comunicar , compreender e se relacionar com os seres_humanos  de uma forma natural . Existem diversas motivações práticas e científicas para o desenvolvimento de robôs sociáveis como plataforma de pesquisas , educação e entretenimento . Entretanto , embora diversos robôs sociáveis já tenham sido desenvolvidos com sucesso , ainda existe muito trabalho para aprimorar a sua eficácia . A utilização de uma arquitetura robótica pode reduzir fortemente o esforço requerido para o desenvolvimento de um robô sociável . Tal arquitetura robótica deve possuir estruturas e mecanismos para permitir a interação social , o controle do comportamento e o aprendizagem a partir do ambiente . Tal arquitetura deve ainda possuir estruturas e mecanismos para permitir a percepção e a atenção , a fim de permitir que um robô sociável perceba a riqueza do comportamento humano e do meio ambiente , e para aprender a partir de interações sociais . Os processos de aprendizado evidenciados na Análise do Comportamento podem levar ao desenvolvimento de métodos e estruturas promissoras para a construção de robôs sociáveis capazes de aprender por meio da interação com o meio ambiente e de exibir comportamento social apropriado . O proposito deste trabalho é o desenvolvimento de uma arquitetura robótica inspirada na Análise do Comportamento . A arquitetura desenvolvida é capaz de simular o aprendizado do comportamento operante e os métodos e estruturas propostos permitem o controlo e a exibição de comportamentos sociais apropriados e o aprendizado a partir da interação com o meio ambiente . A arquitetura proposta foi avaliada no contexto de um problema real não trivial : o aprendizado da atenção compartilhada . Os resultados obtidos_mostram  que a arquitetura é capaz de exibir comportamentos apropriados durante uma interação social real e controlada . Ainda , os resultados mostram também que a arquitetura pode aprender a partir de uma interação social . Este trabalho é a base para o desenvolvimento de uma ferramenta para a construção dos robôs sociáveis . Os resultados obtidos abrem muitas oportunidades de trabalhos 
 Em termos de raciocínio probabilístico clássico , para se realizar inferências de uma base de conhecimento , normalmente é necessário garantir a consistência de tal base . Quando nos deparamos com um conjunto de probabilidades que são inconsistentes entre si , interessa-nos saber onde está a inconsistência , quão grave esta é , e como corrigi-la . Medidas de inconsistência têm sido recentemente propostas como uma ferramenta para endereçar essas questões na comunidade de Inteligência_Artificial  . Este trabalho_investiga  o problema da medição de inconsistência em bases de conhecimento probabilístico . Postulados básicos de racionalidade têm guiado a formulação de medidas de inconsistência na lógica_clássica  proposicional . No caso probabilístico , o carácter quantitativo da probabilidade levou a uma propriedade desejável adicional : medidas de inconsistência devem ser contínuas . Para atender a essa exigência , a inconsistência em bases de conhecimento probabilístico tem sido medida através da minimização de distâncias . Nesta tese , demonstramos que o postulado da continuidade é incompatível com propriedades desejáveis herdadas da lógica_clássica  . Como algumas dessas propriedades são baseadas em conjuntos inconsistentes minimais , nós procuramos por maneiras mais adequadas de localizar a inconsistência em lógica probabilística , analisando os processos de consolidação subjacentes . A teoria AGM de revisão de crenças é estendida para englobar a consolidação pelo ajuste de probabilidades . As novas formas de caracterizar a inconsistência que propomos são empregadas para enfraquecer alguns postulados , restaurando a compatibilidade de todo o conjunto de propriedades desejáveis . Investigações em estatística Bayesiana e em epistemologia formal têm se interessado pela medição do grau de incoerência de um agente . Nesses campos , probabilidades são geralmente interpretadas como graus de crença de um agente , determinando seu comportamento em apostas . Agentes incoerentes possuem graus de crença inconsistentes , que o expõem a transações de apostas desvantajosas - conhecidas como Dutch books . Estatísticos e filósofos sugerem medir a incoerência de um agente através do prejuízo garantido a qual ele está vulnerável . Nós provamos que estas medidas de incoerência via Dutch books são equivalentes a medidas de inconsistência via minimização de distâncias da comunidade de IA 
 A crise mundial de 2008 impulsionou o avanço das políticas de governança do sistema financeiro global . Parte dessas políticas envolve a reformulação de processos de gerenciamento de informações , e neste cenário de reestruturação tecnológica , várias iniciativas se propõem a solucionar alguns dos problemas já conhecidos . Para viabilizar a adoção de um sistema financeiro global integrado e robusto , grandes empresas de tecnologia e instituições financeiras somam esforços para atender melhor às necessidades do setor . A arquitetura da World_Wide  Web é uma constante nessas iniciativas , e parte delas buscam os benefícios previstos pelas tecnologias semânticas , tais como sua alta capacidade de integração de dados heterogêneos e utilização de algoritmos de inferência para a dedução de informações . O objetivo deste estudo é utilizar ontologias e tecnologias semânticas , tais como OWL , na gestão de riscos do sistema financeiro , particularmente para verificar a sua aplicabilidade nas políticas de gestão de riscos específicas do sistema financeiro brasileiro , estabelecidas pelas normas publicadas pelo Banco Central ( BACEN ) 
 A mecânica celular jaz nas propriedades materiais da membrana plasmática , fundamentalmente uma bicamada fosfolipídica com espessura de dimensões moleculares . Além de forças elásticas , tal material bidimensional também experimenta tensões viscosas devido ao seu comportamento fluido ( presumivelmente newtoniano ) na direção tangencial . A despeito da notável concordância entre teoria e experimentos biofísicos sobre a geometria de membranas celulares , ainda não se faz presente um método computacional para simulação de sua ( real ) dinâmica viscosa governada pela lei de Boussinesq-Scriven . Assim sendo , introduzimos uma formulação variacional mista de três campos para escoamentos viscosos de superfícies fechadas curvas . Nela , o fluido circundante é levado em conta considerando-se uma restrição de volume interior , ao passo que uma restrição de área corresponde à inextensibilidade . As incógnitas são a velocidade , o vetor curvatura e a pressão superficial , todas estas interpoladas com elementos finitos lineares contínuos via estabilização baseada na projeção do gradiente de pressão . O método é semi-implícito e requer a solução de apenas um único sistema linear por passo de tempo . Outro ingrediente numérico proposto é uma força que mimetiza a ação de uma pinça óptica , permitindo interação virtual com a membrana , onde a qualidade e o refinamento de malha são mantidos por remalhagem adaptativa automática . Extensivos experimentos numéricos de dinâmica de relaxação são apresentados e comparados com soluções quasi-analíticas . É observada estabilidade temporal condicional com uma restrição de passo de tempo que escala como o quadrado do tamanho de malha . Reportamos a convergência e os limites de estabilidade de nosso método e sua habilidade em predizer corretamente o equilíbrio dinâmico de compridas e finas elongações cilíndricas ( tethers ) que surgem a partir de pinçamentos membranais . A dependência de forma membranal com respeito a uma velocidade imposta de pinçamento também é discutida , sendo que há um valor limiar de velocidade abaixo do qual um tether não se forma de início . Testes adicionais ilustram a robustez do método e a relevância dos efeitos viscosos membranais quando sob a ação de forças externas . Sem dúvida , ainda há um longo caminho a ser trilhado para o entendimento completo da mecânica celular ( há de serem consideradas outras estruturas tais como citoesqueleto , canais iônicos , proteínas transmembranares , etc ) . O primeiro passo , porém , foi dado : a construção de um esquema numérico variacional capaz de simular a intrigante dinâmica das membranas celulares 
 Escoamentos sobre superfícies côncavas , como os que ocorrem no intradorso de uma pá de turbina , estão sujeitos à instabilidade centrífuga . A esse tipo de configuração atribui-se possibilidade de transição à turbulência devido a formação dos vórtices de Görtler . Estudos são propostos no sentido de identificar possível influência do gradiente de pressão nos mecanismos de desenvolvimento desses vórtices e sua interação com outras perturbações na transição . O processo de investigação dá-se numericamente por meio do desenvolvimento e uso de um código numérico paralelizado e de alta ordem de precisão . Resultados obtidos caracterizam o gradiente de pressão adverso como mais instável se comparado ao caso neutro ou favorável . Variações no gradiente de pressão não se mostram eficientes no processo de controle da instabilidade . Ao gradiente adverso atribui-se antecipação da região de saturação dos vórtices . Ressalta-se ainda a natureza desestabilizadora do gradiente adverso quanto aos mecanismos de amplificação dos modos varicoso e sinuoso associados à instabilidade secundária 
 Sistemas baseados em captura de experiências cotidianas têm sido desenvolvidos como forma de apoiar o fato de que as pessoas não conseguem assimilar todas as informações a que são expostas , pelos mais diversos fatores : interrupções , necessidade de anotação e sobrecarga cognitiva . No entanto , esses sistemas em sua maioria não oferecem suporte às funções de autoria interativa de informação  preparação a priori e modificação ou extensão a posteriori das informações  devido à ausência de um modelo de referência para esse suporte . O trabalho aqui apresentado consiste na proposta de um modelo aberto de suporte à autoria de informação multimídia evolucionária , e na implementação de um sistema aberto baseado nesse modelo que suporta a autoria interativa de informação e que pode ser integrado a sistemas de captura 
 Representações computacionais de formas podem ser criadas em ferramentas CAD ou geradas a partir de um objeto físico já existente . Esta última abordagem oferece como vantagens rapidez e fidelidade ao objeto original , que são os aspectos fundamentais em muitas aplicações , como Simulações Numéricas de Equações Diferenciais Parciais e Imagens Médicas . A reconstrução ( ou geração de malhas superficiais ) a partir de pontos amostrados de uma superfície de um objeto é um problema clássico de representação de formas . Nesta dissertação apresentamos um vasto levantamento bibliográfico deste tipo de reconstrução , classificando e descrevendo os principais trabalhos presentes na literatura . A partir do levantamento bibliográfico , selecionamos um conjunto de algoritmos sobre os quais foram realizadas comparações teóricas e empíricas cujos resultados são apresentados . Para finalizar , apresentamos aplicações de nossas implementações em Simulação Numérica de Equações Diferenciais Parciais e processamento de 
 Recentemente , Barak et_al  . ( 2004 ) exibiram construções de extratores e dispersores determinísticos ( funções computáveis em tempo polinomial ) com parâmetros melhores do que era anteriormente possível . Introduziremos os conceitos envolvidos em tal trabalho e mencionaremos suas aplicações ; em particular , veremos como é possível_obter  cotas muito melhores para o problema Ramsey bipartido ( um problema bem difícil ) utilizando as construções descritas no artigo . Também apresentamos resultados originais para melhorar tais construções . Tais idéias são inspiradas no trabalho de Anup Rao ( 2005 ) e utilizam o recente êxito de Jean Bourgain ( 2005 ) em obter extratores que quebram a `` barreira 1/2 '' 
 Este texto apresenta nosso projeto de implementação de um arcabouço de suporte a agentes_móveis  dentro de um ambiente de grade denominado InteGrade . Nosso arcabouço - MobiGrid - foi criado de forma a permitir que aplicações seqüenciais longas possam ser executadas em uma rede de estações de trabalho pessoais . Os agentes_móveis  são utilizados para encapsular essas aplicações com longo tempo de processamento . O encapsulamento de uma aplicação com longo tempo de processamento dentro de um agente móvel é o que denominamos como tarefa . Sendo_assim  , as tarefas podem migrar sempre que a máquina é requisitada por seu usuário local , já que são providas com capacidade de migração automática . Nosso arcabouço também fornece ao usuário um gerente que rastreia as tarefas por ele submetidas . Baseados no ambiente de execução de tarefas descrito , criamos um modelo matemático para efetuarmos simulações de como se comportariam muitas tarefas submetidas a uma grade com grande quantidade de estações de trabalho . Neste trabalho apresentamos também esse modelo , bem como os resultados das simulações nele baseadas 
 Nesta tese apresentamos o projeto e a implementação do KEMS , um provador de teoremas multi-estratégia baseado no método de tablôs KE . Um provador de teoremas multi-estratégia é um provador de teoremas onde podemos variar as estratégias utilizadas sem modificar o núcleo da implementação . Além de multi-estratégia , o KEMS é capaz de provar teoremas em três sistemas lógicos : lógica_clássica  proposicional , mbC e mCi . Listamos abaixo algumas das contribuições deste trabalho : * um sistema KE para mbC que é analítico , correto e completo ; * um sistema KE para mCi que é correto e completo ; * um provador de teoremas multi-estratégia com as seguintes características : - aceita problemas em três sistemas lógicos : lógica_clássica  proposicional , mbC e mCi ; - tem seis estratégias implementadas para lógica_clássica  proposicional , duas para mbC e duas para mCi ; - tem treze ordenadores que são usados em conjunto com as estratégias ; - implementa regras simplificadoras para lógica_clássica  proposicional ; - possui uma interface_gráfica  que permite a visualização de provas ; - é de código_aberto  e está disponível na Internet em http : //kems.iv.fapesp.br ; * benchmarks obtidos através da comparação das estratégias para lógica_clássica  proposicional resolvendo várias famílias de problemas ; - sete famílias de problemas para avaliar provadores de teoremas paraconsistentes ; * os primeiros benchmarks para as famílias de problemas para avaliar provadores de teoremas paraconsistentes 
 Uma das tarefas mais básicas em bioinformática é a comparação de seqüências feita por algoritmos de alinhamento , que modelam as alterações evolutivas nas seqüências biológicas através de mutações como inserção , remoção e substituição de símbolos . Este trabalho trata de generalizações nos algoritmos de alinhamento que levam em consideração outras mutações conhecidas como rearranjos , mais especificamente , inversões , duplicações em tandem e duplicações por transposição . Alinhamento com inversões não tem um algoritmo polinomial conhecido e uma simplificação para o problema que considera somente inversões não sobrepostas foi proposta em 1992 por Schöniger e Waterman . Em 2003 , dois trabalhos independentes propuseram algoritmos com tempo O ( n^4 ) para alinhar duas seqüências com inversões não sobrepostas . Desenvolvemos dois algoritmos que resolvem este mesmo problema : um com tempo de execução O ( n^3 logn ) e outro que , sob algumas condições no sistema de pontuação , tem tempo de execução O ( n^3 ) , ambos em memória O ( n^2 ) . Em 1997 , Benson propôs um modelo de alinhamento que reconhecesse as duplicações em tandem além das inserções , remoções e substituições . Ele propôs dois algoritmos exatos para alinhar duas seqüências com duplicações em tandem : um em tempo O ( n^5 ) e memória O ( n^2 ) , e outro em tempo O ( n^4 ) e memória O ( n^3 ) . Propomos um algoritmo para alinhar duas seqüências com duplicações em tandem em tempo O ( n^3 ) e memória O ( n^2 ) . Propomos também um algoritmo para alinhar duas seqüências com transposons ( um tipo mais geral que a duplicação em tandem ) , em tempo O ( n^3 ) e memória O ( n^2 ) 
 Recentes avanços proporcionaram às grades_computacionais  um bom nível de maturidade . Esses sistemas têm sido implantados em ambientes de produção de qualidade na comunidade de pesquisa acadêmica e vêm despertando um grande interesse da indústria . Entretanto , desenvolver aplicações para essas infra-estruturas heterogêneas e distribuídas ainda é uma tarefa complexa e propensa a erros . As iniciativas de facilitar essa tarefa resultaram , na maioria dos casos , em ferramentas não integradas e baseadas em características específicas de cada grade computacional . O presente_trabalho  tem como objetivo minimizar a dificuldade de desenvolvimento de aplicações para a grade através da construção de um ambiente integrado e extensível de desenvolvimento ( IDE ) para computação em grade chamado InGriDE . O InGriDE fornece um conjunto único de ferramentas compatíveis com diferentes sistemas de middleware , desenvolvidas baseadas na interface de programação Grid Application Toolkit ( GAT ) . O conjunto de funcionalidades do InGriDE foi desenvolvido com base na plataforma Eclipse que , além de fornecer um arcabouço para construção de IDEs , facilita a extensão do conjunto inicial de funcionalidades . Para validar a nossa solução , utilizamos em nosso estudo de caso o middleware InteGrade , desenvolvido no nosso grupo de pesquisa . Os resultados obtidos nesse trabalho mostraram a viabilidade de fornecer independência de middleware para IDEs através do uso de uma interface genérica de programação como o GAT . Além_disso  , os benefícios obtidos com o uso do Eclipse como arcabouço para construção de IDEs indicam que os recursos fornecidos por esse tipo de arcabouço atendem de forma eficiente as necessidades inerentes ao processo de desenvolvimento de aplicações para a grade 
 No sistema brasileiro de saúde , cabe aos Centros de Saúde o papel de órgão provedor de assistência médica primária . Para que esse papel seja cumprido com responsabilidade e eficácia , se mostrou fundamental a condução de programas públicos de atenção primária que envolvam visitas domiciliares aos pacientes . O objetivo desses programas , tais como o Estratégia de Saúde da Família ( ESF ) , também conhecido como Programa de Saúde da Família ( PSF ) , é o de melhorar a qualidade do serviço de saúde prestado à população por meio da aproximação entre equipes de saúde e a comunidade , permitindo , dessa forma , uma migração do paradigma de tratamento de doenças para o de promoção da saúde . No entanto , apesar da importância desses programas para a organização e articulação do sistema de atenção primária , as atividades de atenção domiciliar são normalmente conduzidas com pouco ou nenhum suporte de Tecnologia da Informação ( TI ) . Esta pesquisa de mestrado tem por objetivo mostrar a definição e o desenvolvimento de um sistema móvel que auxilie os profissionais de saúde na coleta de informações dos pacientes que usufruem dos serviços de saúde citados acima . O projeto recebeu o nome de Projeto Borboleta e durante o tempo desta pesquisa várias versões do software foram desenvolvidas . Essas versões geraram protótipos do sistema que foram submetidos a testes em campo e , a partir da avaliação realizada pelos profissionais de saúde , surgiram alterações diversas 
 Este trabalho apresenta uma plataforma para coleta e análise de imagens urbanas , que integra Interfaces de Programação de Aplicativos `` Application Programming Interfaces '' ( APIs ) de sistemas de busca de imagens , Sistemas de Informações Geográficas ( SIGs ) , mapas digitais e técnicas de visão_computacional  . Esta plataforma , INACITY , permite que usuários selecionem regiões de interesse e capturem elementos de relevância para a arquitetura urbana , como , por exemplo árvores e buracos em ruas . A implementação da plataforma foi feita de maneira a permitir que novos módulos possam ser facilmente incluídos ou substituídos possibilitando a introdução de outras APIs de mapas , SIGs e filtros de Visão Computacional . Foram realizados_experimentos  com as imagens obtidas_através  do `` Google Street View '' onde árvores são capturadas em áreas de bairros inteiros em questão de minutos , um ganho significativo quando comparado com o procedimento manual para levantamento deste tipo de dado . Além_disso  , também são apresentados resultados comparativos entre os métodos de visão_computacional  propostos para a detecção de árvores em imagens com outros métodos heurísticos , em um conjunto onde as árvores estão marcadas manualmente e assim as taxas de precisão e de redescoberta de cada algoritmo podem ser avaliadas e comparadas 
 Nesta dissertação estudamos --_-  do ponto de vista algorítmico --_-  o seguinte problema , conhecido como problema da partição conexa balanceada . Dado um grafo conexo G com pesos atribuídos a seus vértices , e um inteiro q > = 2 , encontrar uma partição dos vértices de G em q classes , de forma que cada classe da partição induza um grafo conexo e que , ao considerar as somas dos pesos dos vértices de cada classe , a menor das somas seja o maior possível . Em outras_palavras  , o objetivo é encontrar q classes cujos pesos sejam tão balanceados quanto possível . Sabe-se que este problema é NP-difícil . Mencionamos alguns resultados sobre complexidade computacional e algoritmos que são conhecidos para este problema . Apresentamos algumas heurísticas que desenvolvemos , todas elas baseadas no uso do algoritmo polinomial para árvores , devido a Perl e Schach , que apresentamos com detalhe . Implementamos quatro heurísticas e um algoritmo de 3/4-aproximação conhecido para o caso q=2 . Exibimos os resultados obtidos com os vários testes computacionais conduzidos com instâncias aleatórias , com grafos de diferentes pesos e densidades . Os resultados computacionais indicam que o desempenho dessas heurísticas --_-  todas elas polinomiais --_-  é bem satisfatório . No caso especial em que q=2 , observamos que a heurística mais onerosa sistematicamente produziu soluções melhores ou iguais às do algoritmo de 
 Uma maneira de resolver problemas de programação linear de grande escala é explorar a relaxação lagrangeana das restrições `` difíceis '' e utilizar métodos de subgradientes . Populares por fornecerem rapidamente boas aproximações de soluções duais , eles não produzem diretamente as soluções primais . Para obtê-las com custo_computacional  adequado , pode-se construir seqüências ergódicas ou utilizar uma técnica proposta recentemente , denominada algoritmo do volume . As propriedades teóricas de convergência não foram bem estabelecidas nesse algoritmo , mas pequenas modificações permitem a demonstração da convergência dual . Destacam-se como adaptações o algoritmo do volume revisado , um método de feixes específico , e o algoritmo do volume incorporado ao método de variação do alvo . Este trabalho foi baseado no estudo desses algoritmos e de todos os conceitos envolvidos , em especial , análise convexa e otimização não diferenciável . Estudamos as principais diferenças teóricas desses métodos e realizamos comparações numéricas com problemas lineares e lineares inteiros , em particular , o corte máximo em grafos 
 Mineração de dados tem sido cada vez mais aplicada em distintas áreas com o objetivo de extrair conhecimento interessante e relevante de grandes_conjuntos  de dados . Nesse contexto , aprendizado de máquina fornece alguns dos principais métodos utilizados em mineração de dados . Dentre os métodos empregados em aprendizado de máquina destacam-se os simbólicos que possuem como principal_contribuição  a interpretabilidade . Entretanto , os métodos de aprendizado de máquina tradicionais , como árvores e regras de decisão , não consideram a informação temporal presente nesses dados . Este trabalho propõe uma metodologia para extração de conhecimento de séries_temporais  por meio da extração de características e da identificação de motifs . Características e motifs são utilizados como atributos para a extração de conhecimento por métodos de aprendizado de máquina . Essa metodologia foi avaliada utilizando conjuntos de dados conhecidos na área . Foi realizada uma análise comparativa entre a metodologia e a aplicação direta de métodos de aprendizado de máquina sobre as séries_temporais  . Os resultados mostram que existe diferença estatística significativa para a maioria dos conjuntos de dados avaliados . Finalmente , foi realizado um estudo de caso preliminar referente ao monitoramento ambiental do reservatório da Usina Hidrelétrica Itaipu Binacional . Nesse estudo somente a identificação de motifs foi aplicada . Foram utilizadas séries_temporais  referentes à temperatura da água coletadas em distintas regiões do reservatório . Nesse estudo observou-se a existência de um padrão na distribuição dos motifs identificados para cada região do reservatório , corroborando com resultados consagrados na 
 Schistosoma mansoni é o parasita causador da esquistossomose mansônica que , de acordo com o Ministério da Saúde do Brasil , afeta atualmente vários milhões de pessoas no país . Uma das formas de diagnóstico da esquistossomose é a detecção de ovos do parasita através da análise de lâminas microscópicas com material fecal . Esta tarefa é extremamente cansativa , principalmente nos casos de baixa endemicidade , pois a quantidade de ovos é muito pequena . Nesses casos , uma abordagem computacional para auxílio na detecção de ovos facilitaria o trabalho de diagnóstico . Os ovos têm formato ovalado , possuem uma membrana translúcida , apresentam uma espícula e sua cor é ligeiramente amarelada . Porém nem todas essas características são observadas em todos os ovos e algumas delas são visíveis apenas com uma ampliação adequada . Além_disso  , o aspecto visual do material fecal varia muito de indivíduo para indivíduo em termos de cor e presença de diversos artefatos ( tais como partículas que não são desintegradas pelo sistema digestivo ) , tornando difícil a tarefa de detecção dos ovos . Neste trabalho investigamos , em particular , o problema de detecção das linhas que contornam a borda de vários dos ovos . Propomos um método composto por duas fases . A primeira fase consiste na detecção de estruturas do tipo linha usando operadores morfológicos . A detecção de linhas é dividida em três etapas principais : ( i ) realce de linhas , ( ii ) detecção de linhas , e ( iii ) refinamento do resultado para eliminar segmentos de linhas que não são de interesse . O resultado dessa fase é um conjunto de segmentos de linhas . A segunda fase consiste na detecção de subconjuntos de segmentos de linha dispostos em formato elíptico , usando um algoritmo baseado na transformada Hough . As elipses detectadas são fortes candidatas a contorno de ovos de S. mansoni . Resultados experimentais_mostram  que a abordagem proposta pode ser útil para compor um sistema de auxílio à detecção dos ovos 
 Um dos modelos mais usados para descrever problemas de planejamento probabilístico , i.e. , planejamento de ações com efeitos probabilísticos , é o processo de decisão markoviano ( Markov Decision Process - MDP ) . Soluções tradicionais são baseadas em programação_dinâmica  , sendo as mais ecientes aquelas baseadas em programação_dinâmica  em tempo real ( Real-Time Dynamic Programming - RTDP ) , por explorarem somente os estados alcançáveis a partir de um dado estado inicial . Por outro_lado  , existem soluções ecientes baseadas em métodos de busca heurística em um grafo AND/OR , sendo que os nós AND representam os efeitos probabilísticos das ações e os nós OR representam as escolhas de ações alternativas . Tais soluções também exploram somente estados alcançáveis a partir de um estado inicial porém , guardam um subgrafo solução parcial e usam programação_dinâmica  para a atualização do custo dos nós desse subgrafo . No entanto , problemas com grandes espaços de estados limitam o uso prático desses métodos . MDPs fatorados permitem explorar a estrutura do problema , representando MDPs muito grandes de maneira compacta e assim , favorecer a escalabilidade das soluções . Neste trabalho , apresentamos uma análise comparativa das diferentes soluções para MDPs , com ênfase naquelas que fazem busca heurística e as comparamos com soluções baseadas em programação_dinâmica  assíncrona , consideradas o estado da arte das soluções de MPDs . Além_disso  , propomos um novo algoritmo de busca heurística para MDPs fatorados baseado no algoritmo ILAO* e o testamos nos problemas da competição de planejamento probabilístico IPPC-2011 
 A conversão de voz é um problema emergente em processamento de fala e voz com um crescente_interesse  comercial , tanto em aplicações como Tradução Fala para Fala ( Speech-to-Speech Translation - SST ) e em sistemas Text-To-Speech ( TTS ) personalizados . Um sistema de Conversão de Voz deve permitir o mapeamento de características acústicas de sentenças pronunciadas por um falante origem para valores correspondentes da voz do falante destino , de modo que a saída processada é percebida como uma sentença pronunciada pelo falante destino . Nas últimas duas décadas , o número de contribuições cientícas relacionadas ao problema de conversão de voz tem crescido consideravelmente , e um panorama sólido do processo histórico , assim como de técnicas propostas são indispensáveis para contribuição neste campo . O objetivo deste trabalho é realizar um levantamento geral das técnicas utilizadas para resolver o problema , apontando vantagens e desvantagens de cada método , e a partir deste estudo , desenvolver novas ferramentas . Dentre as contribuições do trabalho , foram desenvolvidos um método para decomposição espectral em termos de bases radiais , mapas fonéticos articiais , agrupamentos k-verossímeis , funções de empenamento em frequência entre outras , com o intuito de implementar um sistema de conversão de voz inter-linguístico independente de texto de alta qualidade 
 O crescente aumento no volume de dados complexos tem se tornado um desafio para pesquisadores . Séries temporais são um tipo de dados complexos que tem tido um crescimento em sua relevância , devido a sua importância para o monitoramento e acompanhamento de safras agrícolas . Assim , a mineração de informação a partir de grandes volumes de séries_temporais  para o apoio a tomada de decisões tem se tornado uma atividade valiosa . Uma das atividades importantes na mineração em séries_temporais  é a descoberta de padrões frequentes . Entretanto , a complexidade dessa atividade requer métodos rápidos e eficientes . Nesse contexto , esta dissertação de mestrado apresenta propostas para novos algoritmos e métodos para minerar e indexar séries_temporais  . Uma das propostas dessa dissertação é o índice Telesto , que utiliza uma estrutura baseada em árvores de sufixo generalizada para recuperar séries_temporais  em uma base de dados de séries_temporais  de modo rápido e eficiente . Outra proposta dessa dissertação é o algoritmo TrieMotif , que se baseia em uma trie para eliminar comparações desnecessárias entre subsequências , agilizando o processo de mineração de padrões frequentes em séries_temporais  . Os algoritmos propostos foram utilizados para a análise de dados climáticos e agrometeorológicos . Os resultados apresentados nessa dissertação de mestrado mostram que os algoritmos são escaláveis , podendo ser utilizados para grandes volumes de 
 Dada a grande quantidade de conteúdo criado por usuários na Web , uma proposta para ajudar na busca e organização é a criação de sistemas de anotações ( tagging systems ) , normalmente na forma de palavras-chave , extraídas do próprio conteúdo ou sugeridas por visitantes . Esse trabalho aplica um algoritmo de mineração de dados em um banco de dados RDF , contendo instâncias que podem fazer referências à rede Linked Data do DBpedia , para recomendação de tags utilizando as medidas de similaridade taxonômica , relacional e literal de descrições RDF . O banco de dados utilizado é o Arquigrafia , um sistema de banco de dados na Web cujo_objetivo  é catalogar imagens de projetos arquitetônicos , e que permite que visitantes adicionem tags às imagens . Foram realizados_experimentos  para a avaliação da qualidade das recomendações de tags realizadas considerando diferentes modelos do Arquigrafia incluindo o modelo estendido do Arquigrafia que faz referências ao DBpedia . Os resultados mostram que a qualidade da recomendação de determinadas tags pode melhorar quando consideramos diferentes modelos ( com referências à rede Linked Data do DBpedia ) na fase de aprendizado 
 Com o avanço da tecnologia , grandes volumes de dados estão sendo coletados e acumulados numa velocidade espantosa . Data_Mining  constitui um campo de pesquisa recente em Inteligência_Artificial  , cujo_objetivo  é extrair conhecimento de grandes bases de dados . Um dos tópicos tratados em Data_Mining  para extrair conhecimento é o uso de algoritmos de Aprendizado de Máquina em grandes volumes de dados 
 Alguns algoritmos de Aprendizado de Máquina são capazes de criar generalizações , ou descrever conceitos , a partir de um conjunto de dados previamente rotulados . Esses algoritmos são conhecidos como indutores e são capazes de induzir uma hipótese ( ou classificador ) . Um classificador pode estar descrito sob uma forma simbólica explícita , e assim , apresentar uma explicação do conceito aprendido de forma inteligível ao ser humano . Uma dessas formas de representação simbólica explícita são as regras de conhecimento 
 Especialmente em Data_Mining  , o volume de regras de conhecimento que descrevem um classificador simbólico pode ser muito grande . Isso dificulta muito a análise de regras individuais ou de um grupo de regras por parte do usuário desse conhecimento . No intuito de propor uma solução para essa dificuldade , a análise automática de regras , utilizando medidas de avaliação e de interessabilidade , destaca-se como uma das fontes de resultados positivos da aplicação do Aprendizado de Máquina na área de Data_Mining  
 Neste trabalho é apresentado o RuleSystem , um sistema computacional protótipo que implementa funcionalidades voltadas para Aprendizado de Máquina e Data_Mining  . Uma dessas funcionalidades , implementadas no RuleSystem , refere-se à análise automática de regras . O Módulo de Análise de regras , proposto neste trabalho , implementa diversas medidas de avaliação e de interessabilidade de regras , permitindo assim realizar uma análise tanto quantitativa quanto qualitativa das regras que constituem a ( s ) hipótese ( s ) induzida ( s ) por algoritmos de Aprendizado de Maquina simbólico 
 Uma investigação sobre como a programação orientada a aspectos combinada com a tecnologia de componentes pode encapsular os interesses_transversais  de um sistema é apresentada . Como resultado desta investigação , um método para o desenvolvimento de software baseado em componentes e aspectos é proposto , cujas etapas , atividades e artefatos são mostrados por meio do exemplo de um Sistema de Reservas de Hotéis , juntamente com sua implementação nas linguagens JAsCO e AspectJ . O método é uma extensão do método UML Components e utiliza a UML com algumas adaptações . Além_disso  , uma estratégia de generalização e documentação de componentes transversais para que possam ser reusados em outras aplicações é proposta . Adicionalmente , o método proposto foi usado para o projeto de um Sistema de Locação de Carros e o resultado foi comparado com uma outra solução para o mesmo problema , baseada em UML Components e uma arquitetura geral para sistemas na Web . O resultado dessa comparação é apresentado e discutido 
 Planejamento probabilístico lida com a tomada de decisão sequencial em ambientes estocásticos e geralmente é modelado por um Processo de Decisão Markoviano ( Markovian Decision Process - MDP ) . Um MDP modela a interação entre um agente e o seu ambiente : em cada estágio , o agente decide executar uma ação , com efeitos probabilísticos e um certo custo , que irá produzir um estado futuro . O objetivo do agente MDP é minimizar o custo esperado ao longo de uma sequência de escolhas de ação . O número de estágios que o agente atua no ambiente é chamado de horizonte , o qual pode ser finito , infinito ou indefinido . Um exemplo de MDP com horizonte indefinido é o Stochastic Shortest Path MDP ( SSP MDP ) , que estende a definição de MDP adicionando um conjunto de estados meta ( o agente para de agir ao alcançar um estado meta ) . Num SSP MDP é feita a suposição de que é sempre possível alcançar um estado meta a partir de qualquer estado do mundo . No entanto , essa é uma suposição muito forte e que não pode ser garantida em aplicações práticas . Estados a partir dos quais é impossível atingir a meta são chamados de becos-sem-saída . Um beco-sem-saída pode ser evitável ou inevitável ( se nenhuma política leva do estado inicial para a meta com probabilidade um ) . Em trabalhos recentes foram propostas extensões para SSP MDP que permitem a existência de diferentes_tipos  de beco-sem-saída , bem como algoritmos para resolvê-los . No entanto , a detecção de becos-sem-saída é feita utilizando : ( i ) heurísticas que podem falhar para becos-sem-saída implícitos ou ( ii ) métodos mais confiáveis , mas que demandam alto custo_computacional  . Neste projeto fazemos uma caracterização formal de modelos de planejamento probabilístico com becos-sem-saída . Além_disso  , propomos uma nova_técnica  para detecção de becos-sem-saída baseada nessa caracterização e adaptamos algoritmos de planejamento probabilístico para utilizarem esse novo método de detecção . Os resultados empíricos mostram que o método proposto é capaz de detectar todos os becos-sem-saída de um dado conjunto de estados e , quando usado com planejadores probabilísticos , pode tornar esses planejadores mais eficientes em domínios com becos-sem-saída difíceis de serem 
 Este trabalho propõe um módulo básico de aquisição e pré-processamento de imagem colorida aplicado a robôs_móveis  , implementado em hardware_reconfigurável  , dentro do conceito de sistemas SoC ( System-on-a-Chip ) . O módulo básico é apresentado em conjunto com funções mais específicas de pré-processamento de imagem , que são utilizadas como base para a verificação das funcionalidades implementadas no trabalho proposto . As principais funções realizadas pelo módulo básico são : montagem de frames a partir dos pixels obtidos da câmera digital CMOS , controle dos diversos parâmetros de configuração da câmera e conversão de padrões de cores . Já as funções mais específicas abordam as etapas de segmentação , centralização , redução e interpretação das imagens adquiridas . O tipo de dispositivo reconfigurável utilizado neste trabalho é o FPGA ( Field-Programmable Gate Array ) , que permite maior adequação das funções específicas às necessidades das aplicações , tendo sempre como base o módulo proposto . O sistema foi aplicado para reconhecer gestos e obteve a taxa 99,57 % de acerto operando a 31,88 frames por segundo 
 Toda medida , desde que feita por um instrumento real , tem uma imprecisão associada . Neste trabalho , abordamos a questão das imprecisões em experimentos de microarranjos de cDNA de dois canais , uma tecnologia que tem sido muito explorada nos últimos_anos  e que ainda é um importante auxiliar nos estudos de expressões gênicas . Dezenas de milhares de representantes de genes são impressos em uma lâmina de vidro e hibridizados simultaneamente com RNA mensageiro de duas amostras diferentes de células . Essas amostras são marcadas com corantes fluorescentes diferentes e a lâmina , após a hibridização , é digitalizada , obtendo-se duas imagens . As imagens são analisadas com programas especiais que segmentam os locais que estavam os genes e extraem estatísticas dos píxeis de cada local . Por exemplo , a média , a mediana e a variância das intensidades do conjunto de píxeis de cada local ( o mesmo é feito normalmente para uma área em volta de cada local , chamada de fundo ) . Estimadores estatísticos como o da variância nos dão uma estimativa de quão precisa é uma certa medida . Uma vez de posse das estimativas das intensidades de cada local , para se obter a efetiva expressão de um gene , algumas transformações são feitas nos dados de forma a eliminar variabilidades sistemáticas . Neste trabalho , mostramos como podem ser feitas as análises a partir de uma medida de expressão_gênica  com um erro estimado . Mostramos como estimar essa imprecisão e estudamos , em termos de propagação da imprecisão , os efeitos de algumas transformações realizadas nos dados , por exemplo , a remoção do viés estimado pelo método de regressão local robusta , mais conhecido como \textit { lowess } . Uma vez obtidas as estimativas das imprecisões propagadas , mostramos também como utilizá-las na determinação dos genes diferencialmente expressos entre as amostras estudadas . Por fim , comparamos os resultados com os obtidos por formas clássicas de análise , em que são desconsideradas as imprecisões das medidas . Concluímos que a modelagem das imprecisões das medidas pode favorecer as análises , já que os resultados obtidos em uma aplicação com dados reais de expressões gênicas foram condizentes com os que encontramos na literatura 
 A área de detecção de outliers ( ou detecção de anomalias ) possui um papel fundamental na descoberta de padrões em dados que podem ser considerados excepcionais sob alguma perspectiva . Detectar tais padrões é relevante de maneira geral porque , em muitas aplicações de mineração de dados , tais padrões representam comportamentos extraordinários que merecem uma atenção especial . Uma importante distinção se dá entre as técnicas supervisionadas e não supervisionadas de detecção . O presente projeto enfoca as técnicas de detecção não supervisionadas . Existem dezenas de algoritmos desta categoria na literatura e novos algoritmos são propostos de tempos em tempos , porém cada um deles utiliza uma abordagem própria do que deve ser considerado um outlier ou não , que é um conceito subjetivo no contexto não supervisionado . Isso dificulta sensivelmente a escolha de um algoritmo em particular em uma dada aplicação prática . Embora seja de conhecimento comum que nenhum algoritmo de aprendizado de máquina pode ser superior a todos os demais em todos os cenários de aplicação , é uma questão relevante se o desempenho de certos algoritmos em geral tende a dominar o de determinados outros , ao menos em classes particulares de problemas . Neste projeto , propõe-se contribuir com o estudo , seleção e pré-processamento de bases de dados que sejam apropriadas para se juntarem a uma coleção de benchmarks para avaliação de algoritmos de detecção não supervisionada de outliers . Propõe-se ainda avaliar comparativamente o desempenho de métodos de detecção de outliers . Durante parte do meu trabalho de mestrado , tive a colaboração intelectual de Erich Schubert , Ira Assent , Barbora Micenková , Michael Houle e , principalmente , Joerg Sander e Arthur Zimek . A contribuição deles foi essencial para as análises dos resultados e a forma compacta de apresentá-los 
 A visualização computacional de informação é um campo em expansão por oferecer meios de se interpretar e analisar vários_tipos  de dados em grande quantidade e/ou de grande complexidade , compreendendo diversas técnicas e ferramentas para fornecer a um usuário formas de interagir e explorar conjuntos de dados a fim de se obter informações úteis ou importantes . A música , por sua vez , é um domínio complexo e de difícil estudo sob o ponto de vista computacional devido à análise de seu conteúdo possuir caráter muitas_vezes  subjetivo e dependente da interpretação humana . Embora vários trabalhos tenham sido publicados a respeito do assunto nos últimos_anos  , a maior_parte  das aplicações de visualização de informação relativas a música tende a analisar conjuntos de composições musicais a fim de agrupar ou classificar dados de acordo com algum tipo de critério . Assim , a visualização das informações contidas em uma única peça musical por si só é uma área que ainda pode ser melhor explorada , sobretudo visando compreender a informação musical envolvida o conteúdo extraído por um músico a partir de partituras e tablaturas . Esta dissertação relata o desenvolvimento de uma abordagem para visualização de dados musicais referentes a melodias em guitarra , com a capacidade de exibir elementos como variações de harmonia , melodia e tempo , tendo como objetivo auxiliar um músico ( ou aprendiz de músico ) na tarefa de interpretar tais dados 
 O Reconhecimento de faces consiste em , a partir de uma imagem , identificar ou verificar um ou mais indivíduos através de um banco de dados de faces . O reconhecimento de faces é uma tarefa de grande interesse , principalmente pelo grande número de possíveis aplicações . Dessa forma , existem diversos métodos para lidar com o problema . No entanto , apesar da maioria dos métodos conseguirem bons_resultados  em ambientes controlados , quando há variações de iluminação , pose ou expressão facial , esse desempenho é reduzido . Buscando lidar com as dificuldades existentes , este trabalho propõe um método para o reconhecimento de faces utilizando os conjuntos-K. Os conjuntos-K são modelos conexionistas baseados em populações de neurônios , concebidos através de estudos e análises do sistema olfativo animal . Estes modelos apresentam estrutura e comportamento biologicamente mais plausíveis que os modelos tradicionais de redes neurais . Os conjuntos-K vêm_sendo  usados em diversas tarefas de aprendizado de máquina , apresentando bons_resultados  principalmente na resolução de problemas complexos ou com ruídos . Devido ao grande potencial dos conjuntos-K para reconhecimento de padrões em ambientes complexos e ruidosos , é levantada a hipótese de que um método baseado nos conjuntos-K alcance um melhor desempenho que os métodos existentes na literatura . O método proposto foi avaliado utilizando dois bancos de dados , AT & T e Yale B , o primeiro com pequenas variações em relação a pose e expressão facial e o segundo com grandes variações de iluminação fornecendo um cenário mais complexo . Os resultados mostraram que o método proposto consegue um desempenho equivalente ou um pouco inferior que os outros métodos avaliados para o primeiro banco de dados . Porém , para o segundo banco de dados , que fornece o cenário mais complexo , o método proposto supera os demais métodos 
 A Sumarização Multidocumento consiste na produção automática de um único sumário a partir de um conjunto de textos que tratam de um mesmo assunto . Essa tarefa vem se tornando cada vez mais importante , já que auxilia o processamento de grandes volumes de informação , permitindo destacar a informação mais relevante para o usuário . Nesse trabalho , são propostas e exploradas modelagens baseadas em Aprendizado Gerativo , em que a tarefa de Sumarização Multidocumento é esquematizada usando o modelo Noisy- Channel e seus componentes de modelagem de língua , de transformação e decodificação , que são apropriadamente instanciados para a tarefa em questão . Essas modelagens são formuladas com atributos superficiais e profundos . Em particular , foram definidos três modelos de transformação , cujas histórias gerativas capturam padrões de seleção de conteúdo a partir de conjuntos de textos e seus correspondentes sumários multidocumento produzidos por humanos . O primeiro modelo é relativamente mais simples , pois é composto por atributos superficiais tradicionais ; o segundo modelo é mais complexo , pois , além de atributos superficiais , adiciona atributos discursivos monodocumento ; finalmente , o terceiro modelo é o mais complexo , pois integra atributos superficiais , de natureza discursiva monodocumento e semântico-discursiva multidocumento , pelo uso de informação proveniente das teorias RST e CST , respectivamente . Além desses modelos , também foi desenvolvido um modelo de coerência ( ou modelo de língua ) para sumários multidocumento , que é projetado para capturar padrões de coerência , tratando alguns dos principais fenômenos multidocumento que a afetam . Esse modelo foi desenvolvido com base no modelo de entidades e com informações discursivas . Cada um desses modelos foi inferido a partir do córpus CSTNews de textos jornalísticos e seus respectivos sumários em português . Finalmente , foi desenvolvido também um decodificador para realizar a construção do sumário a partir das inferências obtidas . O decodificador seleciona o subconjunto de sentenças que maximizam a probabilidade do sumário de acordo com as probabilidades inferidas nos modelos de seleção de conteúdo e o modelo de coerência . Esse decodificador inclui também uma estratégia para evitar que sentenças redundantes sejam incluídas no sumário final . Os sumários produzidos a partir dessa modelagem gerativa são comparados com os sumários produzidos por métodos estatísticos do estado da arte , os quais foram implementados , treinados e testados sobre o córpus . Utilizando-se avaliações de informatividade tradicionais da área , os resultados obtidos_mostram  que os modelos desenvolvidos neste trabalho são competitivos com os métodos estatísticos do estado da arte e , em alguns_casos  , os superam 
 A aplicação de sistemas de múltiplos robôs é desejável em várias tarefas . Algumas delas são : exploração de ambientes , mineração , detecção de minas terrestres , segurança e operações de resgate . Uma estratégia eficiente de coordenação é decisiva para alcançar melhoras no desempenho . Neste projeto , duas novas estratégias são propostas para a coordenação de sistemas de múltiplos robôs , aplicadas para as tarefas de exploração , vigilância e formação . Elas são distribuídas , descentralizadas e ocorrem em tempo de execução . A inspiração para ambas advém de mecanismos biológicos que definem uma organização social de sistemas coletivos . Especificamente , considerou-se nesta tese uma versão modificada do sistema de colônia de formigas . As estratégias são adaptáveis para cenários em que o número de robôs e a estrutura do ambiente mudam . Em relação à primeira estratégia , os experimentos consideram dois critérios de desempenho : a média de ciclos de vigilância e a média de iterações em cada intervalo de segurança . Os resultados de simulação confirmam que a exploração e vigilância emergem da sinergia dos comportamentos individuais dos robôs . Os dados obtidos_mostram  que a estratégia de coordenação é eficiente e satisfatória para realizar as tarefas de exploração e vigilância . Quanto à segunda estratégia , o sistema apresenta as características desejáveis para que a formação seja mantida : separação , alinhamento e coesão . Evidências empíricas mostraram que o sistema possui boa habilidade dispersiva , o que promoveu o aumento da cobertura , e que o mesmo foi capaz de se adaptar a novas topologias de grupo e configurações de 
 A integração de sistemas de Visão Computacional com a Robótica Móvel é um campo de grande interesse na pesquisa . Este trabalho demonstra um método de localização global para Robôs Móveis Autônomos baseado na criação de uma memória visual , através da detecção e descrição de pontos de referência de imagens capturadas , com o método SURF , associados a dados de odometria , em um ambiente interno . O procedimento proposto , associado com conhecimento específico sobre o ambiente , permite que a localização seja obtida posteriormente pelo pareamento entre quadros memorizados e a cena atual observada pelo robô . Experimentos são conduzidos para mostrar a efetividade do método na localização robótica . Aprimoramentos para situações difíceis como travessia de portas são apresentados . Os resultados são analisados , e alternativas para navegação e possíveis futuros refinamentos 
 A computação orientada a serviços propõe o desenvolvimento de software por meio da composição de serviços com os objetivos de aumentar o reúso de software e facilitar a criação de aplicações dinâmicas , flexíveis e com baixo acoplamento . O uso de serviços no desenvolvimento de software só é possível se os desenvolvedores de aplicações ( integradores ) confiarem na qualidade dos serviços oferecidos por terceiros . Uma forma de aumentar a confiança sobre serviços adquirido de terceiros é a realização de testes . Entretanto , o teste de serviços é difícil porque os testadores ficam limitados a usar técnicas de teste baseadas em especificação por causa da indisponibilidade do código fonte . Nesse contexto , os testadores não podem usufruir dos benefícios de combiná-las com técnicas baseadas em implementação , como a técnica estrutural , por exemplo . Uma abordagem para viabilizar o uso da técnica de teste_estrutural  no contexto de aplicações baseadas em serviços sem expor o código fonte dos serviços é apresentada . Ela propõe a criação de serviços testáveis , que são serviços com alta testabilidade e que possuem uma interface de teste cujas operações apoiam o teste_estrutural  . Integradores podem realizar o teste de um serviço testável e obter , sem acessar o código fonte , uma análise de cobertura . Metadados de teste também são fornecidos pelos serviços testáveis para auxiliar integradores na obtenção de uma cobertura estrutural maior . A abordagem também apoia atividades de monitoração ativa de serviços . A abordagem é genérica uma instanciação para apoiar o teste_estrutural  de serviços e aplicações escritos em Java é apresentada . Estudos de casos e experimentos controlados foram realizados para validar a abordagem instanciada . Os resultados mostram que a abordagem é viável e apresenta bons_resultados  quando comparada com o uso apenas da técnica 
 O desenvolvimento orientado a modelos ( Model Driven Development - MDD ) é uma abordagem que tem ganhado cada vez mais espaço na indústria e na academia , trazendo grandes benefícios , como o aumento de produtividade . Uma forma de se trabalhar usando MDD em equipe é usando uma IDE ( Integrated Development Environment ) associada a um sistema de versionamento . Entretanto , trabalhar colaborativamente usando uma IDE associada a um sistema de versionamento pode trazer algumas complicações para o desenvolvimento como : conflitos de modelos , documentação descontinuada , dificuldades por parte dos interessados em usar sistemas de versionamento , etc . Nesse contexto , este trabalho propõe uma abordagem de uso de wiki para desenvolvimento de MDD , de modo que o desenvolvedor seja capaz de criar modelos , gerar código fonte , compartilhar e versionar os modelos e ainda documentar colaborativamente , de maneira mais simples e fácil do que abordagens tradicionais . Isso possibilita que mais usuários não desenvolvedores possam participar mais no processo de desenvolvimento e ainda permite o aumento de produtividade . Para tentar evidenciar de que é possível uma wiki ser usada para desenvolver software , foi criada uma Domain Specific Language - DSL em uma wiki e foram realizados três estudos de caso : um com estudantes do ensino médio e que representam os não desenvolvedores , um com quatro alunos de pós-graduação com experiência de desenvolvimento na indústria e o último estudo de caso foi realizado com 48 participantes entre desenvolvedores e alunos de pósgraduação em Ciências da Computação . Os estudos de caso mostraram que é viável usar uma wiki para desenvolvimento , que não desenvolvedores se adaptam bem à abordagem e que 86 % dos desenvolvedores usariam a abordagem proposta se tivessem que trabalhar com MDD . Os estudos de caso também levantaram as principais barreiras para aumentar a aceitação da abordagem . Com isso , este trabalho apresenta além de uma abordagem relativamente inédita na literatura , resultados sobre uso de sistemas de versionamento , de IDEs e de desenvolvimento 
 Desde sua criação , o Linux trouxe muita atenção ao movimento open-source , e à concreta possibilidade de se usar soluções de baixo custo em missões críticas . Nos últimos_anos  , esta possibilidade tornou-se real com a criação de vários clusters de alta disponibilidade . Atualmente , existem pelo menos 10 soluções de clusters open-source e mais de 25 comerciais . Cada um destes projetos teve uma abordagem diferente para o problema , embora todos tenham enfrentado dificuldades semelhantes . Se houvesse alguma padronização nesta área , esforços poderiam ter sido reaproveitados , e não duplicados . Neste contexto , o Open Clustering Framework ( OCF ) é um projeto open-source que visa definir um padrão para clusters em Linux . Um dos serviços mais importantes em um cluster é o serviço de pertinência . Ele é responsável por criar e manter o grupo , sendo assim importante para inúmeras aplicações . Sistemas de alta disponibilidade baseiam-se no serviço de pertinência para garantir o funcionamento dos recursos oferecidos por um cluster . Esta dissertação visa apresentar vários conceitos relativos a clusters , alta disponibilidade e serviços de pertinência . Com estes conceitos definidos , iremos implementar um serviço de pertinência , que será baseado no framework proposto pelo OCF , de maneira que esta implementação possa ser posteriormente incorporada a qualquer cluster que siga a especificação OCF 
 Apesar de muito trabalho ter sido dispendido sobre linguagens de consulta a Sistemas de Gerenciamento de Bancos de Dados Relacionais , existem somente dois paradigmas básicos para essas linguagens , que são representados pela Structured Query Language  SQL e pela Query by Example  QBE 
 Apesar dessas linguagens de consultas serem computacionalmente completas , elas tem a desvantagem de não permitir ao usuário nenhuma interação gráfica com a informação contida na base de dados . Um dos principais desenvolvimentos na área de base de dados diz_respeito  às ferramentas que proveêm aos usuários um entendimento simples da base de dados e uma extração amigável da informação 
 A linguagem descrita neste trabalho possibilita que usuários criem consultas graficamente por meio de diagramas de fluxo de dados . Além da linguagem de consulta gráfica , este trabalho mostra também a ferramenta de apoio Data Flow Query Language - DFQL , que é um editor/executor de consultas construído para suportar essa linguagem , através de um conjunto de operadores representados graficamente , e a execução desses diagramas , analisando a rede e gerando os comandos correspondentes em SQL para realização da consulta . Esses comandos são submetidos ao sistema de gerenciamento de banco de dados e o resultado é mostrado/gravado conforme a consulta feita 
 Este trabalho de mestrado tem como objetivo o desenvolvimento de uma arquitetura denominada PEESOS-Cloud para realização de experimentos em sistemas orientados a serviços capaz de verificar a carga de trabalho . Possíveis problemas durante a geração de carga em ambientes de experimentos foram identificados e sumarizados . Com base nesses problemas e limitandose às características dos sistemas distribuídos , dois módulos e um modelo para geração de carga foram desenvolvidos e associado à PEESOS-Cloud . Uma avaliação_experimental  foi realizada demonstrando a efetividade da proposta em atuar na avaliação de sistemas orientados a serviços . Para isso , cenários de experimentos foram definidos variando o ambiente de execução da arquitetura . Assim , o desempenho de uma aplicação sintética oferecida como serviço foi estudada e avaliada . Os resultados mostraram como a arquitetura proposta permite identificar a característica da carga de trabalho e verificar seu impacto sobre avaliação de um sistema alvo . Além_disso  , como uma carga de trabalho descaracterizada infere em uma avaliação de desempenho inconsistente . Por outro_lado  , como essa mesma carga de trabalho , sujeita a condições específicas , pode subsidiar uma avaliação de desempenho sólida 
 A Robótica Educacional consiste na utilização de robôs para aplicação prática dos conteúdos teóricos discutidos em sala de aula . Porém , os robôs mais usados apresentam uma carência de interação com os usuários , a qual pode ser melhorada com a inserção de robôs humanoides . Esta dissertação tem como objetivo a combinação de técnicas de visão_computacional  , robótica social e reconhecimento e síntese de fala para a construção de um sistema interativo que auxilie em sessões pedagógicas por meio de um robô humanoide . Diferentes conteúdos podem ser abordados pelos robôs de forma autônoma . Sua aplicação visa o uso do sistema como ferramenta de auxílio no ensino de matemática para crianças . Para uma primeira abordagem , o sistema foi treinado para interagir com crianças e reconhecer figuras geométricas 3D . O esquema proposto é baseado em módulos , no qual cada módulo é responsável por uma função específica e contém um grupo de funcionalidades . No total são 4 módulos : Módulo Central , Módulo de Diálogo , Módulo de Visão e Módulo Motor . O robô escolhido é o humanoide NAO . Para visão_computacional  , foram comparados a rede LEGION e o sistema VOCUS2 para detecção de objetos e SVM e MLP para classificação de imagens . O reconhecedor de fala Google Speech Recognition e o sintetizador de voz do NAOqi API são empregados para interações sonoras . Também foi conduzido um estudo de interação , por meio da técnica de Mágico-de-Oz , para analisar o comportamento das crianças e adequar os métodos para melhores_resultados  da aplicação . Testes do sistema completo mostraram que pequenas calibrações são suficientes para uma sessão de interação com poucos erros . Os resultados mostraram que crianças que tiveram contato com uma maior interatividade com o robô se sentiram mais engajadas e confortáveis nas interações , tanto nos experimentos quanto no estudo em casa para as próximas sessões , comparadas às crianças que tiveram contato com menor nível de interatividade . Intercalar comportamentos desafiadores e comportamentos incentivadores do robô trouxeram melhores_resultados  na interação com as crianças do que um comportamento constante 
 A caracterização de redes e o estudo de sistemas , ambos utilizando grafos , é algo muito usado por várias áreas científicas . Uma das linhas deste estudo é denominada de grafos aleatórios , que por sua vez auxilia na criação de modelos para análise de redes reais . Consideramos um modelo de grafo aleatório não homogêneo criado por Kang , Pachón e Rodríguez ( 2016 ) , cuja construção é feita a partir da realização do grafo binomial G ( n ; p ) . Para este modelo , estudamos argumentos e métodos usados para encontrar resultados sobre o limiar de conectividade , importante propriedade relacionada a existência assintótica de vértices e componentes isolados . Em seguida , constatamos algumas características positivas e negativas a respeito da utilização do grafo para modelar redes reais complexas , onde usamos de simulações computacionais e medidas topológicas 
 Os problemas de corte e empacotamento são frequentes em diferentes processos produtivos , por exemplo , na produção de roupas , de calçados , de peças metálicas e de móveis . Seu objetivo mais frequente e a minimização do desperdício de matéria-prima . No entanto , em algumas situações , o problema de determinação do caminho de corte e fundamental para eciência do planejamento da produção . Este problema consiste em determinar a trajetória de corte que minimize , por exemplo , o tempo total de corte de um plano de corte previamente estabelecido . Devido a existência de poucas abordagens para este problema , nosso objetivo e propor modelos matemáticos para resolver o problema de determinação do caminho de corte . Além_disso  , uma variação do problema que considera a utilização de grafos dinâmicos também é abordada . Os resultados obtidos são comparados com resultados da literatura 
 O setor industrial produtor de papel e celulose tem aumentado sua relevância comercial nas últimas_décadas  devido à demanda constantemente crescente . O aumento na competitividade do setor gerado pela economia globalizada e a dificuldade de desenvolvimento de bons planos de produção em ambientes produtivos cada vez mais complexos têm motivado a pesquisa por novas e efetivas ferramentas de auxílio à tomada de decisão . Considerando estas dificuldades , abordamos neste trabalho o problema de dimensionamento e sequenciamento de lotes com foco em empresas com processo integrado de produção de celulose e de papel . Trata-se de um problema de planejamento de médio a curto prazo , geralmente com maior enfoque no curto prazo por considerar o planejamento detalhado da produção em horizontes de planejamento que não superam 30 dias . No processo integrado de celulose e papel , foram consideradas as decisões de produção do digestor , evaporador , caldeira de recuperação e de múltiplas máquinas produtoras de papel , além do controle de estoque de produtos intermediários e finais . Modelos matemáticos da literatura foram modificados e estendidos para incorporar características adicionais do problema como , por exemplo , processos com múltiplas máquinas de papel . Além_disso  , foram desenvolvidas heurísticas construtivas , heurísticas de melhoria , abordagens de solução híbridas baseadas em algoritmos genéticos combinadas com ferramentas comerciais de solução exata , além de combinações entre os métodos . As abordagens desenvolvidas foram testadas computacionalmente e as melhores combinações de métodos foram definidas . De forma geral , os resultados dessas abordagens foram superiores aos obtidos por ferramentas de solução comerciais puras . Ademais , a variação proposta da heurística de melhoria fixe-e-otimize com mudanças na função objetivo se destacou com relação aos demais métodos , obtendo os melhores_resultados  , independentemente da qualidade da solução inicial utilizada . As principais_contribuições  desta tese são a apresentação de modelos matemáticos para representar apropriadamente o problema estudado , e o desenvolvimento de métodos de solução efetivos para resolver o problema 
 Nas últimas_décadas  , a capacidade das empresas de gerar e coletar informações aumentou rapidamente . Essa explosão no volume de dados gerou a necessidade do desenvolvimento de novas técnicas e ferramentas que pudessem , além de processar essa enorme_quantidade  de dados , permitir sua análise para a descoberta de informações úteis , de maneira inteligente e automática . Isso fez surgir um proeminente campo de pesquisa para a extração de informação em bases de dados denominado Knowledge Discovery in Databases  KDD , no geral técnicas de mineração de dados  DM  têm um papel preponderante . A obtenção de bons_resultados  na etapa de mineração de dados depende fortemente de quão adequadamente o preparo dos dados é realizado . Sendo_assim  , a etapa de extração de conhecimento ( DM ) no processo de KDD , é normalmente precedida de uma etapa de pré-processamento , onde os dados que porventura devam ser submetidos à etapa de DM são integrados em uma única relação . Um problema importante enfrentado nessa etapa é que , na maioria das vezes , o usuário ainda não tem uma idéia muito precisa dos dados que devem ser extraídos . Levando em consideração a grande habilidade de exploração da mente humana , este trabalho propõe uma técnica de visualização de dados armazenados em múltiplas relações de uma base de dados relacional , com o intuito de auxiliar o usuário na preparação dos dados a serem minerados . Esta técnica permite que a etapa de DM seja aplicada sobre múltiplas relações simultaneamente , trazendo as operações de junção para serem parte desta etapa . De uma maneira geral , a adoção de junções em ferramentas de DM não é prática , devido ao alto custo_computacional  associado às operações de junção . Entretanto , os resultados obtidos nas avaliações de desempenho da técnica proposta neste trabalho mostraram que ela reduz esse custo significativamente , tornando possível a exploração visual de múltiplas relações de uma maneira interativa 
 Esta tese apresenta um arcabouço teórico para auxiliar o estudo e o projeto de técnicas de visualização interativa de dados . Tais técnicas , tradicionalmente , têm sido projetadas baseando-se na experiência dos analistas desenvolvedores . Muitos trabalhos , todavia , têm procurado desenvolver um espaço de compreensão coerente para explicar como as visualizações são compostas e para permitir a predição de novas abordagens para técnicas de visualização . No entanto , propostas precursoras apresentam inadequações , não sendo capazes nem de fomentar novas sistematizações nem de explicar a concepção das técnicas mais recentes encontradas na literatura . Numa etapa inicial , esta tese revê conceitos em visualização , percepção e cognição procurando explicar como a análise visual de dados funciona . A revisão destes trabalhos é sintetizada em um processo de expressividade visual que correlaciona estímulos pré-atentivos , percepção visual analítica e interpretação cognitiva . Em seguida , após uma extensa revisão de trabalhos_relacionados  , a discussão prossegue definindo um plano de teorização da constituição dos métodos de representação visual de dados . Este plano impulsiona o desenvolvimento de uma sistematização inicial na forma de uma taxonomia capaz de caracterizar os constituintes pré-atentivos das visualizações . Esta caracterização é orientada à percepção visual analítica , que é parte do processo de expressividade visual . Desta maneira , em uma abordagem orientada a percepções visuais , as técnicas de visualização são classificadas de acordo com um conjunto limitado de características comuns e de processos de espacialização de dados . O próximo passo da discussão prossegue para a construção de um espaço de design com dimensões de posição , forma e cor . O espaço proposto , denominado Espaço de Design Espacial- Perceptivo , considera a possibilidade de múltiplos ciclos de espacialização de dados e também técnicas de interação . Baseando-se no espaço de design introduzido , a tese apresenta um modelo para a definição de parâmetros para o design de visualizações . Este modelo , que é um primeiro resultado da aplicação dos conceitos apresentados , prevê uma ferramenta para a definição , apresentação automática e avaliação empírica de representações_visuais  de dados . O trabalho é encerrado com a descrição de dois sistemas completos para a visualização de grafos e de dados multi variados . Assim , na última parte do texto , os sistemas GMine e VisTree são formalmente apresentados e analisados como estudos de caso à luz da teoria desenvolvida na 
 Uma Gestão de Serviços de Tecnologia da Informação controla e otimiza os processos que possibilitam o fornecimento dos serviços com um nível de qualidade conhecido e gerenciado . O trabalho descrito nesta dissertação envolve a análise dos processos de um serviço específico e o projeto de uma aplicação web para controle e otimização destes processos . A especificação de requisitos e a construção de um protótipo são descritas . A aplicação web proposta será utilizada por técnicos do serviço de Manutenção de Microinformática oferecido pelo Centro de Computação Eletrônica da Universidade de São Paulo 
 O grande aumento do volume de informações gerado pelos sistemas de aquisição de imagens em hospitais tem levado ao desenvolvimento de sistemas que permitam a organização e o acesso à informação de forma organizada e rápida . Com o surgimento dos sistemas PACS ( Picture Archiving and Communication System ) surgiu a possibilidade de armazenar em um só sistema todas as informações dos pacientes . Este projeto implementa , técnicas para a extração de características de imagens para permitir consultas por similaridade . Para uma dada imagem , ela c processada utilizando um subespaço de baixa frequência definido por uma transformada de wavelets . Sobre este subespaço . o método localiza os Minimum Bound Rectangles ( MBR ) de regiões da imagem por meio dos gráficos de energia . Então a análise de textura é feita sobre essas regiões . Esses dados são utilizados na construção dos vetores de características da imagem para permitir a realização de consultas baseadas em conteúdo 
 O emprego de veículos terrestres autônomos tem se tornado cada vez mais comum nos últimos_anos  em aplicações civis e militares . Eles podem ser úteis para as pessoas com necessidades especiais e para reduzir os acidentes de trânsito e o número de baixas em combate . Esta tese aborda o problema da classificação de obstáculos e da localização do veículo em relação a um mapa topológico , sem fazer uso de GPS e de mapas digitais detalhados . Um sensor laser 3D é usado para coletar dados do ambiente . O sistema de classificação de obstáculos extrai as features da nuvem de pontos e usam-nas para alimentar um classificador que separa os dados em quatro classes : veículos , pessoas , construções , troncos de árvores e postes . Durante a extração de features , um método original para transformar uma nuvem 3D em um grid 2D é proposto , o que ajuda a reduzir o tempo de processamento . As interseções de vias de áreas urbanas são detectadas e usadas como landmarks em um mapa topológico . O sistema consegue obter a localização do veículo , utilizando os pontos de referência , e identifica as mudanças de direção do veículo quando este passa pelos cruzamentos . Os experimentos demonstraram que o sistema foi capaz de classificar corretamente os obstáculos e localizar-se sem o uso de sinais de GPS 
 Operações de ponto flutuante representam uma tarefa comum em uma grande variedade de aplicações . Porém , tais operações frequentemente resultam em um gargalo do sistema , devido a grande quantidade de números de ciclos de máquina necessários para executá-las . No caso específico dos sistemas embutidos , essas limitações geralmente proíbem as aplicações de tempo real devido às limitações dos processadores . Visando amenizar o problema , este trabalho apresenta uma solução de aritmética de ponto flutuante baseada em computação reconfigurável denominada FPMU-FIoating-Point Modular Unit . A FPMU foi implementada no padrão IEEE 754 de maneira modular e com técnicas arquiteturais de exploração do paralelismo , cuja finalidade foi explorar os diversos tipos de requisitos das aplicações de tempo real , tais como velocidade , espaço ocupado , funcionalidade e etc . Os resultados obtidos foram considerados satisfatórios , atendendo uma grande classe de aplicações de tempo real 
 Um bom escalonamento de processos é de fundamental_importância  para o bom_desempenho  de um sistema computacional . Porém , a grande diversidade de objetivos de escalonamento , cargas de trabalho e tipos de sistema computacional tomam difícil a existência de um algoritmo de escalonamento que seja bom em todas as situações ou durante todo o tempo . Torna-se importante , portanto , que o software de escalonamento possa avaliar o desempenho do sistema computacional em resposta ao algoritmo de escalonamento utilizado . O PSMS ( Process Scheduling Monitoring System - Sistema de Monitoração do Escalonamento de Processos ) , monitor que implementa a abordagem de monitoração proposta por esta tese , foi desenvolvido com o intuito de oferecer uma infra-estrutura de monitoração que pode ser usada por qualquer software de escalonamento e que permita que o escalonamento de processos possa ser avaliado de maneira sistemática , independente de seu objetivo . O PSMS baseia sua avaliação em métricas de desempenho , as quais são escolhidas de acordo com os objetivos do escalonamento e são de fundamental_importância  dentro da abordagem de monitoração proposta . Também são definidas regras de decisão , que são aplicadas sobre as métricas com o intuito de retornar um valor que quantifique o desempenho do sistema de acordo com o seu objetivo de escalonamento . Este tese apresenta uma descrição detalhada da abordagem de monitoração proposta e do protótipo desenvolvido ; faz uma análise crítica sobre a utilização de métricas de desempenho na avaliação do escalonamento de processo ; e propõe algumas regras de decisão . São apresentados alguns estudos de caso relacionando regras de decisão e métricas , que demonstram a viabilidade da abordagem de monitoração aqui proposta 
 Esta dissertação de mestrado apresenta o trabalho desenvolvido para avaliação e aprimoramento da biblioteca para simulação distribuída , ParSMPL , visando sua inclusão em um ambiente de simulação distribuída automático , ASDA . Durante o trabalho de mestrado desenvolvido foi feita uma análise do uso da ferramenta ParSMPL em plataforma Linux , coletando dados relevantes para o complemento de diretrizes apresentadas em trabalhos anteriores . Tais diretrizes visam auxiliar o desenvolvimento de aplicações utilizando simulação distribuída conservativa , e serão incluídas no módulo avaliador do ambiente ASDA . Novas características foram adicionadas ao ParSMPL , buscando melhorar seu desempenho e também facilitar sua depuração . Esse trabalho de adaptação e aprimoramento do ParSMPL encontra-se descrito nesta dissertação . Foi feito também um estudo do trabalho necessário à inclusão da biblioteca ao ASDA , apresentando uma visão geral das alterações necessárias ao gerador de aplicações existente para o ambiente ASiA , voltado à simulação sequencial , de modo a torná-lo apto à produção de programas de simulação distribuída utilizando protocolo conservativo 
 Estudos empíricos têm mostrado que a Análise de Mutantes  um dos critérios de teste baseado em erros  é bastante eficaz em revelar a presença de erros . Entretanto , seu alto custo , decorrente principalmente do grande número de mutantes gerados , tem motivado a proposição de diversas abordagens alternativas para a sua aplicação . Um estudo relevante nesse sentido resultou na determinação de um conjunto essencial de operadores de mutação para a linguagem Fortran , mostrando-se que é possível reduzir o custo de aplicação do critério , preservando um alto grau de adequação em relação à Análise de Mutantes . Alguns estudos também têm demonstrado que a redução da eficácia não é significativa . Este trabalho tem como objetivo investigar alternativas pragmáticas para a aplicação do critério_Análise  de Mutantes e , nesse contexto , é proposto um procedimento para a determinação de um conjunto essencial de operadores de mutação para a linguagem C , a partir dos operadores implementados na ferramenta Proteum . Procurando aplicar e validar o procedimento proposto , dois grupos distintos de programas são utilizados . Para ambos os grupos , o conjunto essencial obtido apresenta resultados bastante significativos quanto à redução de custo , com um decréscimo muito pequeno no grau de adequação em relação à Análise de Mutantes . Estratégias para evoluir e refinar um conjunto essencial para diferentes domínios de aplicação também são investigadas 
 O problema de seleção de características , no contexto de Reconhecimento de Padrões , consiste na escolha de um subconjunto X de um conjunto S de características , de tal forma que X seja `` ótimo '' dentro de algum critério . Supondo a escolha de uma função custo c apropriada , o problema de seleção de características é reduzido a um problema de busca que utiliza c para avaliar os subconjuntos de S e assim detectar um subconjunto de características ótimo . Todavia , o problema de seleção de características é NP-difícil . Na literatura existem diversos algoritmos e heurísticas propostos para abordar este problema ; porém , quase nenhuma dessas técnicas explora o fato que existem funções custo cujos valores são estimados a partir de uma amostra e que descrevem uma `` curva em U '' nas cadeias do reticulado Booleano ( P ( S ) , < = ) , um fenômeno bem conhecido em Reconhecimento de Padrões : conforme aumenta-se o número de características consideradas , há uma queda no custo do subconjunto avaliado , até o ponto em que a limitação no número de amostras faz com que seguir adicionando características passe a aumentar o custo , devido ao aumento no erro de estimação . Em 2010 , Ris e colegas propuseram um novo algoritmo para resolver esse caso particular do problema de seleção de características , que aproveita o fato de que o espaço de busca pode ser organizado como um reticulado Booleano , assim como a estrutura de curvas em U das cadeias do reticulado , para encontrar um subconjunto ótimo . Neste trabalho estudamos a estrutura do problema de minimização de funções custo cujas cadeias são decomponíveis em curvas em U ( problema U-curve ) , provando que o mesmo é NP-difícil . Mostramos que o algoritmo de Ris e colegas possui um erro que o torna de fato sub-ótimo , e propusemos uma versão corrigida e melhorada do mesmo , o algoritmo U-Curve-Search ( UCS ) . Apresentamos também duas variações do algoritmo UCS que controlam o espaço de busca de forma mais sistemática . Introduzimos dois novos algoritmos branch-and-bound para abordar o problema , chamados U-Curve-Branch-and-Bound ( UBB ) e Poset-Forest-Search ( PFS ) . Para todos os algoritmos apresentados nesta tese , fornecemos análise de complexidade de tempo e , para alguns deles , também prova de corretude . Implementamos todos os algoritmos apresentados utilizando o arcabouço featsel , também desenvolvido neste trabalho ; realizamos experimentos ótimos e sub-ótimos com instâncias de dados reais e simulados e analisamos os resultados obtidos . Por fim , propusemos um relaxamento do problema U-curve que modela alguns tipos de projeto de classificadores ; também provamos que os algoritmos UCS , UBB e PFS resolvem esta versão generalizada do problema 
 Com o avanço tecnológico e econômico , que facilitaram o processo de comunicação e aumento do poder de compra , transações com cartão de crédito tornaram-se o principal meio de pagamento no varejo nacional e internacional ( Bolton e Hand , 2002 ) . Neste aspecto , o aumento do número de transações com cartão de crédito é crucial para a geração de mais oportunidades para fraudadores produzirem novas formas de fraudes , o que resulta em grandes perdas para o sistema financeiro ( Chan et_al  . , 1999 ) . Os índices de fraudes têm mostrado que transações no comércio_eletrônico  ( e-commerce ) são mais arriscadas do que transações presencias em terminais , pois aquelas não fazem_uso  de processos seguros e eficientes de autenticação do portador do cartão , como utilização de senha eletrônica . Como os fraudadores se adaptam rapidamente às medidas de prevenção , os modelos estatísticos para detecção de fraudes precisam ser adaptáveis e flexíveis para evoluir ao longo do tempo de maneira dinâmica . Raftery et_al  . ( 2010 ) desenvolveram um método chamado Dynamic Model Averaging ( DMA ) , ou Ponderação Dinâmica de Modelos , que implementa um processo de atualização contínuo ao longo do tempo . Nesta dissertação , desenvolvemos modelos DMA no espaço de transações eletrônicas oriundas do comércio_eletrônico  que incorporem as tendências e características de fraudes em cada_período  de análise . Também desenvolvemos modelos de regressão logística clássica com o objetivo de comparar as performances no processo de detecção de fraude . Os dados utilizados para tal são provenientes de uma empresa de meios de pagamentos eletrônico . O experimento desenvolvido mostra que os modelos DMA apresentaram resultados melhores que os modelos de regressão logística clássica quando analisamos a medida_F  e a área sob a curva ROC ( AUC ) . A medida_F  para o modelo DMA ficou em 58 % ao passo que o modelo de regressão logística clássica ficou em 29 % . Já para a AUC , o modelo DMA alcançou 93 % e o modelo de regressão logística clássica 84 % . Considerando os resultados encontrados para os modelos DMA , podemos concluir que sua característica de atualização ao longo do tempo se mostra um grande diferencial em dados como os de fraude , que sofrem mudanças de comportamento a todo momento . Deste modo , sua aplicação se mostra adequada no processo de detecção de transações fraudulentas no ambiente de comércio_eletrônico  
 Um dos problemas do paradigma BDI quando se integram agentes a ambientes virtuais ou simuladores é a ausência de controle sobre as suas percepções . Não havendo alguma_forma  de percepção direcionada ao objetivo , o agente pode ser inundado por informações irrelevantes causando um aumento injustificado do tempo de processamento . Com o objetivo de fornecer um maior controle sobre as percepções do agente e reduzir o seu tempo de resposta , este trabalho apresenta um mecanismo de filtragem das percepções para o interpretador Jason que visa eliminar aquelas percepções que podem ser ignoradas . Para tal , foram propostos e implementados alguns tipos de filtros pré-definidos , que foram aplicados a três cenários diferentes . Através de validações estatísticas apropriadas , mostrou-se que a aplicação de filtros de percepção pode reduzir em até 80 % o tempo de processamento de um agente , sem afetar significativamente o seu desempenho medido em termos de sua função de utilidade 
 Contexto : Nos últimos_anos  , o Comprometimento Cognitivo Leve ( CCL ) tem recebido uma grande atenção , pois pode representar um estágio pré-clínico da Doença de Alzheimer ( DA ) . Em termos de distinção entre idosos saudáveis ( CTL ) e pacientes com CCL , vários estudos têm mostrado que a produção de discurso é uma tarefa sensível para detectar efeitos de envelhecimento e para diferenciar indivíduos com CCL dos saudáveis . Ferramentas de Processamento de Língua Natural ( PLN ) têm sido aplicadas em transcrições de narrativas em inglês e também em português brasileiro , por exemplo , o ambiente Coh-Metrix-Dementia . Lacunas : No entanto , a ausência de informações de limites de sentenças e a presença de disfluências em transcrições impedem a aplicação direta de ferramentas que dependem de um texto bem formado , como taggers e parsers . Objetivos : O objetivo principal deste trabalho é desenvolver métodos para segmentar as transcrições em sentenças e detectar/remover as disfluências presentes nelas , de modo que sirvam como uma etapa de pré-processamento para ferramentas subsequentes de PLN . Métodos e Avaliação : Propusemos um método baseado em redes neurais recorrentes convolucionais ( RCNNs ) com informações prosódicas , morfossintáticas e word embeddings para a tarefa de segmentação de sentenças ( SS ) . Já para a detecção de disfluências ( DD ) , dividimos o método e a avaliação de acordo com as categorias de disfluências : ( i ) para preenchimentos ( pausas preenchidas e marcadores discursivos ) , propusemos a mesma RCNN com as mesmas features de SS em conjunto com uma lista pré-determinada de palavras ; ( ii ) para disfluências de edição ( repetições , revisões e recomeços ) , adicionamos features tradicionalmente empregadas em trabalhos_relacionados  e introduzimos um modelo de CRF na camada de saída da RCNN . Avaliamos todas as tarefas intrinsecamente , analisando as features mais importantes , comparando os métodos propostos com métodos mais simples , e identificando os principais acertos e erros . Além_disso  , um método final , chamado DeepBonDD , foi criado combinando todas as tarefas , e foi avaliado extrinsecamente com 9 métricas sintáticas do Coh-Metrix-Dementia . Conclusão : Para SS , obteve-se F1 = 0:77 em transcrições de CTL e F1 = 0:74 de CCL , caracterizando o estado-da-arte para esta tarefa em fala comprometida . Para detecção de preenchimentos , obtevese em média F1 = 0:90 para CTL e F1 = 0:92 para CCL , resultados que estão dentro da margem de trabalhos_relacionados  da língua inglesa . Ao serem ignorados os recomeços na detecção de disfluências de edição , obteve-se em média F1 = 0:70 para CTL e F1 = 0:75 para CCL . Na avaliação extrínseca , apenas 3 métricas mostraram diferença significativa entre as transcrições de CCL manuais e as geradas pelo DeepBonDD , sugerindo que , apesar das variações de limites de sentença e de disfluências , o DeepBonDD é capaz de gerar transcrições para serem processadas por ferramentas de PLN 
 Problemas de horários escolares consistem em alocar encontros entre turmas e professores , com objetivo de minimizar violações a requisitos qualitativos específicos . Esta categoria de problemas tem sido largamente estudada desde 1950 , particularmente via técnicas de programação linear_inteira  mista e metaheurísticas . Entretanto , a computação de soluções_ótimas  ou quase ótimas usando programas inteiro-mistos ou metaheurísticas ainda é um desafio na maioria dos problemas práticos . Nesta tese , nós investigamos novas formulações inteiro-mistas , decomposições por geração de colunas e algoritmos baseados em metaheurísticas paralelas para computar limitantes inferiores e soluções para problemas de horários escolares . Extensivos experimentos computacionais conduzidos com instâncias reais demonstram que nossas melhores formulações são competitivas com as melhores formulações existentes , enquanto nossos algoritmos paralelos são superiores em performance computacional quando comparados com métodos que são estado-da-arte 
 Esta tese aborda um problema de dimensionamento e sequenciamento de lotes de produção baseado em uma indústria alimentícia brasileira que opera por meio de diversas linhas de produção heterogêneas . Nesse ambiente produtivo , as linhas de produção compartilham recursos escassos , tais como , trabalhadores e máquinas e devem ser montadas ( ativadas ) em cada_período  produtivo , respeitando-se a capacidade disponível de cada recurso necessário para ativação das mesmas . Modelos de programação matemática inteira_mista  são propostos para representação do problema , bem como diversos métodos heurísticos de solução , compreendendo procedimentos construtivos e de melhoramento baseados na formulação_matemática  do problema e heurísticas lagrangianas . São propostas heurísticas do tipo relax-and-fix explorando diversas partições das variáveis binárias dos modelos e uma heurística baseada na decomposição do modelo para construção de soluções . Procedimentos do tipo fix-and-optimize e matheuristics do tipo iterative MIP-based neighbourhood search são propostas para o melhoramento das soluções iniciais obtidas pelos procedimentos construtivos . Testes computacionais são realizados com instâncias geradas aleatoriamente e mostram que os métodos propostos são capazes de oferecer melhores soluções do que o algoritmo Branch-and-Cut de um resolvedor comercial para instâncias de médio e grande porte 
 Nos últimos_anos  , houve um grande avanço na computação reconfigurável , em particular em hardware que emprega Field-Programmable Gate Arrays . Porém , esse aumento de capacidade e desempenho aumentou a distância entre a capacidade de projeto e a disponibilidade de tecnologia para o desenvolvimento do projeto . As linguagens de programação imperativas de alto nível , como C , são mais apropriadas para o desenvolvimento de aplicativos complexos que as linguagens de descrição de hardware . Por isso , surgiram diversas ferramentas para o desenvolvimento de hardware a partir de código em C. A ferramenta ChipCflow , da qual faz parte este projeto , é uma delas . A execução dos programas por meio dessa ferramenta será completamente baseada em seu fluxo de dados , seguindo o modelo dinâmico encontrado nas arquiteturas de computadores a fluxo de dados , aproveitando ao máximo o paralelismo considerado natural desse modelo e as características do hardware parcialmente reconfigurável . Neste projeto em particular , o objetivo é a prova de conceito ( proof of concept ) para a criação de instâncias , em forma de operadores , de um algoritmo ChipCflow em hardware parcialmente reconfigurável , tendo como base a plataforma Virtex da 
 Durante séculos , cosméticos têm sido utilizados nas mais diversas sociedades . Entretanto , quando se trata de maquiagem facial , o processo de escolha de um produto ainda é um desafio , pois é um trabalho manual que demanda tempo , além de consumir a maquiagem em si e outros materiais para aplicação e limpeza . Esse processo manual também dificulta a experimentação de vários produtos diferentes devido à necessidade de limpeza da pele para retirada de um produto aplicado anteriormente . Assim , um sistema de simulação de maquiagem utilizando realidade aumentada pode facilitar esse processo , permitindo a experimentação com a combinação de produtos e a comparação dos resultados , além de permitir experimentar os produtos virtualmente , pela internet por exemplo . Trabalhos existentes sobre esse tema permitem ao usuário aplicar a maquiagem sobre uma foto , ou mesmo um vídeo , do próprio usuário . A interação é feita por meio de mouse ou toque de um dedo sobre um monitor sensível a toques como se o usuário aplicasse maquiagem em uma terceira pessoa . Nesta dissertação propomos o desenvolvimento do SelfMakeup , um sistema de realidade aumentada que permite a autoaplicação de maquiagem virtual por meio de toques feitos diretamente na face ao invés de toques no monitor . A nossa hipótese é que essa forma de interação seja mais natural e forneça ao usuário uma melhor experiência ao testar produtos virtuais de maquiagem . O primeiro passo para viabilizar o SelfMakeup foi o desenvolvimento de um método para estimar a po- sição de toques na face utilizando uma câmera RGBD . Realizamos testes para avaliar o desempenho desse método e verificamos que a sua acurácia e precisão se mostraram adequadas para o propósito desta pesquisa . Em seguida , projetamos a interface_gráfica  do sistema para aplicação de maquiagem virtual . A interface per- mite efeitos de destaque e sombreamento que simulam os efeitos provocados pela aplicação de produtos reais de maquiagem . Resultados de um teste piloto do nosso protótipo com 32 usuários sugerem que o SelfMa- keup , por utilizar toques diretamente na face , oferece uma melhor experiência ao usuário na experimentação de produtos virtuais de maquiagem 
 Pequenas e médias empresas muitas_vezes  se deparam com oportunidades de negócios interessantes , porém além de suas capacidades de exploração . A possibilidade de uma união temporária que agregue competências para a realização de projetos em parceria levou os empresários a considerarem o conceito de Empresa Virtual ( EV ) . A EV , por si só , requer gerenciamento de recursos distribuídos entre suas empresas participantes . Esse gerenciamento demanda serviços oferecidos por sistemas de informação que possam ser responsáveis pela organização e distribuição da informação da EV . Os serviços providos pelos sistemas de informação , apesar de oferecerem facilidades e vantagens , apresentam também pontos de vulnerabilidades por onde a empresa pode ser atacada . É necessário que haja formas de proteção contra esses ataques . Este trabalho analisa os processos em uma EV , determina seus pontos de fraqueza e propõe um conjunto de recomendações para efetuar transação segura nesse tipo de ambiente 
 A computação distribuída sobre redes de estações de trabalho tem sido adotada como uma plataforma alternativa para a execução de aplicações paralelas . Essas redes não apresentam apenas uma boa relação custo/benefício , mas também fornecem um ambiente computacional de propósito_geral  , o qual pode ser usado tanto por aplicações paralelas quanto por aplicações não paralelas . Nesses ambientes multiusuários ocorre uma grande variação da carga de trabalho manipulada por cada usuário trazendo sérios problemas para o desempenho global do sistema . Dentro desse contexto , este trabalho descreve detalhadamente a implementação da DPWP , um algoritmo de escalonamento cuja principal finalidade é implantar balanceamento de cargas sobre uma rede de estações de trabalho heterogênea , para aplicações paralelas com grande quantidade de processamento . Os estudos preliminares realizados sobre os testes desenvolvidos serviram , acima de tudo , para validar e avaliar o desempenho da DPWP . Os resultados obtidos demonstram que o algoritmo tem um comportamento seguro e que as aplicações paralelas executadas , tendo a DPWP como algoritmo de escalonamento , apresentaram um excelente resultado , levando a um aumento considerável em seu desempenho final 
 Os algoritmos de simulação de Monte_Carlo  em cadeia de Markov ( MCMC ) têm aplicações em várias áreas da Estatística , entre elas destacamos os problemas de Inferência Bayesiana . A aplicação destas técnicas no entanto , exige uma análise teórica da distribuição a posteriori para assegurar a convergência . Devido ao alto grau de complexidade de certos problemas , essa análise nem sempre é possível . O objetivo deste estudo é destacar estas dificuldades e apresentar alguns aspectos práticos computacionais que podem auxiliar na solução de problemas de inferência Bayesiana . Entre estes ressaltamos os critérios de seleção de amostras , o uso de técnicas de diagnósticos de convergência e métodos de estimativas 
 Este trabalho apresenta um procedimento de auxílio ao primeiro passo do método de engenharia_reversa  FUSION-REI ! - obtenção de informações sobre o sistema . Esse procedimento compreende um processo para a criação de uma base de conhecimento ( Processo de Aquisição de Conhecimento /PAIA , instanciado para o domínio de engenharia_reversa  ) e diretrizes para a utilização dessa base de conhecimento na construção de visões funcionais do sistema 
 O interesse cada vez maior das empresas em adquirir novas tecnologias de processarnento e armazenamento de dados , além de visualizar a informação como seu maior patrimônio , tem direcionado várias pesquisas para o estudo do processo de transformação desses dados em conhecimento , o que pode proporcionar um auxílio efetivamente inteligente à tomada de decisão . Nesse contexto , o processo de Extração de Conhecimento de Bases de Dados ( KDD - Knowledge Discovery in Database ) desponta como uma tecnologia capaz de cooperar amplamente na busca do conhecimento embutido nos dados . Essa busca pode ser realizada utilizando métodos estatísticos e/ou técnicas de Inteligência_Artificial  , especialmente as que manipulam incerteza , que são amplamente aplicados na análise de dados com objetivo de encontrar relações de interesse . As redes Bayesianss representam um dos modelos mais proeminentes para encontrar essas relações . Este trabalho envolve a investigação dos conceitos , técnicas , métodos e ferramentas Bayesianas para auxiliar o processo de extração de conhecimento de bases de dados , considerando a incorporação de conhecimento de fundo , bem como o tratamento de dados incompletos 
 A avaliação de pacotes de software em um ambiente empresarial pode ser realizada pelas pessoas que não são técnicas na área de informática utilizando as características de qualidade de produto de software da ISO/IEC 9126 e de outras encontradas na literatura , através da aplicação de um questionário . Um questionário é apresentado com sua elaboração baseada em estudos realizados conforme a norma ISO/IEC 9126 e o método Goal/Question/Metric ( GQM ) . Um estudo de caso foi realizado usando seis pacotes de software de sistemas de informação em operação na CETERP S/A . O objetivo é proporcionar informação para o gerente no processo de análise e melhorar o apoio da tecnologia da informação disponível na empresa 
 Este trabalho de mestrado apresenta a modelagem , a prototipação e os resultados do desenvolvimento de um middleware para composição dinâmica de Web_Services  denominado DWSC-M ( Dynamic Web service Composition Middleware ) . O objetivo principal desse middleware é permitir que serviços sejam compostos dinamicamente considerando aspectos de qualidade de serviço na escolha dos serviços que fazem parte de um fluxo de composição . Para complementar o funcionamento do middleware DWSC-M foram propostos dois algoritmos para seleção de Web_Services  : o primeiro utiliza seleção aleatória e o segundo utiliza distância Euclidiana para seleção de serviços e considera , para tal finalidade , os parâmetros de QoS enviados pela requisição do cliente do 
 Com a popularização de equipamentos tradicionais de captura de imagens , como câmeras digitais , e o avanço tecnológico dos dispositivos não invasivos , como tomografia e ressonância , cresce também a necessidade e consequente uso de métodos_numéricos  para simulação de fenômenos físicos em domínios definidos por imagens . Um dos pré-requisitos para a aplicação de tais métodos_numéricos  consiste na discretização do domínio em questão , num processo denominado geração de malhas . Embora diversos métodos de geração de malha tenham sido_propostos  para discretizar domínios definidos por primitivas geométricas , pouco tem sido feito no sentido de gerar uma decomposição diretamente a partir de imagens . Neste trabalho , apresentamos uma abordagem de geração de malhas de qualidade a partir de domínios definidos por imagens com textura . Mais especificamente , a pesquisa descrita nesta tese contribui com a melhoria do algoritmo Imesh , ao sanar três de suas principais limitações : tratamento de imagens com texturas ; controle do nível de refinamento da malha e suporte a outros_tipos  de elementos . Estas contribuições flexibilizam o processo de geração da malha , e ainda ampliam o domínio de aplicações do algoritmo Imesh , à medida que são considerados domínios definidos por imagens com textura e o uso de métodos_numéricos  para elementos não simpliciais torna-se possível . O algoritmo de melhoria da malha gerada utiliza uma abordagem inovadora de remalhamento baseada em templates e guiada por retalhos de 
 A mineração de textos vem de encontro à realidade atual de se compreender e utilizar grandes massas de dados textuais . Uma forma de auxiliar a compreensão dessas coleções de textos é construir taxonomias de tópicos a partir delas . As taxonomias de tópicos devem organizar esses documentos , preferencialmente em hierarquias , identificando os grupos obtidos por meio de descritores . Construir manual , automática ou semi-automaticamente taxonomias de tópicos de qualidade é uma tarefa nada trivial . Assim , o objetivo deste trabalho é construir taxonomias de tópicos em domínios de conhecimento restrito , por meio de mineração de textos , a fim de auxiliar o especialista no domínio a compreender e organizar os textos . O domínio de conhecimento é restrito para que se possa trabalhar apenas com métodos de aprendizado estatístico não supervisionado sobre representações bag of words dos textos . Essas representações independem do contexto das palavras nos textos e , conseqüentemente , nos domínios . Assim , ao se restringir o domínio espera-se diminuir erros de interpretação dos resultados . A metodologia_proposta  para a construção de taxonomias de tópicos é uma instanciação do processo de mineração de textos . A cada etapa do processo propôem-se soluções adaptadas às necessidades específicas de construçao de taxonomias de tópicos , dentre as quais algumas contribuições inovadoras ao estado da arte . Particularmente , este trabalho contribui em três frentes no estado da arte : seleção de atributos n-gramas em tarefas de mineração de textos , dois modelos para rotulação de agrupamento hierárquico de documentos e modelo de validação do processo de rotulação de agrupamento hierárquico de documentos . Além dessas contribuições , ocorrem outras em adaptações e metodologias de escolha de processos de seleção de atributos , forma de geração de atributos , visualização das taxonomias e redução das taxonomias obtidas . Finalmente , a metodologia desenvolvida foi aplicada a problemas reais , tendo obtido bons_resultados  
 Nos dias atuais há uma quantidade massiva de dados textuais sendo produzida e armazenada diariamente na forma de e-mails , relatórios , artigos e postagens em redes_sociais  ou blogs . Processar , organizar ou gerenciar essa grande quantidade de dados textuais manualmente exige um grande esforço humano , sendo muitas_vezes  impossível de ser realizado . Além_disso  , há conhecimento embutido nos dados textuais , e analisar e extrair conhecimento de forma_manual  também torna-se inviável devido à grande quantidade de textos . Com isso , técnicas computacionais que requerem pouca intervenção humana e que permitem a organização , gerenciamento e extração de conhecimento de grandes quantidades de textos têm ganhado destaque nos últimos_anos  e vêm_sendo  aplicadas tanto na academia quanto em empresas e organizações . Dentre as técnicas , destaca-se a classificação automática de textos , cujo_objetivo  é atribuir rótulos ( identificadores de categorias pré-definidos ) à documentos textuais ou porções de texto . Uma forma viável de realizar a classificação automática de textos é por meio de algoritmos de aprendizado de máquina , que são capazes de aprender , generalizar , ou ainda extrair padrões das classes das coleções com base no conteúdo e rótulos de documentos textuais . O aprendizado de máquina para a tarefa de classificação automática pode ser de 3 tipos : ( i ) indutivo supervisionado , que considera apenas documentos rotulados para induzir um modelo de classificação e classificar novos documentos ; ( ii ) transdutivo semissupervisionado , que classifica documentos não rotulados de uma coleção com base em documentos rotulados ; e ( iii ) indutivo semissupervisionado , que considera documentos rotulados e não rotulados para induzir um modelo de classificação e utiliza esse modelo para classificar novos documentos . Independente do tipo , é necessário que as coleções de documentos textuais estejam representadas em um formato estruturado para os algoritmos de aprendizado de máquina . Normalmente os documentos são representados em um modelo espaço-vetorial , no qual cada documento é representado por um vetor , e cada posição desse vetor corresponde a um termo ou atributo da coleção de documentos . Algoritmos baseados no modelo espaço-vetorial consideram que tanto os documentos quanto os termos ou atributos são independentes , o que pode degradar a qualidade da classificação . Uma alternativa à representação no modelo espaço-vetorial é a representação em redes , que permite modelar relações entre entidades de uma coleção de textos , como documento e termos . Esse tipo de representação permite extrair padrões das classes que dificilmente são extraídos por algoritmos baseados no modelo espaço-vetorial , permitindo assim aumentar a performance de classificação . Além_disso  , a representação em redes permite representar coleções de textos utilizando diferentes_tipos  de objetos bem como diferentes_tipos  de relações , o que permite capturar diferentes características das coleções . Entretanto , observa-se na literatura alguns desafios para que se possam combinar algoritmos de aprendizado de máquina e representações de coleções de textos em redes para realizar efetivamente a classificação automática de textos . Os principais desafios abordados neste projeto de doutorado são ( i ) o desenvolvimento de representações em redes que possam ser geradas eficientemente e que também permitam realizar um aprendizado de maneira eficiente ; ( ii ) redes que considerem diferentes_tipos  de objetos e relações ; ( iii ) representações em redes de coleções de textos de diferentes línguas e domínios ; e ( iv ) algoritmos de aprendizado de máquina eficientes e que façam um melhor uso das representações em redes para aumentar a qualidade da classificação automática . Neste projeto de doutorado foram propostos e desenvolvidos métodos para gerar redes que representem coleções de textos , independente de domínio e idioma , considerando diferentes_tipos  de objetos e relações entre esses objetos . Também foram propostos e desenvolvidos algoritmos de aprendizado de máquina indutivo supervisionado , indutivo semissupervisionado e transdutivo semissupervisionado , uma vez que não foram encontrados na literatura algoritmos para lidar com determinados tipos de relações , além de sanar a deficiência dos algoritmos existentes em relação à performance e/ou tempo de classificação . É apresentado nesta tese ( i ) uma extensa avaliação empírica demonstrando o benefício do uso das representações em redes para a classificação de textos em relação ao modelo espaço-vetorial , ( ii ) o impacto da combinação de diferentes_tipos  de relações em uma única rede e ( iii ) que os algoritmos propostos baseados em redes são capazes de superar a performance de classificação de algoritmos tradicionais e estado da arte tanto considerando algoritmos de aprendizado_supervisionado  quanto semissupervisionado . As soluções propostas nesta tese demonstraram ser úteis e aconselháveis para serem utilizadas em diversas aplicações que envolvam classificação de textos de diferentes domínios , diferentes características ou para diferentes quantidades de documentos rotulados 
 Os atuais avanços na área das Tecnologias da Informação e da Comunicação ( TIC ) estão mudando a Educação , com a disponibilização de sistemas educativos para apoiar as atividades no processo de ensino e de aprendizagem . Embora o tópico de combinatória seja relevante para o ensino médio e para os cursos de Ciências Exatas , este tópico tem sido desconsiderado nos avanços da Informática na Educação , principalmente ao se considerar o uso de ambientes de aprendizagem via Web . Atualmente , o iComb é , no melhor de nosso conhecimento , o único sistema Web integrável a tais ambientes . Ele é um Módulo de Aprendizagem Interativa ( iMA ) integrável ao Sistema Gerenciador de Cursos ( SGC ) Moodle , por meio do plugin iTarefa . O iComb oferece atividades de autoria de exercícios , bem como , sua avaliação automática . No entanto , considerando sua usabilidade , foram detectados alguns problemas na interface . O foco deste trabalho está relacionado ao reprojeto do sistema iComb e seu impacto na aprendizagem de combinatória . Para a reconstrução do iComb adotou-se o método de design de interação , buscando-se a participação efetiva de usuários finais do sistema , em todas as atividades desse processo , com ( i ) sessões de teste de usabilidade utilizando a versão Java do iComb para identificar problemas e guiar o redesenho do sistema , e ( ii ) prototipagem e avaliação da interface do sistema redesenhado . Esse processo permitiu implementar uma nova versão do iComb utilizando tecnologias associadas ao HTML5 , que será brevemente disponibilizado como software_livre  
 A associação é uma tarefa de mineração de dados que tem sido muito utilizada em problemas reais , porém o grande número de regras de associação que podem ser geradas dificulta a identificação de conhecimento interessante aos usuários . Para apoiar a identificação de regras interessantes podem ser utilizadas medidas de avaliação de conhecimento , que normalmente são classificadas como objetivas ou subjetivas . As medidas objetivas são mais gerais , mas podem não ser suficientes por não considerarem aspectos_relacionados  ao usuário ou ao domínio da aplicação . Por outro_lado  pode haver dificuldade em se obter a subjetividade do usuário necessária para o cálculo das medidas subjetivas . Diante desse contexto , neste trabalho é proposta uma metodologia para identificação de regras de associação interessantes que combina análises com medidas objetivas e subjetivas , visando aproveitar as vantagens de cada tipo e facilitar a participação do especialista . As medidas objetivas são utilizadas para selecionar algumas regras potencialmente interessantes para serem avaliadas por um especialista . As medidas subjetivas são calculadas utilizando essas regras com as avaliações do especialista . Essas medidas subjetivas então são utilizadas para auxiliar a identificação de regras interessantes de acordo com o conhecimento obtido durante a avaliação . Para viabilizar a aplicação dessa metodologia foi desenvolvido um módulo computacional de exploração de regras de associação com medidas subjetivas , denominado RulEE-SEAR . Utilizando esse módulo e outras ferramentas já existentes foi realizado um estudo de caso com uma base de dados real sobre qualidade de vida urbana . Nesse estudo de caso o processo de identificação de regras de associação interessantes foi realizado com especialista da área e verificou-se a viabilidade da metodologia_proposta  
 O desenvolvimento da tecnologia de microarray tornou possível a mediçao dos níveis de expressão de centenas ou até mesmo milhares de genes simultaneamente para diversas condições experimentais . A grande quantidade de dados disponível gerou a demanda por métodos computacionais que permitam sua análise de forma eficiente e automatizada . Em muitos dos métodos computacionais empregados durante a análise de dados de expressão_gênica  é necessária a escolha de uma medida de proximidade apropriada entre genes ou amostras . Dentre as medidas de proximidade disponíveis , coeficientes de correlação têm sido amplamente empregados , em virtude da sua capacidade em capturar similaridades entre tendências das sequências numéricas comparadas ( genes ou amostras ) . O presente_trabalho  possui como objetivo comparar diferentes medidas de correlação para as três principais tarefas envolvidas na análise de dados de expressão_gênica  : agrupamento , seleção de atributos e classificação . Dessa forma , é apresentada nesta dissertação uma visão geral da análise de dados de expressão_gênica  e das diferentes medidas de correlação consideradas para tal comparação . São apresentados também resultados empíricos obtidos a partir da comparação dos coeficientes de correlação para agrupamento de genes , agrupamento de amostras , seleção de genes para o problema de classificação de amostras e classificação de 
 O método ISPH ( do inglês , Incompressible Smoothed Particle Hydrodynamics ) é um método de aproximação livre de malha que , através de um conjunto finito de partículas e uma formulação completamente Lagrangeana , permite a solução de diversos tipos de escoamentos . Entretanto , sua aplicação para escoamentos bifásicos ainda é um desafio , principalmente no que refere-se à manutenabilidade da interface entre fluidos . Diante disso , nesta tese é apresentado o desenvolvimento de um código numérico baseado no método ISPH , sendo propostas duas técnicas de tratamento de interface . Para tanto é realizado um estudo a cerca do método , considerando diferentes metodologias , e analisando pontos específicos , tais como a solução do campo de pressões . São apresentados resultados que mostram a eficácia do método , tanto em escoamentos monofásicos , quanto em escoamentos multifásicos , onde , neste caso , são destacadas as melhorias obtidas_através  das técnicas de tratamento de interface propostas . Por fim , é realizado um estudo do comportamento de misturas bifásicas , com referência ao fenômeno da inversão de 
 Bancos de dados relacionais são fontes de dados rigidamente estruturadas , caracterizadas por relacionamentos complexos entre um conjunto de relações ( tabelas ) . Entender tais relacionamentos é um desafio , porque os usuários precisam considerar múltiplas relações , entender restrições de integridade , interpretar vários atributos , e construir consultas SQL para cada tentativa de exploração . Neste cenário , introduz-se uma metodologia em duas etapas ; primeiro utiliza-se um grafo organizado como uma estrutura hierárquica para modelar os relacionamentos do banco de dados , e então , propõe-se uma nova_técnica  de visualização para exploração relacional . Os resultados demonstram que a proposta torna a exploração de bases de dados significativamente simplificada , pois o usuário pode navegar visualmente pelos dados com pouco ou nenhum conhecimento sobre a estrutura subjacente . Além_disso  , a navegação visual de dados remove a necessidade de consultas SQL , e de toda complexidade que elas requerem . Acredita-se que esta abordagem possa trazer um paradigma inovador no que tange à compreensão de dados 
 Um tópico importante na análise de seqüências biológicas é a busca de genes , ou seja , a identificação de regiões codificantes de proteínas . Esta identificação permite a posterior procura de significado , descrição ou categorização biológica do organismo analisado . Atualmente , vários métodos combinam reconhecimento de padrões com conhecimento coletado de conjuntos de treinamento ou de comparações com banco de dados genômicos . Entretanto , a acurácia desses métodos está ainda longe do satisfatório . Novos métodos de processamento de seqüências de DNA e de identificação de genes podem ser criados através da busca por conteúdo ( search-by-content ) . O padrão periódico de DNA em regiões codificantes de proteína , denominada periodicidade de três bases , vem_sendo  considerado uma propriedade dessas regiões . As técnicas de processamento digital de sinais fornecem uma base robusta para a identificação de regiões com periodicidade de três bases . Nesta dissertação , são apresentados um \pipeline , os conceitos básicos da identificação genômica , e métodos de processamento digital de sinais utilizados para a identificação de regiões codificantes de proteínas . Introduzimos um novo método para a identificação dessas regiões , baseado na transformada proposta , denominada Transformada Modificada de Morlet . Apresentamos vários resultados experimentais obtidos a partir de seqüências de DNA sintéticas e reais . As principais_contribuições  do trabalho consistem no desenvolvimento de um pipeline para projetos genoma e na criação de um método de identificação de regiões codificantes onde a periodicidade de três bases seja latente . O método apresenta desempenho_superior  e vantagens importantes em comparação ao método tradicional baseado na transformada de Fourier de tempo reduzido 
 As reações químicas que resultam da expressão de genes são complexas e ainda não são total- mente compreendidas . Sabe-se que os genes enviam , recebem , e processam informações formando uma complexa rede de comunicação , mas a arquitetura e dinâmica destas redes não são totalmente conhecidas . Dessa forma , um problema importante é determinar como os genes se relacionam dentro da célula . Esse processo de determinar o relacionamento entre os genes é conhecido como inferência de redes gênicas . Uma das formas para representar o relacionamento entre os genes é usar modelos matemáticos e computacionais de Redes Gênicas . Em especial , um dos modelos de grande interesse é o de Redes Booleanas ( BN - do inglês Boolean Networks ) , no qual os genes podem assumir dois estados , ativo ou inativo , se estão , respectivamente , expressos ou não . Estes estados podem variar ao longo do tempo , dependendo de como os genes se relacionam . Nosso interesse está em estudar um caso particular deste modelo , conhecido como Redes Booleanas Limiarizadas , onde apenas uma classe de funções booleanas é utilizada para construir as BNs . Para inferir as Redes Booleanas Limiarizadas , usamos um algoritmo constituído de dois passos . Primeiro , usamos o arcabouço do Problema de Satisfação de Restrições ( CSP - do inglês Constraint Satisfaction Problem ) para inferir conjuntos de soluções consistentes com uma dada série_temporal  de um conjunto de genes . Em seguida analisamos o comportamento dinâmico das soluções encon- tradas , filtrando conjuntos de soluções de maior interesse para testes práticos em laboratório . Usando o arcabouço do CSP , construímos um solver , usando a biblioteca Gecode,1 para inferência de redes consistentes , usando como entrada uma série_temporal  oriunda de dados de microarrays . Em seguida , através da simulação da dinâmica de uma amostra das redes encontradas no passo anterior , fomos capazes de determinar algumas restrições interessantes para filtrar o conjunto de redes . Aplicamos o nosso método para três conjuntos de dados : dois artificiais , e para validação , usamos uma série_temporal  de uma rede artificial conhecida na literatura . Com isso fomos capazes de inferir conjuntos de redes gênicas de possível interesse para testes em laboratório 
 Diferentes sistemas do mundo_real  podem ser representados por redes . As redes são estruturas nas quais seus vértices ( nós ) representam entidades e links representam relações entre essas entidades . Além_disso  , as redes caracterizam-se por ser estruturas dinâmicas , o que implica na rápida aparição e desaparição de entidades e seus relacionamentos . Nesse cenário , um dos problemas importantes a serem enfrentados no contexto das redes , é da predição de links , isto é , prever a ocorrência futura de um link ainda não existente entre dois vértices com base nas informações já existentes . A importância da predição de links deve-se ao fato de ter aplicações na recuperação de informação , identificação de interações espúrias e , ainda , na avaliação de mecanismos de evolução das redes . Para enfrentar o problema da predição de links , a maioria dos métodos utiliza informações da vizinhança topológica das redes para atribuir um valor que represente a probabilidade de conexão futura entre um par de vértices analisados . No entanto , recentemente têm aparecido métodos híbridos , caracterizados por usar outras informações além da vizinhança topológica , sendo as informações das comunidades as normalmente usadas , isso , devido ao fato que , ao serem grupos de vértices densamente ligados entre si e esparsamente ligados com vértices de outros grupos , fornecem informações que podem ser úteis para determinar o comportamento futuro das redes . Assim , neste trabalho são apresentadas duas propostas na linha dos métodos baseados nas informações das comunidades para predição de links . A primeira proposta consiste em um novo índice de similaridade que usa as informações dos vértices pertencentes a mesma comunidade na vizinhança de um par de vértices analisados , bem como as informações dos vértices pertencentes a diferentes comunidades nessa mesma vizinhança . A segunda proposta consiste de um conjunto de índices obtidos a partir da reformulação de algumas propostas já existentes , porém , inserindo neles informações dos vértices pertencentes unicamente à mesma comunidade na vizinhança topológica de um par de vértices analisados . Experimentos realizados em dez redes complexas de diferentes domínios demonstraram que , em geral , os índices propostos obtiveram desempenho_superior  às abordagens 
 O número e a intensidade dos desastres naturais têm aumentado em todo o mundo devido às mudanças climáticas . Atualmente , as inundações representam a maior_parte  dos desastres naturais e causam mais danos , mortes e impactos econômicos do que qualquer outro tipo de fenômeno natural . No Brasil , as inundações se intensificam principalmente no período de chuvas , entre os meses de Dezembro e Março , ocasionando dezenas de mortes , além de impactos econômicos , sociais e ambientais . Assim , prevenir esse tipo de desastre tem se tornado um fator importante para minimizar os impactos por ele provocados . Para isso , são necessárias informações atualizadas , completas e precisas sobre o atual estado das variáveis ambientais . Nesse contexto , as informações voluntárias estão sendo utilizadas como fonte de dados complementar para a gestão de risco de inundação , devido ao número elevado de voluntários que atuam como `` sensores '' . Contudo , outras informações relacionadas a inundação estão dispersas em diferentes fontes de dados , dificultando as ações de resposta das agências de emergência . Neste trabalho e proposta uma abordagem que visa apoiar a gestão de risco de inundação , fornecendo mecanismos para a obtenção de informações voluntárias para esse contexto e permitindo a disponibilização dessas por meio de serviços geoespaciais em tempo real . Para tanto , mecanismos de interpretação foram definidos para apoiar os voluntários durante a análise e medição da altura da água , sendo essas informações fornecidas por meio de uma plataforma de crowdsourcing . Além_disso  , um framework foi desenvolvido a fim de disponibilizar as informações voluntárias de forma interoperável por meio de serviços geoespaciais . Ao final , foram realizados estudos_experimentais  para avaliar a eficácia dos mecanismos desenvolvidos para a obtenção de informações voluntárias para gestão de risco de 
 O controle de versões é uma das principais funcionalidades do Gerenciamento de Configuração de Software ( GCS ) e visa , entre outras coisas , a recuperação e auditoria ( quem , quando e o quê ) de versões anteriores e a redução do espaço de armazenamento dos produtos em desenvolvimento . Existem ferramentas que auxiliam esse controle - o CVS ( Concurrent Versions System ) é uma delas e tem sido amplamente adotado . Como apoio à edição colaborativa na Web , o CVS pode proporcionar benefícios no sentido de recuperar e verificar versões anteriores . Atualmente , ferramentas conhecidas como wiki , que possibilitam edição colaborativa por meio da Web , têm obtido muitos adeptos . Um exemplo desse tipo de ferramenta é a CoTeia , que tem sido utilizada no ambiente acadêmico do Instituto de Ciências_Matemáticas  e de Computação ( ICMC ) da Universidade de São Paulo ( USP ) desde 2001 , e vem_sendo  utilizada também , integrada à ferramenta DocRationale , usada para registro do Design Rationale ( DR ) . Além da edição colaborativa , é possível também o armazenamento de arquivos ( uploads ) associados às páginas_Web  da wiki . A ferramenta DocRationale viabiliza o armazenamento de artefatos relacionados ao processo de desenvolvimento de software , através do upload de arquivos . No entanto , o controle de versões desses artefatos na CoTeia não era provido . De fato , não existe um consenso da literatura a respeito do suporte de controle de versões em áreas de upload nas wikis . Neste trabalho foi realizado um estudo para análise do uso de controle de versões nas páginas e nos uploads em um conjunto de wikis pesquisadas . Já na DocRationale , como os artefatos são alterados durante o processo de desenvolvimento de software , o controle de versões na CoTeia se torna um mecanismo importante . Com isso , foi implementado o controle de versões dos artefatos armazenados na ferramenta DocRationale , através da integração do CVS à CoTeia utilizada na DocRationale 
 Algoritmos para rendering de superfícies são rápidos , mas não são compatíveis com situações em que é necessário investigar estruturas internas em volumes . Algoritmos de rendering volumétrico direto são adequados para a exploração de estruturas volumétricas , mas são lentos quando comparados a um rendering de superfícies . Várias soluções híbridas foram propostas na literatura , sendo que uma delas , conhecida como VoS ( Volume on Surface ) , foi proposta recentemente com o objetivo de aumentar a capacidade de investigação do conteúdo de volumes por meio de superfícies . VoS é uma técnica híbrida que permite mapear o conteúdo de um volume em superfícies extraídas do mesmo . A técnica executa lançamento de raios para mapear as informações do volume na superfície , possibilitando a visualização das estruturas internas do volume utilizando rendering de superfícies convencional 
 No presente_trabalho  estudamos a técnica VoS e propomos diversas modificações com o intuito de generalizar a técnica e tratar algumas de suas limitações . As novas soluções apresentadas permitem a utilização da técnica com volumes de voxels regulares , e geram imagens de melhor qualidade . Assim como a VoS , as duas novas versões implementadas , VoSm e VoSm* , têm o objetivo de melhorar o poder de investigação do rendering de superfícies , permitindo a exploração do conteúdo de volumes . A técnica VoS e suas variações oferecem uma ferramenta alternativa para aplicações em que a utilização de superfícies é uma solução natural 

 O uso de modelos multiníveis é uma alternativa interessante para analisar dados que estão estruturados de forma hierárquica , pois permite a obtenção de diferentes estimativas de parâmetros relativos a grupos distintos e , ao mesmo tempo , leva em consideração a dependência entre as observações em um mesmo grupo . Neste trabalho , desenvolvemos e aplicamos modelos de regressão multiníveis simétricos , a fim de fornecer alternativas ao modelo usual , sob normalidade . Além_disso  , apresentamos uma breve análise de diagnóstico e estudo de simulação . Como motivação , consideramos dados educacionais , a fim de avaliar se o número de reprovações no histórico escolar do aluno e a infraestrutura da escola são variáveis relevantes que afetam o baixo desempenho dos alunos do ensino básico na disciplina de 
 Este trabalho_investiga  a viabilidade de medir a diversão apenas a partir da análise computacional de imagens faciais capturadas de webcams de baixo custo . O estudo e desenvolvimento se baseou em vídeos gravados com as faces de voluntários enquanto jogavam três diferentes jogos populares e independentes ( horror , ação/plataforma e puzzle ) . Os participantes também reportaram seus níveis de frustração , imersão e diversão no intervalo discreto [ 0 , 4 ] , e responderam ao renomado Game Experience Questionnaire ( GEQ ) . Faces foram encontradas nos vídeos coletados utilizando um sistema desenvolvido com implementações existentes do algoritmo de Viola-Jones para a detecção da face e uma variação do algoritmo Active Appearance Model ( AAM ) para o rastreamento das marcas faciais . A diversão foi representada em termos das emoções prototípicas e dos níveis de frustração e imersão . As emoções prototípicas foram detectadas com uma Máquina de Vetores de Suporte ( SVM ) treinada com bases de dados existentes , e os níveis de frustração , imersão e diversão foram detectados com um Perceptron Estruturado treinado com os dados coletados e os níveis reportados de cada afeto , com o gradiente da distância entre a face e a câmera , e com a taxa de piscadas por minuto . A avaliação foi apoiada pela comparação dos níveis reportados com as respostas ao GEQ , e executada com métricas de precisão e revocação ( recall ) obtidas em testes de validação cruzada . O classificador de frustração não obteve uma precisão acima de chance , principalmente porque os dados coletados não tiveram variabilidade suficiente nos níveis reportados desse afeto . O classificador de imersão obteve uma precisão melhor particularmente quando treinado com a taxa de piscadas , com uma média de 0.42 e uma Amplitude Interquartil ( IQR ) entre 0.12 e 0.73 . O classificador de diversão , treinado com as emoções prototípicas e os níveis reportados de frustração e imersão , obteve a melhor precisão , com média de 0.58 e IQR entre 0.28 e 0.84 . Todos os classificadores sofreram de baixa revocação , causada por dificuldades no rastreamento das marcas faciais e pelo desbalanceamento do classificador de emoções , cujos dados de treinamento continham mais exemplos de expressões neutras e de felicidade . Ainda assim , um forte indicador da viabilidade de medir diversão a partir de vídeos está nos padrões de variação dos níveis previstos . Com exceção da frustração , os classificadores de imersão e de diversão foram capazes de prever os aumentos e reduções dos níveis dos respectivos afetos com uma margem de erro média próxima de 1 
 Os Sistemas de Grades Computacionais são intrinsecamente mais vulneráveis às ameaças de segurança que os Sistemas tradicionais , uma vez que abrangem um grande número de usuários e os recursos e as aplicações são geridas por diferentes domínios administrativos . A autenticação e a autorização são fatores imperativos para os Sistemas de Grade Computacional . Da mesma forma , a escalabilidade e a distribuição de dados vêm também sendo objeto de estudo de vários pesquisadores da área . Os serviços providos pelas Grades Computacionais devem evitar implementações centralizadas pela dificuldade do gerenciamento global . Outro importante requisito das Grades Computacionais é prover mecanismos para a delegação de direitos de acesso aos recursos . O proprietário do recurso deve ser capaz de delegar permissões para outro usuário , talvez por um tempo limitado , com base na confiança que possui neste . No entanto , a delegação deve ser usada com cuidado , pois uma longa cadeia de delegações poderia conduzir a uma utilização abusiva dos recursos pelos usuários maliciosos . Para tratar os principais requisitos de segurança das Grades Computacionais , desenvolvemos uma Arquitetura de Segurança denominada Xenia . Esta arquitetura é baseada em SPKI/SDSI , um modelo de segurança flexível , extensível e descentralizado que fornece autenticação , confidencialidade e controle de acesso . Propusemos uma extensão ao modelo SPKI/SDSI baseada em lógica subjetiva para representar relações de confiança entre indivíduos 
 A evolução de sistemas distribuídos resultou em aumento significativo de complexidade para manutenção e gerenciamento , tornando pouco eficientes técnicas convencionais baseadas em intervenções manuais . Isso motivou pesquisas que deram origem ao paradigma de computação autônoma ( Autonomic Computing ) , que provê aspectos de auto-configuração , auto-recuperação , auto-otimização e auto-proteção a fim de tornar sistemas auto-gerenciáveis . Nesse contexto , esta tese teve como objetivo prover autonomia a ambientes distribuídos , sem a necessidade de mudar o paradigma de programação e as aplicações de usuários . Para isso , propôs-se uma abordagem que emprega técnicas para compreensão e predição de dinâmicas comportamentais de processos , utilizando abordagens de sistemas dinâmicos , inteligência_artificial  e teoria do caos . Os estudos realizados no decorrer desta pesquisa demonstraram que , ao predizer padrões comportamentais , pode-se otimizar diversos aspectos de computação distribuída , suportando tomadas de decisão autônomas pelos ambientes . Para validar a abordagem proposta , foi desenvolvida uma política de escalonamento distribuído , denominada PredRoute , a qual utiliza o conhecimento sobre o comportamento de processos para otimizar , transparentemente , a alocação de recursos . Experimentos realizados demonstraram que essa política aumenta o desempenho em até 4 ordens de grandeza e apresenta baixo custo_computacional  , o que permite a sua adoção para escalonamento online de 
 A etiquetagem morfossintática envolve atribuir às palavras de uma sentença suas classes morfossintáticas de acordo com os contextos em que elas aparecem . Cadeias de Markov de Tamanho Variável ( VLMCs , do inglês `` Variable-Length Markov Chains '' ) oferecem uma forma de modelar contextos maiores que trigramas sem sofrer demais com a esparsidade de dados e a complexidade do espaço de estados . Mesmo assim , duas palavras do português apresentam um alto grau de ambiguidade : 'que ' e 'a ' . O número de erros na etiquetagem dessas palavras corresponde a um quarto do total de erros cometidos por um etiquetador baseado em VLMCs . Além_disso  , essas palavras parecem apresentar dois diferentes_tipos  de ambiguidade : um dependendo de contexto não local e outro de contexto direito . Exploramos maneiras de expandir o modelo baseado em VLMCs através do uso de diferentes modelos e métodos , a fim de atacar esses problemas . As abordagens mostraram variado grau de sucesso , com um método em particular ( aprendizado guiado ) se mostrando capaz de resolver boa parte da ambiguidade de 'a ' . Discutimos razões para isso acontecer . Com relação a 'que ' , ao longo desta tese propusemos e testamos diversos métodos de aprendizado de informação contextual para tentar desambiguá-lo . Mostramos como , em todos eles , o nível de ambiguidade de 'que ' permanece praticamente constante 
 Métodos de agrupamento hierárquico de textos são muito úteis para analisar o conhecimento embutido em coleções textuais , organizando os documentos textuais em grupos e subgrupos para facilitar a exploração do conhecimento em diversos níveis de granularidade . Tais métodos pertencem à área de aprendizado não supervisionado de máquina , uma que vez obtêm modelos de agrupamento apenas pela observação de regularidades existentes na coleção textual , sem supervisão humana . Os métodos tradicionais de agrupamento assumem que a coleção textual é representada apenas pela informação técnica , ou seja , palavras e frases extraídas diretamente dos textos . Por outro_lado  , em muitas tarefas de agrupamento existe conhecimento adicional e valioso a respeito dos dados , geralmente extraído por um processo avançado com apoio de usuários especialistas do domínio do problema . Devido ao alto custo para obtenção desses dados , esta informação adicional é definida como privilegiada e usualmente está disponível para representar apenas um subconjunto dos documentos textuais . Recentemente , um novo paradigma de aprendizado de máquina denominado LUPI ( Learning Using Privileged Information ) foi proposto por Vapnik para incorporar informação privilegiada em métodos aprendizado_supervisionado  . Neste trabalho de doutorado , o paradigma LUPI foi estendido para aprendizado não supervisionado , em especial , para agrupamento hierárquico de textos . Foram propostas e avaliadas abordagens para lidar com diferentes desafios existentes em tarefas de agrupamento , envolvendo a extração e estruturação da informação privilegiada e seu uso para refinar ou corrigir modelos de agrupamento . As abordagens propostas se mostraram eficazes em ( i ) consenso de agrupamentos , permitindo combinar diferentes representações e soluções de agrupamento ; ( ii ) aprendizado de métricas , em que medidas de proximidades mais robustas foram obtidas com base na informação privilegiada ; e ( iii ) seleção de modelos , em que a informação privilegiada é explorada para identificar relevantes estruturas de agrupamento hierárquico . Todas as abordagens apresentadas foram investigadas em um cenário de agrupamento incremental , permitindo seu uso em aplicações práticas caracterizadas pela necessidade de eficiência computacional e alta frequência de publicação de novo conhecimento textual 
 Problemas de corte e empacotamento de itens_irregulares  são problemas que visam determinar um leiaute ótimo de objetos pequenos dentro de objetos maiores , a fim de atender a uma demanda . Estes problemas têm grande_importância  prática , já que surgem em vários_tipos  de indústria ( como a têxtil , a de móveis e a de calçados ) . O problema estudado neste trabalho é o problema de corte de itens_irregulares  em recipientes . Os recipientes são delimitados e o objetivo é encontrar um leiaute dos objetos menores , sem sobreposição , dentro dos objetos maiores utilizando a menor quantidade de recipientes . Propomos um novo método de resolução para o problema . Nosso método é um algoritmo que gerencia um conjunto de heurísticas , de baixo_nível  , específicas para a resolução do problema com recipientes retangulares e irregulares . Recipientes irregulares são polígonos convexos e não convexos , que podem ser furados . As heurísticas desenvolvidas utilizam uma malha de pontos sobre a técnica de no-fit polygon para evitar a sobreposição dos itens e encontrar posições viáveis no recipiente retangular ou irregular . Os experimentos computacionais foram feitos para um grande conjunto de instâncias , de recipientes retangulares e irregulares . Os resultados demonstram a competitividade do método , que obtêm resultados bons e algumas soluções_ótimas  , em um tempo computacional aceitável 
 A modelagem da volatilidade desempenha um papel fundamental em Econometria . Nesta dissertação são estudados a generalização dos modelos autorregressivos condicionalmente heterocedásticos conhecidos como GARCH e sua principal generalização multivariada , os modelos DCC-GARCH ( Dynamic Condicional Correlation GARCH ) . Para os erros desses modelos são consideradas distribuições de probabilidade possivelmente assimétricas e leptocúrticas , sendo essas parametrizadas em função da assimetria e do peso nas caudas , necessitando assim de estimar esses parâmetros adicionais aos modelos . A estimação dos parâmetros dos modelos é feita sob a abordagem_Bayesiana  e devido às complexidades destes modelos , métodos computacionais baseados em simulações de Monte_Carlo  via Cadeias de Markov ( MCMC ) são utilizados . Para obter maior eficiência computacional os algoritmos de simulação da distribuição a posteriori dos parâmetros são implementados em linguagem de baixo_nível  . Por fim , a proposta de modelagem e estimação é exemplificada com dois conjuntos de dados 
 Este projeto tem como objetivo a especicação de um enlace de comunicação digital para veículos aéreos não tripulados . Os principais desaos presentes no meio de transmissão serão evidenciados , assim como o impacto acarretado no sistema de comunicação . O projeto foi dividido entre a parte analógica e digital . A parte analógica tratará dos requisitos de potência para o devido funcionamento do canal através do procedimento de link budget . O projeto da parte digital , por sua vez , empregará a técnica de transmissão OFDM . No presente_trabalho  foi sugerido um método de estimação do canal utilizando os tons pilotos . O desempenho desta abordagem será medido através de uma simulação de monte 
 A evolução da computação móvel melhora a capacidade de comunicação e colaboração das pessoas . Os principais pilares desta transformação são : o desenvolvimento e produção de dispositivos_móveis  com capacidade multimídia e equipados com duas ou mais interfaces de rede , a disponibilidade de conectividade sem fio ubíqua e a popularização de aplicações sociais online . As redes_sociais  online merecem destaque pelas funcionalidades que permitem a criação e compartilhamento de conteúdo digital dentro de círculos sociais , também chamado de mídia social . Serviços na web anexam a localização geográfica do dispositivo ao conteúdo digital , criando as chamadas mídias_sociais  baseadas em localização . Equipadas com seus telefones e tablets , as pessoas estão criando e consumindo mídias_sociais  em qualquer lugar . Entretanto , é um desafio manter tais dispositivos_móveis  conectados nos ambientes de rede sem fio atuais e de próxima geração e.g. , múltiplos provedores de acesso e múltiplas tecnologias de comunicação . Pesquisas recentes propõem componentes para o gerenciamento de conectividade sem fio que fazem_uso  simultâneo do contexto de conectividade atual e de um conjunto destes dados coletados no passado . Tais componentes são preditores de mobilidade , mecanismos de handover ou gerenciadores de mobilidade que utilizam dados de contexto de conectividade de forma particular para atingir seus propósitos . Na presente investigação , propomos uma metodologia que orquestra os principais componentes de gerenciamento de conectividade em um laço retro alimentado . Argumentamos que a coleta de dados de contexto de conectividade pode ser projetada como um sistema de sensoreamento , cujo sensores são as interfaces de rede sem fio . Como parte deste sistema de sensoriamento , os círculos sociais podem assistir o gerenciamento de conectividade compartilhando dados de contexto de conectividade . A ideia central é utilizar serviços baseados em localização para compartilhar dados de contexto de conectividade dentro dos círculos sociais . Desta forma , as redes_sociais  online adicionam escala para o sistema e permite colaboração em volta de dados de contexto recentes , locais , personalizados e sociais . O objetivo é melhorar experiências de conectividade sem fio e.g. , métricas de QoS ( Quality of Service ) como : vazão , latência e qualidade do sinal . Relatamos como os dados de contexto de conectividade são manipulados com um modelo baseado em grafos e métricas como : intensidade do vértice e grau centralidade . Com isso , identificamos áreas com alta densidade de handovers , definimos a reputação dos usuários e revelamos a cobertura das redes . Resultados de experimentos mostram que a colaboração pode melhorar métricas de QoS de ~18 a ~30 % se comparado ao uso de um preditor de mobilidade ou um sistema_operacional  moderno , respectivamente . Esta discussão se desdobra com foco na viabilidade da solução em termos de sobrecarga de armazenamento e consumo de energia . Os promissores resultados experimentais indicam que nossa solução pode melhorar experiências de conectividade sem fio de usuários 
 Este projeto apresenta a proposta de uma arquitetura de Metaescalonador que leva em consideração o emprego de qualidade de serviço ( QoS ) para o ambiente de Computação em Nuvem . O Metaescalonador é capaz de realizar a alocação dos recursos dinamicamente , procurando atender as restrições temporais . Em resposta a esse dilema de escalonamento aplicado a Computação em Nuvem , este projeto propõe uma abordagem chamado MACC : Metascheduler Architecture to provide QoS in Cloud Computing . A função principal do MACC é distribuir e gerenciar o processamento das requisições de serviços entre os recursos disponíveis , cumprindo os termos agregados na SLA - Service Level Agreement . São apresentados resultados obtidos considerando-se diferentes algoritmos de roteamento e de alocação de máquinas virtuais . Os resultados apresentados são discutidos e analisados de acordo com as técnicas de planejamento de 
 Com a evolução da informática , diferentes meios de comunicação passaram a explorar a Web como um meio de divulgação de suas informações . Diferentes fontes de informações , diferentes estilos de escrita e a curiosidade nata do ser humano despertam o interesse de leitores por conhecer mais de um relato sobre um mesmo tema . Para que a leitura de diferentes relatos com conteúdo similar seja possível , leitores precisam procurar , ler e analisar informações fornecidas por diferentes fontes de informação . Essa atividade , além de exigir grande investimento de tempo , sobrecarrega cognitivamente usuários . Faz parte das pesquisas da área de Hipermídia investigar mecanismos que apóiem usuários no processo de identificação de informações em repositórios homogêneos , sejam eles disponibilizados na Web ou não . No contexto desta tese , repositórios com informações de conteúdo homogêneo são aqueles cujas informações tratam do mesmo assunto . Esta tese tem por objetivo investigar a especificação , a instanciação e a experimentação de um arcabouço para apoiar a tarefa de criação automática de ligações hipertexto entre repositórios homogêneos . O arcabouço proposto , denominado CARe ( Criação Automática de Relacionamentos ) , é representado por um conjunto de classes que realizam a coleta de informações a serem relacionadas e que processam essas informações para a geração de índices . Esses índices são relacionados e utilizados na criação automática de ligações hipertexto entre a informação original . A definição do arcabouço se deu após uma fase de análise de domínio na qual foram identificados requisitos e construídos componentes de software . Nessa fase , vários protótipos também foram construídos de modo 
 Sistemas computadorizados para o processamento de multimídia em tempo real demandam alta capacidade de processamento . Problemas que exigem grandes capacidades de processamento são comumente abordados através do uso de sistemas paralelos ou distribuídos ; no entanto , a conjunção das dificuldades inerentes tanto aos sistemas de tempo real quanto aos sistemas paralelos e distribuídos tem levado o desenvolvimento com vistas ao processamento de multimídia em tempo real por sistemas computacionais de uso geral a ser baseado em equipamentos centralizados e monoprocessados . Em diversos sistemas para multimídia há a necessidade de baixa latência durante a interação com o usuário , o que reforça ainda mais essa tendência para o processamento em um único nó 
 Neste trabalho , implementamos um mecanismo para o processamento síncrono e distribuído de áudio com características de baixa latência em uma rede local , permitindo o uso de um sistema distribuído de baixo custo para esse processamento . O objetivo primário é viabilizar o uso de sistemas computacionais_distribuídos  para a gravação e edição de material musical em estúdios domésticos ou de pequeno_porte  , contornando a necessidade de hardware dedicado de alto custo 
 O sistema implementado consiste em duas partes : uma , genérica , implementada sob a forma de um middleware para o processamento síncrono e distribuído de mídias contínuas com baixa latência ; outra , específica , baseada na primeira , voltada para o processamento de áudio e compatível com aplicações legadas através da interface padronizada LADSPA . É de se esperar que pesquisas e aplicações futuras em que necessidades semelhantes se apresentem possam utilizar o middleware aqui descrito para outros_tipos  de processamento de áudio bem como para o processamento de outras mídias , como vídeo 
 Existem atualmente diversas propostas para integração de dispositivos_móveis  em uma grade computacional , porém vários problemas são observados em tais ambientes . Esta dissertação mantém o foco em um problema , a restrição sobre a quantidade de energia despendida na execução das aplicações , ao utilizar esses dispositivos_móveis  como provedores de recursos em uma grade computacional que fornece processamento para aplicações paralelas . Para tanto , este trabalho propõe um método para estimar o consumo de energia das aplicações considerando que elas utilizam um determinado conjunto de operações as quais estão presentes na grande maioria das aplicações paralelas ( operações matemáticas e alocação de memória ) . Com base no método proposto , dois dispositivos_móveis  foram estudados e foi criada uma representação do consumo de energia utilizando-se de métodos de regressão . Para validar os modelos , duas aplicações foram analisadas e o consumo de energia real foi comparado ao consumo estimado . O modelo criado apresentou resultados próximos ao medido , mostrando um aumento entre 6 % e 14,24 % em relação ao resultado medido 
 Com o crescimento acelerado da complexidade das aplicações e softwares que exigem alto_desempenho  , o hardware e sua arquitetura passou por algumas mudanças para que pudesse atender essa necessidade . Uma das abordagens propostas e desenvolvidas para suportar essas aplicações , foi a integração de mais de um core de processamento em um único circuito integrado . Inicialmente , a comunicação utilizando barramento foi escolhida , pela sua vantagem de reuso comparado a ponto a ponto . No entanto , com o aumento acelerado da quantidade de cores nos Systems-on-Chip ( SoC ) , essa abordagem passou a apresentar problemas para suportar a comunicação interna . Uma alternativa que vem_sendo  explorada é a Network-on-Chip ( NoC ) , uma abordagem que propõe utilizar o conhecimento de redes comuns em projetos de comunicação interna de SoC . Esse trabalho fornece uma arquitetura de NoC completa , configurável , parametrizável e no padrão Ethernet . Os três_módulos  básicos da NoC , Network Adapter ( NA ) , Link e Switch , são implementados e disponibilizados . Os resultados foram obtidos utilizando o FPGA Stratix IV da Altera . As métricas de desempenho utilizadas para validação da NoC são a área no FPGA e o atraso na comunicação . Os parâmetros disponibilizados são referentes as configurações dos módulos desenvolvidos , considerando características apresentadas de aplicações DSP ( Digital Signal Processing ) . O experimento utilizando dois NAs , dois cores e um Switch precisou de 7310 ALUTs do FPGA EP4SGX230KF40C2ES o que corresponde a 4 % dos seus recursos lógicos . O tempo gasto para a transmissão de um quadro ethernet de 64 Bytes foi de 422 ciclos de clock a uma frequência de 50MHz 
 A diversificação nas formas de acesso à Web , alavancada pelo crescente uso de dispositivos ubíquos , principalmente smartphones e tablets , tem motivado o desenvolvimento de métodos para adaptação do conteúdo disponível na Web , dado que grande parte do conteúdo não foi projetado para ser acessado nesse novo contexto . O conteúdo Web é composto por diversos elementos que , em geral , fornecem interatividade aos usuários . Alguns destes elementos , tais como menus , são responsáveis por auxiliar a navegação no site , ajudando na localização e acesso das informações que por ventura o usuário esteja procurando . No entanto , muitos dos menus disponíveis na Web não foram desenvolvidos de uma forma acessível e nem suportam a interação em dispositivos , criando dificuldades que impedem os usuários interagirem satisfatoriamente com esses elementos . Além da diversificação de modos de acesso , existe também a diversificação do perfil de usuários que fazem_uso  dos recursos da Web . Alguns desses usuários encontram barreiras que tendem a dificultar ou limitar seu acesso às aplicações e conteúdos Web em geral . Nesse sentindo , este trabalho teve como objetivo principal abordar o problema de adaptação de conteúdo Web para dispositivos_móveis  , com foco nos menus , fornecendo uma solução personalizada de acordo com as preferências do usuário , a fim de que as barreiras de interação fossem reduzidas ou eliminadas . Inicialmente estudou-se os diferentes padrões de menus , bem como as diretrizes de acessibilidade e usabilidade para criação de menus acessíveis . A partir desses estudos , foi desenvolvido uma metamodelo que deu origem a linguagem AMenu , contendo todos os detalhes técnicos sobre a acessibilidade e usabilidade . Em seguida , foi desenvolvida a ferramenta AMeG , para validar a linguagem AMenu . Com base na avaliação da linguagem , foi desenvolvido o mecanismo de adaptação , que faz_uso  da linguagem AMenu para geração dos menus acessíveis e adaptados para dispositivos_móveis  . Finalmente , um estudo de caso com usuários idosos foi conduzido , a fim de comparar os menus gerados pelo mecanismo em relação aos menus originais . Os resultados obtidos apontam um melhor desempenho na interação com os menus gerados pelo mecanismo , uma vez que os participantes obtiveram uma redução de 54 % no tempo de realização das tarefas e cometeram 82 % menos erros . Por outro_lado  , os resultados apontam que todos os participantes responderam melhor à abordagem de adaptação desenvolvida neste estudo , uma vez que eles conseguiram concluir com sucesso todas as tarefas definidas para o estudo 
 A segurança nos voos de Veículos Aéreos Não Tripulados ( VANTs ) é uma importante questão e vem ganhando destaque devido a uma série de acidentes com tais aeronaves . O aumento do número de aeronaves no espaço aéreo e a autonomia cada vez maior para realizar missões estão entre outros elementos que merecem destaques . No entanto , pouca atenção tem sido dada a autonomia da aeronave em casos emergenciais [ Contexto ] . Nesse contexto , o desenvolvimento de algoritmos que efetuem o planejamento de rotas na ocorrência de situações críticas é fundamental para obter maior segurança aérea . Eventuais situações de insegurança podem_estar  relacionadas a uma falha nos equipamentos do veículo aéreo que impede a continuação da missão em curso pela aeronave [ Lacuna ] . A presente pesquisa avança o estado da arte considerando um conceito chamado In-Flight Awareness ( IFA ) , que estabelece consciência situacional em VANTs , visando maior segurança de voo . Os estudos também avançam na proposição de modelos matemáticos que representem o estado da aeronave avariada , viabilizando o pouso emergencial e minimizando possíveis danos [ Propósito ] . Este trabalho utiliza técnicas de computação evolutiva como Algoritmos Genéticos ( AG ) e Algoritmos Genéticos Multi-Populacional ( AGMP ) , além de uma Heurística Gulosa ( HG ) e um modelo de Programação Linear Inteira Mista ( PLIM ) no tratamento de falhas críticas juntamente com o conceito de IFA [ Metodologia ] . As soluções obtidas foram avaliadas através de experimentos offline usando os modelos matemáticos desenvolvidos , além de validadas em um simulador de voo e em um voo real . De forma geral , o AG e AGMP obtiveram resultados equivalentes , salvando o VANT em aproximadamente 89 % dos mapas . A HG conseguiu trazer a aeronave até uma região bonificadora em 77 % dos mapas dentro de um tempo computacional abaixo de 1 segundo . No modelo PLIM , o tempo gasto foi de cerca de quatro minutos já que garantia a otimalidade da solução encontrada . Devido ao seu elevado tempo computacional , uma estratégia evolvendo rotas pré-calculadas foi definida a partir do PLIM , mostrando-se bastante promissora . Nos experimentos envolvendo simulador de voo foram testadas diferentes condições de vento e se verificou que mesmo sobre tais condições os métodos desenvolvidos conseguiram efetuar o pouso com segurança [ Resultado ] . O trabalho apresentado colabora com a segurança de Veículos Aéreos Não Tripulados e com a proposta de modelos matemáticos que representem a aeronave em caso de situações críticas . Os métodos , de forma geral , mostraram-se promissores na resolução do problema de pouso emergencial já que trouxeram a aeronave com segurança até regiões interessantes ao pouso em um baixo tempo computacional . Isso foi atestado pelos resultados obtidos a partir das simulações offline , em simulador de voo e em voo real [ Conclusão ] . As principais_contribuições  do trabalho são : modelagem de regiões adequadas ao pouso , modelagem de falhas , arquitetura do sistema planejador de rotas e modelo linear para para pouso emergencial [ Contribuição ] 
 O problema de empacotamento de itens_irregulares  com balanceamento da carga é encontrado no carregamento de aviões , caminhões e navios . O objetivo é empacotar itens_irregulares  utilizando o menor número de recipientes possível de forma que os recipientes estejam balanceados , que os itens não se sobreponham e estejam inteiramente contidos no recipiente . Neste trabalho , propomos três heurísticas bases com três variações cada para o problema com recipientes retangulares e irregulares . As heurísticas utilizam abordagens diferentes para representar os itens e para fazer o balanceamento . Uma das heurísticas utiliza malha para representação dos itens e faz o balanceamento dividindo o recipiente em quadrantes e revezando a alocação dos itens entre eles de forma que o balanceamento é feito de forma indireta . Tal heurística resolve o problema tanto para recipientes retangulares quanto irregulares . A segunda heurística utiliza a representação dos itens por polígonos e impossibilita a sobreposição de itens utilizando a técnica do nofit polygon . A heurística constrói a solução item por item , sem posições fixas e a cada item alocado , os itens são deslocados em direção ao centro de gravidade desejado do recipiente . Esta heurística resolve apenas problemas com recipientes retangulares . A última heurística é uma adaptação da heurística anterior para a resolução do problema com recipientes irregulares , de forma que o problema é resolvido em duas fases . Cada heurística base possui três variações cada , totalizando nove heurísticas . As heurísticas foram comparadas com outro trabalho da literatura e conseguiram melhorar os resultados para nove das dezenove instâncias testadas 
 Atualmente , novas alternativas para as redes de computadores estão surgindo , tais como ReMAVs ( Redes Metropolitanas de Alta Velocidade ) , Internet2 , RNP2 e Advanced ANSP ( Academic Network at São Paulo ) . Neste novo cenário , alta largura de banda e qualidade de serviço são aspectos fundamentais . Tais redes têm como objetivo suportar o tráfego de áudio e vídeo em tempo real com qualidade , o que viabilizará o desenvolvimento de aplicações avançadas - por exemplo , aplicações de videoconferência - que exigem maiores capacidades das mesmas . Este projeto apresenta , na revisão bibliográfica , conceitos de videoconferência ; suas principais características ; e algumas considerações sobre os tipos de cenários e o ambiente dos participantes . Neste projeto foi realizada uma avaliação de sistemas de videoconferência baseando-se em parâmetros que foram estabelecidos de acordo com a recomendação ITU-T F.730 . Com o desenvolvimento deste trabalho foi possível estabelecer e validar um conjunto mínimo de parâmetros para avaliação que poderá auxiliar na escolha de sistemas de videoconferência 
 Diversos problemas práticos envolvendo sistemas de visão_computacional  , tais como vigilância automatizada , pesquisas de conteúdo específico em bancos de dados multimídias ou edição de vídeo , requerem a localização e o reconhecimento de objetos dentro de seqüências de imagens ou vídeos digitais . Mais formalmente , denomina-se rastreamento o processo de determinação da posição de certo ( s ) objeto ( s ) ao longo do tempo numa seqüência de imagens . Já a tarefa de reconhecimento caracteriza-se pela classificação desses objetos de acordo com algum rótulo pré-estabelecido ou apoiada em conhecimento prévio tipicamente introduzido através de um modelo dos objetos de interesse . No entanto , rastrear e classificar objetos em vídeo digital são tarefas desafiadoras , tanto pelas dificuldades inerentes a esse tipo de elemento pictórico , quanto pelo variável grau de complexidade que os quadros sob análise podem apresentar . Este documento apresenta uma metodologia baseada em modelo para rastrear e reconhecer objetos em vídeo digital através de uma representação por grafos relacionais com atributos ( ARGs ) . Tais estruturas surgiram dentro do paradigma de reconhecimento estrutural de padrões e têm se mostrado bastante flexíveis e poderosas para modelar problemas diversos , pois podem transmitir dados quantitativos , relacionais , estruturais e simbólicos . Como modelo e entrada são descritos através desses grafos , a questão de reconhecimento é interpretada como um problema de casamento inexato entre grafos , que consiste em mapear os vértices do ARG de entrada nos vértices do ARG modelo . Em seguida , é realizado o rastreamento dos objetos de acordo com uma transformação afim derivada de parâmetros obtidos da etapa de reconhecimento . Para validar a metodologia_proposta  , resultados sobre seqüências de imagens_digitais  , sintéticas e reais , são apresentados e discutidos 
 Visualização é o processo genérico que utiliza representações_visuais  e interativas para facilitar a análise e o entendimento de informações de conjunto de dados . A maioria das ferramentas de visualização existentes atualmente utiliza exclusivamente recursos visuais para representar informações e isto tem limitado a capacidade exploratória e a apresentação de dados . Vários estudos têm demonstrado que o uso do som como recurso alternativo para representação de dados ( sonificação ) pode ser útil na interpretação de informações e também pode apoiar o aumento da dimensionalidade da apresentação visual . A sonificação é o objeto de estudo deste trabalho . Este trabalho implementa o novo módulo de sonificação de um sistema de exploração visual de dados , o Super Spider ( Watanabe , 2007 ) , que foi estendido com a implementação de recursos que auxiliam a exploração de dados por meio de sons . Um novo sistema , chamado Sonar 2D , também foi desenvolvimento de forma integrada ao Super Spider e apresenta uma nova_técnica  para sonificação de dados . Além_disso  , são apresentados resultados de testes com usuários aplicados para avaliar e validar os mapeamentos visuais e sonoros utilizados nos 
 A comunicação multilíngue é uma tarefa cada vez mais imperativa no cenário atual de grande disseminação de informações em diversas línguas . Nesse contexto , são de grande relevância os sistemas de tradução automática , que auxiliam tal comunicação , automatizando-a . Apesar de ser uma área de pesquisa bastante antiga , a Tradução Automática ainda apresenta muitos problemas . Um dos principais problemas é a ambigüidade lexical , ou seja , a necessidade de escolha de uma palavra , na língua alvo , para traduzir uma palavra da língua fonte quando há várias opções de tradução . Esse problema se mostra ainda mais complexo quando são identificadas apenas variações de sentido nas opções de tradução . Ele é denominado , nesse caso , `` ambigüidade lexical de sentido '' . Várias abordagens têm sido propostas para a desambiguação lexical de sentido , mas elas são , em geral , monolíngues ( para o inglês ) e independentes de aplicação . Além_disso  , apresentam limitações no que diz_respeito  às fontes de conhecimento que podem ser exploradas . Em se tratando da língua_portuguesa  , em especial , não há pesquisas significativas voltadas para a resolução desse problema . O objetivo deste trabalho é a proposta e desenvolvimento de uma nova abordagem de desambiguação lexical de sentido , voltada especificamente para a tradução automática , que segue uma metodologia híbrida ( baseada em conhecimento e em córpus ) e utiliza um formalismo relacional para a representação de vários_tipos  de conhecimentos e de exemplos de desambiguação , por meio da técnica de Programação Lógica Indutiva . Experimentos diversos mostraram que a abordagem proposta supera abordagens alternativas para a desambiguação multilíngue e apresenta desempenho_superior  ou comparável ao do estado da arte em desambiguação monolíngue . Adicionalmente , tal abordagem se mostrou efetiva como mecanismo auxiliar para a escolha lexical na tradução automática 
 Na maioria dos processos de análise de imagens há a necessidade de um pré-processamento , no qual são extraídos e calculados vetores de características que representem as imagens são utilizados no cálculo de similaridade . Uma dificuldade nessas tarefas é o grande número de características que definem um espaço de alta dimensionalidade , afetando fortemente o desempenho das tarefas que seguem , que podem envolver uma análise visual , um agrupamento ou uma classificação de dados , por exemplo . Lidar com esse problema normalmente exige técnicas de redução de dimensionalidade ou seleção de características . O presente_trabalho  dá sequência a trabalhos que utilizam técnicas de visualização como suporte para avaliar espaços de características gerados a partir de coleções de imagens . Nele , objetiva-se aprimorar um método baseado na análise visual de conjuntos de imagens empregando a árvore de similaridade Neighbor-Joining que apoia o usuário a selecionar um subespaço de características que mantenha ou melhore os resultados das visualizações do conjunto de imagens . A partir da metodologia_proposta  , a avaliação e a seleção de características representativas é realizada usando a visualização NJ . A maior_parte  dos experimentos responde positivamente para diferentes conjuntos de imagens representados por vários extratores , obtendo-se processos de seleção personalizados mais precisos e eficazes , em termos de agrupamento , do que abordagens automáticas reportadas na 
 Usuários da Internet estão muito familiarizados que resultados de uma consulta sejam exibidos como uma lista ordenada de snippets . Cada snippet possui conteúdo textual que mostra um resumo do documento referido ( ou página web ) e um link para o mesmo . Esta representação tem muitas vantagens como , por exemplo , proporcionar uma navegação fácil e simples de interpretar . No entanto , qualquer usuário que usa motores de busca poderia reportar possivelmente alguma experiência de decepção com este modelo . Todavia , ela tem limitações em situações particulares , como o não fornecimento de uma visão geral da coleção de documentos recuperados . Além_disso  , dependendo da natureza da consulta - por exemplo , pode ser muito geral , ou ambígua , ou mal expressa - a informação desejada pode ser mal classificada , ou os resultados podem contemplar temas variados . Várias tarefas de busca seriam mais fáceis se fosse devolvida aos usuários uma visão geral dos documentos organizados de modo a refletir a forma como são relacionados , em relação ao conteúdo . Propomos uma técnica de visualização para exibir os resultados de consultas web que visa superar tais limitações . Ela combina a capacidade de preservação de vizinhança das projeções_multidimensionais  com a conhecida representação baseada em snippets . Essa visualização emprega uma projeção_multidimensional  para derivar layouts bidimensionais dos resultados da pesquisa , que preservam as relações de similaridade de texto , ou vizinhança . A similaridade é calculada mediante a aplicação da similaridade do cosseno sobre uma representação bag-of-words vetorial de coleções construídas a partir dos snippets . Se os snippets são exibidos diretamente de acordo com o layout derivado , eles se sobrepõem consideravelmente , produzindo uma visualização pobre . Nós superamos esse problema definindo uma energia funcional que considera tanto a sobreposição entre os snippets e a preservação da estrutura de vizinhanças como foi dada no layout da projeção . Minimizando esta energia funcional é fornecida uma representação bidimensional com preservação das vizinhanças dos snippets textuais com sobreposição mínima . A visualização transmite tanto uma visão global dos resultados da consulta como os agrupamentos visuais que refletem documentos relacionados , como é ilustrado em vários dos exemplos 
 Devido à complexidade das aplicações , a demanda crescente por sistemas que usam milhões de transistores e hardware complexo ; tem sido desenvolvidas ferramentas que convertem C em Linguagem de Descrição de Hardware , tais como VHDL e Verilog . Neste contexto , esta tese apresenta o projeto ChipCflow , o qual usa arquitetura a fluxo de dados , para implementar lógica de alto_desempenho  em Field Programmable Gate Array ( FPGA ) . Maquinas a fluxo de dados são computadores programáveis , cujo hardware é otimizado para computação_paralela  de granularidade fina dirigida por dados . Em outras_palavras  , a execução de programas é determinado pela disponibilidade dos dados , assim , o paralelismo é intrínseco neste sistema . Por outro_lado  , com o avanço da tecnologia da microeletrônica , o FPGA tem sido utilizado principalmente devido a sua flexibilidade , facilidade para implementar sistemas complexos e paralelismo intrínseco . Um dos desafios é criar ferramentas para programadores que usam linguagem de alto nível ( HLL ) , como a linguagem C , e produzir hardware diretamente . Essas ferramentas devem usar a máxima experiência dos programadores , o paralelismo das arquiteturas a fluxo de dados dinâmica , a flexibilidade e o paralelismo do FPGA , para produzir um hardware eficiente , otimizado para alto_desempenho  e baixo consumo de energia . O projeto ChipCflow é uma ferramenta que converte os programas de aplicação escritos em linguagem C para a linguagem VHDL , baseado na arquitetura a fluxo de dados dinâmica . O principal objetivo dessa tese é definir e implementar os operadores do ChipCflow , usando a arquitetura a fluxo de dados dinâmica em FPGA . Esses operadores usam tagged tokens para identificar dados , com base em instâncias de operadores . A implementação dos operadores e das instâncias usam um modelo de implementação assíncrono em FPGA para obter maior velocidade e menor 
 Nesta tese , é pesquisada a integração entre dois paradigmas da área de Inteligência_Artificial  , Raciocínio_Baseado  em Casos e Redes_Neurais  Artificiais . Essa pesquisa dá-se em dois sentidos . Primeiro , o estudo da aplicação da metodologia de Raciocínio_Baseado  em Casos ao problema da escolha e configuração de um modelo de Rede_Neural  Artificial . Em segundo lugar , a viabilidade da introdução de uma Rede_Neural  Artificial no interior do ciclo de funcionamento de um sistema baseado em casos . As soluções para o problema de escolha e configuração de um modelo de Rede_Neural  ainda possuem , até hoje , uma forte componente empírica . Não existe um conhecimento formalizado disponível que forneça suporte a um processo único de implementação destes sistemas , ditos conexionistas . A qualidade da solução depende em muito da habilidade do projetista em ajustar um conjunto de diversos parâmetros envolvidos . A metodologia de Raciocínio_Baseado  em Casos , por sua vez , fundamenta-se na idéia de que um especialista eficiente não é um processador de regras , mas um acumulador de experiências práticas , bem e mal sucedidas . Desta forma , ela torna-se bastante adequada à aplicação em domínios em que o conhecimento é mais difuso , ou seja , não pode ser facilmente explicitado . A partir destas observações , é proposta a representação do problema como uma tarefa tipicamente de projeto { design ) e estabelecida uma estratégia para aplicar a metodologia em sua solução . No outro sentido , a escolha da melhor solução , dentro da metodologia de Raciocínio_Baseado  em Casos , depende de bons processos que permitam a transformação de uma solução anterior em uma solução adequada ao problema atual . Esses processos podem beneficiar-se , conforme é mostrado ao longo do trabalho , de uma boa capacidade de generalização de um conhecimento adquirido . Na maioria dos sistemas existentes , essas transformações , ou adaptações , são executadas através de regras de produção . Essas regras por sua vez exigem também um grau de aquisição de conhecimento em domínios nem sempre bem estruturados . Redes_Neurais  Artificiais possuem como ponto forte a capacidade de aprender a partir de exemplos , extrair características intrínsecas de conjuntos de dados e generalizar esse conhecimento adquirido . Essa capacidade as credencia como boas alternativas para substituição de sistemas baseados em regras . O que pode ser considerado um ponto fraco das Redes_Neurais  , sua carência de justificativas para , por exemplo , associações ou previsões efetuadas , não constitui um empecilho para sua intodução neste ponto específico do ciclo de Raciocínio_Baseado  em Casos . Com base nestas premissas , este trabalho sugere uma abordagem híbrida neurosimbólica como mecanismo de recuperação e adaptação de casos desse ciclo . Para servir como ferramenta de testes , foi também implementado um ambiente de desenvolvimento de sistemas de Raciocínio_Baseado  em Casos 
 Raciocínio_Baseado  em Casos é uma metodologia para a resolução de problemas baseado em experiências passadas . Essa metodologia tenta solucionar um novo problema recuperando e adaptando soluções previamente conhecidas de problemas similares . Porém , cada solução recuperada , em geral , requer adaptações para que possa ser utilizada como solução de um novo problema . Portanto , a adaptação de casos é uma característica desejável em sistemas de Raciocínio_Baseado  em Casos . Um dos maiores desafios da área de RBC é o desenvolvimento de métodos eficientes para a adaptação de casos . Em contraste com a aquisição de casos , o conhecimento para adaptação não é facilmente disponibilizado e é de difícil obtenção ( Hanney , 1996 ; Wiratunga et al. , 2002 ) . A forma de adaptação mais utilizada é a codificação de regras de adaptação , demandando um significativo esforço para a aquisição de conhecimento ( Hanney , 1996 ) . Uma alternativa para superar as dificuldades associadas à aquisição de conhecimento para adaptação de casos tem sido a utilização de abordagens híbridas e de algoritmos de aprendizado automático para a aquisição do conhecimento utilizado para a adaptação . Este trabalho_investiga  a utilização de abordagens híbridas para adaptação de casos empregando algoritmos de Aprendizado de Máquina . As abordagens aprendem o conhecimento necessário para a adaptação de casos automaticamente a partir de uma base de casos e aplicam esse conhecimento para realizar a adaptação de soluções recuperadas 
 A computação ciente de contexto faz_uso  da exploração de informações contextuais para prover serviços relevantes aos usuários . Lacunas dessa área são : a carência de representações contextuais extensíveis , e a falta de exploração de outros_tipos  de informações contextuais , que não o contexto do Usuário . Aplicações que proporcionam acesso ao conteúdo por meio de dispositivos heterogêneos , por exemplo , necessitam imperativamente de informações contextuais referentes a esses dispositivos , classificadas na literatura como informações contextuais referentes à Infra-Estrutura . Com o aumento na quantidade de informações de contexto a ser utili7ada , um outro problema emerge : a necessidade de gerenciamento dessas informações . A literatura aponta alguns trabalhos que convergem para a solução desse problema . Porém , todos eles apresentam limitações em relação à representação de contexto . Este trabalho tem como propósito explorar a representação contextual referente à Infra-Estrutura , sobretudo a representação de contexto de dispositivos e rede e desenvolver um serviço de gerenciamento de informações de contexto . Além_disso  , o trabalho apresenta como contribuições : a especificação do modelo que representa o contexto da Infra-Estrutura , o desenvolvimento do serviço de gerenciamento contextual , seus componentes , especificações da arquitetura , modelagens e fluxogramas 
 Com o aumento da complexidade dos sistemas computacionais , fez-se necessária a utilização de diversas ferramentas para simplificar o processo de detecção de intrusão e de manutenção da segurança . A grande maioria dessas ferramentas baseia-se na geração de mensagens , denominadas alertas , que informam ao administrador sobre eventos ocorridos . Atualmente , não há uma classificação que permita interoperabilidade semântica dos alertas gerados pelas diferentes aplicações , fato que motivou este trabalho de mestrado , que visa estudar e propor o uso de uma ontologia para classificar vulnerabilidades de segurança . A partir dessa motivação , foram iniciados estudos para a modelagem de uma ontologia de vulnerabilidades , especificando suas classes , atributos e relacionamentos . A ontologia proposta foi implementada utilizando a linguagem OWL e as informações catalogadas pelo projeto CVE [ Common Vulnerabilities and Exposures ) . Diversas vulnerabilidades foram classificadas e foram efetuados testes usando a linguagem RDQL , os quais permitiram comprovar a viabilidade na extração de informações da ontologia 
 Neste mestrado estudamos o problema integrado de produção e distribuição ( PIPD ) . Ao resolver o PIPD , tenta-se obter de maneira integrada as decisões de produção , controle de estoque , distribuição e roteamento de veículos . Neste tipo de problema , em cada_período  , um único ou múltiplos itens são produzidos e distribuídos para os clientes com o objetivo de atender uma determinada demanda a um custo total mínimo . Ou seja , no PIPD deve-se decidir quando e quanto produzir de cada item e as rotas e distribuições de cada veículo tal que o custo final , que inclui custos de produção , estoque e distribuição , seja mínimo . Estudamos o problema e modelo matemático considerado em Armentano et_al  . ( 2011 ) . Para tratar o PIPD propomos heurísticas baseadas em programação matemática e geramos instâncias com múltiplos itens para testar o desempenho destas heurísticas . Comparamos os resultados obtidos pelas heurísticas com o solver comercial Cplex . Os resultados mostram que algumas das abordagens propostas obtiveram soluções de boa_qualidade  considerando as instâncias geradas 
 O presente documento tem por objetivo apresentar a tese desenvolvida no Programade Doutorado em Ciência da Computação e Matemática Computacional do ICMC/USP . O tema da tese busca avançar o estado da arte ao resolver os problemas de escalabilidade e representação presentes em algoritmos de planejamento para missões com Veículos Aéreos Não Tripulados ( VANTs ) . Técnicas baseadas em programação matemática e computação evolutiva são propostas . Artigos foram publicados , submetidos ou se encontram em fase final de elaboração . Esses trabalhos reportamos avanços mais significativos obtidos na representação e escalabilidade deste problema.Os planejadores de missão trabalhados na tese lidam com problemas estocásticos em ambientes não convexos , onde os riscos de colisão ou falhas no planejamento da missão são tratados e limitados a um valor tolerado . Os avanços na representação permitiram solucionar violações nos riscos presentes na modelagem original , além de tornar os modelos mais realistas ao incorporar aspectos como efeitos da resistência do ar . Para isso , técnicas eficientes de modelagem matemática permitiram avançar de um modelo de Programação Não-Linear Inteira Mista ( PNLIM ) , originalmente proposto na literatura , para um problema de Programação Linear Inteira Mista ( PLIM ) . A modelagem como um PLIM levou à resolução do problema de forma mais eficiente através do algoritmo branch-and-cut . As novas representações propostas resultaram em melhorias na escalabilidade , solucionando problemas mais complexos em um tempo computacional menor.Além disso , os avanços em escalabilidade mostraram-se mais efetivos quando técnicas combinando programação matemática e metaheurísticas foram aplicadas ao problema 
 A análise do sentimento orientada a aspectos é o campo de estudo que extrai e interpreta o sentimento , geralmente classificado como positivo ou negativo , em direção a algum alvo ou aspecto em um texto de opinião . Esta tese de doutorado detalha um estudo empírico de técnicas e métodos para extração de aspectos em análises de sentimentos baseadas em aspectos com foco na língua_Portuguesa  . Foram exploradas três diferentes abordagens : métodos baseados na frequências , métodos baseados na relação e métodos de aprendizagem de máquina . Em cada abordagem , este trabalho mostra um estudo comparativo entre um córpus para o Português e outro para o Inglês e as diferenças encontradas na aplicação destas abordagens . Além_disso  , o conhecimento linguístico mais rico também é explorado pelo uso de dependências sintáticas e papéis semânticos , levando a melhores_resultados  . Este trabalho resultou no estabelecimento de novos padrões de avaliação para a extração de aspectos em Português 
 Dados temporais são ubíquos em quase todas as áreas do conhecimento humano . A área de aprendizado de máquina tem contribuído para a mineração desse tipo de dados com algoritmos para classificação , agrupamento , detecção de anomalias ou exceções e detecção de padrões recorrentes , dentre outros . Tais algoritmos dependem , muitas_vezes  , de uma função capaz de expressar um conceito de similaridade entre os dados . Um dos mais importantes modelos de classificação , denominado 1-NN , utiliza uma função de distância para comparar uma série_temporal  de interesse a um conjunto de referência , atribuindo à primeira o rótulo da série de referência mais semelhante . Entretanto , existem situações nas quais os dados temporais são insuficientes para identificar vizinhos de acordo com o conceito associado às classes . Uma possível abordagem é transportar as séries para um domínio de representação no qual atributos mais relevantes para a classificação são mais claros . Por exemplo , uma série_temporal  pode ser decomposta em componentes periódicas de diferentes frequências e amplitudes . Para muitas aplicações , essas componentes são muito mais significativas na discriminação das classes do que a evolução da série ao longo do tempo . Nesta Tese , emprega-se diversidade de representações e de distâncias para a classificação de séries_temporais  . Com base na escolha de uma representação de dados adequada para expor as características discriminativas do domínio , pode-se obter classificadores mais fiéis ao conceitoalvo . Para esse fim , promove-se um estudo de domínios de representação de dados temporais , visando identificar como esses domínios podem estabelecer espaços alternativos de decisão . Diferentes modelos do classificador 1-NN são avaliados isoladamente e associados em ensembles de classificadores a fim de se obter classificadores mais robustos . Funções de distância e domínios alternativos de representação são também utilizados neste trabalho para produzir atributos não temporais , denominados atributos de distâncias . Esses atributos refletem conceitos de vizinhança aos exemplos do conjunto de treinamento e podem ser utilizados para treinar modelos de classificação que tipicamente não são eficazes quando treinados com as observações originais . Nesta Tese mostra-se que atributos de distância permitem obter resultados compatíveis com o estado-da-arte 
 Máquinas virtuais de linguagens de programação têm desempenhado um papel_importante  como mecanismo para a implementação de linguagens de programação . Linguagens voltadas para esses ambientes de execução possuem várias vantagens em relação às linguagens compiladas . Essas vantagens fizeram com que tais ambientes de execução se tornassem amplamente_utilizados  pela indústria e academia . Entretanto , a maioria dos estudos nessa area têm se dedicado a aprimorar o desempenho desses ambientes de execução e poucos têm enfocado o desenvolvimento de funcionalidades que automatizem ou facilitem a condução de atividades de engenharia de software , incluindo atividades de teste de software . Este trabalho apresenta indícios de que máquinas virtuais de linguagens de programação podem apoiar a criação de ambientes de teste de software integrado . Para tal , duas funcionalidades que tiram proveito das características de uma máquina virtual Java foram desenvolvidas . O propósito da primeira funcionalidade e automatizar a condução de atividades de mutação fraca . Após a implementação de tal funcionalidade na máquina virtual Java selecionada , observou-se um desempenho até 95 % melhor em relação a uma ferramenta de mutação forte . Afim de apoiar o teste de programas concorrentes , a segunda funcionalidade permite reexecutá-los de forma determinística além de automatizar a exploração de que novas sequências de 
 Nos últimos_anos  , houve um aumento da complexidade e variedade de serviços disponíveis na Internet , fato que tem levado à busca por técnicas eficientes de roteamento de requisições de um cliente ao melhor servidor disponível , sendo uma delas conhecida como application layer anycast ( ALA ) . O objetivo deste mestrado é elaborar meios eficientes de prover anycast na camada de aplicação com qualidade de serviço no contexto de computação em nuvem . Para atingir esse objetivo , um novo sistema foi proposto ( GALA , Global Application Layer Anycast ) . Ele herda características de um outro sistema existente e emprega a geolocalização como diferencial , a fim de melhorar o desempenho geral do algoritmo . Experimentos foram realizados por meio de simulação e os resultados mostraram que esse novo sistema , comparado ao algoritmo herdado , mantém a eficiência das requisições realizadas pelos clientes e diminui consideravelmente o tempo de latência dessas operações . Ainda , o sistema proposto foi desenvolvido em um ambiente real a fim de fortalecer os resultados das simulações . Com os resultados obtidos , o sistema modelado foi validado e sua eficácia confirmada 
 Os recentes avanços da ciência e tecnologia viabilizaram o crescimento de dados em quantidade e disponibilidade . Junto com essa explosão de informações geradas , surge a necessidade de analisar dados para descobrir conhecimento novo e útil . Desse modo , áreas que visam extrair conhecimento e informações úteis de grandes_conjuntos  de dados se tornaram grandes oportunidades para o avanço de pesquisas , tal como o Aprendizado de Máquina ( AM ) e a Mineração de Dados ( MD ) . Porém , existem algumas limitações que podem prejudicar a acurácia de alguns algoritmos tradicionais dessas áreas , por exemplo o desbalanceamento das amostras das classes de um conjunto de dados . Para mitigar tal problema , algumas alternativas têm sido alvos de pesquisas nos últimos_anos  , tal como o desenvolvimento de técnicas para o balanceamento artificial de dados , a modificação dos algoritmos e propostas de abordagens para dados desbalanceados . Uma área pouco explorada sob a visão do desbalanceamento de dados são os problemas de classificação_hierárquica  , em que as classes são organizadas em hierarquias , normalmente na forma de árvore ou DAG ( Direct Acyclic Graph ) . O objetivo deste trabalho foi investigar as limitações e maneiras de minimizar os efeitos de dados desbalanceados em problemas de classificação_hierárquica  . Os experimentos_realizados  mostram que é necessário levar em consideração as características das classes hierárquicas para a aplicação ( ou não ) de técnicas para tratar problemas dados desbalanceados em classificação_hierárquica  
 Especialmente nas últimas quatro décadas , muitos estudos se voltaram às variáveis determinantes para a implementação efetiva de sistemas flexíveis de manufatura , tais como seu design , sequenciamento e controle . Neste ínterim , o manejo apropriado do conjunto de ferramentas necessárias para a fabricação de um respectivo lote de produtos foi destacado como fator crucial no desempenho do sistema de produção como um todo . Neste trabalho , abordamos a otimização do número de inserções e remoções de ferramentas no magazine de uma ou mais máquinas numericamente controladas , admitindo-se que uma parcela significativa do tempo de produção é dispensada com estas trocas de ferramentas . De forma mais precisa , a minimização do número de trocas de ferramentas consiste em determinar a ordem de processamento de um conjunto de tarefas , bem como o carregamento ótimo do ( s ) compartimento ( s ) de ferramentas da ( s ) máquina ( s ) , a fim de que o número de trocas seja minimizado . Como demostrado na literatura , mesmo o caso restrito à existência de apenas uma máquina de manufatura ( MTSP , do inglês Minimization of Tool Switches Problem ) é um problema NP-difícil , o que pode justificar o fato observado de que a maioria dos métodos de solução existentes o abordam de maneira heurística . Consequentemente , concluímos que a extensão ao contexto de múltiplas máquinas é também um problema NP-difícil , intrinsecamente complicado de se resolver . Nosso objetivo consiste em estudar formas eficientes de otimizar o número de trocas de ferramentas em ambientes equipados com máquinas flexíveis de manufatura . Para tanto , abordamos o problema básico , MTSP , e duas de suas variantes , em níveis crescentes de abrangência , que consideram o sequenciamento de tarefas em um conjunto de : ( i ) máquinas_paralelas  e idênticas ( IPMTC , do inglês Identical Parallel Machines problem with Tooling Constraints ) ; e ( ii ) máquinas_paralelas  e idênticas inseridas em um ambiente do tipo job shop ( JSSPTC , do inglês Job Shop Scheduling Problem with Tooling Constraints ) . Classificamos as principais_contribuições  desta tese com respeito a três aspectos . Primeiramente , empurramos as fronteiras da literatura do MTSP propondo formulações_matemáticas  para os problemas IPMTC e JSSPTC . Desenvolvemos , também , algoritmos baseados em diferentes técnicas de resolução , como redução de domínio , Path relinking , Adaptive large neighborhood search e a elaboração de regras de despacho . Por último , com o intuito de bem avaliar a eficiência e o alcance de nossos métodos , propomos três novos conjuntos de instâncias teste . Acreditamos , assim , que este trabalho contribui positivamente com pesquisas futuras em um cenário abrangente dentro da minimização das trocas de ferramentas em um sistema flexível de manufatura 
 No presente_trabalho  , estudamos e desenvolvemos algoritmos com análise de complexidade de avaliação de pior caso para problemas de programação não linear . Para minimização irrestrita , estabelecemos dois algoritmos semelhantes que exploram modelos de ordem superior com estratégia de regularização . Propusemos uma implementação computacional que preserva as boas propriedades teóricas de complexidade , e fizemos experimentos numéricas com problemas clássicos da literatura , a fim de atestar a implementação e avaliar a aplicabilidade de métodos que empreguem modelos de ordem superior . Para minimização com restrições , estabelecemos um algoritmo de duas fases que converge a pontos que satisfazem condições de otimalidade de primeira_ordem  não escaladas para o problema de programação não linear 
 Para sistematizar os testes e contornar as restrições de tempo e custo associadas à atividade de teste , diversas técnicas , critérios e ferramentas têm sido desenvolvidas . Além_disso  , visando ao estabelecimento de uma estratégia de teste incremental , que apresente baixo custo de aplicação e alta eficácia em revelar a presença de erros , estudos teóricos e empíricos vêm_sendo  conduzidos pela comunidade de teste . O presente_trabalho  está inserido nesse contexto e tem como objetivo a realização de estudos empíricos para comparar a adequação entre os critérios baseados em erros - Análise de Mutantes ( teste de unidade ) e Mutação de Interface ( teste de integração ) - visando ao estabelecimento de estratégias de teste de baixo custo e eficazes , que englobem todo o ciclo de desenvolvimento de software . Nessa perspectiva , algumas estratégias incrementais de aplicação dos operadores de mutação de unidade e de integração são definidas , explorando o aspecto complementar dos critérios baseados em mutação , reduzindo com isso os custos da atividade de teste durante as fases do teste de unidade e de integração , sem comprometer sua qualidade . Ainda , um conjunto essencial de operadores de mutação para o critério Mutação de Interface é apresentado 
 Na Web 2.0 são encontrados sistemas com alto volume de interação social . Alguns desses sistemas oferecem cálculo de reputação ou alguma_forma  de classificação de usuários ou do conteúdo compartilhado . Contudo , em muitos_casos  , esse valor de reputação resultante é obtido somente a partir de dados quantitativos ou qualitativos . O objetivo deste trabalho é elaborar um modelo para o cálculo de reputação em comunidades on-line , baseando-se em dados qualitativos e quantitativos provenientes da interação dos próprios participantes da rede , a fim de potencializar a colaboração entre os membros e fornecer um meio de cálculo resistente a algumas das vulnerabilidades comuns em sistemas de reputação , como tolerância a ruídos e ataques Sybil . Para atingir esse objetivo é realizada uma adaptação do algoritmo PageRank , definida como CR ( Collaborative Reputation ) para obter uma ordenação dos usuários a partir de suas interações . Para avaliação , adotamos um conjunto de dados do sítio Epinions.com , com o qual foi realizada uma análise comparativa dos resultados obtidos a partir do modelo proposto com outros três algoritmos correlatos ao trabalho apresentado . Dentre as técnicas usadas na análise estão : diversidade de valores , comparação da ordenação , estudo comparativo de cenários , tolerância a ruídos e robustez contra ataques tipo Sybil . Os algoritmos usados na avaliação são : o PageRank original e o algoritmo ReCop , usados para a identificação de usuários relevantes , e o algoritmo LeaderRank usado para a identificação dos usuários com maior prestígio na rede . Os resultados indicam que o modelo proposto é mais sensível às interações dos usuários em comparação aos outros modelos usados na avaliação , mas é mais eficiente a ataques Sybil 
 A formalização e o compartilhamento do conhecimento tem incentivado cada vez mais o uso de ontologias em diversas_áreas  da computação . Na Engenharia de Software , por exemplo , elas são usadas em diferentes fases do ciclo de vida do software . Especificamente no desenvolvimento de software a ontologia pode ser considerada como um artefato de software que atua na formalização do conhecimento e requisitos , na geração_automática  de código , na integração contínua e na transformação de dados em conhecimento . No entanto , poucos_estudos  abordam esses fatores de maneira sistematizada na construção do software baseado em ontologia , ao associar os conceitos da Engenharia de Software à Engenharia de Ontologias . Além_disso  , as abordagens atuais não inserem princípios ágeis em suas definições . Portanto , este trabalho tem por objetivo definir um processo de desenvolvimento considerando os princípios e valores ágeis para o desenvolvimento de software baseado em ontologia . No processo , denominado OntoSoft , fases , atividades , tarefas , papeis e modelos de artefatos foram definidos de maneira detalhada para guiar as equipes de desenvolvimento . Ademais , foram especificados três cenários de desenvolvimento considerando a complexidade do software a ser desenvolvido , a fim de evidenciar possibilidades distintas na sequência das atividades durante o fluxo de desenvolvimento do software baseado em ontologia . Com base nos estudos de caso conduzidos em diferentes cenários de desenvolvimento , os resultados sugerem que o processo OntoSoft contribui positivamente na produção dos artefatos do software baseado em ontologia , colaborando para a efetividade e produtividade da equipe 
 Este trabalho tem como objetivo o desenvolvimento de um método_numérico  para simular_escoamentos  viscoelásticos tridimensionais com superfícies_livres  governados pelo modelo constitutivo Giesekus . As equações governantes são resolvidas pelo método de diferenças_finitas  numa malha_deslocada  . A superfície_livre  do fluido é modelada por partículas marcadoras , possibilitando assim a visualização e localização da superfície_livre  do fluido . A equação_constitutiva  de Giesekus é resolvida utilizando as seguintes formulações : método de Runge-Kutta de segunda_ordem  ( também conhecido como método de Euler modificado ) e transformação logarítmica do tensor conformação . O método_numérico  apresentado é verificado comparando-se os resultados obtidos por meio de refinamento de malha para os escoamentos em um tubo e de um jato incidindo em uma placa plana . Resultados de convergência foram obtidos por meio de refinamento de malha do escoamento totalmente desenvolvido em um tubo . Os resultados numéricos obtidos incluem a simulação de um jato incidindo em uma caixa vazia e a simulação do inchamento do extrudado ( dieswell ) para vários números de Weissenberg utilizando diferentes valores do fator de mobilidade do fluido . Resultados adicionais incluem simulações do fenômeno delayed dieswell para altos números de Weissenberg e altos valores do número de Reynolds . Uma comparação qualitativa com resultados experimentais é apresentada 
 Sistemas-de-sistemas ( SoS ) englobam sistemas diversos e independentes que cooperam entre si para executar uma ação combinada que supera suas competências individuais . Em paralelo , descrições arquiteturais são artefatos que expressam arquiteturas de software , desempenhando no contexto de SoS um importante papel na promoção da interoperabilidade entre constituintes ao facilitar a comunicação entre interessados e apoiar atividades de inspeção e análise desde o início de seu ciclo de vida . O principal problema abordado nessa tese consiste na falta de descrições arquiteturais adequadas para SoS que estão sendo desenvolvidos sem um devido cuidado à sua arquitetura de software . Uma vez que os sistemas constituintes não são necessariamente conhecidos em tempo de projeto devido à natureza evolucionária dos SoS , a descrição arquitetural precisa definir em tempo de projeto quais coalisões entre sistemas constituintes são possíveis em tempo de execução . Como muitos desses sistemas são desenvolvidos para o domínio crítico de segurança , medidas adicionais precisam ser adotadas para garantir a correção e completude da descrição arquitetural . Visando tratar esse problema , esse projeto de doutorado emprega SosADL , uma linguagem formal criada especialmente para o domínio de SoS que permite expressar arquiteturas de software como associações dinâmicas entre sistemas independentes em que as interações devem ser mediadas para desempenhar uma ação conjunta . Em particular , é proposto um novo método formal , denominado Ark , para sistematizar os passos necessários na síntese de arquiteturas concretas aderentes a essa descrição . Para isso , o método cria um modelo formal intermediário , denominado TASoS , que expressa a arquitetura do SoS em termos de um problema de satisfatibilidade de restrições , possibilitando desse modo a verificação automática de um conjunto inicial de propriedades . O resultado obtido por essa análise pode ser utilizado em refinamentos e revisões subsequentes da descrição arquitetural . Uma ferramenta de apoio denominada SoSy também foi desenvolvida para automatizar a geração de modelos intermediários e arquiteturas concretas , ocultando o uso de solucionadores de restrições no projeto e desenvolvimento de SoS . O método e sua ferramenta foram aplicados em um modelo de SoS para monitoramento de rios em áreas urbanas em que a viabilidade de arquiteturas abstratas foi investigada . Ao formalizar e automatizar os passos necessários para a síntese arquitetural de SoS , é possível adotar métodos formais no projeto arquitetural de SoS , que são necessários para alcançar níveis maiores de confiabilidade 
 O citoesqueleto é a estrutura celular mais importante em células eucariotas e é responsável por manter a forma da célula e as junções celulares , auxiliando nos movimentos celulares . Esta é composta de filamentos de Actina , Microtúbulos e filamentos intermediários . Recentemente , a análise de duas dessas estruturas tornaram-se importantes , pois é possível_obter  micrografias usando microscópios de alta_resolução  , que contém microscopia de fluorescência , em combinação com métodos complexos de aplicação de substâncias de contraste para rotulagem e posterior análises visuais . A combinação dessas técnicas , entretanto , limita-se a ser descritiva e subjetiva . Neste trabalho , são avaliadas cinco técnicas de análise de imagens , as quais são : Bag of Visual Words ( BoVW ) , Local Binary Local ( LBP ) , Textons baseados em Discrete Fourier Transform ( TDFT ) , Textons baseados em Gabor Filter Banks ( TGFB ) e Textons baseados em Complex Networks ( TCN ) sobre o conjunto de dados 2D Hela e FDIG Olympus . Experimentos extensivos foram conduzidos em ambos os conjuntos de dados , e seus resultados podem servir de base para futuras pesquisas como análises do citoesqueleto em imagens de microscopia fluorescente . Neste trabalho , é apresentada uma comparação quantitativa e qualitativa dos métodos acima mencionados para entender o comportamento desses métodos e propriedades dos microfilamentos de actina ( MA ) e Microtúbulos ( MT ) em ambos os conjuntos de dados . Os resultados obtidos evidenciam que é possível classificar o conjunto de dados da FDIG Olympus com uma precisão de até 90:07 % e 98:94 % para 2D Hela , além de obter 86:05 % e 96:84 % , respectivamente , de precisão , usando teoria de redes complexas 
 Este trabalho apresenta uma metodologia para caracterização do perfil de consumo de recursos demandados por um programa de computador a partir da análise do código binário do arquivo executável . A categorização de processos de acordo com seus perfis de consumo de recursos durante a execuçãotais como uso de CPU e memóriaé uma informação muito desejada para objetivos de projeto e gerenciamento de sistemas . Técnicas convencionais para este propósito são baseadas em testes de caixa branca ( que avaliam o código fonte da aplicação ) , que tendem a ser de difícil aplicação dado a complexidade das arquiteturas de software além da necessidade de acesso ao código fonte ; ou detecção de perfis baseada em dados de execução , que depende da disponibilidade de dados de execução confiáveis e da seleção de características que de fato vão correlacionar o perfil de consumo . A abordagem baseada em mineração de dados proposta neste trabalho evita estas dificuldades uma vez que manipula somente os arquivos binários executáveis . O método combina técnicas provindas da teoria da informação , redes complexas e filogenia para produzir um agrupamento hierárquico de um conjunto de arquivos de programas executáveis que pode ser utilizado para prever potenciais similaridades em termos de consumo de recursos em tempo de execução . As questões de pesquisa deste trabalho investigam se a transformação feita pelo compilador preserva similaridades entre código fonte e binário que podem ser detectadas através de algoritmos de compressão ; em caso positivo , verificar se as similaridades encontradas no código binário estão relacionadas com o perfil de execução das aplicações , permitindo inferir o comportamento dos programas a partir da análise do código binário . Este trabalho apresenta a sistematização do método assim como os resultados da aplicação para caracterizar aplicações em termos de consumo de CPU e Entrada/Saída em uma plataforma PC padrão . Diversos experimentos foram executados em um repositório de 80 programas de várias fontes obtendo-se resultados significativos que evidenciam que a similaridade dos perfis de execução obtidas com esta abordagem é consistente com as obtidas experimentalmente por aferição . A aplicação do método também é exemplificado através de casos de estudo que caracterizam o perfil de execução de programas executáveis 
 Neste trabalho apresentamos uma implementação de revisão de crenças baseada em comparação de modelos ( estados ) em uma ferramenta de verificação automática de sistemas de estados finitos . Dada uma fórmula ( na lógica CTL ) inconsistente com o modelo do sistema , revisamos esse modelo de tal maneira que essa fórmula temporal se torne verdadeira . Como temos oito operadores temporais ( AG , AF , AX , AU , EG , EF , EX e EU ) , foram criados algoritmos especícos para cada um deles . Como o modelo do sistema deriva do seu código na linguagem SMV , a sua revisão passa obrigatoriamente por mudanças na sua descrição . A nossa implementação contempla três tipos de mudanças : acréscimo de linhas , eliminação de linhas e mudança no estado inicial , sendo que as duas primeiras provocam modicações nas transições entre os estados que compõe o modelo . Alguns testes foram aplicados para comprovar a contribuição da revisão de crenças ( revisão de modelos ) como ferramenta de auxílio ao usuário durante o processo de modelagem de sistemas 
 O teste de software é considerado uma importante atividade na garantia da qualidade de produtos de software . No entanto , há uma carência de profissionais qualificados nessa área . Isso pode ser ocasionado pela dificuldade de ensinar teste de software por meio de abordagens que utilizem apenas aulas teóricas e ferramentas de teste tradicionais . Além_disso  , há uma desmotivação decorrente do ambiente de trabalho e das estratégias de alocação e responsabilidade desses profissionais nas equipes de desenvolvimento e teste . Para amenizar esses problemas , têm sido utilizado outras abordagens de apoio ao ensino de teste de software , tais como : jogos educacionais , ensino de teste com programação , módulos educacionais , entre outras . O objetivo deste projeto de mestrado foi desenvolver um jogo educacional , denominado Testing Game , para auxiliar o ensino de teste de software , especificamente : teste funcional , teste_estrutural  e teste de mutação . Para auxiliar o desenvolvimento do Testing Game , foi realizado um mapeamento sistemático para selecionar um motor de jogos . Na primeira versão do jogo foi utilizado o motor de jogos Cocos2D e na segunda versão foi utilizado o Construct 2 . Para avaliar a eficiência do Testing Game , realizou-se um estudo de viabilidade com o intuito de avaliar a qualidade com relação à motivação , experiência do usuário e aprendizagem sob o ponto de vista dos estudantes . Além_disso  , avaliou-se a usabilidade do Testing Game . Aproximadamente 85,64 % das pessoas que participaram do estudo avaliaram a qualidade do jogo de forma positiva com relação à motivação , experiência do usuário e aprendizagem sob o ponto de vista dos estudantes . Quanto à usabilidade do jogo , foram identificados poucos problemas , o que possibilita a liberação do jogo . Por meio deste trabalho , percebeu-se que o jogo Testing Game poderia ser utilizado como um recurso complementar de apoio ao ensino de teste de software , e sua efetividade ser avaliada 
 O Protocolo Internet ( IP ) , base de toda arquitetura da Internet , está envelhecendo . Projetado numa época onde comércio_eletrônico  , computadores_pessoais  , mobilidade e segurança eram ficção científica , o 112 está se tornando incapaz de suportar os próximos passos da Revolução da Informação . Em resposta a esse espantoso crescimento da Internet e à necessidade das aplicações que vão ser executadas na rede no próximo milênio , desenvolveu-se uma nova versão do protocolo 112 . O IPv6 , a versão 6 do Protocolo Internet , oferece um enorme espaço de endereçamento , melhorias de segurança , suporte à mobilidade e diversas outras características que , juntas , vão possibilitar a verdadeira interconexão global . Este trabalho descreve a implementação de uma Aplicação de Transmissão Segura de Dados que utiliza tecnologia de smart cards para armazenar chaves criptográficas , e a integração desta tecnologia com o IPSec , um protocolo de segurança embutido no IPv6 
 A escolha da topologia de uma Rede_Neural  RBF é geralmente realizada por tentativa e erro baseado na experiência do projetista . Os algoritmos de treinamento existentes que determinam a topologia da rede utilizam métodos locais , que apresentam uma grande possibilidade de cair em mínimos locais gerando soluções sub-ótimas . Algoritmos Genéticos representam um método de busca global apropriado para encontrar boas soluções em espaços de busca complexos , como o espaço de busca das topologias das Redes_Neurais  . Este trabalho propõe um Algoritmo Genético para otimizar a topologia de redes RBF limitando o espaço de busca através de uma técnica de aglomeração . Os resultados obtidos sugerem que esta otimização melhora o desempenho de redes RBF em aplicações financeiras 
 Nesta dissertação apresentamos e implementamos um método de relaxamento para resolver o problema de roteamento de dados em redes de comutação . Este problema pode ser formulado como um problema de multifluxo , a critério convexo . O algoritmo apresentado resolve iterativamente o problema de multifluxo , decompondo-o da forma mais independente possível em subproblemas de simples fluxo . Esta independência entre os cálculos permite que a resolução dos subproblemas seja simultânea ; isto nos permitiu a implementação em paralelo . Os resultados do algoritmo paralelo foram usados para estabelecer uma comparação com o algoritmo seqüencial e assim analisar o speedup . A biblioteca paralela utilizada foi o PVM 
 Dentro da área da Robótica , Robôs Móveis têm recebido crescente atenção . Robôs móveis se propõem a realizar uma variedade de tarefas mais complexas que seus antecessores , os robôs industriais . Para tal , são necessárias técnicas que lhes permitam interagir de forma efetiva com o ambiente . A parte mais essencial desta interação é o Sistema de Navegação que é um conjunto de métodos e procedimentos que o robô utiliza para se locomover e encontrar seu caminho no mundo . Infelizmente , as pesquisas até agora têm demonstrado pouco sucesso quando os robôs são submetidos a tarefas do mundo_real  . Métodos baseados em modelagem matemática são inadequados para os robôs_móveis  , porque seu ambiente é dinâmico e mutável . Já os métodos que rejeitam inteiramente os modelos do mundo e simplesmente reagem às contingências do ambiente , não conseguem ser escalados para problemas complexos . Apesar destas dificuldades , a natureza parece ter se saído particularmente bem ao dotar animais e seres_humanos  da capacidade de navegação . Uma abordagem recente é buscar inspiração nela . Esta abordagem é representada pelo estudo dos Mapas Cognitivos - estruturas mentais , encontradas em desde ratos até seres_humanos  , que permitem registrar fatos e raciocinar a respeito dos espaços . Os Mapas Cognitivos da natureza são implementados em Redes_Neurais  Naturais - os cérebros . Pesquisadores de computação e engenharia procuram imitá-lo com as Redes_Neurais  Artificiais . Este trabalho propõe criar um sistema de navegação para robô_móvel  , inspirado no mecanismo de mapa cognitivo , implementado através de Redes_Neurais  Artificiais . O objetivo é obter um sistema robusto , capaz de responder as exigências de desempenho presentes em tarefas do mundo_real  
 Este trabalho trata de um problema característico de organizações constituídas por diversas unidades operacionais com autonomia para definir as próprias regras de desenvolvimento de suas atividades e formas de atuação , mas que também precisam interoperar . Esse problema pode ser traduzido em necessidades de flexibilidade e especificidade dos sistemas de informação . Uma maneira de permitir o desenvolvimento de sistemas de informação com a flexibilidade necessária é a construção de sistemas modulares , onde cada unidade operacional escolhe os módulos que lhe convém , podendo escolher diferentes versões de um módulo para cada função . Para alcançar autonomia de gerenciamento de dados por parte dos subsistemas que compõem urna aplicação é necessário modularizar a base de dados , fazendo com que cada módulo possa ter seu próprio repositório de dados que conterá apenas os dados relevantes as suas transações . Desse modo , este trabalho propõe diretrizes para a modularização de bases de dados , contemplando o compartilhamento e a interoperabilidacle da informação . A modularização foi incorporada ao processo genérico de projeto de base de dados através da inclusão de duas novas fases nesse processo . Como conseqüência da modularização , surgem relacionamentos entre módulos , propondo-se , então , a integração desses através de objetos integradores , que contemplem a possível heterogeneidade do ambiente de gerenciamento de dados . Para caracterizar e delimitar o problema , bem como validar a solução proposta por este trabalho , foi utilizado um estudo de caso de urna associação de cooperativas médicas 
 Esta dissertação investiga a possibilidade de integração de Redes_Neurais  Artificiais ( RNAs ) e Método Estocásticos para previsão de séries_temporais  . O problema de previsão é geralmente abordado através de Métodos Estocásticos . Ultimamente , as RNAs têm sido muito utilizadas para a construção de previsores não lineares em diferente áreas de aplicações . Contudo , as arquiteturas da RNAs devem também ser parcimoniosas , ou seja , apenas considerar as entradas mais relevantes para realizar uma boa previsão . Assim , várias abordagens vêm_sendo  propostas para melhorar o projeto de arquitetura em problemas de previsão . Alguns exemplos destas abordagens são a combinação de RNAs e métodos Box 8c Jenkins , as técnicas de seleção usando métodos de poda de RNAs e modelos de RNAs com capacidade de processamento temporal . Além_disso  , as vantagens particulares dos previsores construídos seguindo tais abordagens podem ser combinadas através de comitês ou combinadores de previsão . Os experimentos desta dissertação foram realizados com dados sobre séries_temporais  de cotação de moedas e ações 
 Em simulação de filas diferentes métodos de coleta de dados podem ser aplicados com características e problemas distintos . Um dos métodos mais utilizados é o batch means que consiste de uma grande rodada de simulação dividida em batches consecutivos de observações , sendo sua principal fonte de erro causada pela correlação entre as médias dos batches . Existem duas regras principais para definir a relação entre número e tamanho de batches : a regra FNB ( o número de batches é fixo enquanto seu tamanho cresce linearmente com o tamanho da amostra ) produzindo amplos intervalos de confiança que tendem a conter a média real do processo e com menor correlação entre as médias dos batches ; a regra SQRT ( número e tamanho dos batches cresce segundo a raiz quadrada do tamanho amostrai ) produzindo pequenos intervalos de confiança com uma convergência mais rápida da distribuição das médias dos batches para uma distribuição Normal . Com o objetivo de melhorar a qualidade dos intervalos de confiança ( boa cobertura e redução de amplitude ) obtidos para o método batch means foram propostas duas novas regras denominadas LBATCH e ABATCH ( Fislunan & Yarberry , 1994 ) que , de formas diferentes , utilizam alternadamente as regras FIVB e SQRT conforme o resultado de um teste de correlação entre as médias dos batches . Utilizando-se linguagens não especificas de simulação foi programada a fila M/M/1 e , simulada por Monte-Carlo , conseguiu-se boa redução na amplitude dos intervalos de confiança mantendo-se boa cobertura . Neste trabalho são utilizadas as regras LBATCH e ABATCH para a fila M/M/1 programada na linguagem de simulação GPSS/H , buscando estender a aplicabilidade dessas regras devido a facilidade em programar sistemas , mesmo mais complexos , nesta linguagem . Ainda que utilizando-se um tamanho amostrai bem menor , obteve-se uma maior redução na amplitude dos intervalos de confiança , que a fornecida por linguagens não específicas , contudo houve também significativa redução na taxa de cobertura 
 Apresentamos neste trabalho , uma análise_bayesiana  para dados clínicos exponenciais com variáveis auxiliares . Formulamos uma abordagem_bayesiana  com densidades a priori informativas , obtidas_através  das variáveis auxiliares sob o contexto de modelos lineares generalizados , para estimar os parâmetros de interesse , testar o modelo e prever a sobrevivência de pacientes com doenças graves . Diferentes funções de ligações são consideradas . O método que iremos examinar consiste na obtenção de informações a priori para a média das respostas , com correspondentes variáveis auxiliares fixas de modo que se possa induzir uma distribuição a priori sobre os coeficientes de regressão a partir de médias condicionais a priori . Esta abordagem utiliza os algoritmos computacionais do tipo Gibbs Sampling/Metropolis-Hastings e será comparada com a inferência bayesiana exata . Finalizamos com aplicações em dados clínicos exponenciais para pacientes com leucemia utilizando amostras completas e amostras censuradas 
 Este trabalho apresenta um ambiente integrado para simulação de escoamentos bidimensionais incompressíveis com superfícies_livres  , denominado Freeflow-2D . Este sistema é compostos por quatro módulos : um modelador de moldes e escoamentos - Modflow- 2D , um simulador de escoamentos - Simflow-2D , um visualizador de escoamentos - Visflow- 2D e um reiniciador de escoamentos - Resimflow-2D . A comunicação entre os módulos do sistema é feita por arquivos . O Freeflow-2D implementa o método GENSMAC e foi baseado na estrutura de dados do Freeflow-3D . Os objetos geométricos ( fluidos , contêineres , injetores e ejetores ) são representados pela estrutura de dados B-Rep ( Boundary Representation ) . Alguns resultados de simulação utilizando este sistema são apresentados e comparados com os do sistema Freeflow 
 Os avatars , ou ferramentas de análise de desempenho de sistemas distribuídos baseadas em tecnologias de realidade virtual , surgem como novas ferramentas de monitoramento e visualização gráfica do comportamento dos sistemas paralelos e distribuídos . Neles o formato de arquivo utilizado é o SDDF ( Self-Defining Data Format ) que vem se tornando padrão de armazenamento entre as ferramentas baseadas nesta tecnologia . A finalidade desde trabalho é investigar e desenvolver um sistema de captura e rastreamento das operações no formato ODBC ( Open Database Connectivity ) , que ocorrem no ambiente intranet e disponibilizar tais dados no formato especifico , o SDDF possibilitando a visualização em avatars 
 Este trabalho apresenta um método de apoio ao processo de mapeamento da abstração de generalização para o Modelo Relacional , o qual proporciona um conjunto completo de opções de mapeamento . A definição e avaliação destas opções de mapeamento foram realizadas considerando-se alguns aspectos da estrutura conceituai da abstração de generalização . Tais aspectos estão relacionados às restrições de sobreposição e participação , e em particular , ao atributo critério , o qual define a especialização de um tipo genérico em tipos específicos . Além_disso  , considerou-se outros fatores , representados por propriedades , que fazem parte das características básicas de uma ocorrência da abstração de generalização . Para conduzir os analistas e projetistas na escolha de uma opção de mapeamento apropriada , é apresentada uma árvore de decisão , que foi construída de acordo com as restrições e propriedades definidas , as quais são identificadas como relevantes em cada ocorrência da abstração de generalização . Para a validação do desenvolvimento teórico das técnicas de mapeamento que foram propostas , e também , para o tratamento completamente automatizado da abstração de generalização , desde a representação em um modelo orientado a objetos até sua representação relacional , foi construída uma fermenta 
 Independentemente do tipo de manutenção conduzida - conetiva , preventiva , adaptativa ou evolutiva , as atividades de teste de regressão são necessárias para testar as modificações realizadas e as eventuais novas_funcionalidades  de um programa , e , principalmente , para testar se as funcionalidades já existentes não foram afetadas adversamente pelas modificações . Visando a aplicar o teste de regressão de uma maneira sistemática a um baixo custo e com eficácia , muitas técnicas têm sido propostas na literatura . Essas diversas técnicas são divididas em duas_abordagens  : retest-all e seletiva . A abordagem retest-ali utili7a todo o conjunto de casos de teste disponível para testar as modificações , enquanto que a abordagem seletiva utiliza um subconjunto dos casos de teste disponíveis para testar as modificações . As técnicas baseadas na abordagem seletiva têm sido muito estudadas , pois visam a diminuir os esforços despendidos no teste de regressão reduzindo o número de casos de teste a serem reexecutados . Diante da diversidade das técnicas seletivas , fazem-se necessários estudos empíricos para avaliar e comparar a aplicação dessas técnicas . Assim , este trabalho visa a aplicar e avaliar empiricamente duas técnicas de teste de regressão que têm se mostrado promissoras : a Técnica baseada em Modificação ( Wong et ai. , 1997a ) e a Técnica baseada em Mutação Seletiva ( Wong et 1997b ) . Para auxiliar nessa avaliação , um framework , proposto por Rothennel e Harrold ( 1996 ) , é utilizado . Com a realização desses experimentos , espera-se contribuir para o estabelecimento de estratégias de teste de regressão efetivas e de baixo custo 
 Este trabalho apresenta o Método para Projeto de Hiperdocumentos para Ensino , ou EHDM ( Educational Hyperdocuments Design Method ) , que proporciona uma abordagem sistemática para apoiar o projeto e desenvolvimento de aplicações hipennídia para ensino . O método utiliza o modelo proposto por Michener e a técnica de mapeamento conceitual para modelar o domínio de conhecimento do hiperdocumento . As três fases que compõem o método modelagem conceitual hierárquica , projeto navegacional de contextos e construção e teste são apresentadas . Uma ferramenta denominada Educational Hyperdocuments Development Tool ( EHDT ) foi desenvolvida para auxiliar o desenvolvimento de hiperdocumentos para ensino destinados ao sistema SASHE . Essa ferramenta utiliza o EHDM como base metodológica . O EHDT fornece mecanismos para facilitar laços de realimentação rápidos entre as fases do método e para apoiar abordagens de projeto bottom-up e top-down 
 Aplicações de Processamento de Língua Natural ( PLN ) , como revisores ortográficos , gramaticais e tradutores , geralmente precisam consultar extensos dicionários contendo , por exemplo , informações morfossintáticas de várias centenas de milhares de palavras de uma língua . Autômatos finitos , largamente utilizados na construção de eficientes analisadores léxicos para compiladores , apresentam-se como ótimos candidatos para o problema de representação desse tipo de léxico de língua natural . Este trabalho investigou os métodos de codificação de léxicos utilizando autômatos finitos , as técnicas de minimização de autômatos determinísticos acíclicos e as estruturas de dados necessárias para uma representação compacta , resultando em um sistema computacional eficiente e versátil de representação de grandes léxicos de língua natural . Um léxico de mais de 430.000 palavras da língua_portuguesa  pode ser convertido em um autômato de menos de 220Kb , em menos de 5 minutos usando um computador doméstico 
 Redes_Neurais  Artificiais ( RNAs ) têm proporcionado uma solução eficiente para uma grande variedade de problemas práticos . Infelizmente , a seleção dos parâmetros ideais para o processo de aprendizado , bem como a escolha da topologia adequada , não são tarefas triviais Geralmente , o processo de escolha do número de parâmetros livres é informal , e as redes são treinadas com diferentes topologias e complexidades até que a de melhor desempenho seja encontrada . Este procedimento nem sempre produz redes de tamanho mínimo , o que em muitos_casos  inviabiliza a implementação . Nesta dissertação é apresentado um estudo comparativo de diversas técnicas de Pruning , as quais têm como objetivo minimizar a complexidade da rede , sem degradar sua capacidade de generalização . Um grande número de experimentos foi realizado , utilizando diversas técnicas previamente selecionadas . Uma análise dos resultados obtidos é também apresentada , indicando o comportamento das técnicas de Pruning em geral , e identificando as de melhor desempenho 
 Este trabalho contribui com questões relativas à integração das tecnologias de Hipermídia e Trabalho Cooperativo Suportado por Computador ( Computer Supported Cooperative Work - CSCW ) , explorando a utilização de hiperdocumentos estruturados no suporte a sessões de trabalho cooperativo . São apresentados conceitos da área de Sistemas Hipermídia Distribuídos e a evolução das linguagens de especificação de hiperdocumentos até a XML ( Extensible Markup Language ) . A área de CSCW é apresentada , com ênfase nas principais funcionalidades encontradas nas aplicações dessa área . Essa pesquisa motivou a proposta da metodologia CSCW-SH ( CSCW design based on Structured Hypermedia ) a qual , visando auxiliar a construção de aplicações de CSCW , explora hiperdocumentos estruturados para capturar o conteúdo das sessões de trabalho . Essa metodologia foi utilizada para orientar o projeto e a implementação do DocConf , um ambiente de apoio ao trabalho cooperativo , que , por implementar funcionalidades CSCW como componentes , configura-se como um ambiente extensível e aberto . Para ilustrar a utilização do DocConf , o trabalho apresenta a integração do DocConf ao StudyConf , o qual é um ambiente que apoia a navegação e discussão de hiperdocumentos didáticos 
 Este trabalho consiste no desenvolvimento de uma unidade de controle , cuja função é gerenciar vários elementos de processamento que compõem uma arquitetura computacional classificada como arranjo sistólico , com o propósito de solucionar problemas que envolvam sistemas_lineares  . A partir de uma formulação_matemática  de alto nível de abstração , estabeleceu-se uma sequência de operações que possibilitou a codificação do modelo matemático em linguagem VHDL . Foram empregadas metodologias e ferramentas avançadas para o projeto de hardware que aceleraram o ciclo de desenvolvimento do projeto , e para a implementação utilizaram-se dispositivos reprogramáveis FPGAs ( Field Programmable Gate Arrays ) . São apresentados resultados numéricos na forma de diagrama de tempo que evidencia o sincronismo da técnica de Pipeline , indicando que a abordagem e a metodologia adotada é viável e eficiente para a solução do problema 
 Problemas com a escrita podem afetar o desempenho de profissionais de maneira marcante , principalmente no caso de pesquisadores e acadêmicos que precisam escrever com proficiência e desembaraço não apenas na língua materna , mas também em uma ou mais línguas estrangeiras . Atualmente , o inglês é a língua dominante para a escrita e divulgação de pesquisas técnicas e científicas . Para amenizar os problemas desses usuários da língua inglesa , que sofrem interferência da língua materna quando escrevem em inglês ou se sentem incapazes de gerar construções e composições lingüísticas de tal idioma , foi desenvolvido o ambiente modular de auxílio e ensino da escrita técnica , chamado AMADEUS ( Alvfiable Árticle DEvelopment for User Support ) . A estrutura do AMADEUS consiste em várias ferramentas inter-relacionadas ferramenta de referência , ferramenta de suporte , ferramenta de crítica e ferramenta tutorial . A ferramenta de critica , objeto de estudo deste trabalho , fornece conhecimento estrutural em um nível textual maior que um parágrafo , ou seja , visa indicar o conjunto mais apropriado de componentes das estruturas esquemática e de componentes específicos de cada uma das 'tias seções de um artigo . Essa ferramenta proporciona o desenvolvimento da criatividade no processo de escrita de um texto , assim como a escrita de textos apropriados a um propósito e audiência específicos ( por exemplo , para a comunidade de Inteligência_Artificial  ) . Este trabalho explora a abordagem de críticas para ensinar a escrita técnica utilizada por uma comunidade de pesquisa específica , a CHI ( Conference on Human Factors in Computing Systems ) . Apresentamos tanto um modelo para a construção de ferramentas de critica para a escrita técnica , quanto um modelo de documentos ( artigos técnicos ) que deixa explícito os objetivos e convenções da comunidade escolhida e usamos essa teoria explícita para ensinar a escrita de novos artigos neste domínio . Baseado neste modelo de ferramenta foi construído um protótipo de uma ferramenta de crítica para a comunidade CHI , utilizando um corpus de artigos da CHF96 . Finalmente , o protótipo foi avaliado de acordo com os critérios de operacionalidade e praticidade . A questão da operacionalidade foi avaliada através da análise da interação de usuários com diferentes graus de competência da escrita técnica em inglês . Para a questão da praticidade , a estendibilidade , portabilidade , personalização , e custo de implementação foram estudados 
 Neste trabalho estudamos o problema de corte de estoque inteiro . Para o caso unidimensional , apresentamos alguns métodos heurísticos selecionados por Wãscher e Gau ( 1996 ) , os quais realizaram um estudo computacional . Tais métodos partem da solução ótima do problema relaxado por programação linear e buscam uma solução inteira em sua 'vizinhança ' . Neste presente_trabalho  , estendemos um dos métodos para o caso bidimensional , que consiste em resolver o problema original relaxado , impondo padrões de corte 2-estágios e irrestritos e utilizando a geração de colunas proposta por Gilmore e Gomory ( 1965 ) . Em seguida , um arredondamento para o inteiro inferior é feito , resultando em um problema residual . Para resolução deste problema , abandonamos novamente a condição de integralidade e utilizamos a técnica de geração de colunas impondo agora padrões de corte 2-estágios e restritos . O arredondamento é realizado , resultando em um novo problema residual , que será tratado da mesma forma . Este procedimento é repetido até que o arredondamento ' resulte somente em freqüências nulas . Por fim , padrões restritos são utilizados até toda a demanda restante ser atendida . Os resultados dos testes computacionais obtidos com a implementação deste método são apresentados , onde foram observadas fortes indicações da propriedade M1RUP ser também válida para problemas de corte bidimensional 2-estágios 
 Neste trabalho é apresentado o estudo de desenvolvimento de um ambiente integrado para programação paralela . O ambiente contém as ferramentas apropriadas para o desenvolvimento de novos programas paralelos , para a paralelização de programas seqüenciais , para a análise e otimização de desempenho e a depuração de programas paralelos . O trabalho cobre as fases de análise de requisitos , especificação e projeto do ambiente proposto . Através de uma pesquisa extensa sobre as ferramentas para programação paralela e da análise de algumas formas de classificação existentes , foi proposta uma nova classificação que possibilita enquadrar um grande número de ferramentas que fizeram parte da pesquisa . Com base nessa classificação foi desenvolvido um modelo gráfico do sistema , determinando-se os módulos que se fazem necessários para cobrir todas as fases de desenvolvimento de programas paralelos . Também é apresentado neste trabalho três opções de implementação do sistema baseadas em ferramentas existentes que podem fazer parte do projeto proposto com ou sem alteração do seu código fonte 
 A computação reconfigurável está se fortalecendo cada vez mais devido ao grande avanço dos dispositivos reprogramáveis e ferramentas de projeto de hardware utilizadas atualmente . Isso possibilita que o desenvolvimento de hardware torne-se bem menos trabalhoso e complicado , facilitando assim a vida do desenvolvedor . A tecnologia utilizada atualmente em projetos de computação reconfigurável é denominada FPGA ( Field Programmable Gate Array ) , que une algumas características tanto de software ( flexibilidade ) , como de hardware ( desempenho ) . Isso fornece um ambiente bastante propício para desenvolvimento de aplicações que precisam de um bom_desempenho  , sem que estas devam possuir uma configuração definitiva . O objetivo deste trabalho foi implementar um barramento eficiente para possibilitar a comunicação entre diferentes CORES de um robô reconfigurável , que podem_estar  dispersos em diferentes dispositivos FPGAs . Tal barramento seguirá o padrão AMBA ( Advanced Microcontroller Bus Architecture ) , pertencente à ARM . Todo o desenvolvimento do core completo do AMBA foi realizado utilizando-se a linguagem VHDL ( Very High Speed Integrated Circuit Hardware Description Language ) e ferramentas EDAs ( Electronic Design Automation ) apropriadas . É importante notar que , embora o barramento tenha sido projetado para ser utilizado em um robô , o mesmo pode ser usado em qualquer sistema on-chip 
 A falta de materiais instrucionais digitais disponíveis em larga_escala  e com baixo custo levou à criação da tecnologia de Objetos de Aprendizagem ( OAs ) [ Wiley 2000 ] . Nessa tecnologia , materiais para ensino e aprendizagem são construídos na forma de pequenos componentes ( objetos ) para que possam ser combinados de várias maneiras diferentes , ou seja , possam ser reutilizados na composição de seqüências didáticas que cubram um determinado domínio de aprendizagem , por exemplo , Geometria Euclidiana ou Eletrostática . Nessa visão , cabe ao professor ( ou ao próprio aluno ) decidir qual é a seqüência de OAs mais adequada para um determinado contexto instrucional ( ou de acordo com as preferências do aluno ) . OAs podem_estar  disponíveis em grandes repositórios na Internet , de maneira que qualquer usuário - professor ou estudante - possa buscá-los e recuperá-los de acordo com suas necessidades . Para tornar a busca e recuperação mais ecientes , cada objeto precisa ser descrito num formato padrão o que permite a sua utilização em diversas plataformas . Atualmente , cada uma das tarefas associadas à descoberta e utilização de OAs é realizada de forma não automatizada , ou seja , um professor ao elaborar um curso , deve fazer uma busca nos repositórios de OAs disponíveis , ou na própria Web , e selecionar os objetos de interesse . Nessa etapa , é preciso que o professor analise cada OA recuperado a m de decidir : ( a ) se ele satisfaz ou não os objetivos didáticos em questão ; ( b ) em que ponto do curso o OA pode ser inserido , dadas as suposições sobre o conhecimento prévio feitas no projeto de construção do OA ; ( c ) qual é o nível de diculdade , o contexto instrucional e o tipo de recurso de aprendizagem do OA desejados ( por exemplo , exercício , simulação , notas de aula , testes etc. ) . Além_disso  , quando o professor faz a busca por OAs , ele utiliza somente palavras-chave , sem nenhuma referência sobre a estrutura do domínio de conhecimento . Essa é a principal limitação dessa tecnologia : não existem padrões para a representação do conteúdo de um OA , que possibilite automatizar a recuperação e análise de OAs em termos do seu conteúdo relacionado ao domínio no qual ele se insere . Esse trabalho propõe uma ferramenta , chamada SEQUOA ( Seqüenciamento de OAs ) , que dê suporte ao professor para a tarefa de seleção e composição de OAs , com base na tecnologia da Web Semântica . Com esse objetivo , foi feito um estudo de caso para o domínio da Eletrostática . Foram construídas ontologias para : formalizar as descrições de OAs , baseando-se em padrões largamente adotados , e descrever o domínio de aprendizagem de uma maneira didática ( segundo educadores experientes no domínio em questão ) . O uso dessas ontologias permite que a ferramenta SEQUOA seja capaz de realizar inferências sobre as diferentes características dos OAs e gerar um conjunto de seqüências alternativas de OAs , para um determinado domínio de aprendizagem , promovendo assim o reuso de OAs 
 As recentes tecnologias de desenvolvimento e distribuição de componentes possibilitaram o aumento do número de componentes disponíveis no mercado . No entanto , eles muitas_vezes  estão dispersos e não publicados adequadamente para a comunidade de pesquisa e desenvolvimento de software . Encontrar componentes apropriados para solucionar um problema particular não é uma tarefa simples e novas técnicas devem ser desenvolvidas para o reuso efetivo de componentes . Um dos maiores desafios em reusar componentes consiste em classificá-los corretamente para futuras consultas . Classificar componentes para possibilitar uma busca eficaz depende da qualidade das informações adquiridas , que viabilizam melhor precisão e cobertura das consultas ao encontrar componentes reutilizáveis em potencial . Ao mesmo tempo , mecanismos de classificação e busca devem ser fáceis o suficiente para convencer os desenvolvedores a reusar componentes . Este trabalho estuda as técnicas de classificação de componentes de software , repositórios e métodos de busca . é apresentada uma proposta de modelo de classificação de componentes que considera não apenas sua função , mas o negócio onde ele está inserido e seus atributos de qualidade . Um método de preenchimento semi-automático das informações é proposto , de modo a diminuir os custos de classificação . O protótipo REUSE+ foi construído para exemplificar o uso do modelo e do método de classificação semi-automática , de forma a validar a proposta , destacando , por fim , as principais_contribuições  do trabalho 
 Linha de produtos de software ( LPS ) corresponde a uma das mais bem sucedidas formas de reúso , pois permite a reutilização de requisitos e arquitetura . Embora o desenvolvimento , manutenção e evolução de uma LPS ainda possua um custo alto quando comparado ao desenvolvimento de sistemas únicos ( single systems ) , um lucro significativo pode ser obtido com a venda de diversos produtos derivados da LPS . No projeto de uma LPS analisa-se os sistemas coletivamente , ou seja , o domínio . Geradores de aplicações são ferramentas capazes de gerar artefatos a partir de uma especificação , e no caso de se ter a especificação de um domínio , é possível gerar aplicações para esse domínio . Web_services  representam uma tecnologia promissora para disponibilização de serviços na Web e desenvolvimento de software com arquitetura flexível e de fácil manutenção . Neste trabalho é proposta uma abordagem de desenvolvimento de linha de produtos com arquitetura orientada a serviços , na qual a geração de produtos é apoiada por um gerador de aplicações . A abordagem chama-se SoProL-WS e possui o objetivo de reduzir os custos e prazos de desenvolvimento da LPS e facilitar a sua manutenção , evolução e derivação de seus membros . SoProLWS apresenta as atividades e artefatos necessários para partir dos requisitos da LPS , projetar , implementar , configurar um gerador de aplicações e gerar seus membros a partir do gerador ou por meio de uma configuração manual . Além_disso  , é apresentado um estudo de caso com o desenvolvimento de uma linha de produtos de leilões Web seguindo os passos da abordagem , bem como são discutidas as alternativas de projeto relevantes para esse tipo de 
 O Paradigma Orientado a Objetos tem sido atualmente a abordagem dominante de desenvolvimento de software . Contudo , ela sofre da Tirania da Decomposição Dominante , pois não permite uma modularização adequada da implementação relativa a interesses estruturais . Como consequência , a implementação relativa a cada interesse estrutural fica espalhada pelos módulos do programa e entrelaçada com a implementação relativa a outros interesses estruturais . Outras abordagens de desenvolvimento de software , como o Desenvolvimento de Software Orientado a Aspectos com AspectJ e a Separação Multidimensional de Interesses em Hiperespaços com Hyper/J e CME , atingem sucesso moderado em oferecer mecanismos que permitem superar as deficiências do Paradigma Orientado a Objetos . No entanto , tais abordagens também possuem deficiências e omissões que devem ser reparadas para que elas possam se tornar utilizáveis em contextos típicos de desenvolvimento de software complexo . Este trabalho especifica uma nova abordagem , denominada Desenvolvimento de Software Orientado a Temas ( DSOT ) , que tem como objetivo superar algumas deficiências das abordagens anteriores por meio de mecanismos que permitem a manipulação da implementação de cada interesse estrutural de forma separada e a manipulação da implementação de cada tipo de dado de forma separada . Além_disso  , DSOT possui operadores que são ortogonais , isto é , podem ser utilizados de forma combinada ou separada , para efetuar a composição de módulos do programa . Mostra-se o modelo_conceitual  do DSOT e descrevese um estudo de caso que consiste no desenvolvimento de um programa para demonstrar mais concretamente como o DSOT funciona na prática . Não se demonstra a superioridade do DSOT para o caso geral , mas os resultados alcançados evidenciam que o DSOT é uma abordagem promissora que merece ser investigada mais aprofundadamente em pesquisas 
 Muitos dos problemas de classificação descritos na literatura de Aprendizado de Máquina e Mineração de Dados dizem respeito à classificação de dados em que cada exemplo a ser classificado pertence a um conjunto finito , e geralmente pequeno , de classes que estão em um mesmo nível . Vários problemas de classificação , entretanto , são de natureza hierárquica , em que classes podem ser subclasses ou superclasses de outras classes . Em muitos problemas hierárquicos , principalmente no campo da Bioinformática , um ou mais exemplos podem ser associados a mais de uma classe simultaneamente . Esses problemas são conhecidos como problemas de classificação_hierárquica  tirrótulo . Nesta pesquisa , foram investigadas diferentes técnicas para lidar com esses tipos de problemas . Essas técnicas são baseadas em duas_abordagens  : local ou Top-Down e global ou One-Shot . Três técnicas descritas na literatura foram utilizadas . A primeira delas , chamada HMC-BR , é baseada na abordagem Top-Down , e utiliza uma estratégia de classificação binária chamada Um-Contra-Todos . As outras duas técnicas , baseadas na abordagem One-Shot , são chamadas C4.5H ( uma extensão do algoritmo de indução de àrvores de decis~ao C4.5 ) , e de Clus-HMC ( baseada na noção de Predictive Clustering Trees , em que àrvores de decisão são estruturadas como uma hierarquia de grupos ( clusters ) ) . Além das técnicas descritas na literatura , duas novas técnicas foram propostas e implementadas nesta pesquisa , chamadas de HMC-LP e HMC-CT. Essas técnicas são variações hierárquicas de técnicas de classificação multirrótulo não hierárquicas . A técnica HMC-LP utiliza uma estratégia de combinação de classes e a técnica HMC-CT utiliza uma estratégia de decomposição de classes . Para a avaliação das técnicas , foram utilizadas medidas específicas para esse tipo de classificação . Os resultados experimentais mostraram que as técnicas propostas obtiveram desempenhos superiores ou semelhantes aos das técnicas descritas na literatura , dependendo da medida de avaliação utilizada e das características dos conjuntos de 
 Este trabalho é dedicado ao estudo do erro de estimação em filtragem linear para sistemas_lineares  com parâmentros sujeitos a saltos markovianos a tempo discreto . Indroduzimos o conceito de alcançabilidade média para uma classe de sistemas . Construímos um conjunto de matrizes de alcançabilidade e mostramos que o conceito usual de alcan- çabilidade definido através da positividade do gramiano é caracterizado pela definição por posto completo destas matrizes . A alcançabilidade média funciona como condição necessária e suficiente para positividade do segundo momento do estado do sistema , resultado esse que auxilia na caracterização da positividade uniforme da matriz de covariância do erro de estimação . Abordamos a estabilidade de estimadores com a interpretação de que a covariância do erro permanece limitada na presença de erro de qualquer magnitude no modelo do ruído , que é uma característica relevante para aplicações . Apresentamos uma prova de que filtros markovianos são estáveis sempre que o segundo momento condicionado é positivo . Exemplos numéricos encontram-se inclusos 
 Aprender conceitos provenientes de fluxos de dados é uma tarefa significamente diferente do aprendizado tradicional em lote . No aprendizado em lote , existe uma premissa implicita que os conceitos a serem aprendidos são estáticos e não evoluem significamente com o tempo . Por outro_lado  , em fluxos de dados os conceitos a serem aprendidos podem evoluir ao longo do tempo . Esta evolução é chamada de mudança de conceito , e torna a criação de um conjunto fixo de treinamento inaplicável neste cenário . O aprendizado incremental é uma abordagem promissora para trabalhar com fluxos de dados . Contudo , na presença de mudanças de conceito , conceitos desatualizados podem causar erros na classificação de eventos . Apesar de alguns métodos incrementais baseados no modelo de misturas gaussianas terem sido_propostos  na literatura , nota-se que tais algoritmos não possuem uma política explicita de descarte de conceitos obsoletos . Nesse trabalho um novo algoritmo incremental para fluxos de dados com mudanças de conceito baseado no modelo de misturas gaussianas é proposto . O método proposto é comparado com vários algoritmos amplamente_utilizados  na literatura , e os resultados mostram que o algoritmo proposto é competitivo com os demais em vários cenários , superando-os em alguns_casos  
 Neste trabalho são apresentadas , complementadas e melhoradas duas técnicas de restauração de imagens : uma abordando o problema de retoque digital/remoção de objetos enquanto a segunda é direcionada ao problema deneliminação de ruído . Em ambas as técnicas , a ideia é trabalhar com imagens contendo texturas e outras características de interesse para um observador humano como a preservação de padrões , bordas , estruturas e regiões de natureza oscilatória . A técnica descrita sobre retoque digital de imagens combina difusão anisotrópica , síntese de texturas , busca dinâmica e um novo termo empregado no mecanismo de atribuição da ordem de prioridade durante o processo de reconstrução . Assim , dada uma imagem com regiões a serem recompostas , uma técnica de difusão anisotrópica é aplicada à imagem afim de se obter um mapa de saliência contendo bordas , estruturas e demais informações de baixa frequência da imagem . Na sequência , um mecanismo de prioridade baseado em um novo termo de confiabilidade regularizado é calculado a partir da combinação do mapa anteriormente gerado com a equação do transporte . Tal mecanismo é utilizado para determinar a ordem de preenchimento das partes faltantes da imagem . Para essa tarefa , a abordagem apresentada utiliza uma nova medida de similaridade entre blocos de pixels ( amostrados dinamicamente para acelerar o processo ) , afim de encontrar os melhores candidatos a serem alocados nas regiões danificadas . A técnica destinada à remoção de ruídos alia a teoria da difusão anisotrópica , técnicas de análise harmônica e modelos numéricos de discretização de EDPs não-lineares em uma equação diferencial parcial regularizada , a qual atua de forma incisiva em regiões mais homogêneas da imagem e de forma mais suave em regiões caracterizadas como textura e bordas , preservando , assim , essas regiões . Além da natureza anisotrópica , a EDP procura recompor partes texturizadas perdidas no processo de eliminação de ruído através da aplicação de técnicas robustas de análise harmônica . Uma validação teórica e experimental para esta EDP e um estudo do ajuste paramétrico do método de eliminação de ruído baseado nesta EDP foram realizados neste trabalho . A eficiência e a performance das técnicas propostas são atestadas por meio das análises experimentais quantitativas e qualitativas com outras abordagens clássicas da literatura 
 Aplicações de captura e acesso exploram o paradigma de computação ubíqua --_-  que consiste em popular o ambiente com aplicações e dispositivos computacionais a fim de auxiliar transparentemente as pessoas na realização de suas atividades --_-  para dar_apoio  à captura automática de informação em experiências `` ao vivo ' e à correspondente geração de documentos passíveis de armazenamento , recuperação , visualização e extensão ao longo do tempo . Devido à sua natureza distribuída , à heterogeneidade dos dispositivos computacionais envolvidos e à diversidade nas funcionalidades providas , essas aplicações são difíceis de se construir e requerem infra-estruturas e serviços de software que auxiliem o desenvolvedor nessa tarefa . Este trabalho_investiga  a construção e o uso de aplicações de captura e acesso por meio do desenvolvimento da xINCA , uma infra-estrutura estendida baseada em componentes de software reutilizáveis que englobam as funcionalidades recorrentes nessa classe de aplicações . A xINCA é uma extensão da infra-estrutura INCA --_-  uma infra-estrutura de baixo_nível  que provê abstrações de comunicação para aplicações de captura e acesso . Complementares , as infra-estruturas INCA e xINCA provêem um modelo simplificado para o desenvolvimento de aplicações de captura e acesso , considerando aspectos de projeto , implementação e reuso . Associada ao modelo de armazenamento do serviço StRES , a xINCA tem ainda papel na estruturação da informação capturada com o uso de XML e tecnologias correlatas 
 A recuperação de imagens baseada em conteúdo ( Content-based Image Retrieval - CBIR ) embasa-se sobre dois aspectos primordiais , um extrator de características o qual deve prover as características intrínsecas mais significativas dos dados e uma função de distância a qual quantifica a similaridade entre tais dados . O grande desafio é justamente como alcançar a melhor integração entre estes dois aspectos chaves com intuito de obter maior precisão nas consultas por similaridade . Apesar de inúmeros esforços serem continuamente despendidos para o desenvolvimento de novas técnicas de extração de características , muito pouca atenção tem sido direcionada à importância de uma adequada associação entre a função de distância e os extratores de características . A presente Dissertação de Mestrado foi concebida com o intuito de preencher esta lacuna . Para tal , foi realizada a análise do comportamento de diferentes funções de distância com relação a tipos distintos de vetores de características . Os três principais tipos de características intrínsecas às imagens foram analisados , com respeito a distribuição de cores , textura e forma . Além_disso  , foram propostas duas novas técnicas para realização de seleção de características com o desígnio de obter melhorias em relação à precisão das consultas por similaridade . A primeira técnica emprega regras de associação estatísticas e alcançou um ganho de até 38 % na precisão , enquanto que a segunda técnica utilizando a entropia de Shannon alcançou um ganho de aproximadamente 71 % ao mesmo tempo em que reduz significantemente a dimensionalidade dos vetores de características . O presente_trabalho  também demonstra que uma adequada utilização das funções de distância melhora efetivamente os resultados das consultas por similaridade . Conseqüentemente , desdobra novos caminhos para realçar a concepção de sistemas 
 A universalização do acesso ao conteúdo disponibilizado em sistemas Web tem se tornado crucial para que todas as pessoas , independente de deficiências ou de outras restrições possam ter acesso a ele . Diversos estudos indicam que , apesar da instituição de leis federais sobre acessibilidade para conteúdo Web em diversos países , muitos sítios ainda apresentam problemas . A falta de conscientização das pessoas envolvidas em projetos de desenvolvimento Web sobre a acessibilidade e a não utilização de técnicas adequadas para desenvolvimento de aplicações têm um impacto considerável sobre a acessibilidade . Levantamentos foram realizados com o objetivo de identificar características dos desenvolvedores sobre o conhecimento e uso de técnicas para acessibilidade . Entretanto , os estudos realizados investigaram somente o uso de um conjunto restrito de técnicas e , além disso , também não investigaram a correlação entre as respostas obtidas pelos participantes e o nível de acessibilidade das páginas desenvolvidas por eles . Neste trabalho , propõe-se efetuar um levantamento sobre a percepção de acessibilidade e uso de técnicas para desenvolvimento de sistemas Web considerando acessibilidade com pessoas envolvidas em projetos de desenvolvimento Web no Brasil de diferentes áreas de atuação . Este levantamento foi acompanhado de avaliações de acessibilidade automatizadas com uso de métricas sobre sítios desenvolvidos pelos participantes , para verificar a influência dos fatores investigados na acessibilidade dos sítios e na percepção de acessibilidade dos participantes . O levantamento realizado contou com a participação de 613 participantes de todo o Brasil . Os resultados indicaram que no Brasil a percepção da acessibilidade por pessoas que participam de projetos de desenvolvimento Web ainda é bastante limitada . Mais do que promover o treinamento das pessoas envolvidas em projetos sobre questões técnicas , é necessário promover maior conscientização sobre a acessibilidade e sobre os problemas que pessoas com diferentes restrições e habilidades enfrentam ao utilizar a Web 
 Em relação aos significativos resultados em Qualidade de Serviço ( QoS ) para servidores_Web  , existem ainda muitos problemas não resolvidos . Enquanto as abordagens atuais se limitam a prover QoS relativa através de diferenciação de serviço , este projeto apresenta e compara três modelos que tem por objetivo prover QoS absoluta para um array de servidores_Web  heterogêneos por meio de uma arquitetura de escalonamento ortogonal : A Multiple Queue ( MQ ) , a Single Queue ( SQ ) e a Dynamic Single Queue ( DSQ ) . A MQ consiste em receber a requisição HTTP e enviá-la para o servidor escolhido do array de servidores através do balanceamento de carga . A SQ e a DSQ possuem uma única fila gerenciada de forma centralizada . Enquanto a SQ envia a requisição somente quando o servidor esta livre , a DSQ seleciona o servidor com mais curto tempo de término mediante o uso de filas virtuais . Os modelos foram simulados considerando diferentes parâmetros e configurações para o ambiente . A avaliação de desempenho da arquitetura ortogonal demonstra que a mesma provê um bom_desempenho  na provisão de QoS absoluta com relação as mudanças instantâneas das cargas de trabalho no ambiente Web . Esta pesquisa estende os resultados da politica de escalonamento chamada EBS , concebida para provisão de garantias de tempo de resposta estocásticas em ambientes interativos online , especificamente para os servidores_Web  . Os resultados demonstram que a combinação da EBS na política de fila com a disciplina de recurso proposta neste trabalho é superior às outras combinações examinadas . Um modelo de política adaptativa é também 
 Aprendizado de máquina é uma área de pesquisa na qual se investiga como desenvolver sistemas capazes de aprender com a experiência . Muitos algoritmos de aprendizado possuem parâmetros cujos valores devem ser especificados pelo usuário . Em geral , esses valores influenciam diretamente no processo de aquisição do conhecimento , podendo gerar diferentes modelos . Recentemente , algoritmos de otimização bioinspirados têm sido aplicados com sucesso no ajuste de parâmetros de técnicas de aprendizado de máquina . Essas técnicas podem apresentar diferentes sensibilidades em relação aos valores escolhidos para seus parâmetros e diferentes algoritmos de ajuste de parâmetros podem apresentar desempenhos singulares . Esta dissertação investiga a utilização de algoritmos bioinspirados para o ajuste de parâmetros de redes neurais_artificiais  e máquinas de vetores de suporte em problemas de classificação . O objetivo dessa investigação é verificar quais são as técnicas que mais se beneficiam do ajuste de parâmetros e quais são os algoritmos mais eficientes para essas técnicas . Os resultados experimentais_mostram  que os algoritmos bioinspirados conseguem encontrar melhores clasificadores que outras abordagens . Porém , essa melhoria é estatisticamente significativa para alguns conjuntos de dados . Foi possível verificar que o uso dos valores padrão para os parâmetros das técnicas de classificação leva a desempenhos similares aos obtidos com os algoritmos bioinspirados . Entretanto , para alguns conjuntos de dados , o ajuste de parâmetros pode melhorar significativamente o desempenho dos 
 O estudo do comportamento de fluidos é um antigo domínio das ciências da natureza . Ultimamente , fenômenos de engenharia que eram estudados empiricamente passaram a ser estudados com auxílio computacional . A Dinâmica de Fluidos Computacional ( DFC ) é a área da ciência da computação que estuda métodos computacionais para simulação de escoamento de fluidos , e muitas_vezes  é a forma mais prática , ou a única , de se observar fenômenos de interesse no escoamento . Este projeto de Mestrado procurou investigar , no âmbito da simulação de um escoamento bifásico , métodos computacionais para representar a interface entre dois fluidos imiscíveis . A separação dos fluidos por meio de uma interface é necessária para assegurar que , propriedades como viscosidade e densidade , específicas de cada fluido , sejam utilizadas corretamente para o cálculo do movimento de seus respectivos fluidos . Desenvolvemos um método lagrangeano sem a utilização de malhas com o objetivo de suprir algumas restrições de trabalhos prévios . Para representar a interface entre os dois fluidos , este método utiliza uma técnica de reconstrução de superfícies baseada em aproximações de superfícies algébricas de alta ordem . Os resultados numéricos reportados neste documento evidenciam o potencial da nossa 
 Selecionar atributos é , por vezes , uma atividade necessária para o correto desenvolvimento de tarefas de aprendizado de máquina . Em Mineração de Textos , reduzir o número de atributos em uma base de textos é essencial para a eficácia do processo e a compreensibilidade do conhecimento_extraído  , uma vez que se lida com espaços de alta dimensionalidade e esparsos . Quando se lida com contextos nos quais a coleção de textos é não-rotulada , métodos não-supervisionados de redução de atributos são utilizados . No entanto , não existe forma geral predefinida para a obtenção de medidas de utilidade de atributos em métodos não-supervisionados , demandando um esforço maior em sua realização . Assim , este trabalho aborda a seleção não-supervisionada de atributos por meio de um estudo exploratório de métodos dessa natureza , comparando a eficácia de cada um deles na redução do número de atributos em aplicações de Mineração de Textos . Dez métodos são comparados - Ranking porTerm Frequency , Ranking por Document Frequency , Term Frequency-Inverse Document Frequency , Term Contribution , Term Variance , Term Variance Quality , Método de Luhn , Método LuhnDF , Método de Salton e Zone-Scored Term Frequency - sendo dois deles aqui propostos - Método LuhnDF e Zone-Scored Term Frequency . A avaliação se dá em dois focos , supervisionado , pelo medida de acurácia de quatro classificadores ( C4.5 , SVM , KNN e Naïve Bayes ) , e não-supervisionado , por meio da medida estatística de Expected Mutual Information Measure . Aos resultados de avaliação , aplica-se o teste estatístico de Kruskal-Wallis para determinação de significância estatística na diferença de desempenho dos diferentes métodos de seleção de atributos comparados . Seis bases de textos são utilizadas nas avaliações experimentais , cada uma relativa a um grande domínio e contendo subdomínios , os quais correspondiam às classes usadas para avaliação supervisionada . Com esse estudo , este trabalho visa contribuir com uma aplicação de Mineração de Textos que visa extrair taxonomias de tópicos a partir de bases textuais não-rotuladas , selecionando os atributos mais representativos em uma coleção de textos . Os resultados das avaliações mostram que não há diferença estatística significativa entre os métodos não-supervisionados de seleção de atributos comparados . Além_disso  , comparações desses métodos não-supervisionados com outros supervisionados ( Razão de Ganho e Ganho de Informação ) apontam que é possível utilizar os métodos não-supervisionados em atividades supervisionadas de Mineração de Textos , obtendo eficiência compatível com os métodos supervisionados , dado que não detectou-se diferença estatística nessas comparações , e com um custo_computacional  
 ChipCflow é o projeto de uma ferramenta para execução de algoritmos escritos em linguagem C utilizando o modelo a fluxo de dados dinâmico em hardware com reconfiguração parcial . O objetivo principal do projeto ChipCflow é a aceleração da execução de programas por meio da execução direta em hardware , aproveitando ao máximo o paralelismo considerado natural do modelo a fluxo de dados . Em particular nesta parte do projeto , realizou-se a prova de conceito para a programação a fluxo da dados em hardware_reconfigurável  . O modelo de fluxo de dados utilizado foi o estático em plataforma sem reconfiguração parcial , dada a complexidade desse sistema , que faz parte de outro módulo em desenvolvimento no projeto 
 Hierarquias de tópicos são formas eficientes de organização de coleções de documentos , auxiliando usuários a gerir o conhecimento materializado nessas publicações textuais . Tais hierarquias são usualmente construídas por meio de algoritmos de agrupamento hierárquico não supervisionado . Entretanto , por não considerarem o contexto do usuário na formação dos grupos , hierarquias de tópicos não supervisionadas nem sempre conseguem atender as suas expectativas . Uma solução para este problema e o emprego de algoritmos de agrupamento semissupervisionado , os quais incorporam o conhecimento de domínio do usuário por meio de restrições . Entretanto , para o contexto de agrupamento hierárquico semissupervisionado , não são eficientemente explorados na literatura métodos de seleção de casos ( instâncias ou grupos ) para receber restrições , bem como não há formas eficientes de interação do usuário com o processo de agrupamento hierárquico . Dessa maneira , neste trabalho , dois algoritmos de agrupamento hierárquico semissupervisionado são propostos : HCAC ( Hierarchical Confidence-based Active Clustering ) e HCAC-LC ( Hierarchical Confidence-based Active Clustering with Limited Constraints ) . Estes algoritmos empregam uma abordagem de aprendizado ativo baseado na confiança de uma junção de clusters . Quando uma junção de baixa confiança e detectada , o usuário e convidado a decidir , em um conjunto de pares de grupos candidatos , a melhor junção naquele ponto . Estes algoritmos são aqui utilizados na extração de hierarquias de tópicos por meio do framework SMITH , também proposto nesse trabalho . Este framework fornece uma série de atividades bem definidas que possibilitam a interação do usuário para a obtenção de hierarquias de tópicos . A abordagem de aprendizado ativo utilizado nos algoritmos HCAC e HCAC-LC , o tipo de restrição utilizada nestes algoritmos , bem como o framework SMITH para obtenção de hierarquias de tópicos semissupervisionadas são inovações ao estado da arte propostos neste trabalho . Os resultados obtidos indicam que os algoritmos HCAC e HCAC-LC superam o desempenho de outros algoritmos hierárquicos semissupervisionados em diversos cenários . Os resultados também indicam que hierarquias de tópico semissupervisionadas obtidas por meio do framework SMITH são mais intuitivas e fáceis de navegar do que aquelas não 
 Esta dissertação de mestrado tem como finalidade descrever o trabalho realizado em uma pesquisa que envolve a análise de expressões gênicas provenientes de microarrays com o objetivo de encontrar genes importantes em um organismo ou em uma determinada doença , como o câncer . Acreditamos que a descoberta desses genes , que chamamos aqui de genes de predição intrinsicamente multivariada ( genes IMP ) , possa levar a descobertas de importantes processos biológicos ainda não conhecidos na literatura . A busca por genes IMP foi realizada em conjunto com estudos de modelos e conceitos matemáticos e estatísticos como redes Booleanas , cadeias de Markov , Coeficiente de Determinação ( CoD ) , Classificação em análise de expressões gênicas e métodos de estimação de erro . No modelo de redes Booleanas , introduzido na Biologia por Kauffman , as expressões gênicas são quantizadas em apenas dois_níveis  : `` ligado '' ou `` desligado '' . O nível de expressão ( estado ) de cada gene , está relacionado com o estado de alguns outros genes através de uma função lógica . Adicionando uma perturbação aleatória a este modelo , temos um modelo mais geral conhecido como redes Booleanas com perturbação . O sistema dinâmico representado pela rede é uma cadeia de Markov ergódica e existe então uma distribuição de probabilidade estacionária . Temos a hipótese de que os experimentos de microarray seguem esta distribuição estacionária . O CoD é uma medida normalizada de quanto a expressão de um gene alvo pode ser melhor predita observando-se a expressão de um conjunto de genes preditores . Uma determinada configuração de CoDs caracteriza um gene alvo como sendo um gene IMP . Podemos trabalhar não somente com genes alvo , mas também com fenótipos alvo , onde o fenótipo de um sistema biológico poderia ser representado por uma variável aleatória binária . Por exemplo , podemos estar interessados em saber quais genes estão relacionados ao fenótipo de vida/morte de uma célula . Como a distribuição de probabilidade das amostras de microarray é desconhecida , o estudo dos CoDs é feito através de estimativas . Entre os métodos de estimação de erro estudados para este propósito podemos citar : Holdout , Resubstituição , Cross-validation , Bootstrap e .632 Bootstrap . Os métodos foram implementados para calcular os CoDs , permitindo então a busca por genes IMP . Os programas implementados na pesquisa foram usados em conjunto com uma pesquisa realizada pelo Prof. Dr. Hugo A. Armelin do Instituto de Química da USP . Este estudo em particular envolve a busca de genes importantes relacionados à morte de células tumorigênicas de camundongo disparada por FGF2 ( Fibroblast Growth Factor 2 ) . Nesta pesquisa observamos sub-redes de genes envolvidos no processo biológico em questão e também encontramos genes que podem_estar  relacionados ao fenômeno de morte das células de camundongo ou que estão , de fato , participando de alguma via disparada pelo FGF2 . Esta abordagem de análise de expressões gênicas , juntamente com a pesquisa realizada pelo Prof. Armelin , resulta em uma metodologia para buscas de genes envolvidos em novos mecanismos de células tumorigênicas , ativados pelo FGF2 . Na realidade esta metodologia pode ser aplicada em qualquer processo biológico de interesse científico , desde que seja possível modelar o problema proposto no contexto de redes Booleanas , coeficientes de determinação e genes IMP 
 Nesta dissertação estudou-se o modelo GARMA para modelar séries_temporais  de dados de contagem com as distribuições condicionais de Poisson , binomial e binomial negativa . A principal finalidade foi analisar no contexto clássico e bayesiano , o desempenho e a qualidade do ajuste dos modelos de interesse , bem como o desempenho dos percentis de cobertura dos intervalos de confiança dos parâmetros para os modelos adotados . Para atingir tal finalidade considerou-se a análise dos estimadores pontuais bayesianos e foram analisados intervalos de credibilidade . Neste estudo é proposta uma distribuição a priori conjugada para os parâmetros dos modelos e busca-se a distribuição a posteriori , a qual associada a certas funções de perda permite encontrar estimativas bayesianas para os parâmetros . Na abordagem clássica foram calculados estimadores de máxima_verossimilhança  , usandose o método de score de Fisher e verificou-se por meio de simulação a consistência dos mesmos . Com os estudos desenvolvidos pode-se observar que , tanto a inferência clássica quanto a inferência bayesiana para os parâmetros dos modelos em questão , apresentou boas propriedades analisadas por meio das propriedades dos estimadores pontuais . A última etapa do trabalho consiste na análise de um conjunto de dados reais , sendo uma série real correspondente ao número de internações por causa da dengue em Campina Grande . Estes resultados mostram que tanto o estudo clássico , quanto o bayesiano , são capazes de descrever bem o comportamento da 
 O desenvolvimento de conteúdos educacionais e a adoção de mecanismos de modelagem representam fatores importantes a serem considerados no contexto de ensino e aprendizagem . Nesse cenário , a utilização de ontologias proporciona vantagens tais como definção formal do conhecimento , reusabilidade e interoperabilidade de informações . Além_disso  , facilidades como recuperação dos objetos de aprendizagem , reúso do conteúdo de aprendizagem e personalização do conteúdo a partir do desempenho do usuário também são observadas . Diante disso , diversas aplicações com base em ontologias têm sido utilizadas tanto para modelar domínios educacionais como para construir , organizar e atualizar objetos de aprendizagem e perfis de aluno . Ainda , associado ao uso de ontologias em sistemas educacionais , verifica-se um crescente_interesse  na construção da personalização do conteúdo , de acordo com as preferências e características do usuário envolvendo a utilização de ontologias . O presente projeto está inserido nesse contexto , em que um conjunto de ontologias foi desenvolvido para prover a personalização do conteúdo , com base no perfil do usuário . Foi construído um exemplo na ontologia global ( que compõe o conjunto de ontologias ) , em que o domínio de análise de ponto de função foi instanciado para três usuários com diferentes níveis de conhecimento . A ideia é elicitar a granularidade distinta do conteúdo para cada usuário , em conformidade com suas preferências e nível de conhecimento no domínio , a partir das relações e das inferências estabelecidas nas 
 Coleções digitais de mídias , tanto pessoais como online , crescem rapidamente . Para que grandes quantidades de músicas sejam acessíveis à usuários , serviços populares como iTunes , Last.fm e Pandora oferecem recomendações . Essa abordagem livra usuários de lembrarem de músicas , e permite a descoberta de canções novas ou esquecidas . Mas recomendações apresentam problemas com usuários , como credibilidade e falta de controle . A motivação deste trabalho é melhorar a experiência de usuários com recomendações de música através do uso de explicações . Ao usar um sistema de recomendação , a satisfação e aprovação de usuários não depende só da eficácia do algoritmo , mas também de explicações . Pesquisas mostram que estas podem beneficiar sistemas de recomendação , aumentando a credibilidade e satisfação de usuários , ao oferecer mais transparência e formas de correção . O objetivo deste trabalho é projetar e desenvolver uma nova forma de visualização de tags , e testar sua viabilidade para explicar e filtrar recomendações de músicas . Mais precisamente , investigamos se esta visualização pode favorecer as metas de inspeção ( scrutability ) , eficiência , eficácia e satisfação . A partir da pesquisa em necessidades de usuários para recomendações e música , a visualização Tag Strings foi projetada e desenvolvida . Tag Strings inclui tanto a interface da visualização , quanto o processo de coleta e cálculo de relevância de tags e músicas . Para a avaliação da visualização Tag Strings , dois tipos de experimentos foram construídos : a comparação entre uma lista de recomendações com Tag Strings , e a comparação entre o design de referência ( baseado nos serviços Last.fm e Pandora ) e Tag Strings . A construção desses dois experimentos permitiu a avaliação de Tag Strings como uma forma de explicação para recomendações de música . Os resultados dos experimentos evidenciam que a nova forma de visualização Tag Strings favorece as metas de inspeção ( scrutability ) , eficiência , eficácia e satisfação , melhorando a usabilidade e experiência de usuários com recomendações de música 
 Neste trabalho , os métodos de pontos_interiores  primal-dual e preditor corretor são estudados e desenvolvidos para o problema de minimização de custos na geração e perdas na transmissão do pré-despacho DC ( fluxo de carga em corrente contínua ) de um sistema de potência hidroelétrico , com base no modelo de fluxo em redes e no princípio do mínimo esforço . A estrutura matricial , resultante da simplificação do problema proposto pela inclusão do princípio do mínimo esforço , é estudada visando implementações eficientes 
 Resolver um problema de processamento de imagens pode ser uma tarefa bastante complexa . Em geral , isto depende de diversos fatores , como o conhecimento , experiência e intuição de um especialista , e o conhecimento do domínio da aplicação em questão . Motivados por tal complexidade , alguns grupos de pesquisa têm trabalhado na criação de técnicas para projetar operadores de imagens automaticamente , a partir de uma coleção de exemplos de entrada e saída do operador desejado 

 A abordagem multirresolução tem sido empregada com sucesso no projeto estatístico de W-operadores de janelas grandes . Esta metodologia usa uma estrutura piramidal de janelas para auxiliar na estimação das distribuições de probabilidade condicional para padrões não observados no conjunto de treinamento . No entanto , a qualidade do operador projetado depende diretamente da pirâmide escolhida . Tal escolha é feita pelo projetista a partir de sua intuição e de seu conhecimento prévio sobre o problema 

 Neste trabalho , investigamos o uso da entropia condicional como um critério para determinar automaticamente uma boa pirâmide a ser usada no projeto do W-operador . Para isto , desenvolvemos uma técnica que utiliza o arcabouço piramidal multirresolução como um modelo na estimação da distribuição conjunta de probabilidades . Experimentos com o problema de reconhecimento de dígitos manuscritos foram realizados para avaliar o desempenho do método . Utilizamos duas bases de dados diferentes , com bons_resultados  . Além_disso  , outra contribuição deste trabalho foi a experimentação com mapeamentos de resolução da teoria de pirâmides de imagens no contexto do projeto de W-operadores multirresolução 

 A crescente demanda por sistemas e a alta velocidade com que seus requisitos evoluem têm evidenciado que desenvolvimento de software exige flexibilidade , pois muitas decisões precisam ser tomadas durante o projeto . Além_disso  , as dificuldades para a produção de sistemas vão muito além das questões técnicas . Fatores estratégicos , comerciais e humanos são responsáveis por algumas das variáveis que contribuem para tornar o desenvolvimento de sistemas de software uma atividade altamente complexa . Modelos tradicionais de desenvolvimento de software propõem processos prescritivos que não consideram toda essa complexidade . Por outro_lado  , Métodos Ágeis de desenvolvimento de software sugerem uma abordagem mais humanística com foco na entrega rápida e constante de software com valor de negócios . Porém , para conseguir isto , é preciso escolher um conjunto de práticas de desenvolvimento adequado às características do projeto e da equipe . Desta forma , a natureza única de cada projeto e a necessidade de alta qualidade e produtividade tornam importante a busca por práticas de desenvolvimento . A partir de projetos que conduzimos usando métodos ágeis na academia e na indústria , identificamos e descrevemos 22 práticas para desenvolvimento de software que podem ser adotadas por equipes para aumentar o seu desempenho e/ou a qualidade do software 
 Muitos problemas de otimização envolvem tanto variáveis inteiras quanto contínuas e podem ser modelados como problemas de programação não-linear inteira_mista  . Problemas dessa natureza aparecem com freqüência em engenharia química e incluem , por exemplo , síntese de processos , projeto de colunas de destilação , síntese de rede de trocadores de calor e produção de óleo e gás . Neste trabalho , apresentamos algoritmos baseados em Lagrangianos Aumentados e branch and bound para resolver problemas de programação não-linear inteira_mista  . Duas abordagens são consideradas . Na primeira delas , um algoritmo do tipo Lagrangianos Aumentados é usado como método para resolver os problemas de programação não-linear que aparecem em cada um dos nós do método branch and bound . Na segunda abordagem , usamos o branch and bound para resolver os problemas de minimização em caixas com variáveis inteiras que aparecem como subproblemas do método de Lagrangianos Aumentados . Ambos os algoritmos garantem encontrar a solução ótima de problemas convexos e oferecem recursos apropriados para serem usados na resolução de problemas não convexos , apesar de não haver garantia de otimalidade nesse caso . Apresentamos um problema de empacotamento de retângulos em regiões convexas arbitrárias e propomos modelos para esse problema que resultam em programas não-lineares com variáveis inteiras e contínuas . Realizamos alguns experimentos numéricos e comparamos os resultados obtidos pelo método descrito neste trabalho com os resultados alcançados por outros métodos . Também realizamos experimentos com problemas de programação não-linear inteira_mista  encontrados na literatura e comparamos o desempenho do nosso método ao de outro disponível publicamente 
 Este documento apresenta uma dissertação sobre o desenvolvimento de um sistema de integração de dados para geração de estruturas tridimensionais a partir de vídeo 3D . O trabalho envolve a extensão de um sistema de vídeo 3D em tempo real proposto recentemente . Esse sistema , constituído por projetor e câmera , obtém imagens de profundidade de objetos por meio da projeção de slides com um padrão de faixas coloridas . Tal procedimento permite a obtenção , em tempo real , tanto do modelo 2,5 D dos objetos quanto da textura dos mesmos , segundo uma técnica denominada luz estruturada . Os dados são capturados a uma taxa de 30 quadros por segundo e possuem alta qualidade : resoluções de 640 x 480 pixeis para a textura e de 90 x 240 pontos ( em média ) para a geometria . A extensão que essa dissertação_propõe  visa obter o modelo tridimensional dos objetos presentes em uma cena por meio do registro dos dados ( textura e geometria ) dos diversos quadros amostrados . Assim , o presente_trabalho  é um passo intermediário de um projeto maior , no qual pretende-se fazer a reconstrução dos modelos por completo , bastando para isso apenas algumas imagens obtidas a partir de diferentes pontos de observação . Tal reconstrução deverá diminuir a incidência de pontos de oclusão ( bastante comuns nos resultados originais ) de modo a permitir a adaptação de todo o sistema para objetos móveis e deformáveis , uma vez que , no estado atual , o sistema é robusto apenas para objetos estáticos e rígidos . Até onde pudemos averiguar , nenhuma técnica já foi aplicada com este propósito . Este texto descreve o trabalho já desenvolvido , o qual consiste em um método para detecção , rastreamento e casamento espacial de componentes conexas presentes em um vídeo 3D . A informação de imagem do vídeo ( textura ) é combinada com posições tridimensionais ( geometria ) a fim de alinhar partes de superfícies que são vistas em quadros subseqüentes . Esta é uma questão chave no vídeo 3D , a qual pode ser explorada em diversas aplicações tais como compressão , integração geométrica e reconstrução de cenas , dentre outras . A abordagem que adotamos consiste na detecção de características salientes no espaço do mundo , provendo um alinhamento de geometria mais completo . O processo de registro é feito segundo a aplicação do algoritmo ICP -- -Iterative Closest Point -- -introduzido por Besl e McKay em 1992 . Resultados experimentais bem sucedidos corroborando nosso método são apresentados 
 A gestão eficaz de recursos_computacionais  em nuvem está diretamente ligada a gerir corretamente o desempenho das aplicações hospedadas na Máquina Virtual ( Virtual Machine - VM ) , criando um ambiente capaz de controlá-la e redimensionar recursos de Memória , Disco , CPU e outros que se façam necessários , individualmente em resposta a carga de trabalho . Neste trabalho considera-se também a gestão eficaz a qual é possível realizar o retorno sobre o investimento realizado para a contratação do serviço de IaaS . Nesta pesquisa de mestrado , foi proposto o gerenciamento da infraestrutura computacional em nuvem , através de dois modelos que facilitam o provisionamento auto-adaptativo de recursos em um ambiente virtualizado : alocação de recursos utilizando modelo para previsão da carga de trabalho futura e a gestão auto-adaptativa de capacidade utilizando agentes computacionais para monitorarem constantemente as VMs . Além_disso  , é proposto o retorno do investimento , que trata a relação entre o valor que o cliente contratou do serviço de IaaS e o quanto efetivamente ele está utilizando . Desta forma , a cada_período  é contabilizado a taxa do valor gasto em unidades monetárias . Para contemplar esta proposta , foram desenvolvidos algoritmos que são o núcleo de todo gerenciamento . Também foram realizados_experimentos  e os resultados mostram a capacidade do autogerenciamento das máquinas virtuais , com reconfiguração dinâmica da infraestrutura através de previsões baseadas em histórico e também da reconfiguração e monitoramento com o uso de agentes computacionais . Após a análise e avaliação dos resultados obtidos nos experimentos , é possível afirmar que houve uma significativa melhora da reconfiguração dos recursos com agentes computacionais se comparado a reconfiguração com previsão de carga futura 
 Um agente de planejamento em Inteligência_Artificial  deve estar preparado para lidar com aspectos dinâmicos do domínio , ou seja , com os efeitos de suas ações , bem como com mudanças provocadas por outros agentes ( eventos exógenos ) . Neste caso , é possível identificar duas situações distintas : todas as informações necessárias sobre a dinâmica do ambiente são , de alguma_forma  , modeladas pelo agente e consideradas durante o planejamento ; ou o agente não possui conhecimento completo do ambiente , sendo que as ações exógenas só são percebidas durante a execução do plano . Neste último caso , podem ocorrer falhas na execução das ações do agente , por exemplo , as pré-condições nas quais o plano se baseia deixam de ser verdadeiras e a meta do agente pode não ser mais alcançável . Para garantir que o agente saia da situação de plano inválido e alcance seu conjunto de metas originais , é preciso fazer um `` reparo de plano '' ou `` replanejamento '' . Enquanto no replanejamento o agente abandona o plano original e constrói um novo plano para o estado atual , o reparo de plano tenta fazer um compromisso com o plano original , gerando o menor número de mudanças possível para que o novo plano atinja as metas do problema original . O processo de reparo de plano envolve duas operações básicas : ( a ) remover do plano as ações que estejam impedindo a execução do mesmo e ( b ) adicionar novas ações a fim de atingir as metas . A proposta deste trabalho é aplicar e implementar o método de reparo de plano chamado de `` refinamento reverso '' . O sistema de reparo implementado será capaz de realizar duas operações : adicionar ações por meio dos procedimentos clássicos de refinamento de planos e remover ações por refinamento reverso com a adição de heurísticas para melhorar o desempenho da tarefa de reparo 
 Esta dissertação tem como objetivo estudar a combinação de duas técnicas de planejamento em inteligência_artificial  : planejamento hierárquico e planejamento sob incerteza Knightiana . Cada uma delas possui vantagens distintas , mas que podem ser combinadas , permitindo um ganho de eficiência para o planejamento sob incerteza e maior robustez a planos gerados por planejadores hierárquicos . Primeiramente , estudamos um meio de efetuar uma transformação , de modo sistemático , que permite habilitar algoritmos de planejamento determinístico com busca progressiva no espaço de estados a tratar problemas com ações não-determinísticas , sem considerar a distribuição de probabilidades de efeitos das ações ( incerteza Knightiana ) . Em seguida , esta transformação é aplicada a um algoritmo de planejamento hierárquico que efetua decomposição a partir das tarefas sem predecessoras , de modo progressivo . O planejador obtido é competitivo com planejadores que representam o estado-da-arte em planejamento sob incerteza , devido à informação adicional que pode ser fornecida ao planejador , na forma de métodos de decomposição de tarefas 
 Esta dissertação de mestrado aborda um estudo sobre a caracterização de cargas de trabalho para Web_services  por meio da análise de Documentos WSDL ( Web Service Description Language ) . Esses documentos , que representam a interface para os serviços , foram obtidos na Web e seus conteúdos analisados estatisticamente . A metodologia utilizada para alcançar a caracterização desejada , constitui-se da coleta e análise dos dados de 1346 arquivos WSDL válidos . O resultado do estudo proposto nesta dissertação contribui para a avaliação de desempenho no que diz_respeito  a caracterizar os diversos aspectos das cargas de trabalho de Web_services  . Dentre os dados obtidos ressaltam-se as porcentagens da ocorrência de algumas características tais como o número de operações , tipos de Binding , quantidade de parâmetros de entrada e saída e tipos de dados mais utilizados . Para auxiliar na busca e avaliação das características das WSDLs uma nova ferramenta denominada WSDLAnalyzer é proposta e 
 Coreografias de serviços_Web  representam uma forma mais escalável e flexível de compor serviços do que uma abordagem centralizada como a orquestração , e seu papel na integração e comunicação de sistemas de larga_escala  é vital para os objetivos da SOC ( Computação Orientada a Serviços ) e da Internet do Futuro . Atualmente coreografias de serviços_Web  possuem vários desafios de pesquisa , dos quais a qualidade de serviço ( QoS ) e o monitoramento de coreografias de serviçosWeb são linhas importantes . O objetivo deste trabalho é propor e implementar um mecanismo de monitoramento não intrusivo de coreografias de serviços_Web  baseado em SLAs ( Acordos de Nível de Serviço ) que especificam as restrições de atributos de QoS de maneira probabilística . Esta dissertação_propõe  um mecanismo para coreografias de serviços_Web  que : ( 1 ) define requisitos de QoS ; ( 2 ) especifica contratos probabilísticos sobre parâmetros de QoS usando SLA ; e ( 3 ) realiza um monitoramento não intrusivo de coreografias de serviços_Web  para detectar violações de SLA 
 Neste trabalho estudamos as noções de alcançabilidade e controlabilidade para sistemas_lineares  a tempo contínuo com perturbações aditivas e saltos nos parâmetros sujeitos a uma cadeia de Markov geral . Definimos conceitos de alcançabilidade e controlabilidade médios de maneira natural exigindo que os valores esperados dos gramianos correspondentes sejam definidos positivos . Visando obter uma condição testável para ambos os conceitos , introduzimos conjuntos de matrizes de alcançabilidade e de controlabilidade para esta classe de sistemas e usamos certas propriedades de invariância para mostrar que : o sistema é alcançável em média , e , analogamente , controlável em média , se e somente se as matrizes respectivas , de alcançabilidade e de controlabilidade , têm posto completo . Usamos alcançabilidade média de sistemas para mostrar que a matriz de segundo momento do estado é definida positiva com uma margem uniforme . Uma consequência deste resultado no problema de estimação linear do estado é que a matriz de covariância do erro de estimação é positiva definida em média , no sentido que existe um nível mínimo de ruído nas estimativas . Na sequência , para estimadores lineares markovianos , estudamos a limitação do valor esperado da matriz de covariância do erro para mostrar que o filtro é estável num certo sentido , sendo esta uma propriedade desejável em aplicações reais . Quanto às aplicações da controlabilidade média , usamos este conceito para estabelecer condições necessárias e suficientes que garantem a existência de um processo de controle que leva a componente contínua do estado do sistema para a origem em tempo finito e com probabilidade positiva 
 O surgimento de novos serviços e aplicações baseados na Web tem provocado um aumento desenfreado na quantidade de usuários da World_Wide  Web que , por sua vez , se torna cada vez mais popular no mundo dos negócios . Sites de e-commerce , que demandam grande tráfego de requisições , têm adotado sistemas de servidores_Web  distribuídos , como a arquitetura Web Cluster . Isso se deve ao fato de enfrentarem frequentemente situações de sobrecarga , durante as quais podem deixar de atender requisições de transação ( com grande probabilidade de gerar renda ) por conta do aumento na demanda de requisições de navegação ( geram renda apenas de forma indireta ) . A utilização ineficiente de recursos pode comprometer o desempenho do sistema e é nesse contexto que este trabalho se insere . Neste trabalho foi desenvolvido um modelo de Servidor Web para E-Commerce ( SWE-C ) , validado por meio de um modelo de simulação e uma carga sintética gerada a partir de um modelo desenvolvido com os principais tipos de requisições que caracterizam um site de e-commerce . Foram realizadas simulações no sistema com diversas combinações de algoritmos de escalonamento e disciplinas de atendimentos para filas , dentre as quais de destaca uma nova disciplina que utiliza um mecanismo de prioridades orientado ao consumo de CPU proposto neste trabalho . O objetivo é aumentar o throughput de requisições de transação e melhorar os tempos de resposta em situações de sobrecarga . Uma avaliação de desempenho foi realizado e constatou-se que o mecanismo de prioridades proposto é adequado às necessidades de um site de e-commerce 
 A Dimensão Fractal pode ser utilizada para medir algumas características ligadas a complexidade da imagem , permitindo seu uso em análise de formas e texturas e reconhecimento de padrões . Neste trabalho é apresentado um estudo comparativo entre alguns dos principais métodos de estimativa da Dimensão Fractal . Foi realizada uma análise experimental e um estudo de casos para cada uma das técnicas , levando em consideração aspectos de implementação , precisão , variação de resultados segundo ajuste de parâmetros e tolerância a ruídos . Neste trabalho também foi desenvolvido um estudo sobre a Dimensão Fractal Multiescala , visando seu emprego como metodologia de assinatura de complexidade . Na literatura a técnica de multiescala é limitada ao método de Bouligand-Minkowski , sendo aqui ela estendida para outras metodologias de estimativa de Dimensão Fractal . Por meio de análise experimental as metodologias propostas foram comparadas e os resultados discutidos , enfatizando as vantagens e desvantagens destas técnicas 
 Os Métodos Ágeis surgiram no final da década passada como uma alternativa aos métodos tradicionais de desenvolvimento de software . Eles propõem uma nova abordagem para o desenvolvimento , eliminando gastos com documentação excessiva e burocrática , enfatizando a comunicação , colaboração com o cliente e as atividades que trazem valor imediato na produção de software com qualidade . Por meio de um processo empírico , com ciclos constantes de inspeção e adaptação , a equipe trabalha sempre num ambiente de melhoria contínua . Uma das atividades propostas pela Programação Extrema ( XP ) para guiar a equipe em direção à melhoria é conhecida como tracking . O papel do tracker é coletar métricas para auxiliar a equipe a entender o andamento do projeto . Encontrar maneiras eficazes de avaliar o processo e a equipe de desenvolvimento não é uma tarefa simples . Além_disso  , alguns dos possíveis problemas não são facilmente reconhecidos a partir de dados quantitativos . Este trabalho_investiga  o uso de métricas no acompanhamento de projetos utilizando Métodos Ágeis de desenvolvimento de software . Um estudo de caso da aplicação de XP em sete projetos acadêmicos e governamentais foi conduzido para validar algumas dessas métricas e para avaliar o nível de aderência às práticas propostas , com o objetivo de auxiliar o tracker de uma equipe ágil 
 O objetivo deste trabalho é desenvolver uma plataforma para anotação e classificação automática de entidades nomeadas para notícias escritas em português do Brasil . Para restringir um pouco o escopo do treinamento e análise foram utilizadas notícias esportivas do Campeonato Paulista de 2011 do portal UOL ( Universo Online ) . O primeiro artefato desenvolvido desta plataforma foi a ferramenta WebCorpus . Esta tem como principal intuito facilitar o processo de adição de metainformações a palavras através do uso de uma interface rica web , elaborada para deixar o trabalho ágil e simples . Desta forma as entidades nomeadas das notícias são anotadas e classificadas manualmente . A base de dados foi alimentada pela ferramenta de aquisição e extração de conteúdo desenvolvida também para esta plataforma . O segundo artefato desenvolvido foi o córpus UOLCP2011 ( UOL Campeonato Paulista 2011 ) . Este córpus foi anotado e classificado manualmente através do uso da ferramenta WebCorpus utilizando sete tipos de entidades : pessoa , lugar , organização , time , campeonato , estádio e torcida . Para o desenvolvimento do motor de anotação e classificação automática de entidades nomeadas foram utilizadas três diferentes técnicas : maximização de entropia , índices invertidos e métodos de mesclagem das duas técnicas anteriores . Para cada uma destas foram executados três passos : desenvolvimento do algoritmo , treinamento utilizando técnicas de aprendizado de máquina e análise dos melhores_resultados  
 Um problema importante na área de Biologia Sistêmica é o de inferência de redes de regulação gênica . Os avanços científicos e tecnológicos nos permitem analisar a expressão_gênica  de milhares de genes simultaneamente . Por `` expressão_gênica  '' , estamos nos referindo ao nível de mRNA dentro de uma célula . Devido a esta grande quantidade de dados , métodos matemáticos , estatísticos e computacionais têm sido desenvolvidos com o objetivo de elucidar os mecanismos de regulação gênica presentes nos organismos vivos . Para isso , modelos matemáticos de redes de regulação gênica têm sido_propostos  , assim como algoritmos para inferir estas redes . Neste trabalho , focamos nestes dois aspectos : modelagem e inferência . Com relação à modelagem , estudamos modelos existentes para o ciclo celular da levedura ( Saccharomyces cerevisiae ) . Após este estudo , propomos um modelo baseado em redes Booleanas probabilísticas sensíveis ao contexto , e em seguida , um aprimoramento deste modelo , utilizando cadeias de Markov não homogêneas . Mostramos os resultados , comparando os nossos modelos com os modelos estudados . Com relação à inferência , propomos um novo algoritmo utilizando o paradigma de crescimento de semente de genes . Neste contexto , uma semente é um pequeno subconjunto de genes de interesse . Nosso algoritmo é baseado em dois passos : passo de crescimento de semente e passo de amostragem . No primeiro passo , o algoritmo adiciona outros genes à esta semente , seguindo algum critério . No segundo , o algoritmo realiza uma amostragem de redes , definindo como saída um conjunto de redes potencialmente interessantes . Aplicamos o algoritmo em dados artificiais e dados biológicos de células HeLa , mostrando resultados_satisfatórios  
 Este trabalho contempla o estudo de algoritmos criptográficos para assegurar a comunicação entre sistemas embarcados_críticos  tendo em vista o grande crescimento na utilização e disseminação desse tipo de sistema , bem como a alta necessidade em se assegurar as informações que são enviadas e recebidas . Um dos desafios a serem contemplados é o estudo e a avaliação do impacto no desempenho desses sistemas , levando em consideração limitações de recursos inerentes a esta plataforma e a criticidade da comunicação em sistemas de tempo real . Os experimentos_realizados  são de cunho prático por meio de um protótipo implementado em kits Gumstix Overo EVM . Os resultados avaliam os principais algoritmos de criptografia , provendo informações que podem auxiliar na escolha de uma solução criptográfica própria para ambientes 
 Uma decomposição de um grafo G é um conjunto D = { H_1 , ... , H_k } de subgrafos de G dois-a-dois aresta-disjuntos que cobre o conjunto das arestas de G. Se H_i é isomorfo a um grafo fixo H , para 1 < =i < =k , então dizemos que D é uma H-decomposição de G. Neste trabalho , estudamos o caso em que H é um caminho de comprimento fixo . Para isso , primeiramente decompomos o grafo dado em trilhas , e depois fazemos uso de um lema de desemaranhamento , que nos permite transformar essa decomposição em trilhas numa decomposição somente em caminhos . Com isso , obtemos resultados para três conjecturas sobre H-decomposição de grafos no caso em que H=P_\ell é o caminho de comprimento \ell . Dois desses resultados resolvem versões fracas das Conjecturas de Kouider e Lonc ( 1999 ) e de Favaron , Genest e Kouider ( 2010 ) , ambas para grafos regulares . Provamos que , para todo inteiro positivo \ell , ( i ) existe um inteiro positivo m_0 tal que se G é um grafo 2m\ell-regular com m > =m_0 , então G admite uma P_\ell-decomposição ; ( ii ) se \ell é ímpar , existe um inteiro positivo m_0 tal que se G é um grafo m\ell-regular com m > =m_0 , e G contém um m-fator , então G admite uma P_\ell-decomposição . O terceiro resultado diz_respeito  a grafos altamente aresta- conexos : existe um inteiro positivo k_\ell tal que se G é um grafo k_\ell-aresta-conexo cujo número de arestas é divisível por \ell , então G admite uma P_\ell-decomposição . Esse resultado prova que a Decomposition Conjecture de Barát e Thomassen ( 2006 ) , formulada para árvores , é verdadeira para caminhos 
 Para realizar a correção de atenuação em uma tomografia computadorizada por emissão de fóton único ( SPECT , em inglês ) é necessário medir e reconstruir o mapa dos coeficientes de atenuação utilizando uma leitura de um tomógrafo de transmissão , feita antes ou simultaneamente à leitura de emissão . Essa abordagem encarece a produção da imagem e , em alguns_casos  , aumenta consideravelmente a duração do exame , sendo a imobilidade do paciente um fator importante para o sucesso da reconstrução . Uma alternativa que dispensa a leitura de transmissão é reconstruir tanto a imagem de atividade quanto o mapa de atenuação somente através dos dados de uma leitura de emissão . Dentro dessa abordagem propusermos um método baseado no algoritmo criado por Censor , cujo_objetivo  é resolver um problema misto de viabilidade côncavo-convexo para reconstruir simultaneamente as imagens . O método proposto é formulado como um problema de minimização , onde a função objetivo é dada pela variação total das imagens sujeita à viabilidade mista de Censor . Os teste foram feitos em imagens simuladas e os resultados obtidos na ausência de ruídos , mesmo para uma pequena quantidade de dados , foram satisfatórios . Na presença de dados ruidosos com distribuição de Poisson o método foi instável e a escolha das tolerâncias , nesse caso , ainda é um problema aberto 
 O estudo do comportamento assintótico de densidades de algumas subestruturas é uma das principais áreas de estudos em combinatória . Na Teoria das Permutações , fixadas permutações ? 1 e ? 2 e um inteiro n > 0 , estamos interessados em estudar o comportamento das densidades de ? 1 e ? 2 na família de permutações de tamanho n. Assim , existem duas direções naturais que podemos seguir . Na primeira direção , estamos interessados em achar a permutação de tamanho n que maximiza a densidade das permutações ? 1 e ? 2 simultaneamente . Para n suficientemente grande , explicitamos a densidade máxima que uma família de permutações podem assumir dentre todas as permutações de tamanho n. Na segunda direção , estamos interessados em achar a permutação de tamanho n que minimiza a densidade de ? 1 e ? 2 simultaneamente . Quando ? 1 é a permutação identidade com k elementos e ? 2 é a permutação reversa com l elementos , Myers conjecturou que o mínimo é atingido quando tomamos o mínimo dentre as permutações que não possuem a ocorrência de ? 1 ou ? 2 . Mostramos que se restringirmos o espaço de busca somente ao conjunto de permutações em camadas , então a Conjectura de Myers é verdadeira . Por outro_lado  , na Teoria dos Grafos , o problema de encontrar um circuito Hamiltoniano é um problema NP-completo clássico e está entre os 21 problemas Karp . Dessa forma , uma abordagem comum na literatura para atacar esse problema é encontrar condições que um grafo deve satisfazer e que garantem a existência de um circuito Hamiltoniano em tal grafo . O célebre resultado de Dirac afirma que se um grafo G de ordem n possui grau mínimo pelo menos n/2 , então G possui um circuito Hamiltoniano . Seguindo a linha de Dirac , mostramos que , dados inteiros 1 6 l 6 k/2 e ? > 0 existe um inteiro n0 > 0 tal que , se um hipergrafo k-uniforme H de ordem n satisfaz ? k-2 ( H ) > ( ( 4 ( k - l ) - 1 ) / ( 4 ( k - l ) 2 ) + ? ) ( n 2 ) , então H possui um l-circuito Hamiltoniano 
 A qualidade de dados é uma das principais preocupações em Aprendizado de Máquina - AM -cujos algoritmos são freqüentemente utilizados para extrair conhecimento durante a fase de Mineração de Dados - MD - da nova área de pesquisa chamada Descoberta de Conhecimento de Bancos de Dados . Uma vez que a maioria dos algoritmos de aprendizado induz conhecimento estritamente a partir de dados , a qualidade do conhecimento_extraído  é amplamente determinada pela qualidade dos dados de entrada 

 Diversos aspectos podem influenciar no desempenho de um sistema de aprendizado devido à qualidade dos dados . Em bases de dados reais , dois desses aspectos estão relacionados com ( i ) a presença de valores desconhecidos , os quais são tratados de uma forma bastante simplista por diversos algoritmos de AM , e ; ( ii ) a diferença entre o número de exemplos , ou registros de um banco de dados , que pertencem a diferentes classes , uma vez que quando essa diferença é expressiva , sistemas de aprendizado podem ter dificuldades em aprender o conceito relacionado com a classe minoritária 

 O problema de tratamento de valores desconhecidos é de grande interesse prático e teórico . Em diversas aplicações é importante saber como proceder quando as informações disponíveis estão incompletas ou quando as fontes de informações se tornam indisponíveis . O tratamento de valores desconhecidos deve ser cuidadosamente planejado , caso contrário , distorções podem ser introduzidas no conhecimento induzido . Neste trabalho é proposta a utilização do algoritmo k-vizinhos mais próximos como método de imputação . Imputação é um termo que denota um procedimento que substitui os valores desconhecidos de um conjunto de dados por valores plausíveis . As análises conduzidas neste trabalho indicam que a imputação de valores desconhecidos com base no algoritmo k-vizinhos mais próximos pode superar o desempenho das estratégias internas utilizadas para tratar valores desconhecidos pelos sistemas C4.5 e CN2 , bem como a imputação pela média ou moda , um método amplamente utilizado para tratar valores desconhecidos 

 O problema de aprender a partir de conjuntos de dados com classes desbalanceadas é de crucial importância , uma vez que esses conjuntos de dados podem ser encontrados em diversos_domínios  . Classes com distribuições desbalanceadas podem se constituir em um gargalo significante no desempenho obtido por sistemas de aprendizado que assumem uma distribuição balanceada das classes . Uma solução para o problema de aprendizado com distribuições desbalanceadas de classes é balancear artificialmente o conjunto de dados . Neste trabalho é avaliado o uso do método de seleção unilateral , o qual realiza uma remoção cuidadosa dos casos que pertencem à classe majoritária , mantendo os casos da classe minoritária . Essa remoção cuidadosa consiste em detectar e remover casos considerados menos confiáveis , por meio do uso de algumas heurísticas 

 Uma vez que não existe uma análise matemática capaz de predizer se o desempenho de um método é superior aos demais , análises experimentais possuem um papel_importante  na avaliação de sistema de aprendizado . Neste trabalho é proposto e implementado o ambiente computacional Discover Learning Environmnet - DLE - o qual é um em framework para desenvolver e avaliar novos métodos de pré-processamento de dados . O ambiente DLE é integrado ao projeto Discover , um projeto de pesquisa em desenvolvimento em nosso laboratório para planejamento e execução de experimentos relacionados com o uso de sistemas de aprendizado durante a fase de Mineração de dados do processo de KDD 
 Para o sucesso do processo de mineração de dados é importante que o conhecimento extraí ? do seja compreensível e interessante para que o usuário final possa utilizá-lo em um sistema inteligente ou em processos de tomada de decisão . Um grande problema , porém , é identificado quando a tarefa de mineração de dados denominada associação é utilizada : a geração de um grande volume de regras . Taxonomias podem ser utilizadas para facilitar a análise e interpretação das regras de associação , uma vez que as mesmas provêm uma visão de como os itens podem ser hierarquicamente classificados . Em função dessa hierarquia é possível_obter  regras mais gerais que representem um conjunto de itens . Dentro desse contexto , neste trabalho é apresentada uma metodologia para construção semi-automática de taxonomias , que inclui procedimentos automáticos e interativos para a realização dessa tarefa . Essa combinação possibilita a utilização do conhecimento do especialista e também o auxilia na identificação de grupos . Entre os principais resultados deste trabalho , pode-se destacar a proposta e implementação do algoritmo SACT ( Semi-automatic Construction of Taxonomies - Construção Semi-automática de Taxonomias ) , que provê a utilização da metodologia_proposta  . Para viabilizar a utilização do algoritmo , foi desenvolvido o módulo computacional RulEESACT . Com o objetivo de viabilizar e analisar a qualidade da metodologia_proposta  e do módulo desenvolvido , foi realizado um estudo de caso no qual foram construída taxonomias para duas bases de dados utilizando o RulEE-SACT . Uma das taxonomias foi analisada e validada por uma especialista do domínio . Posteriormente , as taxonomias e as bases de transações foram fornecidas para dois algoritmos de generalização de regras de associação a fim de analisar a aplicação das taxonomias 
 Há várias décadas , pesquisadores em ciências sociais buscam formas gráficas para expressar as relações humanas na sociedade . O advento do computador e , mais recentemente , da internet , possibilitou o surgimento de um campo que tem despertado a atenção de estudiosos das áreas de visualização de informação e de ciências sociais , o da visualização de redes_sociais  . Esse campo tem o potencial de revelar e explorar padrões que podem beneficiar um número muito grande de aplicações e indivíduos em áreas tais como comércio , segurança em geral , redes de conhecimento e pesquisa de mercado . Grande parte dos algoritmos de visualização de redes_sociais  são baseados em grafos , destacando relacionamentos entre indivíduos e grupos de indivíduos , mas dando pouca atenção aos seus demais atributos . Assim , este trabalho apresenta um conjunto de soluções para representar e explorar visualmente redes_sociais  levando em consideração tais atributos . A primeira solução faz_uso  de redes heterogêneas , onde tanto indivíduos quanto comunidades são representados no grafo ; a segunda solução utiliza técnicas de visualização baseadas em projeção_multidimensional  , que promovem o posicionamento dos dados no plano de acordo com algum critério de similaridade baseado em atributo ; e a última solução coordena múltiplas visões para focar rapidamente em regiões de interesse . Os resultados indicam que as soluções proveem um poder de representação e identificação de conceitos não facilmente detectados por formas convencionais de visualização e exploração de grafos , com indícios fornecidos através dos estudos de caso e da realização de avaliações com usuários . Este trabalho fornece um estudo das áreas de visualização em grafos para a análise de redes_sociais  bem como uma implementação das soluções de integração da visualização em redes com as projeções_multidimensionais 
 Na natureza e na ciência , dados e informações que desviam significativamente da média frequentemente possuem grande relevância . Esses dados são usualmente denominados na literatura como outliers . A identificação de outliers é importante em muitas aplicações reais , tais como detecção de fraudes , diagnóstico de falhas , e monitoramento de condições médicas . Nos últimos_anos  tem-se testemunhado um grande interesse na área de Redes Complexas . Redes complexas são grafos de grande escala que possuem padrões de conexão não trivial , mostrando-se uma poderosa maneira de representação e abstração de dados . Embora um grande montante de resultados tenham sido reportados nesta área de pesquisa , pouco tem sido explorado acerca de detecção de outliers em redes complexas . Considerando-se a dinâmica de uma caminhada aleatória , foram propostos neste trabalho uma medida de distância e um método de ranqueamento de outliers . Através desta técnica , é possível detectar como outlier não somente nós periféricos , mas também nós centrais ( hubs ) , depedendo da estrutura da rede . Também foi identificado que existem características bem definidas entre os nós outliers , relacionadas a funcionalidade dos mesmos para a rede . Além_disso  , foi descoberto que nós outliers têm papel_importante  para a rotulação a priori na tarefa de detecção de comunidades semi-supervisionada . Isto porque os nós centrais são bons difusores de informação e os nós periféricos encontram-se em regiões de borda de comunidade . Baseado nessa observação , foi proposto um método de detecção de comunidades semi-supervisionado . Os resultados de simulações mostram que essa abordagem é 
 Técnicas baseadas na Equação de Fluxo Bem-Balanceada têm sido muitas_vezes  empregadas como eficientes ferramentas para eliminação de ruídos e preservação de arestas em imagens_digitais  . Embora efetivas , essas técnicas demandam alto custo_computacional  . Este trabalho objetiva propor uma técnica baseada na abordagem multigrid para acelerar a solução_numérica  da Equação de Fluxo Bem-Balanceada . A equação de difusão é resolvida em uma malha grossa e uma correção do erro na malha grossa para as mais finas é aplicada para gerar a solução desejada . A transferência entre malhas grossas e finas é feita pelo filtro de Mitchell , um esquema bem conhecido que é projetado para preservação de arestas . Além_disso  , a equação do transporte e a Equação do Fluxo de Curvatura são adaptadas à nossa técnica para retoque em imagens e eliminação de ruí ? dos . Resultados_numéricos  são comparados quantitativamente e qualitativamente com outras abordagens , mostrando que o método aqui introduzido produz qualidade de imagens similares com muito menos tempo computacional 
 O trabalho apresentado nesta dissertação trata do desenvolvimento de um mecanismo para adaptação automática de ví ? deo MPEG-4 ao vivo , de modo a atender as necessidades ou capacidades atuais de usuários e do sistema . Um dos desafios dessa área é capturar e representar as informações necessárias para realizar a adaptação . Assim , utilizando técnicas da área de computação ciente de contexto , foi desenvolvido um modelo extensível para representação de dispositivos . Também foram desenvolvidos métodos automáticos e semi-automáticos para capturar as informações necessárias . Neste trabalho foi adotado o modelo de recodificação de vídeo , o qual pode gerar atrasos que inviabilizam a adaptação de vídeo ao vivo em aplicações interativas . Assim , este trabalho realizou uma avaliação do impacto causado pela recodificação no atraso total , fim-a-fim , percebido pelo usuário 
 No contexto da Internet convencional , são várias as soluções oferecidas para transferência de fluxos de áudio e vídeo . Este trabalho é realizado no contexto de uma rede de grande largura de banda e de baixa latência , como aquela que está sendo investigada no contexto do Programa FAPESP TIDIA . O grupo de pesquisa no qual este trabalho foi realizado propôs a arquitetura S-MOJOHON ( Lobato et al. , 2007a ) , elaborada com o intuito de permitir transmissão de vários_tipos  de dados , principalmente áudio e vídeo , no ambiente TIDIA . Os componentes intermediários de uma arquitetura voltada para transmissão de fluxos de áudio e vídeo na Internet , como é o caso da S-MOJOHON , precisam ser selecionados de maneira que possibilitem a entrega de informações entre componentes finais da melhor forma possível . Nesse sentido , o trabalho reportado nesta dissertação envolveu a avaliação dos algoritmos de seleção utilizados nas diferentes implementações da arquitetura S-MOJOHON . Analisando os resultados obtidos , criou-se um novo algoritmo de seleção de componentes que explora as vantagens dos algoritmos avaliados anteriormente . Este texto apresenta a arquitetura S-MOJOHON , a avaliação dos algoritmos de seleção , o novo algoritmo proposto e sua 
 No contexto do método MAC e baseado em esquemas de diferenças_finitas  , este trabalho apresenta três estudos : i ) uma análise de estabilidade , ii ) o desenvolvimento de técnicas implícitas e , iii ) a construção de métodos de projeção para escoamentos com superfície_livre  . Na análise de estabilidade , o principal resultado mostra que o método de Crank-Nicolson torna-se condicionalmente estável quando aplicado para uma malha_deslocada  com a discretiza ção explícita das condições de contorno do tipo Dirichlet . Entretanto , o mesmo método com condições de contorno implícitas é incondicionalmente estável . Para obter métodos mais estáveis , formulações implícitas são desenvolvidas para a equação da pressão na superfície_livre  , derivada da condição de tensão normal . Esta estratégia resulta no acoplamento dos campos de velocidade e pressão , o que exige a introdução de novos métodos de projeção . Os métodos de projeção assim desenvolvidos resultam em novas metodologias para escoamentos com superfície_livre  que são apropriados para o tratamento de problemas com baixo número de Reynolds . Além_disso  , mostra-se que os métodos propostos podem ser aplicados para fluidos viscoelásticos . Novas estratégias são derivadas para obter métodos de projeção de segunda_ordem  de precisão para escoamentos com superfícies_livres  . Além dos resultados teóricos sobre a estabilidade de esquemas numéricos , técnicas implícitas e métodos de projeção , testes computacionais são realizados e comparados para consolidação da teoria apresentada . Os resultados numéricos são obtidos no sistema FREEFLOW . A eficiência e robustez das técnicas desenvolvidas neste trabalho são demonstradas na solução de problemas tridimensionais complexos com superfície_livre  e baixo número de Reynolds , incluindo os problemas do jato oscilante e do inchamento do 
 A Geração de Língua Natural ( GLN ) ocupa-se de atribuir forma lingüística a dados em representação não-lingüística ( Reiter & Dale , 2000 ) ; a Realização Lingüística ( RL ) , por sua vez , reúne as subtarefas da GLN estritamente dependentes das especificidades da língua-alvo . Este trabalho objetiva a investigação em RL , uma de cujas aplicações mais proeminentes é a construção de módulos geradores de língua-alvo na tradução automática baseada em transferência semântica . Partimos da identificação de três requisitos fundamentais para modelos de RL quais sejam generalidade , instanciabilidade e complexidade e da tensão entre esses requisitos no estado da arte . Argumentamos pela relevância da avaliação formal dos modelos da literatura contra esses critérios e focalizamos em modelos baseados em restrições ( Schulte , 2002 ) como promissores para reconciliar os três requisitos . Nesta classe de modelos , identificamos o recente modelo de Debusmann ( 2006 ) Extensible Dependency Grammar ( XDG ) e sua implementação - o XDG Development Toolkit ( XDK ) - como uma plataforma especialmente promissora para o desenvolvimento em RL , apesar de jamais utilizada para tal . Nossas contribuições práticas se resumem ao esforço de tornar o XDK mais eficiente e uma formulação da disjunção inerente à lexicalização adequada à XDG , demonstrando suas potenciais vantagens numa sistema de GLN mais 
 Nessa dissertação , é investigado o armazenamento e a 
 de padrões de forma biologicamente inspirada no cérebro . 
 modelos estudados consistiram de redes neurais realimentadas , 
 tentam modelar certos aspectos dinâmicos do funcionamento 
 cérebro . Em particular , atenção especial foi dada às Redes_Neurais 
 Celulares , que constituem uma versão localmente acoplada do 
 clássico modelo de Hopfield . Além da análise de estabilidade 
 redes consideradas , foi realizado um teste com o intuito 
 avaliar o desempenho de diversos métodos de construção de 
 endereçáveis por conteúdo ( memórias associativas ) em Redes_Neurais 
 Celulares 
 A produção de mosaicos fotográficos é uma atividade de apoio bastante importante em diversas_áreas  tais como a geração de mapas , o monitoramento ambiental e o gerenciamento agrícola . A fotogrametria , e em especial a aerofotogrametria , é a ciência que trata , entre outros tópicos , da geração de mosaicos através de procedimentos trabalhosos , o que torna sua manutenção uma tarefa_difícil  e de alto custo 
 O Projeto ARARA ( Aeronaves de Reconhecimento Assistidas por Rádio e Autônomas ) representa uma alternativa de baixo custo para a aquisição de fotografias aéreas . Câmeras digitais de pequeno formato a bordo das aeronaves permitem a obtenção automática das fotografias necessárias para a geração de mosaicos 
 Este trabalho propõe uma metodologia para a geração e a atualização de mosaicos compostos por fotografias aéreas oblíquas digitais e de pequeno formato , adquiridas com o auxílio das aeronaves do Projeto ARARA . As fotografias podem ser submetidas a procedimentos que associam técnicas de ortoretificação e processamento digital de imagens para corrigir suas distorções geométricas e radiométricas . A metodologia apresentada neste trabalho procura evitar a necessidade de pontos de controle no solo e focaliza a geração_automática  ou semi-automática dos mosaicos . Procedimentos automáticos têm o potencial de permitir a utilização de uma grande quantidade de fotografias de pequeno formato em substituição às fotografias normalmente utilizadas pela aerofotogrametria convencional 
 A recuperação de imagens baseada em conteúdo , amplamente conhecida como CBIR ( do inglês Content-Based Image Retrieval ) , é um ramo da área da computação que vem crescendo muito nos últimos_anos  e vem contribuindo com novos desafios . Sistemas que utilizam tais técnicas propiciam o armazenamento e manipulação de grandes volumes de dados e imagens e processam operações de consultas de imagens a partir de características visuais extraídas automaticamente por meio de métodos computacionais . Esses sistemas devem prover uma interface de usuário visando uma interação fácil , natural e atraente entre o usuário e o sistema , permitindo que o usuário possa realizar suas tarefas com segurança , de modo eficiente , eficaz e com satisfação . Desse modo , o design da interface firma-se como um elemento fundamental para o sucesso de sistemas CBIR . Contudo , dentro desse contexto , a interface do usuário ainda é um elemento constituído de pouca pesquisa e desenvolvimento . Um dos obstáculos para eficácia de design desses sistemas consiste da necessidade em prover aos usuários uma interface de alta qualidade para permitir que o usuário possa consultar imagens similares a uma dada imagem de referência e visualizar os resultados . Para atingir esse objetivo , este trabalho visa analisar a interação do usuário em sistemas de recuperação de imagens por conteúdo e avaliar sua funcionalidade e usabilidade , aplicando técnicas de interação humano-computador que apresentam bons_resultados  em relação à performance de sistemas com grande complexidade , baseado em um estudo de caso aplicado à 
 Existem várias linguagens e plataformas que permitem a programação baseada no modelo de atores , uma solução elegante para a programação concorrente proposta há algumas décadas . Segundo esse modelo , implementa-se o programa na forma de uma série de agentes que são executados em paralelo e se comunicam entre si somente por meio da troca de mensagens , sem a necessidade de memória_compartilhada  ou estruturas tradicionais de sincronização como semáforos e mutexes . Uma das áreas nas quais esse modelo seria particularmente adequado é a programação de aplicações web , isto é , aplicações cujas lógicas de negócios e de dados residem num servidor e que são acessadas pelo usuário por intermédio de um navegador . Porém , existem muitos obstáculos ao desenvolvimento de aplicações desse tipo , entre eles a falta de linguagens e ferramentas que permitam integrar tanto o servidor quanto o cliente ( navegador ) no modelo de atores , as dificuldades de conversões de dados que se fazem necessárias quando o servidor e o cliente são desenvolvidos em linguagens diferentes , e a necessidade de contornar as dificuldades inerentes aos detalhes do protocolo de comunicação ( HTTP ) . O PAWEB é uma proposta de uma plataforma para o desenvolvimento e execução de aplicações web que fornece a infraestrutura necessária para que tanto o lado cliente quanto o lado servidor do aplicativo hospedado possam ser escritos numa mesma linguagem ( Python ) , e possam criar e gerenciar atores que trocam mensagens entre si , tanto local quanto remotamente , de maneira transparente e sem a necessidade de implementar conversões de dados ou outros detalhes de baixo_nível  
 Em Aprendizagem Colaborativa com Suporte Computacional ( ACSC ) , o planejamento instrucional consiste em obter uma sequência de interações instrucionais que definem o conteúdo instrucional como a representação do que deve ser ensinado e da forma em que os participantes devem interagir , denominada informação de planejamento instrucional . O desenvolvimento , adaptação e personalização de unidades concisas de estudo compostas por recursos instrucionais e informação de planejamento instrucional , denominadas unidades de aprendizagem , envolve um processo de planejamento instrucional complexo que consome muito tempo e apresenta um conjunto de tarefas repetitivas a serem efetuadas pelos projetistas instrucionais . Neste trabalho , o planejamento instrucional em ACSC é modelado como um problema de planejamento hierárquico para dar suporte ao desenvolvimento , adaptação e personalização das unidades de aprendizagem de forma automática . A modelagem consiste na representação do domínio a ser ensinado , das caraterísticas dos estudantes e das estratégias de planejamento instrucional na linguagem do sistema JSHOP2ip , um sistema de planejamento hierárquico desenvolvido para dar solução aos problemas específicos de planejamento instrucional . Para avaliar a modelagem proposta , efetuamos o desenvolvimento de um gerador de cursos colaborativos como um serviço Web usando a modelagem proposta e o sistema JSHOP2ip , no qual foram avaliados o desempenho , a modelagem das estratégias e a saída do planejador . Além_disso  , para demonstrar a viabilidade do modelo proposto em situações reais , efetuamos o desenvolvimento de uma ferramenta de autoria de unidades de aprendizagem que emprega o gerador de cursos colaborativos 
 Este trabalho tem como objetivo propor e desenvolver uma plataforma para ensino e treinamento em técnicas de projeto e implementação de sistemas operacionais . Após mais de uma década de hegemonia de alguns poucos produtos comerciais , o estabelecimento do paradigma do software_livre  e a proliferação de arquiteturas embarcadas capazes de executar um sistema_operacional  ( SO ) implicam em demanda de especialistas para atuarem diretamente no desenvolvimento de novos SOs , adequados a novos requisitos das aplicações emergentes . Assim , para além de disciplina de formação teórica , o conhecimento em sistemas operacionais tem reforçado seu caráter prático como competência técnica perspectiva que este trabalho atende mediante uma abordagem de aprendizado baseado em projetos . A principal_contribuição  para o estado da arte nesse domínio é um roteiro de instrução que associa teoria e prática por meio do processo de desenvolvimento integral de um sistema_operacional  
 A navegação_autônoma  é uma tarefa fundamental na robótica_móvel  . Para que esta tarefa seja realizada corretamente é necessário um sistema inteligente de controle e navegação associado ao sistema sensorial . Este projeto apresenta o desenvolvimento de um sistema de controle para a navegação de veículos e robôs_móveis  autônomos . A abordagem utilizada neste trabalho utiliza Redes_Neurais  Artificiais para o aprendizado de Autômatos Finitos de forma que os robôs possam lidar com os dados provenientes de seus sensores mesmo estando sujeitos a imprecisões e erros e ao mesmo tempo permite que sejam consideradas as diferentes situações e estados em que estes robôs se encontram ( contexto ) . Dessa forma , é possível decidir como agir para realizar o controle da sua movimentação , e assim executar tarefas de controle e navegação das mais simples até as mais complexas e de alto nível . Portanto , esta dissertação visa utilizar Redes_Neurais  Artificiais para reconhecer o estado atual ( contexto ) do robô em relação ao ambiente em que está inserido . Uma vez que seja identificado seu estado , o que pode inclusive incluir a identificação de sua posição em relação aos elementos presentes no ambiente , o robô será capaz de decidir qual a ação/comportamento que deverá ser executado . O sistema de controle e navegação irá implementar um Autômato Finito que a partir de um estado atual define uma ação corrente , sendo capaz de identificar a mudança de estados , e assim alternar entre diferentes comportamentos previamente definidos . De modo a validar esta proposta , diversos experimentos foram realizados através do uso de um simulador robótico ( Player-Stage ) , e através de testes realizados com robôs reais ( Pioneer P3-AT , SRV-1 e veículos automatizados 
 Robôs sociáveis devem ser capazes de interagir , se comunicar , compreender e se relacionar com os seres_humanos  de uma maneira natural . Embora diversos robôs sociáveis tenham sido desenvolvidos com sucesso , ainda existem muitas limitações a serem superadas . São necessários importantes avanços no desenvolvimento de mecanismos que possibilitem interações mais realísticas , bem como regulem o relacionamento entre robôs e humanos . Um forma de tornar mais realísticas as interações é através de expressões faciais de emoção . Nesse contexto , este trabalho fornece capacidade de imitação de expressão facial de emoções a uma cabeça robótica virtual , com o objetivo de permitir interações mais realísticas e duradouras com o ser humano . Para isso , é incorporado à mesma aprendizado por imitação , no qual a cabeça robótica imita expressões faciais apresentadas por um usuário durante a interação social . O aprendizado por imitação foi realizado atráves de redes neurais_artificiais  . As expressões faciais consideradas neste trabalho são : neutra , alegria , raiva , surpresa e tristeza . Os resultados experimentais são apresentados , os quais mostram o bom_desempenho  do sistema de imitação 
 Objetos de aprendizagem ( OA ) compõem uma das abordagens adotadas para tratar a crescente complexidade da educação com auxílio de computadores , promovendo o reúso e a qualidade de materiais didáticos . Tais benefícios podem ser ampliados ao incluir questões de projeto instrucional e associá-las às características multimídias e interativas dos dispositivos computacionais , sistematizando-se a produção de objetos de aprendizagem . Nesse contexto , esta tese define uma abordagem para o desenvolvimento de objetos de aprendizagem multimídias e interativos considerando o cenário de televisão digital . Essa abordagem , denominada LOD ( Learning Object Development ) consiste de um modelo de processo ou processo-padrão para desenvolvimento de objetos de aprendizagem ( SPLOD Standard Process for Learning Object Development ) , a partir do qual se define um processo ( LODP Learning Object Development Process ) adequado para a construção de objetos de aprendizagem como recursos educacionais abertos ( análogos a software_livre  ) ; de um método de desenvolvimento dirigido a modelos , denominado LODM ( Learning Object Development Method ) , compreendendo a modelagem conceitual , instrucional e de interação do objeto de aprendizagem ; e de um conjunto de ferramentas que estabelecem um protótipo de ambiente para o desenvolvimento de objetos de aprendizagem ( LODE Learning Object Development Environment ) considerando o método LODM e o processo LODP . A abordagem LOD foi preliminarmente avaliada quanto à engenharia de objetos de aprendizagem multimídia e interativos , representados por apresentações multimídia e aplicações interativas para televisão digital para a plataforma Ginga do Sistema Brasileiro de Televisão Digital ( SBTVD ) . Os resultados apontam para os benefícios da abordagem integrada para o desenvolvimento , com a geração de objetos de aprendizagem multimídias e interativos de forma mais ágil e sistemática , além de promover o reúso desde os primeiros passos da modelagem dos objetos de 
 Conjuntos de dados reais muitas_vezes  apresentam um grande número de atributos preditivos ou de entrada , o que leva a uma grande quantidade de informação . Entretanto , essa quantidade de informação nem sempre significa uma melhoria em termos de desempenho de técnicas de agrupamento . Além_disso  , alguns atributos podem_estar  correlacionados ou adicionar ruído , reduzindo a qualidade do agrupamento de dados . Esse problema motivou o desenvolvimento de técnicas de seleção de atributos , que tentam encontrar um subconjunto com os atributos mais relevantes para agrupar os dados . Neste trabalho , o foco está no problema de seleção de atributos não supervisionados . Esse é um problema difícil , pois não existe informação sobre rótulos das classes . Portanto , não existe um guia para medir a qualidade do subconjunto de atributos . O principal objetivo deste trabalho é definir um método para identificar quanto atributos devem ser selecionados ( após ordená-los com base em algum critério ) . Essa tarefa é realizada por meio da técnica de Falsos Vizinhos Mais Próximos , que tem sua origem na teoria do caos . Resultados experimentais_mostram  que essa técnica informa um bom número aproximado de atributos a serem selecionados . Quando comparado a outras técnicas , na maioria dos casos analisados , enquanto menos atributos são selecionados , a qualidade da partição dos dados é 
 O aumento na quantidade e na intensidade de desastres naturais é um problema que está se agravando em todo o mundo . As consequências desses desastres são significantemente ampliadas quando ocorrem em regiões urbanas ou com atuação humana devido à perda de vidas e à quantidade de bens materiais afetados . O uso de redes de sensores sem fio para a coleta de dados e o uso de técnicas de aprendizado de máquina para a previsão de desastres naturais são opções viáveis , porém novas tendências tecnológicas têm se mostrado promissoras e podem agregar na tarefa de monitoramento de ambientes e na previsão de desastres naturais . Uma dessas tendências é adotar redes de sensores baseadas em IP e utilizar padrões emergentes para IoT . Nesse contexto , esta Tese propõe e analisa uma abordagem chamada SENDI ( System for dEtecting and forecasting Natural Disasters based on IoT ) , um sistema tolerante a falhas baseado em IoT , WSN e AM para a detecção e a previsão de desastres naturais . O SENDI foi modelado empregando o ns-3 e validado utilizando dados coletados por uma WSN real instalada na cidade de São Carlos - Brasil , a qual realiza a coleta de dados de rios da região . Esse sistema também prevê a possibilidade de falhas na comunicação e a perda de nós durante a ocorrência de desastres , além de agregar inteligência aos nós para realizar a distribuição de dados e de previsões , mesmo nesses casos . Esta Tese também apresenta um estudo de caso sobre previsão de enchentes que utiliza a modelagem do sistema e os dados colhidos pela WSN . Os resultados dos experimentos mostram que o SENDI permite gerar alertas para a tomada de decisões em tempo hábil , realizando as previsões mesmo com falhas parciais no sistema , porém com acurácia variável dependendo do nível de degradação do mesmo 
 A demanda por novas tecnologias , melhoria de segurança e conforto para veículos urbanos cresceu consideravelmente nos últimos_anos  , motivando a indústria na criação de sistemas destinados ao apoio de motoristas ( ADAS - Advanced Driver Assistance Systems ) . Este fato contribuiu para o desenvolvimento de diversos sistemas embarcados na área automobilística destacando-se , à prevenção de colisão a pedestres por veículos . Através do avanço em diversas pesquisas , começaram a circular pelas ruas veículos com sistemas anticolisão e com navegação_autônoma  . Contudo , para alcançar objetivos cada vez mais desafiadores , os projetistas precisam de ferramentas que permitam unir tecnologias e conhecimentos de áreas distintas de forma eficiente . Nesse contexto , há uma demanda para a construção de sistemas que aumentem o nível de abstração da modelagem de projetos para o processamento de imagens em sistemas embarcados e assim , possibilitando uma melhor exploração do espaço de projetos . A fim de contribuir para minimizar este problema , este trabalho de pesquisa demonstra o desenvolvimento de um framework para coprojeto de hardware e software específico para a construção de sistemas ADAS que utilizam visão_computacional  . O Framework visa facilitar o desenvolvimento dessas aplicações permitindo a exploração o espaço de projeto ( DSE - Design Space Exploration ) , e assim contribuindo para um ganho de desempenho no desenvolvimento de sistemas embarcados quando comparados à construção totalmente de um modo manual . Uma das características deste projeto é a possibilidade da simulação da aplicação antes da síntese em um sistema reconfigurável . Os principais desafios deste sistema foram relacionados à construção do sistema de intercomunicação entre os diversos blocos de Propriedade Intelectual ( IP ) e os componentes de software , abstraindo do usuário final inúmeros detalhes de hardware , tais como gerenciamento de memória , interrupções , cache , tipos de dados ( ponto flutuante , ponto fixo , inteiros ) e etc , possibilitando um sistema mais amigável ao projetista 
 A Web 2.0 alterou o desenvolvimento de aplicações para internet . Contudo , os pesquisadores e desenvolvedores ainda replicam as ideias uns dos outros com pouco reuso . Esse cenário ilustra a necessidade de uma engenharia de domínio , na qual as similaridades e as variabilidades de uma família de aplicações são identificadas e documentadas , com a finalidade de obter o reuso dos componentes desenvolvidos . Neste trabalho , e feita uma engenharia de domínio para Redes Sociais na Web 2.0 , com o foco nas funcionalidades colaborativas relativas ao compartilhamento de conteúdo . Como método , e utilizado o FODA ( Feature Oriented Domain Analysis ) adaptado com o modelo 3C de colaboração para classificar e padrões para interação mediada por computador para descrever as funcionalidades colaborativas . No modelo 3C , a colaboração e analisada a partir da comunicação , coordenação e cooperacao , e padroes descrevem e detalham o contexto de uso das funcionalidades levantadas . Para a implementação das funcionalidades colaborativas comuns nessas aplicações , são desenvolvidos componentes de software compatíveis com a plataforma Groupware Workbench . Um experimento foi realizado para avaliar os artefatos gerados na engenharia de domínio e um estudo de caso para avaliar a aplicabilidade e abrangência dos componentes desenvolvidos em um contexto real , a rede social para compartilhamento de imagens de arquitetura , chamada Arquigrafia Brasil . Os experimentos e o estudo de caso indicaram que os artefatos gerados são reusáveis , uteis e abrangem boa parte das funcionalidades presentes nas redes_sociais  atuais 
 Do ponto de vista do usuário , a interface é uma das partes mais importantes dos sistemas computacionais , porque por meio dela o usuário vê , ouve e sente . Essa relevância motiva pesquisadores da área de Interação Humano-Computador a estudarem maneiras de se criarem interfaces com design focado em usabilidade . A avaliação da usabilidade de interfaces visa verificar se elas atendem aos requisitos do usuário de forma que as funcionalidades do sistema sejam realizadas de modo efetivo , eficiente e que satisfaça as expectativas do usuário . Tendo em vista que o ciclo de desenvolvimento de software costuma ser longo , avaliações da usabilidade de diferentes versões de interfaces devem ser realizadas no decorrer do processo , como forma de minimizar erros e reduzir custos de produção do sistema . Uma das avaliações de usabilidade mais conhecidas é a avaliação heurística , criada por Jacob Nielsen , que se destaca pelo baixo custo e rapidez de aplicação . Nela , especialistas avaliam as interfaces e os diálogos do sistema com base em um conjunto de regras gerais , as heurísticas , que lhes permitem identificar problemas de usabilidade . Apesar de respeitadas e amplamente usadas , as heurísticas de Nielsen foram criadas sem foco em interfaces de dispositivos_móveis  , muito difundidos atualmente . Por meio deste trabalho , verificou-se que as heurísticas de Nielsen têm limitações para encontrarem problemas de usabilidade em interfaces de dispositivos_móveis  . Por conta disso , propôs-se um conjunto de heurísticas para avaliação de interfaces de dispositivos_móveis  e se definiu um conjunto de diretrizes para o desenvolvimento dessas interfaces . A validação das heurísticas propostas indicou que elas foram mais efetivas que as de Nielsen para encontrarem problemas de usabilidade considerados pelos especialistas como catastróficos ou de baixa 
 A Desambiguação Lexical de Sentido ( DLS ) consiste em determinar o sentido mais apropriado da palavra em um contexto determinado , utilizando-se um repositório de sentidos pré-especificado . Esta tarefa é importante para outras aplicações , por exemplo , a tradução automática . Para o inglês , a DLS tem sido amplamente explorada , utilizando diferentes abordagens e técnicas , contudo , esta tarefa ainda é um desafio para os pesquisadores em semântica . Analisando os resultados dos métodos por classes gramaticais , nota-se que todas as classes não apresentam os mesmos resultados , sendo que os verbos são os que apresentam os piores resultados . Estudos ressaltam que os métodos de DLS usam informações superficiais e os verbos precisam de informação mais profunda para sua desambiguação , como frames sintáticos ou restrições seletivas . Para o português , existem poucos trabalhos nesta área e só recentemente tem-se investigado métodos de uso geral . Além_disso  , salienta-se que , nos últimos_anos  , têm sido desenvolvidos recursos lexicais focados nos verbos . Nesse contexto , neste trabalho de mestrado , visou-se investigar métodos de DLS de verbos em textos escritos em português do Brasil . Em particular , foram explorados alguns métodos tradicionais da área e , posteriormente , foi incorporado conhecimento linguístico proveniente da Verbnet.Br . Para subsidiar esta investigação , o córpus CSTNews foi anotado com sentidos de verbos usando a WordNet-Pr como repositório de sentidos . Os resultados obtidos mostraram que os métodos de DLS investigados não conseguiram superar o baseline mais forte e que a incorporação de conhecimento da VerbNet.Br produziu melhorias nos métodos , porém , estas melhorias não foram estatisticamente significantes . Algumas contribuições deste trabalho de mestrado foram um córpus anotado com sentidos de verbos , a criação de uma ferramenta que auxilie a anotação de sentidos , a investigação de métodos de DLS e o uso de informações especificas de verbos ( provenientes da VerbNet.Br ) na DLS de verbos 
 A sumarização de opiniões , também conhecida como sumarização de sentimentos , é a tarefa que consiste em gerar automaticamente sumários para um conjunto de opiniões sobre uma entidade específica . Uma das principais abordagens para gerar sumários de opiniões é a sumarização baseada em aspectos . A sumarização baseada em aspectos produz sumários das opiniões para os principais aspectos de uma entidade . As entidades normalmente referem-se a produtos , serviços , organizações , entre outros , e os aspectos são atributos ou componentes das entidades . Nos últimos_anos  , essa tarefa tem ganhado muita relevância diante da grande quantidade de informação online disponível na web e do interesse cada vez maior em conhecer a avaliação dos usuários sobre produtos , empresas , pessoas e outros . Infelizmente , para o Português do Brasil , pouco se tem pesquisado nessa área . Nesse cenário , neste projeto de mestrado , investigou-se o desenvolvimento de alguns métodos de sumarização de opiniões com base em aspectos . Em particular , foram implementados quatro métodos clássicos da literatura , extrativos e abstrativos . Esses métodos foram analisados em cada uma de suas fases e , como consequência dessa análise , produziram-se duas propostas para gerar sumários de opiniões . Essas duas propostas tentam utilizar as principais vantagens dos métodos clássicos para gerar melhores sumários . A fim de analisar o desempenho dos métodos implementados , foram realizados_experimentos  em função de três medidas de avaliação tradicionais da área : informatividade , qualidade linguística e utilidade do sumário . Os resultados obtidos_mostram  que os métodos propostos neste trabalho são competitivos com os métodos da literatura e , em vários casos , os superam 
 A técnica de Análise de Componentes Principais ( PCA ) tem como objetivo principal a descrição da variância e covariância entre um conjunto de variáveis . Essa técnica é utilizada para mitigar redundâncias no conjunto de variáveis e para redução de dimensionalidade em várias aplicações nas áreas científica , tecnológica e administrativa . Por outro_lado  , o modelo de dados multidimensionais é composto por relações de fato e dimensões ( tabelas ) que descrevem um evento usando métricas e a relação entre suas dimensões . No entanto , o volume de dados armazenados e a complexidade de suas dimensões geralmente envolvidas neste modelo , especialmente no ambiente de data warehouse , tornam a tarefa de interpretar a correlação entre dimensões muito difícil e às vezes impraticável . Neste trabalho , propomos o desenvolvimento de uma Interface de Programação de Aplicação ( API ) para a aplicação da PCA no modelo de dados multidimensionais para facilitar a tarefa de caracterização e redução de dimensionalidade , integrando essa técnica com ambientes de Data Warehouses . Para verificar a eficácia desta API , um estudo de caso foi realizado utilizando dados de produção científica e suas citações obtidas das Plataformas Lattes , Web of Science , Google Scholar e Scopus , fornecidas pela Superintendência de Tecnologia da Informação da Universidade de São Paulo 
 O QC-MDPC McEliece foi considerado um dos mais promissores esquemas criptográficos de chave_pública  que oferecem segurança contra ataques por computadores quânticos . O tamanho das chaves públicas do QC-MDPC McEliece é competitivo com o das chaves do RSA , e o esquema tem uma redução de segurança aparentemente forte . Por três anos , o esquema não sofreu ataques críticos , até que na Asiacrypt de 2016 Guo , Johansson , e Stankovski mostraram um ataque de reação contra o QC-MDPC McEliece que explora um aspecto não considerado em sua redução de segurança : a probabilidade de o algoritmo de decriptação falhar é menor quando a chave secreta e o vetor usado para encriptar a mensagem compartilham certas propriedades , chamadas de espectros . Dessa forma , um atacante pode , ao detectar falhas de decriptação , obter informação sobre o espectro , que será usada para reconstruir a chave secreta . Guo et_al  . apresentaram um algoritmo para a reconstrução da chave a partir do espectro recuperado , para o qual é possível apontar três problemas . O primeiro é que seu algoritmo não é eficiente quando o espectro da chave não foi recuperado quase completamente , o que resulta em o atacante ter que enviar um grande número de testes de decriptação à portadora da chave secreta . O segundo problema é que o desempenho de seu algoritmo não escala bem para níveis de segurança mais altos . O terceiro e último problema é que , por ser baseado numa busca em profundidade , seu algoritmo não pode ser paralelizado trivialmente . Para aumentar a eficiência do ataque , dois novos algoritmos de reconstrução são propostos neste trabalho . Estes algoritmos são mais eficientes , usam menos informação sobre a chave secreta , e podem ser paralelizados trivialmente . O primeiro algoritmo é probabilístico e tem complexidade assintótica ligeiramente melhor do que a do original . Entretanto , o desempenho do algoritmo probabilístico piora rapidamente , embora mais lentamente do que o algoritmo de Guo et al. , conforme a quantidade de informação sobre o espectro diminui . O segundo algoritmo explora uma relação linear entre os blocos da chave secreta . Este é mais eficiente , tanto assintoticamente quanto na prática , que os dois outros algoritmos , e é eficiente mesmo com 50 % menos informação sobre o espectro do que o necessário para o algoritmo original . Isso permite que o atacante encontre a chave secreta fazendo apenas em torno de 20 % do número de testes necessários pelo algoritmo de Guo 's et al. , considerando-se o nível de segurança de 80 bits . O desempenho de ambos os algoritmos são analisados e comparados com o do algoritmo original , e as análises são feitas tanto para a complexidade teórica quanto para o desempenho na prática , considerando a implementação dos algoritmos em linguagem C 
 O presente_trabalho  é vinculado a duas áreas de grande pesquisa e enfoque na comunidade cientifica , a área de navegação de robôs_móveis  e a área de computação reconfigurável . Este trabalho tem como principal finalidade implementar uma técnica de mapeamento para o sistema de navegação de um robô_móvel  , em hardware_reconfigurável  , objetivando a melhora do desempenho na execução da técnica chamada mapeamento topológico , além de fornecer a capacidade de um sistema robótico poder-se auto reconfigurar em tempo real 
 Para que seja realizada esta tarefa , foram necessários pesquisas e estudos a estes dois assuntos , podendo ser encontrada uma explanação dos mesmos nos capítulos 3 e 4 
 O primeiro tema abordado foi o sistema de navegação de robôs_móveis  com análise inicial sobre as formas de navegação e mapeamento associadas com o estudo dos ambientes que serão realizadas as tarefas de navegação 
 O segundo tema abordado é sobre sistemas reconfiguráveis que tem como ênfase à construção , implementação , reconfiguração assim como os principais fabricantes 
 Depois de realizado todo o estudo inerente à pesquisa , anteriormente citado , é implementado um sistema de navegação de robôs_móveis  em um hardware_reconfigurável  utilizando o conjunto de ferramentas de desenvolvimento de hardwares reconfiguráveis da empresa chamada Altera 
 A identificação taxonômica de algas verdes de água doce é um problema de extrema relevância na Ficologia . Identificar espécies de algas da família Selenastraceae é uma tarefa complexa devido às inconsistências existentes em sua taxonomia , reconhecida como problemática . Os biólogos analisam manualmente imagens de microscópio de cepas de algas e realizam diversos procedimentos demorados que necessitamde conhecimento sólido . Tais limitaçõesmotivaramo estudo da aplicabilidade de técnicas de processamento de imagens , reconhecimento de padrões e mineração visual de dados para apoiar os biólogos em tarefas de identificação de espécies de algas . Esta tese descreve metodologias computacionais para a classificação de imagens de algas verdes , nas abordagens tradicional e baseada em classificação visual incremental com participação do usuário . Nesta última , os usuários interagem com visualizações baseadas em árvores filogenéticas para utilizar seu conhecimento no processo de classificação , como por exemplo , na seleção de instâncias relevantes para o conjunto de treinamento de um classificador , como também na avaliação dos resultados . De forma a viabilizar o uso de classificadores e técnicas de visualização , vetores de características devem ser obtidos das imagens de algas verdes . Neste trabalho , utiliza-se extração de características de forma , uma vez que a taxonomia da família Selenastraceae considera primordialmente as características morfológicas na identificação das espécies . No entanto , a obtenção de características representativas requer que as algas sejam precisamente segmentadas das imagens . Esta é , de fato , uma tarefa altamente desafiadora considerando a baixa qualidade das imagens e a maneira pelas quais as algas se organizam nas imagens . Duas metodologias de segmentação foram introduzidas : uma baseada no método Level Set e outra baseada no algoritmo de crescimento de regiões . A primeira se mostrou robusta e consegue identificar com alta precisão as algas nas imagens , mas seu tempo de execução é alto . A outra apresenta maior precisão e é mais rápida , uma vez que as técnicas de pré-processamento são especializadas para as imagens de algas verdes . Uma vez segmentadas as algas , dois descritores para caracterizar as imagens foram propostos : um baseado em características geométricas básicas e outro que utiliza medidas quantitativas calculadas a partir das assinaturas de forma . Resultados experimentais indicaram que as soluções propostas têm um bom potencial para serem utilizadas em tarefas de identificação taxonômica de algas verdes , uma vez que reduz o esforço nos procedimentos manuais e obtém-se classificações satisfatórias 
 A rápida evolução dos recursos_computacionais  vem permitindo que grandes_conjuntos  de dados sejam armazenados e recuperados . No entanto , a exploração , compreensão e extração de informação útil ainda são um desafio . Com relação às ferramentas computacionais que visam tratar desse problema , a Visualização de Informação possibilita a análise de conjuntos de dados por meio de representações gráficas e a Mineração de Dados fornece processos automáticos para a descoberta e interpretação de padrões . Apesar da recente popularidade dos métodos de visualização de informação , um problema recorrente é a baixa escalabilidade visual quando se está analisando grandes_conjuntos  de dados , resultando em perda de contexto e desordem visual . Com intuito de representar grandes_conjuntos  de dados reduzindo a perda de informação relevante , o processo de agregação visual de dados vem_sendo  empregado . A agregação diminui a quantidade de dados a serem representados , preservando a distribuição e as tendências do conjunto de dados original . Quanto à mineração de dados , visualização de informação vêm se tornando ferramental essencial na interpretação dos modelos computacionais e resultados gerados , em especial das técnicas não-supervisionados , como as de agrupamento . Isso porque nessas técnicas , a única forma do usuário interagir com o processo de mineração é por meio de parametrização , limitando a inserção de conhecimento de domínio no processo de análise de dados . Nesta dissertação , propomos e desenvolvemos uma metáfora visual baseada na TableLens que emprega abordagens baseadas no conceito de agregação para criar representações mais escaláveis para a interpretação de dados tabulares . Como aplicação , empregamos a metáfora desenvolvida na análise de resultados de técnicas de agrupamento . O ferramental resultante não somente suporta análise de grandes bases de dados com reduzida perda de contexto , mas também fornece subsídios para entender como os atributos dos dados contribuem para a formação de agrupamentos em termos da coesão e separação dos grupos formados 
 A detecção e reconhecimento de objetos é uma tarefa fundamental em aplicações relacionadas à navegação_autônoma  de robôs_móveis  e veículos inteligentes . Com a evolução tecnológica nos sistemas sensoriais , surgiram equipamentos capazes de detectar e representar os elementos presentes no ambiente de forma tridimensional , em estruturas chamadas nuvem de pontos . Os sensores 3D geralmente capturam um grande volume de pontos em curtos intervalos de tempo , o que demanda técnicas robustas para processamento dessa informação além de tolerância a eventuais ruídos nos dados . Uma abordagem frequentemente utilizada na área de Visão Computacional para redução de dimensionalidade é a extração de features robustas , armazenando um subconjunto de informações representativas e simplificadas do conjunto de dados . Esta tese apresenta uma metodologia de classificação de objetos em nuvens de pontos 3D através da extração de features 3D globais . Foi desenvolvido um novo descritor 3D invariante à escala , translação e rotação denominado 3D-CSD ( 3D-Contour Sample Distances ) para representação da superfície dos objetos presentes no ambiente , e utilizado um método de aprendizado_supervisionado  para reconhecimento de padrões . Os experimentos_realizados  envolveram o uso de Redes_Neurais  Artificiais para o reconhecimento de diferentes classes de objetos , avaliando e validando a metodologia_proposta  . Os resultados obtidos demostraram a viabilidade da aplicação desta abordagem para o reconhecimento de objetos em sistemas de percepção 3D 
 Os volumes de dados armazenados em grandes bases de dados aumentam em ritmo sempre crescente , pressionando o desempenho e a flexibilidade dos Sistemas de Gerenciamento de Bases de Dados ( SGBDs ) . Os problemas de se tratar dados em grandes quantidades , escopo , complexidade e distribuição vêm_sendo  tratados também sob o tema de big data . O aumento da complexidade cria a necessidade de novas formas de busca - representar apenas números e pequenas cadeias de caracteres já não é mais suficiente . Buscas por similaridade vêm se mostrando a maneira por excelência de comparar dados complexos , mas até recentemente elas não estavam disponíveis nos SGBDs . Agora , com o início de sua disponibilidade , está se tornando claro que apenas os operadores de busca por similaridade fundamentais não são suficientes para lidar com grandes volumes de dados . Um dos motivos disso é que similaridade ' é , usualmente , definida considerando seu significado quando apenas poucos estão envolvidos . Atualmente , o principal foco da literatura em big data é aumentar a eficiência na recuperação dos dados usando paralelismo , existindo poucos_estudos  sobre a eficácia das respostas obtidas . Esta tese visa propor e desenvolver variações dos operadores de busca por similaridade para torná-los mais adequados para processar big data , apresentando visões mais abrangentes da base de dados , aumentando a eficácia das respostas , porém sem causar impactos consideráveis na eficiência dos algoritmos de busca e viabilizando sua execução escalável sobre grandes volumes de dados . Para alcançar esse objetivo , este trabalho apresenta quatro frentes de contribuições : A primeira consistiu em um modelo de diversificação de resultados que pode ser aplicado usando qualquer critério de comparação e operador de busca por similaridade . A segunda focou em definir técnicas de amostragem e de agrupamento de dados com o modelo de diversificação proposto , acelerando o processo de análise dos conjuntos de resultados . A terceira contribuição desenvolveu métodos de avaliação da qualidade dos conjuntos de resultados diversificados . Por fim , a última frente de contribuição apresentou uma abordagem para integrar os conceitos de mineração visual de dados e buscas por similaridade com diversidade em sistemas de recuperação por conteúdo , aumentando o entendimento de como a propriedade de diversidade pode ser aplicada 
 A tarefa de planejamento de trajetórias de robôs_móveis  autônomos consiste em determinar objetivos intermediários para que um robô seja capaz de partir de sua localização inicial e alcançar seu objetivo final . Além do planejamento , é importante definir um método de controle da navegação ( seguimento da trajetória ) do robô para que ele seja capaz de realizar seu trajeto de forma segura . Este projeto propõe uma abordagem híbrida para planejamento exploratório e execução de trajetórias de robôs_móveis  autônomos em ambientes indoor . Para o planejamento de trajetória , foram investigados algoritmos de busca em espaço de estados , dando ênfase ao uso de algoritmos_evolutivos  e algoritmos de otimização por colônia de formigas para a descoberta e otimização da trajetória . O controle da navegação é realizado por meio de comportamentos locais reativos , baseado na exploração e uso de mapas topológicos , os quais permitem uma maior flexibilidade em termos de definição da localização da posição do robô_móvel  e sobre os detalhes do mapa do ambiente ( mapas com informações aproximadas e não métricos ) . Assim , foi proposto e desenvolvido um método robusto capaz de planejar , mapear e explorar um caminho ótimo ou quase ótimo para que o robô possa navegar e alcançar seu objetivo de forma segura , com pouca informação prévia do ambiente ou mesmo sobre sua localização . Além_disso  , o robô pode reagir a ambientes com alterações dinâmicas em sua estrutura , considerando por exemplo , elementos dinâmicos como portas que possam ser abertas ou fechadas e passagens que são obstruídas . Por fim , foram realizados diversos testes e simulações a fim de validar o método proposto , com a avaliação da qualidade das soluções encontradas e comparação com outras abordagens tradicionais de planejamento de trajetórias ( algoritmos A* e D* ) 
 Os algoritmos de navegação , para robôs_móveis  , baseados em células , ainda são de alto custo_computacional  
 Depois de uma pesquisa dentre os algoritmos disponivéis , realizando comparações de desempenho entre salas e corredores chegou-se a um algoritmo que além de ótimo era altamente usado dentro do universo da robótico 
 Este algoritmo , baseado em Campos Potenciais , usado para desvio de obstáculos e planejamento de caminhos locais , foi implementado em hardware_reconfigurável  usando uma FPGA Altera 
 Comparações entre este hardware_reconfigurável  e processadores de propósito_geral  foram realizadas para concluir o trabalho 
 A utilização de redes de alta velocidade e de ferramentas multimídia tem se tomado comum em grandes corporações e instituições de ensino , disponibilizando um meio rápido e interativo de comunicação . Grandes quantidades de dados de diferentes_tipos  e prioridades trafegam nestas redes , sendo que um controle da Qualidade de Serviço se faz necessário para grande parte deles , como , por exemplo , dados de mídia contínua ( audio e vídeo ) . Este projeto de mestrado tem como foco_principal  um estudo abrangente da tecnologia de rede ATM na transmissão de streams de áudio e vídeo , comprimidos ou não , considerando aspectos como Qualidade de Serviço , controle de admissão de conexões , congestionamento na rede , as diferentes prioridades de transmissão , os tipos de tráfego gerados e respectivas taxas de transmissão , além da problemática envolvida no setup da rede . Neste estudo será utilizada toda a infra-estrutura oferecida pela tecnologia de rede ATM , notadamente no que tange ao suporte oferecido para a manutenção de uma Qualidade de Serviço específica e essencial para aplicações de transporte de mídia contínua . Desta maneira , será possível também averiguar quão adequada é a utilização da tecnologia nas novas aplicações multimídia surgidas recentemente , como TV Interativa , Video on Demand , entre outras 
 Este trabalho apresenta a modelagem de um processo de engenharia_reversa  e a definição de um modelo de qualidade associado , visando à obter indicativos da qualidade do produto atual e dos artefatos produzidos durante este processo . Para a realização da modelagem foi utilizado o método FUSION-RE/I , o qual tem como objetivo auxiliar na atividade de manutenção , recuperando visões funcionais e estruturais de um sistema , e a abordagem SQUID ( método e ferramenta ) , que é uma abordagem de desenvolvimento de software que visa a garantir e controlar a qualidade de um software através de medições de artefatos intermediários produzidos pelo processo de desenvolvimento envolve tanto a visão de qualidade ( modelo de qua um particular projeto . qual tem como objetivo auxiliar na atividade de manutenção , is e estruturais de um sistema , e a abordagem SQUID ( método e ferramenta ) , que é uma abordagem de desenvolvimento de software que visa a garantir e controlar a qualidade de um software através de medições de artefatos intermediários produzidos pelo processo de desenvolvimento ao qual o software está submetido . A abordagem SQUID envolve tanto a visão de desenvolvimento ( modelo de desenvolvimento ) quanto a visão de qualidade ) , que devem estar associadas em função das características de um particular projeto 
 O avanço tecnológico teve como consequência a geração e o armazenamento de quantidades abundantes de dados . Para conseguir extrair o máximo de informação possível dos dados tornou-se necessária a formulação de novas ferramentas de análise de dados . Foi então introduzido o Processo de Descoberta de Conhecimento em Bancos de Dados , que tem como objetivo a identificação de padrôes válidos , novos , potencialmente úteis e compreensíveis em grandes bancos de dados . Nesse processo , a etapa responsável por encontrar padrões nos dados é denominada de Mineração de Dados . A acurácia e eficiência de algoritmos de mineração de dados dependem diretamente da quantidade e da qualidade dos dados que serão analisados . Nesse sentido , atributos redundantes e/ou não-informativos podem tornar o processo de mineração de dados ineficiente . Métodos de Seleção de Atributos podem remover tais atributos . Nesse trabalho é proposto um algoritmo para seleção de atributos e algumas de suas variantes . Tais algoritmos procuram identificar redundância por meio do agrupamento de atributos . A identificação de atributos redundantes pode auxiliar não apenas no processo de identificação de padrões , mas também pode favorecer a compreensibilidade do modelo obtido . O algoritmo proposto e suas variantes são comparados com dois algoritmos do mesmo gênero descritos na literatura . Tais algoritmos foram avaliados em problemas típicos de mineração de dados : classificação e agrupamento de dados . Os resultados das avaliações mostram que o algoritmo proposto , e suas variantes , fornecem bons_resultados  tanto do ponto de vista de acurácia como de eficiência computacional , sem a necessidade de definição de parâmetros críticos pelo 
 Detecção de outliers desempenha um importante papel para descoberta de conhecimento em grandes bases de dados . O estudo é motivado por inúmeras aplicações reais como fraudes de cartões de crédito , detecção de falhas em componentes industriais , intrusão em redes de computadores , aprovação de empréstimos e monitoramento de condições médicas . Um outlier é definido como uma observação que desvia das outras observações em relação a uma medida e exerce considerável influência na análise de dados . Embora existam inúmeras técnicas de aprendizado de máquina para tratar desse problemas , a maioria delas não faz_uso  de conhecimento prévio sobre os dados . Técnicas de aprendizado semissupervisionado para detecção de outliers são relativamente novas e incluem apenas um pequeno número de rótulos da classe normal para construir um classificador . Recentemente um modelo semissupervisionado baseado em rede foi proposto para classificação de dados empregando um mecanismo de competição e cooperação de partículas . As partículas são responsáveis pela propagação dos rótulos para toda a rede . Neste trabalho , o modelo foi adaptado a fim de detectar outliers através da definição de um escore de outlier baseado na frequência de visitas . O número de visitas recebido por um outlier é significativamente diferente dos demais objetos de mesma classe . Essa abordagem leva a uma maneira não tradicional de tratar os outliers . Avaliações empíricas sobre bases artificiais e reais demonstram que a técnica proposta funciona bem para bases desbalanceadas e atinge precisão comparável às obtidas pelas técnicas tradicionais de detecção de outliers . Além_disso  , a técnica pode fornecer novas perspectivas sobre como diferenciar objetos , pois considera não somente a distância física , mas também a formação de padrão dos 
 A estimação da razão entre duas densidades de probabilidade é uma importante ferramenta no aprendizado de máquina supervisionado . Neste trabalho , novos métodos de estimação da razão de densidades são propostos baseados na solução de uma equação integral multidimensional . Os métodos resultantes usam o conceito de matriz-V , o qual não aparece em métodos anteriores de estimação da razão de densidades . Experimentos demonstram o bom potencial da nova abordagem com relação a métodos anteriores . A estimação da Informação Mútua - IM - é um componente importante em seleção de atributos e depende essencialmente da estimação da razão de densidades . Usando o método de estimação da razão de densidades proposto neste trabalho , um novo estimador - VMI - é proposto e comparado experimentalmente a estimadores de IM anteriores . Experimentos conduzidos na estimação de IM mostram que VMI atinge melhor desempenho na estimação do que métodos anteriores . Experimentos que aplicam estimação de IM em seleção de atributos para classificação evidenciam que uma melhor estimação de IM leva as melhorias na seleção de atributos . A tarefa de seleção de parâmetros impacta fortemente o classificador baseado em kernel Support Vector Machines - SVM . Contudo , esse passo é frequentemente deixado de lado em avaliações experimentais , pois costuma consumir tempo computacional e requerer familiaridade com as engrenagens de SVM . Neste trabalho , procedimentos de seleção de parâmetros para SVM são propostos de tal forma a serem econômicos em gasto de tempo computacional . Além_disso  , o uso de um kernel não linear - o chamado kernel min - é proposto de tal forma que possa ser aplicado a casos de baixa e alta dimensionalidade e sem adicionar um outro parâmetro a ser selecionado . A combinação dos procedimentos de seleção de parâmetros propostos com o kernel min produz uma maneira conveniente de se extrair economicamente um classificador SVM com boa performance . O método de regressão Regularized Least Squares - RLS - é um outro método baseado em kernel que depende de uma seleção de parâmetros adequada . Quando dados de treinamento são escassos , uma seleção de parâmetros tradicional em RLS frequentemente leva a uma estimação ruim da função de regressão . Para aliviar esse problema , é explorado neste trabalho um kernel menos suscetível a superajuste - o kernel INK-splines aditivo . Após , são explorados métodos de seleção de parâmetros alternativos à validação cruzada e que obtiveram bom_desempenho  em outros métodos de regressão . Experimentos conduzidos em conjuntos de dados reais mostram que o kernel INK-splines aditivo tem desempenho_superior  ao kernel RBF e ao kernel INK-splines multiplicativo previamente proposto . Os experimentos também mostram que os procedimentos alternativos de seleção de parâmetros considerados não melhoram consistentemente o desempenho . Ainda assim , o método Finite Prediction Error com o kernel INK-splines aditivo possui desempenho comparável à validação cruzada 
 A computação em nuvem tem recebido um grande destaque , ao propor um novo e eficiente mecanismo para disponibilizar recursos_computacionais  . Dos investimentos cada vez maiores nessa plataforma , inclusive pela academia , surge a oportunidade de compartilhar estes recursos_computacionais  entre diferentes instituições . As grades_computacionais  são um mecanismo bem estabelecido para o compartilhamento de alguns tipos de recursos_computacionais  . Através do entendimento de como isso é feito nestas estruturas , esse trabalho avalia as soluções existentes e propõe um arquitetura alternativa que permite a criação das federações de nuvens computacionais 
 Recentemente estudiosos como Benkler , Lessig , Boyle , Hess e Ostrom retomaram o uso do conceito de commons , mas agora relacionado à informação em geral ou à informação científica . Nesse trabalho , nós lançamos mão desse termo para destacar o caráter cooperativo da pesquisa científica , a importância da transparência e neutralidade no acesso ao commons da Ciência e a natureza anti-rival da informação científica . O conceito de commons nos é muito útil para focar todo o conjunto dos artigos científicos já publicados , quer estejam na forma impressa ou na digital . Ainda permite um estudo através de prismas multidisciplinares e , finalmente , enfatiza a dinâmica das comunidades científicos como um todo . Em qualquer commons de informação , quanto maior a distribuição do conhecimento , mais dinâmico e eficiente é o processo de evolução do conhecimento . A tecnologia da imprensa tem desempenhado um papel fundamental na divulgação de informação e o seu surgimento marcou uma revolução no conhecimento e na cultura da nossa civilização . A tecnologia digital tem se mostrado mais eficiente ainda , uma vez que a natureza da sua implementação em bits se aproxima mais da natureza anti-rival das idéias do que qualquer outra tecnologia hoje empregada para preservação e distribuição de informação . Em nosso estudo , constatamos que o commons da Ciência pode ser enormemente enriquecido através de práticas cooperativas e de acesso aberto na publicação da academia . Percebemos também que o uso da tecnologia digital no commons científico , especialmente na publicação dos resultados da pesquisa , aumenta grandemente a distribuição do conhecimento acadêmico , suas oportunidades de escrutínio e validação , a dinâmica de amadurecimento das idéias científicas e , conseqüentemente , pode tornar o desenvolvimento da Ciência mais veloz e eficiente . No entanto , o meio digital tem sido utilizado tanto para criar um ambiente de livre circulação de idéias quanto para controlá-las . Por um lado , código computacional tem sido implementado para garantir o acesso apenas aos que pagam pelos altos preços das revistas científicas . Por outro_lado  , a publicação de revistas on-line de acesso aberto e outras formas alternativas de disseminação de conteúdo científico têm se proliferado . Ainda , o decrescente orçamento das bibliotecas , o crescente preço das assinaturas de revistas científicas e as crescentes restrições aplicadas pelas leis de propriedade intelectual têm minado a natureza livre das idéias científicas e colocado a Comunicação Científica numa crise . Estamos no meio de uma transição de paradigmas quanto à publicação dos resultados de pesquisa científica , onde aspectos legais , tecnológicos e sócio-econômicos estão em renegociação . À luz das oportunidades da tecnologia digital e da publicação em acesso aberto , as formas de disseminação dos resultados da pesquisa científica presentemente estabelecidas tem sido repensadas . Inserimos essa análise num contexto maior , o paradigma da Comunicação Científica . Isso nos auxilia a fazer um estudo mais abrangente das complexas questões envolvendo nosso tema , analisando os aspectos tecnológicos , legais e sócio-econômicos de uma possível transição para o modelo de publicação de acesso aberto . Tão grandes são as oportunidades desse novo modelo que ele tem agregado em torno de si iniciativas sócio-acadêmicas conhecidas por Movimento de Acesso Aberto à literatura científica . Atualmente , há muitos testes e modelos de publicação dessa literatura . Em especial , nesse trabalho focamos o modelo de acesso aberto aos resultados científicos , suas vantagens , as dificuldades para seu estabelecimento e como ele tem se desenvolvido . Analisamos a viabilidade de criação de um ecossistema de bibliotecas digitais de acesso aberto , especializadas em cada ramo da Ciência . Nossos modelos de partida baseiam-se em alguns aspectos de serviços como arXiv , CiteSeer e Google Scholar . Entre as muitas conclusões desse estudo , constatamos que bibliotecas desse tipo aumentam sobremaneira a dinâmica de circulação , geração , transformação e renovação do conhecimento científico . Assim , o processo de produção de recursos no commons científico pode se tornar muito mais eficiente 
 O presente_trabalho  desenvolve uma extensão do problema de corte de estoque unidimensional no caso em que a demanda pelos vários_tipos  de itens não é exatamente conhecida . Para considerar a aleatoriedade , foi proposto um modelo de programação estocástica de dois estágios com recurso . As varáveis de primeiro estágio são os números de barras cortadas por padrão de corte , e as variáveis de segundo estágio , os números de itens produzidos em escassez e em escassez . O objetivo do modelo é minimizar o custo total esperado . Para resolver a relaxação linear do modelo , foram propostos um método exato baseado no método Simplex com geração de colunas e uma estratégia heurística , que considera o valor esperado da demanda na resolução do problema de corte de estoque . As duas estratégias foram comparadas , assim como a possibilidade de resolver o problema de corte ignorando as incertezas . Finalmente , observou-se que é mais interessante determinar o valor ótimo do modelo recurso quando o problema sofre mais influência da 
 Esta tese apresenta um modelo de custo para estimar o número de acessos a disco ( custo de I/O ) e o número de cálculos de distância ( custo de CPU ) para consultas por similaridade executadas sobre métodos de acesso métricos dinâmicos . O objetivo da criação do modelo é a otimização de consultas por similaridade em Sistemas de Gerenciamento de Bases de Dados relacionais e objeto-relacionais . Foram considerados dois tipos de consultas por similaridade : consulta por abrangência e consulta aos k-vizinhos mais próximos . Como base para a criação do modelo de custo foi utilizado o método de acesso métrico dinâmico Slim-Tree . O modelo estima a dimensão intrínseca do conjunto de dados pela sua dimensão de correlação fractal . A validação do modelo é confirmada por experimentos com conjuntos de dados sintéticos e reais , de variados tamanhos e dimensões , que mostram que as estimativas obtidas em geral estão dentro da faixa de variação medida em consultas 
 Sistemas PACS ( Picture Archieving and Communication Systems ) têm sido desenvolvidos para armazenar de maneira integrada tanto os dados textuais e temporais dos pacientes quanto as imagens dos exames médicos a que eles se submetem para ampliar o uso das imagens no auxílio ao diagnóstico . Outra ferramenta valiosa para o auxílio ao diagnóstico médico são os sistemas CAD ( Computer-Aided Diagnosis ) , para os quais pesquisas recentes mostram que o seu uso melhora significativamente a performance dos radiologistas em detectar corretamente anomalias . Dentro deste contexto , muitos trabalhos têm buscado métodos que possam reduzir o problema do `` gap semântico '' , que refere-se ao que é perdido pela descrição sucinta da imagem e o que o usuário espera recuperar/reconhecer utilizando tal descrição . A grande maioria dos sistemas CBIR ( do inglês Content-based image retrieval ) utiliza características primárias ( baixo_nível  ) para descrever elementos relevantes da imagem e proporcionar recuperação baseada em conteúdo . É necessário `` fundir '' múltiplos vetores com uma caracterí ? stica em um vetor composto de características que possui baixa dimensionalidade e que ainda preserve , dentro do possível , as informações necessárias para a recuperação de imagens . O objetivo deste trabalho é propor novos extratores de características , baseados nos subespaços de imagens médicas gerados por transformadas wavelets . Estas características são armazenadas em vetores de características , os quais representam numericamente as imagens e permitindo assim sua busca por semelhança utilizando o conteúdo das próprias imagens . Esses vetores serão usados em um sistema de mineração de imagens em desenvolvimento no GBdI-ICMC-USP , o StARMiner , permitindo encontrar padrões pertencentes às imagens que as levem a ser classificadas em 
 A crescente procura por software de qualidade vem causando grande pressão sobre as empresas que trabalham com desenvolvimento de software . As entregas de produtos de software dentro do prazo e custo previstos vêm se tornando , a cada dia , um diferencial importante nesse ramo de atividade . Nesse sentido , as empresas procuram por metodologias que propiciem o desenvolvimento de produtos com qualidade , e que respeitem o custo e prazo previstos . Em resposta a essas necessidades , surgiu uma nova classe de metodologias de desenvolvimento de software , conhecidas como metodologias ágeis . Este trabalho apresenta um estudo realizado sobre as principais características existentes nessa nova classe de metodologias . Uma análise permitiu a identificação de semelhanças e diferenças existentes entre elas , o que possibilitou a criação de um modelo de referência para o desenvolvimento ágil de software . O modelo foi utilizado em uma avaliação de processo baseada no modelo de avaliação da ISO/IEC 15504 . A avaliação permitiu a identificação de forças e fraquezas no processo avaliado e possibilitou a definição de ações de melhoria para que o processo avaliado se assemelhasse à um processo de desenvolvimento ágil . Palavra-chave : Metodologia ágil de desenvolvimento . Modelo de referência . Processo de desenvolvimento de software . Avaliação de processo de 
 A SOA está se tornando uma abordagem difundida no desenvolvimento de sistemas . Em sistemas maiores onde se utiliza SOA pode ocorrer problemas de alta demanda , que podem ser resolvidos com arquiteturas distribuídas como clusters . Esta dissertação apresenta a proposta de uma nova arquitetura para distribuição de requisições em clusters de web services . A arquitetura proposta tem foco na distribuição flexível de requisições , possibilitando o emprego de políticas diversificadas , estas voltadas a diferentes objetivos , aplicações e plataformas . A arquitetura também propõe trabalhar de forma dinâmica , possibilitando decisões baseadas na coleta de estado dos elementos que compõem a estrutura de atendimento . A transparência , do ponto de vista do cliente , também é tratada na arquitetura . Os testes da arquitetura foram realizados através de um protótipo que a implementa . O protótipo instancia os objetivos propostos na arquitetura , apresentando ganhos de desempenho em relação à solução existente atualmente . A flexibilidade das políticas de distribuição é destacada através da construção de uma política , cujo_objetivo  é melhorar a eficiência no consumo de energia de clusters . Dentre as contribuições do trabalho está a criação de uma nova arquitetura de software que atua como facilitadora para a criação de políticas de distribuição de requisições mais eficiente para web 
 Uma representação retangular de um grafo plano G é uma representação de G , onde cada vértice é desenhado como um retângulo de modo que dois retângulos devem compartilhar algum segmento de seus lados se e somente se existe uma aresta em G entre os vértices correspondentes aos retângulos . Ainda , a representação de G deve formar um retângulo e não deve existir buracos , ou seja , toda região interna deve corresponder a algum vértice de G. Um desenho retangular de um grafo plano H é um desenho de H , onde todas as arestas são desenhadas como segmentos horizontais ou verticais . Ainda , todas as faces internas são retângulos e as arestas que incidem na face externa também formam um retângulo . Nesta dissertação , apresentamos os principais trabalhos existentes na literatura para problemas associados à representação retangular . Também apresentamos resultados para problemas associados ao desenho retangular . Por fim , apresentamos o algoritmo que desenvolvemos para determinar as coordenadas dos vértices de um desenho retangular quando a orientação das arestas já foram determinadas 
 O uso de aeronaves não tripuladas ( VANTs ) tem crescido substancialmente nos últimos_anos  , tanto no campo militar quanto no civil . Roadmaps preveem que em um futuro próximo essas aeronaves compartilhem o espaço aéreo com aeronaves convencionais , exigindo novas arquiteturas de sistemas embarcados que possam garantir uma operação coordenada e segura desses robôs . A maior_parte  das suas missões baseia-se fortemente em um conjunto de sensores transportados pela aeronave como parte da payload da missão . Contudo , não é trivial a integração de diferentes missões em diferentes aeronaves , visto que ainda não há uma padronização para a comunicação nesses robôs . Para possibilitar essa associação foi proposto neste trabalho a criação de um middleware . Para que se pudesse entender sobre a área de conhecimento dos VANTs realizou-se uma pesquisa sobre esses veículos e suas aplicações e então um protocolo chamado Smart Sensor Protocol ( SSP ) foi modelado , utilizando-se de técnicas formais para isso . O comportamento do protocolo está modelado com diagrama de estados , seguindo uma gramática escrita utilizando a forma BNF . Este modelo foi verificado com a ferramenta UPPAAL e sua implementação testada em placas Arduino . Os resultados dos testes mostraram que o modelo é viável para o ambiente de embarcados_críticos  visto que ele provê as funcionalidades necessárias neste cenário sem acrescentar um overhead na 
 A sumarização_automática  multidocumento consiste em produzir um sumário ou resumo ( como mais comumente é conhecido ) a partir de um grupo de textos que versam sobre um mesmo assunto , contendo as informações mais relevantes de acordo com o interesse do usuário . No cenário atual , com a quantidade imensa de informação em constante crescimento e atualização , e o tempo cada vez mais reduzido disponível para apreender o conteúdo de interesse , sumários multidocumento têm se tornado um recurso importante . Nesta dissertação , foram explorados métodos de seleção de conteúdo para sumarização multidocumento com base no modelo de relacionamento multidocumento CST ( Cross-document Structure Theory ) , proposto recentemente e já difundido na área de Processamento de Línguas_Naturais  . Em particular , neste trabalho , foram definidos e formalizados operadores de seleção de conteúdo para sumarização multidocumento com base no modelo CST . Estes operadores representam possíveis preferências de sumarização e focam-se no tratamento dos principais desafios presentes no processamento de múltiplos documentos : redundância , complementaridade e informações contraditórias . Estes operadores são especificados em templates contendo regras e funções que relacionam essas preferências às relações CST . Especificamente , foram definidos operadores para extrair a informação principal , apresentar informação de contexto , identificar autoria , tratar redundâncias e identificar informação contraditória . Também foi avaliado o impacto do uso do modelo CST em métodos de sumarização superficiais . Experimentos foram realizados com textos jornalísticos escritos em português brasileiro . Os resultados das avaliações mostram que o uso da teoria CST melhora a informatividade e a qualidade dos sumários 
 A substituição de valores ausentes , também conhecida como imputação , é uma importante tarefa para a preparação dos dados em aplicações de mineração de dados . Este trabalho propõe e avalia um algoritmo para substituição de valores ausentes baseado em um algoritmo_evolutivo  para agrupamento de dados . Este algoritmo baseia-se na suposição de que grupos ( previamente desconhecidos ) de dados podem prover informações úteis para o processo de imputação . Para avaliar experimentalmente o algoritmo proposto , simulações de valores ausentes foram realizadas em seis bases de dados , para problemas de classificação , com a aplicação de dois mecanismos amplamente usados em experimentos controlados : MCAR e MAR . Os algoritmos de imputação têm sido tradicionalmente avaliados por algumas medidas de capacidade de predição . Entretanto , essas tradicionais medidas de avaliação não estimam a influência dos métodos de imputação na etapa final em tarefas de modelagem ( e.g. , em classificação ) . Este trabalho descreve resultados experimentais obtidos sob a perspectiva de predição e inserção de tendências ( viés ) em problemas de classificação . Os resultados de diferentes cenários nos quais o algoritmo proposto , apresenta em geral , desempenho semelhante a outros seis algoritmos de imputação reportados na literatura . Finalmente , as análises estatísticas reportadas sugerem que melhores_resultados  de predição não implicam necessariamente em menor viés na 
 O principal objetivo do desenvolvimento de aplicações multimodais é possibilitar uma maneira mais natural dos seres_humanos  se comunicarem com as máquinas , por meio de interfaces mais eficientes , intuitivas , fáceis de usar e , de certa forma , mais inteligentes . No entanto , a literatura da área mostra que a reutilização , tanto de conhecimento como de código fonte , ainda apresenta problemas , dados a complexidade do código em sistemas multimodais , a falta de mecanismos eficientes de testes de usabilidade e a dificuldade em se gerenciar a captura , o armazenamento e a recuperação de conhecimento de projeto . Nesta tese argumenta-se que a utilização de uma abordagem sistemática , centrada no usuário , apoiada por uma ferramenta computacional e com um modelo bem definido que permita o desenvolvimento de interfaces multimodais com a reutilização de Design Rationale , aumenta e melhora os níveis de usabilidade , promove a identificação e utilização de padrões de projeto e o reúso de componentes . Para demonstrar esta tese , apresenta-se neste texto a abordagem para o desenvolvimento de interfaces multimodais Web , MMWA , e o seu ambiente de autoria , o MMWA-ae , ambos compostos por atividades que auxiliam a equipe de projeto durante as fases de projeto , desenvolvimento e avaliações de usabilidade . São discutidos também os resultados obtidos com a execução de três estudos de caso , realizados no ambiente acadêmico , nos quais se buscou determinar a viabilidade da abordagem e os benefícios que podem ser alcançados com a combinação de diferentes técnicas , a saber : design rationale , padrões de projeto , modelagem de tarefas , componentes de software , princípios de usabilidade , avaliações heurísticas , testes com usuários , regras de associação , entre outras . Os resultados evidenciam que a abordagem e seu ambiente de autoria podem proporcionar diferentes benefícios para organizações que desenvolvem sistemas multimodais , incluindo o aumento da usabilidade e consequentemente da qualidade do produto , bem como a diminuição de custos e da complexidade do desenvolvimento com a reutilização de código e de conhecimento capturado em projetos 
 Nas últimas_décadas  , as técnicas de mineração de dados têm desempenhado um importante papel em diversas_áreas  do conhecimento humano . Mais recentemente , essas ferramentas têm encontrado espaço em um novo e complexo domínio , nbo qual os dados a serem minerados estão fisicamente distribuídos . Nesse domínio , alguns algorithmos específicos para agrupamento de dados podem ser utilizados - em particular , algumas variantes do algoritmo amplamente Fuzzy C-Means ( FCM ) , as quais têm sido investigadas sob o nome de agrupamento_fuzzy  colaborativo . Com o objetivo de superar algumas das limitações encontradas em dois desses algoritmos , cinco novos algoritmos foram desenvolvidos nesse trabalho . Esses algoritmos foram estudados em dois cenários específicos de aplicação que levam em conta duas suposições sobre os dados ( i.e. , se os dados são de uma mesma npopulação ou de diferentes populações ) . Na prática , tais suposições e a dificuldade em se definir alguns dos parâmetros ( que possam ser requeridos ) , podemn orientar a escolha feita pelo usuário entre os algoitmos diponíveis . Nesse sentido , exemplos ilustrativos destacam as diferenças de desempenho entre os algoritmos estudados e desenvolvidos , permitindo derivar algumas conclusões que podem ser úteis ao aplicar agrupamento_fuzzy  colaborativo na prática . Análises de complexidade de tempo , espaço , e comunicação também foram 
 Nos últimos_anos  , clusters de computadores vêm ganhando popularidade , principalmente nos meios corporativos e académicos , como servidores_Web  , Isso se deve , principalmente , por eles oferecerem um custo/benefício bastante atraente . No entanto , esses clusters ( conhecidos como Web Farms ) possuem uma maior complexidade de gerenciamento e , principalmente os clusters de alta disponibilidade , necessitam de um monitoramento intensivo para manter os seus serviços sempre disponíveis . Este trabalho apresenta a implementação de um Web Farm Cluster e o desenvolvimento de um sistema de monitoramento e controle desse tipo de cluster ( iCluster ) utilizando a tecnologia de agentes de software . O sistema utiliza a tecnologia de agentes de software para implementar módulos de computação autónoma , permitindo dessa forma o monitoramento , o controle e a visualização de estados e serviços de um Web Farm Cluster com o mínimo de intervenção humana 
 Este trabalho aborda o Problema da Mochila Compartimentada que é uma variação do clássico problema da mochila e pode ser enunciado considerando-se a seguinte situação hipotética : um alpinista deve carregar sua mochila com possíveis itens de seu interesse . A cada item atribui-se o seu peso e um valor de utilidade ( até aqui , o problema coincide com o clássico Problema da Mochila ) . Entretanto , os itens são de agrupamentos distintos ( alimentos , medicamentos , utensílios , etc . ) e devem estar em compartimentos separados na mochila . Os compartimentos da mochila são flexíveis e têm capacidades limitadas . A inclusão de um compartimento tem um custo fixo que depende do agrupamento com que foi preenchido , além de introduzir uma perda da capacidade da mochila . O problema consiste em determinar as capacidades adequadas de cada compartimento e como esses devem ser carregados , maximizando o valor de utilidade total , descontado o custo de incluir compartimentos . Neste trabalho propomos um modelo de otimização não linear inteiro para o problema e algumas heurísticas para sua resolução , para as quais apresentamos os resultados computacionais obtidos . Uma aplicação prática que surge no corte de bobinas de aço , sujeito à laminação é detalhada 
 Devido às tendências de crescimento da quantidade de dados processados e a crescente necessidade por computação de alto_desempenho  , mudanças significativas estão acontecendo no projeto de arquiteturas de computadores . Com isso , tem-se migrado do paradigma sequencial para o paralelo , com centenas ou milhares de núcleos de processamento em um mesmo chip . Dentro desse contexto , o gerenciamento de energia torna-se cada vez mais importante , principalmente em sistemas embarcados , que geralmente são alimentados por baterias . De acordo com a Lei de Moore , o desempenho de um processador dobra a cada 18 meses , porém a capacidade das baterias dobra somente a cada 10 anos . Esta situação provoca uma enorme lacuna , que pode ser amenizada com a utilização de arquiteturas multi-cores heterogêneas . Um desafio fundamental que permanece em aberto para estas arquiteturas é realizar a integração entre desenvolvimento de código embarcado , escalonamento e hardware para gerenciamento de energia . O objetivo geral deste trabalho de doutorado é investigar técnicas para otimização da relação desempenho/consumo de energia em arquiteturas multi-cores heterogêneas single-ISA implementadas em FPGA . Nesse sentido , buscou-se por soluções que obtivessem o melhor desempenho possível a um consumo de energia ótimo . Isto foi feito por meio da combinação de mineração de dados para a análise de softwares baseados em threads aliadas às técnicas tradicionais para gerenciamento de energia , como way-shutdown dinâmico , e uma nova política de escalonamento heterogeneity-aware . Como principais_contribuições  pode-se citar a combinação de técnicas de gerenciamento de energia em diversos níveis como o nível do hardware , do escalonamento e da compilação ; e uma política de escalonamento integrada com uma arquitetura multi-core heterogênea em relação ao tamanho da memória cache L1 
 Homomorfismos de grafos são funções do conjunto de vértices de um grafo no conjunto de vértices de outro grafo que preservam adjacências . O estudo de homomorfismos de grafos é bastante abrangente , existindo muitas linhas de pesquisa sobre esse tópico . Nesta dissertação , apresentaremos resultados sobre homomorfismos de grafos relacionados a pseudo-aleatoriedade , convergência de seqüência de grafos e matrizes de conexão de invariantes de grafos . Esta linha tem se mostrado muito rica , não apenas pelos seus resultados , como também pelas técnicas utilizadas nas demonstrações . Em especial , destacamos a diversidade das ferramentas matemáticas que são usadas , que incluem resultados clássicos de álgebra , probabilidade e análise 
 O Job Shop Scheduling Problem é um problema NP-Difícil que chama a atenção de muitos pesquisadores devido seu desafio matemático e sua aplicabilidade em contextos reais . Geralmente , principalmente em cenários próximos aos de fábricas e indústrias , obter um escalonamento ótimo por meio de métodos computacionais exatos implica em um alto desprendimento de tempo . Em contrapartida , devido às exigências de um mercado cada vez mais competitivo , as decisões de onde , como , quando e com o que produzir devem ser tomadas rapidamente . O presente_trabalho  propõe o desenvolvimento de um método heurístico Beam Search para solucionar o Job Shop Scheduling Problem e o Flexible Job Shop Scheduling Problem . Para isso , inicialmente um algoritmo do tipo list scheduling é definido e então o método Beam Search é construído baseado neste algoritmo . Os métodos propostos foram avaliados em diferentes níveis de complexidade utilizando instâncias da literatura que retratam diferentes cenários de planejamento . Em linhas gerais , as soluções encontradas se mostraram bastante competitivas quando comparadas a outras soluções da literatura 
 Em diversos escoamentos sobre superfícies há a presença de protuberâncias , como por exemplo rebites , parafusos e juntas . Estas protuberâncias podem influenciar a camada limite , acelerando a transição do escoamento do estado laminar para o estado turbulento . Em alguns_casos  isto pode ser indesejável , já que o escoamento turbulento implica necessariamente em uma força de atrito maior do que aquela referente ao escoamento laminar . Existem alguns aspectos neste tipo de escoamento que ainda não estão bem compreendidos . O objetivo deste trabalho é estudar a influência de uma rugosidade isolada no escoamento sobre uma superfície . Este estudo contribui para se entender o que ocorre em casos de maior complexidade . O estudo é de natureza computacional , em que se utiliza simulação_numérica  direta das equações de Navier-Stokes . A técnica de fronteiras imersas é utilizada para representar a rugosidade no escoamento sobre a superfície . O código numérico é verificado por meio do método de soluções manufaturadas . Comparações entre resultados experimentais , da teoria de estabilidade linear e numéricos também são utilizados para a validação do código . Resultados obtidos com diferentes alturas de rugosidade e variações no gradiente de pressão permitiram analisar a influência de elemento rugoso tridimensional em escoamentos de camada limite 
 De acordo com a Organização Mundial da Saúde , cerca de 1,2milhões de pessoas no mundo morrem em acidentes de trânsito . Sistemas de assistência ao motorista e veículos_autônomos  podem diminuir o número de acidentes . Dentre as várias demandas existentes para viabilizar essa tecnologia , sistemas computacionais de percepção ainda permanecem sem uma solução definitiva . Dois deles , detecção de obstáculos e de via navegável , normalmente fazem_uso  de algoritmos sofisticados como técnicas de aprendizado_supervisionado  , que mostram resultados impressionantes quando treinados com bases de dados bem definidas e diversificadas.Entretanto , construir , manter e atualizar uma base de dados com exemplos de vários lugares do mundo e em diversas situações é trabalhoso e complexo . Assim , métodos adaptativos e auto-supervisionados mostram-se como boas alternativas para sistemas de detecção do futuro próximo . Neste contexto , esta tese apresenta um método para estimar obstáculose via navegável através de sensores de baixo custo ( câmeras estereoscópicas ) , sem o uso de técnicas de aprendizado de máquina e de diversas suposições normalmente utilizadas por trabalhos já disponíveis na literatura . Esses métodos utilizando sensor estereoscópico foram comparados fazendo_uso  de sensores do tipo 3D-LIDAR e mostraram resultados semelhantes . Este sistema poderá ser usado como uma fase pré-processamento de dados para melhorar ou viabilizar métodos adaptativos de aprendizado 
 A disponibilidade de ferramentas de teste de software propicia maior qualidade e produtividade para as atividades de teste . Diversas ferramentas têm sido propostas para o apoio ao teste_estrutural  , mas nota-se a falta de ferramentas que ofereçam suporte ao teste funcional e implementem os diversos critérios dessa técnica . Visando a preencher essa lacuna , uma ferramenta para apoio ao teste funcional de programas Java foi desenvolvida e é apresentada . A ferramenta apóia os principais critérios funcionais , tais como o Particionamento de Equivalência e Análise de Valor-limite , e oferece análise de cobertura baseada nos critérios da técnica funcional . A análise de cobertura é uma característica normalmente encontrada nas ferramentas que apóiam o teste_estrutural  , mas raramente disponível nas ferramentas que oferecem suporte ao teste funcional . O projeto da ferramenta é apresentado , no qual o conceito de separação de interesses é utilizado . A Programação Orientada a Aspectos é utilizada nas etapas de instrumentação e execução dos critérios de teste . O uso dessa técnica permite a separação clara entre o código de teste e o código do programa , assim como torna mais fácil a adição e remoção dos aspectos que implementam o teste , além de ser uma solução eficaz para o projeto da ferramenta . Um exemplo de uso da ferramenta é apresentado 
 A abordagem de linha de produto de software ( LP ) tem como objetivo principal promover a geração de produtos específicos de um determinado domínio com base na reutilização de uma infraestrutura central , chamada núcleo de artefatos . Um dos principais artefatos do núcleo de uma LP é a Arquitetura de LP ( ALP ) , que representa a abstração de todas as arquiteturas de sistemas únicos que podem ser gerados , para um domínio específico . Avaliações de ALP são importantes , pois permitem aumentar a produtividade e a qualidade dos produtos da LP , bem como , seus resultados permitem a análise de metas de negócio e de retorno de investimento . Este trabalho propõe um método sistemático para avaliação de ALP , o SystEM-PLA ( a Systematic Evaluation Method for Software Product Line Architectures ) . Tal método considera modelos de ALP em UML , por ser uma notação amplamente conhecida e consolidada . SystEM-PLA é composto por um metaprocesso de avaliação , diretrizes que guiam o usuário em como avaliar uma ALP e métricas básicas para modelos UML e atributos de qualidade . O método utiliza a abordagem SMarty ( Stereotype-based Management of Variability ) , para gerenciar variabilidades em LP baseadas em UML . Análises de trade-off com o objetivo de priorizar atributos de qualidade para o desenvolvimento e evolução dos produtos de uma LP são realizadas com base na aplicação e coleta das métricas do SystEM-PLA em configurações de uma ALP . As métricas propostas para os atributos de qualidade complexidade e extensibilidade foram validadas por meio de um estudo_experimental  . Evidências indicaram a viabilidade de aplicação do método SystEM-PLA na indústria com base em um estudo_experimental  realizado com profissionais de uma empresa de grande porte no setor de desenvolvimento de 
 Os desastres naturais , como inundações , secas e tempestades causam muitas mortes e danos em todo o mundo . Mais recentemente , alguns países sofreram com o aumento das inundações , comparado com outros_tipos  de desastres . Para melhor gerenciá-las , agências governamentais têm fornecido dados históricos de redes de sensores estáticas para ajudar comunidades que vivem em áreas de risco . No entanto , tais redes de sensores apenas ajudam a verificar propriedades específicas ( por exemplo , temperatura e pressão ) e pouco contribuem com a falta de informações presente nesse contexto . Além dos sensores estáticos , sensores móveis também têm sido utilizados para monitorar inundações , uma vez que podem fornecer imagens e alcançar distâncias onde sensores estáticos não funcionam adequadamente . Para combinar esses sensores , deve ser utilizado uma iniciativa chamada Sensor Web Enablement ( SWE ) que isola as aplicações das idiossíncrasias da implementação desses sensores heterogêneos . Entretanto , a SWE não gerencia completamente contextos em que sensores são inseridos e removidos dinamicamente . Este contexto dinâmico torna complexo o controle , o acesso e a descoberta de novos sensores . Logo , o objetivo deste trabalho é gerenciar dinamicamente e próximo do tempo-real sensores heterogêneos envolvidos na gestão de inundações , permitindo um acesso interoperável para seus dados usando componentes abertos e de re-uso . Para alcançar esse objetivo , um middleware orientado a serviços contendo um protocolo de mensagens comum , um componente de gerenciamento dinâmico de sensores e um repositório foi desenvolvido . A avaliação dessa abordagem foi feita considerando uma aplicação que prioriza geograficamente dados de mídias_sociais  baseados em dados de sensores 
 As organizações públicas e privadas são constantemente expostas a fatores internos e externos , que podem comprometer sua estabilidade diante das oscilações da economia e dos concorrentes . Nestas empresas , os tomadores de decisão são essenciais para analisar e avaliar todas as variáveis que envolvem estes fatores , com o objetivo de identificar o melhor caminho para os negócios . Entretanto , conseguir gerenciar os dados internos e externos à organização não é uma atividade simples . Neste contexto , os Sistemas de Suporte à Decisão ( SSD ) tornaram-se fundamentais para auxiliar os tomadores de decisão na solução de problemas mal estruturados ou sem nenhuma estruturação . Porém , a complexidade que envolve os projetos de implantação ou desenvolvimento de um SSD , geralmente compromete a efetividade dos testes que garantem a conformidade do sistema em relação às especificações previamente definidas . Uma solução para esse problema é considerar os atributos ou fatores de testabilidade nestes projetos , pois podem elevar o grau de eficácia e eficiência da atividade de teste e consequentemente contribuírem para redução do tempo e custos do projeto . Portanto , conseguir identificar esses atributos ou fatores que tenham influência na testabilidade dos SSD e algum método que permita analisar e avaliar o quanto estão presentes neste sistema , é essencial para aumentar a qualidade do sistema . Diante desta necessidade , este trabalho investigou e selecionou os principais fatores que podem influenciar no grau de testabilidade de um software e propôs um método para analisar e avaliar o quanto o SSD está considerando esses fatores em sua arquitetura . Com o objetivo de avaliar e validar o método de análise e avaliação , foram realizados testes de aplicabilidade em empresas de pequeno , médio e grande porte , bem como no meio acadêmico . Com os resultados obtidos nos testes , foi possível concluir que o método é específico para SSD , que pode ser usado como um guia durante o processo de desenvolvimento e auxiliar na classificação de SSD quanto a sua testabilidade 
 Técnicas de Agrupamento vêm obtendo bons_resultados  quando utilizados em diversos problemas de análise de dados , como , por exemplo , a análise de dados de expressão_gênica  . Porém , uma mesma técnica de agrupamento utilizada em um mesmo conjunto de dados pode resultar em diferentes formas de agrupar esses dados , devido aos possíveis agrupamentos iniciais ou à utilização de diferentes valores para seus parâmetros livres . Assim , a obtenção de um bom agrupamento pode ser visto como um processo de otimização . Esse processo procura escolher bons agrupamentos iniciais e encontrar o melhor conjunto de valores para os parâmetros livres . Por serem métodos de busca global , Algoritmos Genéticos podem ser utilizados durante esse processo de otimização . O objetivo desse projeto de pesquisa é investigar a utilização de Técnicas de Agrupamento em conjunto com Algoritmos Genéticos para aprimorar a qualidade dos grupos encontrados por algoritmos de agrupamento , principalmente o k-médias . Esta investigação será realizada utilizando como aplicação a análise de dados de expressão_gênica  . Essa dissertação de mestrado apresenta uma revisão bibliográfica sobre os temas abordados no projeto , a descrição da metodologia utilizada , seu desenvolvimento e uma análise dos resultados obtidos 
 Assumir suposições especiais sobre a função de risco tem sido a estratégia adotada por vários autores , com intuito de garantir modelos gerais e abrangentes , tanto para a análise de dados de sobrevivência quanto de conDabilidade . Neste estudo , modelos aplicados a dados da área de sobrevivência e conDabilidade são considerados . A Dnalidade deste estudo é propor modelos mais Pexíveis e/ou mais abrangentes de forma a generalizar modelos já existentes , bem como estudar suas propriedades e propor possíveis comparações entre os modelos via testes de hipóteses . Considera-se nesta tese , três classes de modelos baseados na função de risco ( modelos de risco ) . A primeira classe apresenta-se como um caso particular do modelo de risco estendido ( Louzada-Neto , 1999 ) , formada por modelos que relacionam o parâmetro de escala a covariáveis , sendo que esse relacionamento pode ser considerado log-linear ou log-nãolinear . Considera-se um modelo particular onde a dependência do parâmetro de escala se dá de forma log-não-linear . Na segunda classe considera-se modelos que estão vinculados a dados de riscos competitivos , quando se tem ou não informação sobre qual tipo de risco foi responsável pela falha de um equipamento ou pelo óbito de um paciente . A terceira classe de modelos foi proposta , nesta tese , relacionando o contexto de modelos de longa duração 
 Degradação do design é um problema central investigado na área de evolução de software . A densa rede de interdependências que emerge entre classes e módulos ao longo do tempo resulta em código difícil de mudar , não reutilizável e que não comunica por si só sua intenção . Dentre outros motivos , designs degradam porque requisitos mudam de maneiras não antecipadas pelo design inicial , ou seja , as modificações no código introduzem dependências novas e não planejadas entre classes e módulos do sistema . A gerência de dependências visa reduzir a degradação do design por meio de uma série de mecanismos que auxiliam na administração da complexidade estrutural inerente de sistemas orientados a objetos . Neste trabalho , investigamos as técnicas de identificação de dependências estruturais e lógicas . Em particular , por meio de um estudo de larga_escala  , comparamos os conjuntos desses dois tipos de dependências . Em seguida , conduzimos um estudo de caso a fim de identificar as origens de dependências lógicas . Por fim , fazemos um levantamento das técnicas de visualização de dependências e mostramos a ferramenta XFlow 
 O Problema de Escalonamento de Médicos ( Physician Scheduling Problem ) consiste em atribuir tarefas a médicos num horizonte de planejamento respeitando regras laborais , contratuais e de preferências pessoais de modo a satisfazer a demanda de serviços de um hospital . O problema lida majoritariamente com o objetivo de maximizar o atendimento dos requisitos de preferência pessoal , respeitando as restrições laborais e organizacionais . Sobre esta classe de problemas , vários métodos de resolução e suas variantes têm sido_propostos  na literatura . Ademais , mais características têm sido agregadas ao problema , tornando-o mais complexo e deste modo fazendo-se mais necessária a aplicação de métodos mais elaborados para a sua resolução . Neste trabalho são estudados , reformulados e propostos métodos de resolução baseados em programação matemática para tratar o problema de escalonamento acíclico de médicos em departamento de emergência de hospitais . O primeiro modelo tem como objetivo a minimização da soma ponderada dos desvios das restrições de distribuição . O segundo modelo tem como objetivo , a minimização do máximo dos desvios obtidos nas restrições de distribuição , a fim de se obter escalas mais equilibradas entre os médicos . Foram também propostas heurísticas baseadas na formulação_matemática  cujos resultados não foram competitivos com as dos modelos . Os modelos foram testados sobre um conjunto de instâncias fictícias resultantes de uma mescla entre instâncias benchmark e características do problema . Os resultados computacionais demonstram que formulação ponderada obteve solução ótima para grande parte das instâncias , embora os limitantes inferiores tenham sido majoritariamente fracos . Em relação ao segundo modelo , soluções_ótimas  não foram obtidas e os limitantes inferiores foram igualmente fracos . Relativamente a qualidade das escalas , o segundo modelo teve melhor comportamento comparando ao modelo de somas ponderadas . Dada a qualidade das soluções , nota-se a viabilidade da solução baseada em técnicas de otimização em detrimento da manual , pois esta ainda é mais suscetível de erros e acarreta um alto tempo para obtenção de solução 
 A exploração de conjuntos de dados é um problema abordado com frequência em diversos_domínios  e tem como objetivo uma melhor compreensão de fenômenos simulados ou medidos . Tal atividade é precedida pelas etapas de coleta e armazenamento de dados que buscam registrar o máximo de detalhes sobre algum fenômeno observado . Porém , a exploração efetiva dos dados envolve uma série de desafios . Um deles é a dificuldade em identificar quais dados são realmente relevantes para as análises . Outro problema está relacionado com a falta de garantias de que os fatores fundamentais para a compreensão do problema tenham sido coletados . A transformação interativa de dados é uma abordagem que utiliza técnicas de visualização computacional para resolver ou minimizar esses problemas . No entanto , os trabalhos disponíveis na literatura possuem limitações , como interfaces demasiadamente complexas e mecanismos de interação pouco flexíveis . Assim , este projeto de mestrado teve como objetivo desenvolver novas técnicas visuais interativas para a transformação de dados multidimensionais . A metodologia desenvolvida se baseou no uso de biplots e na ação conjunta dos mecanismos de interação para superar as limitações das técnicas do estado da arte . Os resultados dos experimentos_realizados  sobre diversos conjuntos de dados dão indícios de que os métodos desenvolvidos possibilitam a obtenção de conjuntos de dados mais representativos . Mais especificamente , foram obtidos melhores_resultados  em tarefas de classificação de dados ao utilizar os métodos desenvolvidos 
 Uma série_temporal  é uma sequência ordenada pelo tempo de valores reais . Dado que inúmeros fenômenos do dia-a-dia podem ser representados por séries_temporais  , há grande interesse na mineração de dados temporais , em especial na tarefa de classificação . Recentemente foi introduzida uma nova primitiva de séries_temporais  chamada shapelet , que é uma subsequência que permite a classificação de séries_temporais  de acordo com padrões locais . Na transformada shapelet estas subsequências se tornam atributos em uma matriz de distância que mede a dissimilaridade entre os atributos e as séries_temporais  . Para obter a transformada é preciso escolher alguns shapelets dos inúmeros possíveis , seja pelo efeito de evitar overfitting ou pelo fato de que é computacionalmente caro obter todos . Sendo_assim  , foram elaboradas medidas de qualidade para os shapelets . Tradicionalmente tem se utilizado a medida de ganho de informação , porém recentemente foi proposto o uso da f-statistic , e nós propomos neste trabalho uma nova denominada in-class transitions . Em nossos experimentos demonstramos que a inclass transitions costuma obter a melhor acurácia , especialmente quando poucos atributos são utilizados . Além_disso  , propomos o uso de amostragem aleatória nos shapelets para reduzir o espaço de busca e acelerar o processo de obtenção da transformada . Contrastamos a abordagem de amostragem aleatória contra uma em que só são exploradas shapelets de determinados tamanhos . Nossos experimentos mostraram que a amostragem aleatória é mais rápida e requer a computação de um menor número de shapelets . De fato , obtemos os melhores_resultados  ao amostrarmos 5 % dos shapelets , mas mesmo a uma amostragem de 0,05 % não foi possível notar uma degradação significante da acurácia 
 Esta tese de doutorado apresenta investigações , modelos e implementações de um middleware para grades_computacionais  denominado Grid Anywhere . Essa plataforma tem como objetivo viabilizar a construção de grades_computacionais  que permitam um maior número de provedores e consumidores de recursos . Para isso , são apresentadas soluções para gerenciamento de segurança , carregamento de aplicações , hospedagem de objetos , execução remota de métodos , desenvolvimento de aplicações e transporte alternativo de mensagens SOAP ( utilizando o sistema de televisão digital interativa e encapsulando uma mensagem SOAP dentro de outro documento desse mesmo protocolo ) . Como aplicações da plataforma foram criadas duas grades_computacionais  com arquiteturas distintas . A primeira permite que um ambiente de compartilhamento de recursos possa ser utilizado como infraestrutura para prover plataforma como serviço ( PaaS ) para usuários convencionais ( domésticos ou corporativos ) em um ambiente de computação em nuvem . A outra arquitetura tem como foco o fortalecimento de grades_computacionais  desktop por meio da utilização de receptores digitais de TV ( set-top box ) como provedores de recursos onde a distribuição de objetos e as mensagens SOAP ocorrem por difusão . Os modelos foram validados por meio de testes reais feitos utilizando as respectivas implementações , o que demonstrou que são funcionais . Tais implementações disponibilizam produtos que cooperam com a inovação do desenvolvimento de aplicações para grades_computacionais  e também para outras categorias de sistemas 
 Atualmente , é comum que usuários levem em consideração a localização geográfica dos documentos , é dizer considerar o escopo geográfico que está sendo tratado no contexto do documento , nos processos de Recuperação de Informação . No entanto , os sistemas convencionais de extração de informação que estão baseados em palavras-chave não consideram que as palavras podem representar entidades geográficas espacialmente relacionadas com outras entidades nos documentos . Para resolver esse problema , é necessário viabilizar o georreferenciamento dos textos , ou seja , identificar as entidades geográficas presentes e associá-las com sua correta localização espacial . A identificação e desambiguação das entidades geográficas apresenta desafios importantes , principalmente do ponto de vista linguístico , já que um topônimo , pode possuir variados tipos de ambiguidade associados . Esse problema de ambiguidade causa ruido nos processos de recuperação de informação , já que o mesmo termo pode ter informação relevante ou irrelevante associada . Assim , a principal estratégia para superar os problemas de ambiguidade , compreende a identificação de evidências que auxiliem na identificação e desambiguação das localidades nos textos . O presente_trabalho  propõe uma metodologia que permite identificar e determinar a cobertura espacial dos documentos , denominada SpatialCIM . A metodologia SpatialCIM tem o objetivo de organizar os processos de resolução de topônimos . Assim , o principal objetivo deste trabalho é avaliar e selecionar técnicas de desambiguação que permitam resolver a ambiguidade dos topônimos nos textos . Para isso , foram propostas e desenvolvidas as abordagens de ( 1 ) Desambiguação por Pontos e a ( 2 ) Desambiguação Textual e Estrutural . Essas abordagens , exploram duas técnicas diferentes de desambiguação de topônimos , as quais , geram e desambiguam os caminhos geográficos associados aos topônimos reconhecidos para cada documento . Assim , a hipótese desta pesquisa é que o uso das técnicas de desambiguação de topônimos viabilizam uma melhor localização espacial dos documentos . A partir dos resultados obtidos neste trabalho , foi possível demonstrar que as técnicas de desambiguação melhoram a precisão e revocação na classificação espacial dos documentos . Demonstrou-se também o impacto positivo do uso de uma ferramenta linguística no processo de reconhecimento das entidades geográficas . Assim , foi demostrada a utilidade dos processos de desambiguação para a obtenção da cobertura espacial dos 
 Estudos de águas subterrâneas enfrentam limitações computacionais ao fornecer detalhes locais em modelos regionais . Os pesquisadores estão concentrados na aplicação dos modelos numéricos para minimizar a diferença entre a realidade física e o modelo numérico implementado considerando o custo_computacional  mínimo . Este trabalho consiste no estudo de elementos de linha ( como line-doublets , círculos , polígonos , fraturas ) usando o Método de Elemento Analítico ( AEM ) para o fluxo de águas subterrâneas . Neste trabalho , consideramos o estudo do fluxo bidimensional de águas subterrâneas em meios porosos fraturados pelo Método dos Elementos Analíticos . Desenvolvemos uma solução_numérica  baseada em uma expansão em série para um problema com mais de uma fratura . Cada fratura tem uma influência que pode ser expandida em uma série que satisfaça exatamente a equação de Laplace . Na expansão da série , os coeficientes desconhecidos são obtidos a partir dos potenciais de descarga de todos os outros elementos que estão relacionados aos coeficientes de expansão . Tamanhos , locais e condutividades para todas as não-homogeneidades são arbitrariamente selecionados . Este trabalho também discute o método da matriz obtido impondo as condições de contorno do interno para o Método do Elemento Analítico . A análise de convergência de um método iterativo tipo Gauss-Seidel também é discutida 
 A Web Semântica é uma extensão da Web em que as informações tem um significado explícito , permitindo que computadores e pessoas trabalhem em cooperação . Para definir os significados explicitamente , são usadas ontologias na estruturação das informações . À medida que mais campos científicos adotam tecnologias da Web Semântica , mais ontologias complexas são necessárias . Além_disso  , a garantia de qualidade das ontologias e seu gerenciamento ficam prejudicados quanto mais essas ontologias aumentam em tamanho e complexidade . Uma das causas para essas dificuldades é a existência de problemas , também chamados de anomalias , na estrutura das ontologias . Essas anomalias englobam desde problemas sutis , como conceitos mal projetados , até erros mais graves , como inconsistências . A identificação e a eliminação de anomalias podem diminuir o tamanho da ontologia e tornar sua compreensão mais fácil . Contudo , métodos para identificar anomalias encontrados na literatura não visualizam anomalias , muitos não trabalham com OWL e não são extensíveis por usuários . Por essas razões , um novo método para identificar e visualizar anomalias em ontologias , o ONTO-Analyst , foi criado . Ele permite aos desenvolvedores identificar automaticamente anomalias , usando consultas SPARQL , e visualizá-las em forma de grafos . Esse método usa uma ontologia proposta , a METAdata description For Ontologies/Rules ( MetaFOR ) , para descrever a estrutura de outras ontologias , e consultas SPARQL para identificar anomalias nessa descrição . Uma vez identificadas , as anomalias podem ser apresentadas na forma de grafos . Um protótipo de sistema , chamado ONTO-Analyst , foi criado para a validação desse método e testado em um conjunto representativo de ontologias , por meio da verificação de anomalias representativas . O protótipo testou 18 tipos de anomalias retirados da literatura científica , em um conjunto de 608 ontologias OWL de 4 repositórios públicos importantes e dois artigos . O sistema detectou 4,4 milhões de ocorrências de anomalias nas 608 ontologias : 3,5 milhões de ocorrências de um mesmo tipo e 900 mil distribuídas em 11 outros_tipos  . Essas anomalias ocorreram em várias partes das ontologias , como classes , propriedades de objetos e de dados , etc . Num segundo teste foi realizado um estudo de caso das visualizações geradas pelo protótipo ONTO-Analyst das anomalias encontradas no primeiro teste . Visualizações de 11 tipos diferentes de anomalias foram automaticamente geradas . O protótipo mostrou que cada visualização apresentava os elementos envolvidos na anomalia e que pelo menos uma solução podia ser deduzida a partir da visualização . Esses resultados demonstram que o método pode eficientemente encontrar ocorrências de anomalias em um conjunto representativo de ontologias OWL , e que as visualizações facilitam o entendimento e correção da anomalia encontrada . Para estender os tipos de anomalias detectáveis , usuários podem escrever novas consultas SPARQL 
 Neste trabalho estudamos as equações de Riccati para a filtragem de sistemas_lineares  com saltos Markovianos a tempo discreto . Obtemos uma condição geral para estabilidade do filtro ótimo obtido pela equação algébrica de filtragem , e que também é válida para que não haja multiplicidade de soluções . Revisitamos também a questão da existência , chegando a uma condição em termos da sequência de ganhos de um observador de Luenberger . Estes resultados usaram cadeias de Markov em escala reversa de tempo , inspirando a explorar a dualidade entre filtragem e controle em sistemas com reversão na cadeia , chegando a uma relação simples de dualidade 
 Séries temporais são ubíquas no dia-a-dia do ser humano . Dados organizados no tempo são gerados em uma infinidade de domínios de aplicação , como medicina , biologia , economia e processamento de sinais . Devido ao grande interesse nesse tipo de dados , diversos métodos de mineração de dados temporais foram propostos nas últimas_décadas  . Muitos desses métodos possuem uma característica em comum : em seu núcleo , há uma função de ( dis ) similaridade utilizada para comparar as séries . Dynamic Time Warping ( DTW ) é indiscutivelmente a medida de distância mais relevante na análise de séries_temporais  . A principal dificuldade em se utilizar a DTW é seu alto custo_computacional  . Ao mesmo tempo , algumas tarefas de mineração de séries_temporais  , como descoberta de motifs , requerem um alto número de cálculos de distância . Essas tarefas despendem um grande tempo de execução , mesmo utilizando-se medidas de distância menos custosas , como a distância Euclidiana . Esta tese se concentra no desenvolvimento de algoritmos eficientes que permitem a análise de dados temporais em larga_escala  , utilizando métodos baseados em similaridade . As contribuições desta tese têm implicações em variadas tarefas de mineração de dados , como classificação , agrupamento e descoberta de padrões frequentes . Especificamente , as principais_contribuições  desta tese são : ( i ) um algoritmo para acelerar o cálculo exato da distância DTW e sua incorporação ao processo de busca por similaridade ; ( ii ) um novo algoritmo baseado em DTW para prover invariância a prefixos e sufixos espúrios no cálculo da distância ; ( iii ) uma representação de similaridade musical com implicações em diferentes tarefas de mineração de dados musicais e um algoritmo eficiente para computá-la ; ( iv ) um método eficiente e anytime para encontrar motifs e discords baseado na medida DTW invariante a prefixos e sufixos 
 Na ultima década o paradigma de computação orientada a serviços ( SOC - Service Oriented Computing ) tem ganhado cada vez mais espaço na indústria e na academia , a fim de solucionar o problema da falta de comunicação entre os diversos sistemas de informação presentes dentro de um ambiente corporativo . Graças aos recentes avanços da chamada `` Web 2.0 '' um novo estilo arquitetural chamado de Arquitetura orientada a Web ( WOA - Web Oriented Architecture ) foi proposto a fim de garantir uma maneira simples de conectar os componentes de software dinamicamente . Esse estilo tem como um dos princípios o uso de serviços_Web  RESTful , a fim de conseguir uma interface funcional simples e uniforme . Este trabalho apresenta um ambiente colaborativo de apoio ao desenvolvimento de serviços_Web  RESTful utilizando o paradigma de desenvolvimento orientado a modelos ( MDD - Model Driven Development ) . Pretende-se mostrar os benefícios do MDD aplicado a Engenharia Web e também as vantagens apresentadas pela colaboração nesse cenário . Foi realizado ainda um experimento a fim de comprovar a eficiência do ambiente colaborativo e benefícios alcançados por 
 Uma variedade de algoritmos de aprendizado semissupervisionado baseado em grafos e métodos de geração de grafos foram propostos pela comunidade_científica  nos últimos_anos  . Apesar de seu aparente sucesso empírico , a área de aprendizado semissupervisionado carece de um estudo empírico detalhado que avalie o impacto da geração de grafos na classificação semissupervisionada . Neste trabalho , é provido tal estudo empírico . Para tanto , combinam-se uma variedade de métodos de geração de grafos com uma variedade de algoritmos de aprendizado semissupervisionado baseado em grafos para compará-los empiricamente em seis bases de dados amplamente usadas na literatura de aprendizado semissupervisionado . Os algoritmos são avaliados em tarefas de classificação de dígitos , caracteres , texto , imagens e de distribuições gaussianas . A avaliação_experimental  proposta neste trabalho é subdividida em quatro partes : ( 1 ) análise de melhor caso ; ( 2 ) avaliação da estabilidade dos classificadores semissupervisionados ; ( 3 ) avaliação do impacto da geração de grafos na classificação semissupervisionada ; ( 4 ) avaliação da influência dos parâmetros de regularização no desempenho de classificação dos classificadores semissupervisionados . Na análise de melhor caso , avaliam-se as melhores taxas de erro de cada algoritmo semissupervisionado combinado com os métodos de geração de grafos usando uma variedade de valores para o parâmetro de esparsificação , o qual está relacionado ao número de vizinhos de cada exemplo de treinamento . Na avaliação da estabilidade dos classificadores , avalia-se a estabilidade dos classificadores semissupervisionados combinados com os métodos de geração de grafos usando uma variedade de valores para o parâmetro de esparsificação . Para tanto , fixam-se os valores dos parâmetros de regularização ( quando existirem ) que geraram os melhores_resultados  na análise de melhor caso . Na avaliação do impacto da geração de grafos , avaliam-se os métodos de geração de grafos combinados com os algoritmos de aprendizado semissupervisionado usando uma variedade de valores para o parâmetro de esparsificação . Assim como na avaliação da estabilidade dos classificadores , para esta avaliação , fixam-se os valores dos parâmetros de regularização ( quando existirem ) que geraram os melhores_resultados  na análise de melhor caso . Na avaliação da influência dos parâmetros de regularização na classificação semissupervisionada , avaliam-se as superfícies de erro geradas pelos classificadores semissupervisionados em cada grafo e cada base de dados . Para tanto , fixam-se os grafos que geraram os melhores_resultados  na análise de melhor caso e variam-se os valores dos parâmetros de regularização . O intuito destes experimentos é avaliar o balanceamento entre desempenho de classificação e estabilidade dos algoritmos de aprendizado semissupervisionado baseado em grafos numa variedade de métodos de geração de grafos e valores de parâmetros ( de esparsificação e de regularização , se houver ) . A partir dos resultados obtidos , pode-se concluir que o grafo k- vizinhos mais próximos mútuo ( mutKNN ) pode ser a melhor opção dentre os métodos de geração de grafos de adjacência , enquanto que o kernel RBF pode ser a melhor opção dentre os métodos de geração de matrizes ponderadas . Em adição , o grafo mutKNN tende a gerar superfícies de erro que são mais suaves que aquelas geradas pelos outros métodos de geração de grafos de adjacência . Entretanto , o grafo mutKNN é instável para valores relativamente pequenos de k. Os resultados obtidos neste trabalho indicam que o desempenho de classificação dos algoritmos semissupervisionados baseados em grafos é fortemente influenciado pela configuração de parâmetros . Poucos padrões evidentes foram encontrados para auxiliar o processo de seleção de parâmetros . As consequências dessa instabilidade são discutidas neste trabalho em termos de pesquisa e aplicações 
 Na última_década  diversos algoritmos baseados em busca foram desenvolvidos para a geração de níveis em diferentes_tipos  de jogos . O espaço de busca para geração de níveis geralmente possui restrições , uma vez que a mecânica de um jogo define regras de factibilidade para os níveis . Em alguns métodos , a avaliação de factibilidade requer uma simulação com um agente inteligente que controla o jogo . Esse processo de avaliação geralmente possui ruído , causado por componentes aleatórios no simulador ou na estratégia do agente . Diversos trabalhos têm utilizado simulação como forma de avaliação de conteúdo , no entanto , nenhum deles discutiu profundamente a presença de ruído neste tipo de abordagem . Assim , esse trabalho apresenta um algoritmo genético capaz de gerar níveis factíveis que são avaliados por um agente inteligente em uma simulação ruidosa . O algoritmo foi aplicado a jogos de quebra-cabeças baseados em física com a mecânica do Angry Birds . Uma representação dos níveis em forma de indivíduos é introduzida , a qual permite que o algoritmo genético os evolua com características diferenciadas . O ruído na função de aptidão é tratado por uma nova abordagem , baseada em uma sistema de cache , que auxilia o algoritmo genético a encontrar boas soluções candidatas . Três conjuntos de experimentos foram realizados para avaliar o algoritmo . O primeiro compara o método de cache proposto com outros métodos de redução de ruído da literatura . O segundo mede a expressividade do algoritmo genético considerando as características estruturais dos níveis gerados . O último avalia os níveis gerados considerando aspectos de design ( como dificuldade , imersão e diversão ) , os quais são medidos por meio de questionários respondidos por jogadores humanos via Internet . Os resultados mostraram que o algoritmo genético foi capaz de gerar níveis distintos que são tão imersíveis quanto níveis produzidos manualmente . Além_disso  , a abordagem de cache lidou apropriadamente com o ruído nos cálculos de aptidão , permitindo uma correta evolução elitista 
 A robótica é uma área multidisciplinar que cresce continuamente com a contribuição do avanço científico e aumento frequente do poder_computacional  do hardware . As pesquisas em robótica estão divididas em diversas linhas de investigação . A visão_computacional  é uma das linhas de pesquisa de grande interesse devido à farta variedade de métodos e técnicas oferecidas . Um dos maiores desafios para os robôs é descobrir e analisar o ambiente em que estão inseridos . Dentre os principais sensores que podem ser utilizados , as câmeras digitais oferecem um bom benefício : podem ser leves , pequenas e baratas , características fundamentais para alguns robôs . Este trabalho propõe o desenvolvimento e análise de um sistema de visão_computacional  para rastrear veículos usando sistemas de detecção e classificação de segmentos em imagens . Para atingir os objetivos são investigados métodos de extração de informações das imagens , modelos de atenção visual e modelos de aprendizado bioinspirados para detecção e classificação de veículos . Para a tarefa de atenção visual foram utilizadas as técnicas de geração de mapas de saliência iNVT e VOCUS2 , enquanto que para classificação foi empregada a técnicas bag-of-features e finalmente , para o rastreamento do veículo especificado , durante seu percurso em uma rodovia , foi adotada a técnica Camshift com filtro de Kalman . O sistema desenvolvido foi implementado com um robô aéreo e testado com imagens reais contendo diferentes veículos em uma rodovia e os resultados de classificação e rastreamento obtidos foram muito satisfatórios 
 Diversas pesquisas e estudos científicos mostram que uma grande porcentagem das pessoas com deficiência é excluída do mercado de trabalho , sobretudo em países em desenvolvimento . Com o intuito de alterar essa realidade , destacam-se , entre outras medidas , a criação de Centros de Trabalho para Deficientes ( CTDs ) . Tais organizações empregam trabalhadores com deficiência em vários setores empresariais , dando-lhes oportunidades iniciais e preparando-os para que possam , mais tarde , ser inseridos no mercado de trabalho convencional . Vários destes centros operam linhas de produção , principal objeto de estudo desta tese . Nosso estudo é situado em uma etapa idealmente posterior aos CTDs , referente à inserção de trabalhadores com deficiência em linhas de produção convencionais . A demanda por estudos neste contexto tem crescido nos últimos_anos  , devido sobretudo a políticas corporativas de responsabilidade social e exigências legislativas , como a `` Lei das Cotas '' , presentes em diversos países . O planejamento da operação de linhas de produção na presença de trabalhadores com deficiência envolve uma série de desafios , devido à heterogeneidade entre trabalhadores , que faz com que o tempo de execução das tarefas seja dependente de cada indivíduo . Nos deparamos , assim , com um problema de dupla alocação , em que as variáveis de decisão determinam as tarefas a serem inseridas em estações e a alocação de trabalhadores para as mesmas , de modo a otimizar alguma medida de eficiência . O balanceamento de linhas de produção convencionais com uma parcela de trabalhadores com deficiência é denominado problema de balanceamento de linhas de produção e integração de trabalhadores ( ALWIBP , do inglês : assembly line worker integration and balancing problem ) , sendo um caso particular do problema de balanceamento de linhas de produção e designação de trabalhadores ( ALWABP , do inglês : assembly line worker assignment and balancing problem ) , cuja ocorrência é mais comum em linhas de CTDs . Nosso objetivo consiste em estudar formas eficientes de proporcionar a integração de trabalhadores com deficiência em linhas convencionais . Para tanto , abordamos variações do ALWIBP que consideram : ( i ) minimização de diferentes funções objetivo ( número de estações ou tempo de ciclo ) ; ( ii ) linha de produção com leiautes distintos ( simples ou em U ) ; ( iii ) incertezas quanto ao tempo de execução de cada tarefa ( abordagem robusta ) ; ( iv ) estratégias de rotação de tarefas ou alocação de trabalhadores com deficiência na linha com espaçamento regular . Para cada uma destas extensões , foram desenvolvidos formulações_matemáticas  , métodos de resolução e novos conjuntos de instâncias teste . Experimentos computacionais indicam possibilidades de adaptação de linhas de produção convencionais à inserção de trabalhadores com deficiência , a custos adicionais baixos ou quase nulos . Portanto , este trabalho oferece alternativas para uma maior flexibilidade na integração de pessoas com deficiência , tornando-os tão eficientes quanto qualquer outro trabalhador denominado `` convencional '' 
 Os sistemas computacionais_distribuídos  aplicados à computação_paralela  permitem uma melhor relação custo/benefício para a computação_paralela  . Esses sistemas oferecem a potência computacional adequada às aplicações que não necessitam de uma máquina maciçamente paralela , porém necessitam de uma potência computacional maior que uma máquina seqüencial pode oferecer . P.V.M . ( Parallel Virtual Machine ) e M.P.I . ( Message Passage Inteface ) são exemplos de ambiente de paralelos virtuais amplamente discutido na literatura . Tendo em vista a grande utilização desses ambientes tanto em nível acadêmico quanto em níveis comerciais e industriais , torna-se interessante a criação de uma ferramenta que apoie o desenvolvimento de programas para esses ambientes . Poucas são as ferramentas desse tipo que aparecem na literatura ; uma delas e que permite ser estendida para dar suporte a tais ambientes é a F.A.P.P . ( Ferramenta de Apoio à Programação Paralela ) . Dentro desse contexto , este trabalho apresenta a modelagem dos ambientes paralelos virtuais segundo a abordagem proposta na definição da F.A.P.P. , para que arcabouços de programas P.V.M . e M.P.I possam ser gerados . Essa ferramenta permite a utilização da computação_paralela  a um maior número de usuários , ou seja , auxiliando os iniciante na confecção dos programa e os experientes na manutenção , além de permitir maior produtividade . Foram realizados estudos visando a validação e a avaliação da ferramenta . Os resultados obtidos demonstram que a ferramenta possui comportamento estável e tem potencial para ser utilizada livremente em ambientes P.V.M . e 

 Muitos dos atuais sistemas computacionais de apoio ao ensino podem ser considerados parte de uma evolução que tem enfatizado a exploração de sistemas hipermídia em geral , e da Web em particular . A pesquisa associada ao trabalho aqui reportado tem como objetivo explorar as tecnologias de Hipermídia e Computer Supported Cooperative Work ( CSCW ) para viabilizá-las em um ambiente que suporte o acesso de alunos a hiperdocumentos de conteúdo didático de forma cooperativa ¾ o ambiente StudyConf . Para promover a interação entre alunos que visitam um determinado hiperdocumento , o StudyConf controla suas navegações e gera , dinamicamente , sessões de discussão entre os mesmos . O StudConf mantém o registro das discussões realizadas na forma de hiperdocumentos estruturados , os quais podem ser utilizados , por exemplo , para a geração cooperativa de documentos , conforme proposto em várias ferramentas Computer Supported Cooperative Learning ( CSCL ) . O trabalho aqui reportado colaborou , ainda , para o desenvolvimento de uma técnica que tem como objetivo orientar o projeto de aplicações_hipermídia  que manipulam informações na Web 
 Agentes móveis são programas que podem ser disparados de um computador ( cliente ) e transmitidos através de uma rede de comunicação para uma outra maquina ( servidor ) com o objetivo de executar tarefas a eles designadas . Esta dissertação apresenta uma avaliação de plataformas de desenvolvimento e servidores para agentes_móveis  . A tecnologia de agentes_móveis  tem sido alvo de grandes pesquisas , inclusive nos setores de Segurança da Informação e Comércio Eletrônico . Foram executados testes e com as informações foi feita uma análise comparativa , levando-se em consideração questões como características de desempenho dos agentes , desempenho e segurança . Para efetuar os testes foram necessários o entendimento do funcionamento do servidor e o desenvolvimento de seus agentes . Os testes de desempenho serviram para definir quais agentes são mais ágeis e quais são os gastos de processamento dos servidores . Já o teste de segurança teve a finalidade de classificar os servidores quanto à segurança . Os resultados obtidos serviram para indicar qual a melhor plataforma a ser utilizada no desenvolvimento do Sistema de Detecção de Intrusão ( SDI ) do ICMC . As plataformas que obtiveram destaques nos testes foram o ASDK 1.1 e Grasshopper . A plataforma escolhida para o SDI foi o ASDK 1.1 
 Este trabalho tem como objetivo desenvolver um método_numérico  para simular_escoamentos  incompressíveis , isotérmicos , confinados ou com superfícies_livres  , de fuidos viscoelásticos governados pelos modelos integrais de Maxwell e K-BKZ ( Kaye-Bernstein , Kearsley e Zapas ) . A técnica numérica apresentada é uma extensão do método GENSMAC ( Tomé McKee - J. Comp . Phys. , ( 110 ) , pp 171 -- 186 , 1994 ) para a solução das equações de conservação , juntamente com as equações constitutivas integrais de Maxwell e K-BKZ . As equações governantes são resolvidas pelo método de diferenças_finitas  em uma malha_deslocada  . O tensor de Finger , B_t ' ( t ) é calculado com base nas idéias do método de campos de deformação ( Peters et_al  . - J. Non-Newtonian Fluid Mech . ( 89 ) , de maneira que não há a necessidade de seguir a trajetória da partícula de fuido para descrever a história de deformação da partícula . Uma abordagem diferente para a discretização do instante passado é utilizada e o tensor de Finger e o tensor das tensões são calculados utilizando um método de segunda_ordem  . A validação do método_numérico  descrito nesse trabalho foi feita utilizando o escoamento em um canal bidimensional e a solução_numérica  obtida para a velocidade e para as componentes de tensão com o modelo de Maxwell foram comparadas com as respectivas soluções analíticas no estado estacionário , mostrando excelente concordância . Os resultados numéricos para a simulação do escoamento em uma contração planar 4 : 1 mostraram bons_resultados  , tanto qualitativos quanto quantitativos , quando comparados com os resultados experimentais de Quinzani et_al  . ( J. Non-Newtonian Fluid Mech . ( 52 ) , pp 1 ? 36 , 1994 ) . Além_disso  , utilizando os modelos Maxwel e K-BKZ , o escoamento em uma contração planar 4 : 1 foi simulado para vários números de Weissenberg e os resultados obtidos estão de acordo com os encontrados na literatura . Resultados_numéricos  de escoamentos com superfícies_livres  modelados pelas equações integrais de Maxwell e K-BKZ são apresentados . Em particular , a simulação_numérica  do jato oscilante para diferentes números de Weissenberg e diferentes números de Reynolds é apresentada 
 A interação entre instituições heterogêneas tem sido cada vez mais necessária para obter e disponibilizar informações e serviços para seus usuários internos e externos . Esta interação tem sido sustentada principalmente pelo uso das novas tecnologias da informação e comunicação . A interoperabilidade entre instituições heterogêneas garante esta interação e proporciona vários benefícios como , por exemplo , utilizar toda a plataforma legada das instituições e ainda permitir a interação entre os sistemas . Entretanto , para que esta interoperabilidade seja possível é necessária a definição de conceitos comuns que padronizam e orientam as interações entre as instituições . Através destes conceitos comuns , as instituições podem trocar informações entre si e ainda manter sua independência e as particularidades em seus sistemas internos . Em nosso trabalho , propomos um Modelo para Interoperabilidade entre Instituições Heterogêneas ( MIIH ) . A especificação das regras de interação e , especificamente , os protocolos de interoperabilidade entre as instituições são escritas usando JamSession , que é uma plataforma para a coordenação de serviços de software heterogêneos e distribuídos . O modelo também define uma arquitetura baseada em Artefatos do Conhecimento Institucionais para lidar com as conexões com os sistemas das instituições . Estes Artefatos do Conhecimento Institucionais são baseados no conceito geral de Artefatos do Conhecimento , ou seja , `` objetos que contêm e transmitem uma representação utilizável do conhecimento '' . Os Artefatos do Conhecimento Institucionais são padrões arquitetônicos recorrentes que são observados no projeto de mecanismos de interoperabilidade para conectar instituições heterogêneas e são usados como uma descrição de alto nível da arquitetura para um projeto de sistema . Eles funcionam como padrões arquiteturais pré-concebidos que norteiam e padronizam as interações e , portanto , a interoperabilidade organizacional e semântica entre as instituições . Os Artefatos do Conhecimento Institucionais são fundamentados sobre uma ontologia de conceitos relevantes para os serviços destas instituições , cujo nível de abstração pode variar , dependendo do nível de integração necessário para as instituições - quanto mais sofisticada a interação , mais detalhes devem ser representados explicitamente na ontologia . Os Artefatos do Conhecimento Institucionais implementados também se comunicam com a camada de interação com o usuário , baseada em mundos virtuais , para garantir a comunicação adequada com estes usuários . Além do modelo_conceitual  proposto , apresentamos como resultado deste trabalho , um exemplo de uso do MIIH no contexto das instituições relacionadas à herança cultural ( museus , galerias , colecionadores , etc. ) . Tendo reconhecido que este contexto dos museus é importante para toda a sociedade , verificamos mais profundamente o funcionamento dos museus e suas interações entre si e com seus usuários . Identificamos neste cenário a aplicação direta de nosso projeto , uma vez que a interoperabilidade entre os museus é fundamental para o desempenho de suas funções e a interoperabilidade com seus usuários define a razão de sua existência , conforme identificamos na definição de museu apresentada pela UNESCO . Este exemplo de uso é construído seguindo a metodologia_proposta  neste trabalho e serve para mostrar a utilização do nosso modelo no desenvolvimento de uma aplicação concreta para uso em instituições de arte e também por seus usuários 
 A Avaliação Heurística ( AH ) é um método popular de inspeção de usabilidade . Entretanto , seus resultados são dependentes da experiência dos avaliadores . Este estudo explorou e descreveu a diferença na qualidade de resultados ( relatórios ) de AH colaborativa conduzida por grupos de avaliadores de composição distinta , considerando diferentes quantidades de avaliadores experientes em cada grupo . Vinte e sete ( 27 ) avaliadores contribuíram voluntariamente com este estudo , nove ( 9 ) experientes e 18 novatos . Assim , foram organizados sete ( 7 ) grupos de AH , de acordo com quatro ( 4 ) níveis diferentes do fator presença de avaliador experiente , variando de nenhum experiente até três ( 3 ) avaliadores experientes no mesmo grupo . Cada grupo de avaliadores concordou em entregar seus relatórios de AH para este estudo . A partir de tais relatórios , foi conduzida uma análise comparativa baseada em métodos específicos da área , e também baseado em uma análise de agrupamento com base em medidas de similaridade . Como resultado , descreveu-se as medidas F ( F-measure ) referentes ao relatório de cada grupo respeitando critérios estritos e relaxados de comparação . Além disto , foram descritos os dendrogramas resultados das análises de agrupamento . Os resultados mostraram que a qualidade de relatórios de AH colaborativas conduzidas por avaliadores experientes e novatos juntos pode ser mais similar à qualidade de relatórios de AH tradicional conduzida por múltiplos avaliadores experientes ( Grupo Benchmark ) do que à qualidade de relatórios de AH colaborativa conduzida por grupos formados apenas por avaliadores novatos ( Grupo Baseline ) . Finalmente , discutiu-se resultados adicionais e implicações para pesquisas futuras na área 
 No contexto da Engenharia de Software para a Computação em Nuvem as pesquisas relacionadas ao tema são cada vez mais crescentes e investiga-se como a Computação em Nuvem influenciará no desenvolvimento de sistemas de uma maneira geral . A atividade de construir sistemas para nuvem é uma tarefa complexa , criar aplicações de múltiplas nuvens , sobretudo , no contexto do modelo de serviço Plataforma-como-um-Serviço ( PaaS ) , é ainda mais agravada devido especificidades de plataformas de nuvem que podem tornar a tarefa de desenvolvimento repetitiva , custosa e dependente de um provedor específico . As abordagens dirigidas por modelos ( MDE ) resolvem alguns desses problemas , elas propõem que a modelagem e mecanismos de transformação utilizados para gerar código a partir de modelos são uma melhor maneira de desenvolver sistemas de software , ao invés da codificação pura . Portanto , visando investigar como combinar os benefícios da Computação em Nuvem alinhados ao MDE , foi desenvolvida uma abordagem dirigida por modelos para desenvolvimento de aplicações multi-PaaS . Em direção a este objetivo foi realizado um Estudo de Caso em colaboração com uma empresa da indústria . Essa colaboração permitiu a criação de implementações de referencia que possibilitaram o desenvolvimento de uma Linguagem Específica de Domínio ( DSL ) e metaprogramas que compõem a abordagem . Para avaliar a abordagem desenvolvida foi realizado um Estudo de Caso . Os resultados mostram que MDE pode não só resolver o problema , mas trazer benefícios adicionais em relação a abordagens tradicionais de desenvolvimento de sistemas . Este trabalho explora esses benefícios , apresenta uma maneira de unir recursos heterogêneos de nuvem por meio de uma abordagem dirigida por modelos e aplicações orientadas a serviço 
 Arquiteturas de software e teste de software desempenham um papel essencial no processo de desenvolvimento de produtos de software de alta qualidade . Com base em um mapeamento sistemático , pode-se observar que as atividades de estabelecimento de arquiteturas de software não consideram representar informações de teste de software associadas às visões da arquitetura , em particular de arquiteturas de referência . Este trabalho tem por objetivo propor um processo que induza a representação de informações de teste de software no estabelecimento de arquiteturas de referência . Dessa forma , além de se abstrair a essência de um conjunto de arquiteturas de software de um domínio de aplicação , facilitando o projeto de novas arquiteturas por meio do reúso e padronização de elementos arquiteturais , promove-se também o reúso e a padronização de informações de teste . Este trabalho é realizado a partir do modelo RAModel e do processo ProSA-RA . O RAModel apresenta um conjunto de elementos essenciais para o projeto de arquiteturas de referência . O ProSA-RA por sua vez apresenta um processo que sistematiza o estabelecimento de arquiteturas de referência , considerando os elementos do RAModel . Em particular , propõe-se um processo denominado ProSA-RAT . Esse processo viabiliza a definição de processos de teste adequados a domínios de aplicação específicos promovendo o reúso e padronização de informações de teste nas etapas iniciais de processos de desenvolvimento baseados em arquitetura . Um estudo de viabilidade do ProSA-RAT foi conduzido e um exemplo no domínio de robótica é apresentado 
 Um espaço de tuplas tem como função criar uma abstração de memória_compartilhada  sobre um sistema distribuído . Por propiciar modelos de programação muito simples e com baixo acoplamento entre os elementos do sistema , espaços de tuplas têm sido empregados na construção de sistemas distribuídos complexos . O espaço de tuplas JavaSpaces é um dos mais populares espaços de tuplas para a linguagem Java . Ele tem como características relevantes a conformidade a objetos , a persistência e o emprego de transações 
 As atuais implementações de JavaSpaces apresentam restrições como : complexidade de configuração , limitação de alcance e não serem abertas . Por complexidade de configuração entende-se ter que usar boa parte da infra-estrutura Jini ( feita para facilitar o desenvolvimento e administração de sistemas distribuídos ) e o Remote Method Invocation ( mecanismo de chamadas remotas padrão no ambiente Java ) , mesmo quando eles seriam dispensáveis . Por `` limitação de alcance '' , entende-se não poder usar as implementações sobre redes amplas , como a Internet . Por não ser aberto entende-se que : ou o código fonte não está disponível ou o código fonte e o aplicativo são distribuídos por licenças de software proprietárias ou o uso do software requer algum componente proprietário 
 Um projeto de espaço de tuplas em conformidade com a especificação JavaSpaces e que busca contornar as restrições acima é apresentado neste trabalho . São destaques do projeto proposto 
 1 . Dispensar o Remote Method Invocation pois utiliza sockets diretamente 
 2 . Implementar a persistência sobre bases de dados relacionais 
 3 . Suscitar o emprego de um mecanismo direto para obtenção de proxies Jini 
 As características 1 e 3 simplificam a configuração do espaço de tuplas e viabilizam o seu emprego da Internet . A característica 2 viabiliza uma implementação baseada em software aberto . Um protótipo foi implementado para verificar as idéias propostas 
 A representação atributo-valor de documentos usada no processo de mineração de textos é uma estrutura adequada à maioria das tarefas de classificação e agrupamento de documentos . No contexto de algoritmos de aprendizado de máquina , a representação atributo-valor de documentos freqüentemente utiliza a abordagem bag-of-words . Essa abordagem é caracterizada pela alta dimensionalidade na representação dos dados , pois toda palavra presente no documento pode ser um possível atributo . Deve ser considerado , portanto , que uma boa representação de documentos tem uma influência fundamental no desempenho dos algoritmos de aprendizado ( supervisionado ou não supervisionado ) 

 Como uma das principais_contribuições  deste trabalho , é apresentada uma ferramenta para pré-processamento que eficientemente decompõe textos em palavras usando a abordagem bag-of-words , bem como o uso de métodos para reduzir a dimensionalidade da representação gerada . Essa ferramenta transforma os documentos em um formato acessível à maioria dos algoritmos de aprendizado , nos quais os dados são descritos como um vetor de dimensão fixa 

 A ferramenta computacional implementada , entre as diversas funcionalidades , reduz a dimensionalidade da representação de documentos com o objetivo de obter um melhor desempenho dos algoritmos de aprendizado de máquina utilizados . A escolha do algoritmo de aprendizado a ser utilizado , supervisionado e não supervisionado , é dependente do problema em questão . Algoritmos de aprendizado_supervisionado  podem ser aplicados a documentos rotulados , enquanto algoritmos de aprendizado não supervisionado são freqüentemente aplicados a dados não rotulados 

 No caso do aprendizado não supervisionado , para avaliar se um dado cluster corresponde a um certo conceito , neste trabalho é utilizada uma abordagem usando algoritmos de aprendizado indutivo para auxiliar na interpretação dos clusters . Nesta abordagem o interesse consiste em compreender como o sistema representa e raciocina sobre o conhecimento adquirido . Essa compreensão é necessária tanto para o usuário aceitar a solução gerada pelo sistema quanto para analisar o raciocínio utilizado 

 As redes complexas têm recebido um crescente_interesse  nas mais diversas_áreas  do conhecimento . Esse crescimento se deve principalmente a sua flexibilidade em modelar e simular estruturas topológicas que aparecem em nosso cotidiano . Na maioria das vezes , a caracterização das redes complexas é baseada em medidas básicas , como média dos graus , graus hierárquicos , coeficiente de aglomeração , entre outras . Muitas das medidas propostas são correlacionadas , implicando em redundância . Este trabalho propõe o uso das caminhadas determinísticas do turista como uma medida de representação robusta e eficiente de redes complexas . Nesta medida , caminhadas são iniciadas por exploradores que partem de um dos vértices da rede e em seguida , informações são extraídas sobre essas caminhadas . Experimentos foram realizados em redes complexas artificiais e em redes modelando imagens de textura . No reconhecimento de redes artificiais , o método proposto foi aplicado em quatro modelos de redes complexas teóricos : redes aleatórias , pequenomundo , livre de escala e geográficas . No reconhecimento de textura , o método foi avaliado em bancos de texturas sintéticas e reais ( texturas de folhas de plantas ) . Em ambas as aplicações , o método alcançou excelentes resultados comparados com o estado da 
 O agrupamento de dados é uma técnica fundamental em aplicações de diversos campos do mercado e da ciência , como , por exemplo , no comércio , na biologia , na psiquiatria , na astronomia e na mineração da Web . Ocorre que em um subconjunto desses campos , como engenharia industrial , ciências sociais , engenharia sísmica e recuperação de documentos , as bases de dados são usualmente descritas apenas pelas proximidades entre os objetos ( denominadas bases de dados relacionais ) . Mesmo em aplicações nas quais os dados não são naturalmente relacionais , o uso de bases relacionais permite que os dados em si sejam mantidos sob sigilo , o que pode ser de grande valia para bancos ou corretoras , por exemplo . Nesta dissertação é apresentada uma revisão de algoritmos de agrupamento de dados que lidam com bases de dados relacionais , com foco em algoritmos que produzem partições rígidas ( hard ou crisp ) dos dados . Particular ênfase é dada aos algoritmos_evolutivos  , que têm se mostrado capazes de resolver problemas de agrupamento de dados com relativa acurácia e de forma computacionalmente eficiente . Nesse contexto , propõe-se nesta dissertação um novo algoritmo_evolutivo  de agrupamento capaz de operar sobre dados relacionais e também capaz de estimar automaticamente o número de grupos nos dados ( usualmente desconhecido em aplicações práticas ) . É demonstrado empiricamente que esse novo algoritmo pode superar métodos tradicionais da literatura em termos de eficiência computacional e 
 Uma abordagem de teste_estrutural  de integração contextual para programas_OO  e OA escritos em Java e AspectJ é apresentada . A finalidade dessa abordagem é descobrir defeitos que possam existir nas interfaces entre uma determinada unidade ( método ou adendo ) e todas as outras que interagem diretamente com ela , bem como descobrir defeitos que possam ocorrer na hierarquia de chamadas dessas unidades . Para programas_OO  , esse tipo de teste envolve testar a interação entre métodos ; já para programas OA , o teste_estrutural  de integração nível um ( como também pode ser chamado ) deve considerar as interações método-método , método-adendo , adendo-adendo e adendo-método . Para efetuar o teste_estrutural  de integração nível um deve-se considerar todo o fluxo de execução ( fluxo de controle e de dados ) que ocorre entre uma unidade chamadora e as unidades que interagem diretamente com ela . Para isso é definido o grafo Def-Uso IN1P , que é uma abstração formada pela integração dos grafos Def-Uso Orientado a Aspectos ( AODU ) da unidade chamadora e das unidades que ela chama ou que a afeta . Além_disso  , são propostos três critérios para derivar os requisitos de teste , dois baseados em fluxo de controle ( todos-nós-integrados-N1 e todas-arestas-integradas-N1 ) e um baseado em fluxo de dados ( todos-usos-integrados-N1 ) . A ferramenta JaBUTi/AJ foi estendida para dar_apoio  à abordagem de teste de integração proposta . Exemplos são apresentados para ilustrar o uso da ferramenta para o teste de profundidade um e também seu uso no contexto de uma abordagem que leva em consideração também o teste de unidades e o teste baseado em conjuntos de 
 Esta tese de Doutorado tem como objetivo apresentar uma arquitetura para metaescalonamento verde com provisão de qualidade de serviço em uma nuvem privada denominada GreenMACC . Essa nova arquitetura oferece a automatização na escolha de políticas em quatro estágios de escalonamento de uma nuvem privada , permitindo cumprir a negociação que foi estabelecida com o usuário . Devido a essa função , é possível garantir que o GreenMACC se comporte seguindo os princípios da computação verde sem deixar de se preocupar com a qualidade do serviço . Nesta tese o GreenMACC é apresentado , detalhado , discutido , validado e avaliado . Com os resultados apresentados pode-se concluir que a arquitetura proposta mostrou-se consistente , permitindo a execução dos serviços requisitados com diversas políticas de escalonamento em todos os seus estágios . Além_disso  , demonstrou flexibilidade em receber novas políticas , com focos verde e de qualidade de serviço , e eficiência na escolha das políticas de escalonamento de acordo com a negociação feita com o usuário 
 Com o crescimento do volume e dos tipos de dados , a necessidade de analisar e entender o que estes representam e como estão relacionados tem se tornado crucial . Técnicas de visualização baseadas em projeções_multidimensionais  ganharam espaço e interesse como uma das possíveis ferramentas de auxílio para esse problema , proporcionando um forma simples e rápida de identificar padrões , reconhecer tendências e extrair características antes não óbvias no conjunto original . No entanto , a projeção do conjunto de dados em um espaço de menor dimensão pode não ser suficiente , em alguns_casos  , para responder ou esclarecer certas perguntas feitas pelo usuário , tornando a análise posterior à projeção crucial para a correta interpretação da visualização observada . Logo , a interatividade , aplicada à necessidade do usuário , é uma fator essencial para análise . Neste contexto , este projeto de mestrado tem como principal objetivo criar metáforas visuais baseadas em atributos , através de medidas estatísticas e artefatos para detecção de ruídos e grupos similares , para auxiliar na exploração e análise dos dados projetados . Além_disso  , propõe-se disponibilizar , em navegadores Web , as técnicas de visualização de dados multidimensionais desenvolvidas pelo Grupo de Processamento Visual e Geométrico do ICMC-USP . O desenvolvimento do projeto como plataforma Web inspira-se na dificuldade de instalação e execução que certos projetos de visualização possuem , como problemas causados por diferentes versões de IDEs , compiladores e sistemas operacionais . Além_disso  , o fato do projeto estar disponível online para execução tem como propósito facilitar o acesso e a divulgação das técnicas propostas para o público geral 
 Expressed Sequence Tags ( ESTs ) são amostras de trechos de genes , que funcionam como moldes na síntese de proteínas . Como a quantidade de ESTs coletados nos últimos_anos  é muito grande , o uso de computadores tornou-se imprescindível para a identificação de genes , proteínas e para a descoberta de genes homólogos . Este trabalho propõe uma metodologia e implementa uma ferramenta para a visualização de ESTs através de um grafo para auxiliar biólogos na exploração e na descoberta de conhecimento sobre estas seqüências . A metodologia inclui agrupamento usando um programa montador de seqüências e , conseqüentemente , a transformação dos grupos em nós de um grafo . O algoritmo BLAST é usado para procurar alinhamentos entre seqüências , representando-os posteriormente por arestas entre as seqüências mais similares . Para a visualização do grafo utilizamos e modificamos a ferramenta TG WikiBrowser conectada a um banco de dados . O resultado é uma ferramenta interativa baseada em código livre e robusto que funciona em ambientesWindows e Linux . Ela possibilita a fácil exploração do grafo , com diversas funcionalidades como , por exemplo : a expansão e filtragem do grafo , a busca por rótulos ou trechos de seqüências e a visualização detalhada de seqüências e grupos de seqüências . Com isso , os biólogos e especialistas em bioinformática ganham mais uma alternativa de investigação da 
 A adoção em larga_escala  de redes de computadores e gerenciadores de banco de dados contribuiu para o surgimento de sistemas de informação complexos . Atualmente , estes sistemas tornaram-se elementos essenciais na vida das pessoas , dando suporte a processos de negócio e serviços corporativos indispensáveis à sociedade , como automação bancária e telefonia . A utilização de componentes na estruturação destes sistemas promove maior qualidade e flexibilidade ao produto e agiliza o processo de desenvolvimento . Entretanto , para que estes benefícios sejam totalmente observados , é fundamental que os provedores de componentes de prateleira projetem especificações precisas , completas e consistentes . Geralmente , as especificações omitem ou negligenciam o comportamento dos componentes nas situações de falha . Desta forma , a utilização de componentes não confiáveis , cujos comportamentos não podem ser inteiramente previstos , compromete seriamente o projeto de sistemas tolerantes a falhas . Uma estratégia para a especificação de componentes tolerantes a falhas é informar a ocorrência de erros através de exceções e realizar a recuperação dos mesmos por rotinas de tratamento correspondentes . A especificação deve separar claramente o comportamento normal do excepcional , destinado à recuperação do erro . Entretanto , em sistemas concorrentes e distribuídos , a especificação apenas deste tratamento local não é suficiente . Uma exceção pode ser lançada em decorrência de erros sistêmicos ( i.e . problemas de rede ) que afetam todo o sistema . Assim , determinadas exceções devem ser tratadas em nível arquitetural , envolvendo os demais componentes no tratamento . O modelo_conceitual  de ações Atômicas Coordenadas ( ações CA - Coordinated Atomic actions ) , bastante aplicado na estruturação de sistemas tolerantes a falhas , define um mecanismo geral para a coordenação do tratamento excepcional dos componentes , que cooperam na execução das atividades e competem por recursos compartilhados . Portanto , o modelo de ações CA oferece uma solução potencialmente viável para a especificação do tratamento de exceções em nível arquitetural . Este trabalho propõe um framework para a especificação do tratamento de exceções em nível arquitetural , baseando-se no modelo de aninhamento de ações CA e utilizando a linguagem orientada a eventos CSP ( Communicating Sequential Processes ) . Sua principal característica é prover um protocolo padronizado para a coordenação do tratamento de exceções , que envolve a cooperação dos componentes do sistema . Além_disso  , é apresentada uma estratégia para a verificação formal dos sistemas na ferramenta FDR ( Failure Divergence Refinement ) , com base no modelo de refinamento por rastros 
 A segmentaçãoo de imagens é fundamental para a visão_computacional  . Com essa finalidade , a textura tem sido uma propriedade bastante explorada por pesquisadores . Porém , a existência de diversos métodos de extração de textura , muitas_vezes  específicos para determinadas aplicações , dificulta a implementação de sistemas de escopo mais geral . Tendo esse contexto como motivação e inspirado no sucesso dos sistemas de visão naturais e em sua generalidade , este trabalho propõe a combinação de métodos por meio da seleção de características baseada na saliência das sinapses de um perceptron multicamadas ( MLP ) . É proposto , também , um método alternativo baseado na capacidade do MLP de apreender textura que dispensa o uso de técnicas de extração de textura . Como principal_contribuição  , além da comparação da heurística de seleção proposta frente à busca exaustiva segundo o critério da distância de Jeffrey-Matusita , foi introduzida a técnica de Equalização da Entrada , que melhorou consideravelmente a qualidade da medida de saliência . É também apresentada a segmentação de imagens de cenas naturais , como exemplo de 
 Este trabalho_investiga  como o desenvolvimento de linhas de produtos de software pode ser beneficiado pela utilização da programação orientada a aspectos para reduzir o acoplamento e aumentar a coesão das features da linha de produtos . Como resultado dessa investigação , uma abordagem para desenvolvimento incremental de linhas de produtos baseado em aspectos é proposta . São apresentadas as etapas , atividades e artefatos dessa abordagem . Por ser uma abordagem incremental , reduz-se a carga de trabalho necessária no início da produção da linha de produtos . Isso é conseguido graças à utilização de aspectos . Com isso , tem-se as vantagens de linhas de produtos ao mesmo tempo amenizando a desvantagem do risco do alto investimento inicial não ter o retorno esperado . A abordagem foi proposta com base em práticas estabelecidas de desenvolvimento de linhas de produtos de software e no estudo das práticas atuais para análise e projeto orientado a aspectos . Foi dada ênfase à abordagem Tema , que é utilizada neste trabalho como parte do ferramental para análise e projeto . A abordagem desenvolvida especifica práticas desde a análise de domínio até a implementação . Os aspectos são tratados desde os estágios iniciais do desenvolvimento . Técnicas para implementação com orientação a aspectos são propostas . Um estudo de caso utilizando as linguagens Java e AspectJ é apresentado para ilustrar as idéias 
 Relações min-max são objetos centrais em otimização combinatória . Elas basicamente afirmam que , numa dada estrutura , o valor ótimo de um certo problema de minimização é igual ao valor ótimo de um outro problema de maximização . Relações desse tipo fornecem boas caracterizações e descrições poliédricas para diversos problemas importantes , além de geralmente virem acompanhadas de algoritmos eficientes para os problemas em questão . Muitas vezes , tais algoritmos eficientes são obtidos naturalmente das provas construtivas dessas relações ; mesmo quando isso não ocorre , essas relações revelam o suficiente sobre a estrutura combinatória dos problemas , levando ao desenvolvimento de algoritmos eficientes . O foco_principal  desta dissertação é o estudo dessas relações em grafos . Nossa ênfase é sobre grafos orientados . Apresentamos o poderoso arcabouço poliédrico de Edmonds e Giles envolvendo fluxos submodulares , bem como o algoritmo de Frank para um caso especial desse arcabouço : o teorema de Lucchesi-Younger . Derivamos também diversas relações min-max sobre o empacotamento de conectores , desde o teorema de ramificações disjuntas de Edmonds até o teorema de junções disjuntas de Feofiloff-Younger e Schrijver . Apresentamos também uma resenha completa sobre as conjecturas de Woodall e sua versão capacitada , conhecida como conjectura de Edmonds-Giles . Derivamos ainda algumas relações min-max clássicas sobre emparelhamentos , T-junções e S-caminhos . Para tanto , usamos um teorema de Frank , Tardos e Sebö e um arcabouço bastante geral devido a Chudnovsky , Geelen , Gerards , Goddyn , Lohman e Seymour . Ao longo do texto , ilustramos vários aspectos recorrentes , como o uso de ferramentas da combinatória poliédrica , a técnica do descruzamento , o uso de funções submodulares , matróides e propriedades de troca , bem como alguns resultados envolvendo subestruturas proibidas 
 A Teoria das equações_diferenciais  faz parte de uma área da Matemática muito rica em aplicações . Os métodos_numéricos  para a solução de equações_diferenciais  ordinárias são , da mesma forma que as próprias equações , fontes importantes de problemas a serem pesquisados . Como destaque tem-se os métodos multiderivadas de passo múltiplo , que são importantes na solução de problemas stiff . Os métodos_numéricos  mais conhecidos para a solução desses problemas são os BDF , que compõem , para L = 1 , a família dos métodos ( K , L ) de Brown . Algumas questões relacionadas à estabilidade dos métodos ( K , L ) ainda não foram solucionadas como , por exemplo , uma conjectura de Jeltsch . Para analisá-la , é necessário estudar o comportamento dos zeros dos polinômios característicos associados aos métodos ( K , L ) . Neste trabalho é apresentado um estudo sobre zeros de polinômios com o objetivo de demonstrar a validade da conjectura de Jeltsch para K ' < OU = ' 'K IND ; L ' . As regiões de estabilidade para alguns valores de K e L fixos são apresentadas e também é utilizada a teoria das order stars para mostrar algumas propriedades dos métodos ( K , L ) . Portanto , este trabalho apresenta um estudo sobre os métodos ( K , L ) de Brown e usa uma ferramenta pouco utilizada na literatura , que são as order stars , para demonstrar alguns 
 Neste trabalho , abordamos métodos de resolução para o problema de dimensionamento de lotes que contempla o planejamento da produção de vários produtos em múltiplas máquinas . A fabricação dos produtos consome tempo de produção e preparação de uma capacidade de produção limitada . A demanda pelos produtos é conhecida e pode ser atendida com atraso durante um horizonte de planejamento finito . O objetivo é minimizar a soma dos custos de produção , preparação para a produção , estoque dos produtos e atraso na entrega destes . Em uma primeira etapa , desenvolvemos uma busca tabu determinística baseada em outra , aleatória , que foi apresentada na literatura . Com isso , realizamos uma análise sobre a influência de fatores aleatórios sobre heurísticas do tipo busca tabu quando aplicadas ao problema estudado . Posteriormente , desenvolvemos um método híbrido baseado em busca tabu , branch-and-cut e programação linear para a resolução do problema . Nos testes computacionais realizados , o método proposto mostrou-se competitivo quando comparado a outras heurísticas apresentadas na 
 Apresentando um papel de destaque no cenário nacional e internacional , o eucalipto possui rápido crescimento , alta produtividade , ampla diversidade de espécies , grande capacidade de adaptação e é aplicado em diferentes processos industriais , como por exemplo , produção de madeira , celulose e papel . No Brasil existem extensas áreas plantadas , principalmente nos estados de Minas Gerais , São Paulo e Paraná . Entretanto , eucaliptos são suscetíveis a doenças e pragas , o que pode trazer grandes prejuízo aos produtores . Tendo em vista esse contexto , surge a necessidade de detectar e diagnosticar doenças prematuramente , permitindo um combate ais eficaz e preciso a essas patologias . Visto que as plantações de eucalipto cobrem áreas muito extensas , o uso de VANTs ( Veículos Aéreos Não-Tripulados ) pode agilizar o processo de monitoramento , uma vez que podem sobrevoar grandes distâncias em pouco tempo . Sendo_assim  , esse trabalho desenvolveu um sistema de diagnóstico automático de doenças de eucalipto . Baseando-se em técnicas de detecção de ataques digitais , o diagnóstico é feito comparando assinaturas espectrais de plantas doentes com assinaturas conhecidas armazenadas em uma base de dados seguindo um modelo de assinaturas espectrais inspirado em um modelo de assinaturas de ataque . O sistema foi desenvolvido e validade utilizando dados de espectroradiômetros , apresentando precisão de até 96 % em alguns_casos  
 A crescente presença da Internet nas tarefas do dia a dia , juntamente com a evolução dos sistemas computacionais , contribuiu para aumentar a exposição dos dados . Esse cenário evidencia a necessidade de sistemas de autenticação de usuários mais seguros . Uma alternativa para lidar com isso é pelo uso de sistemas biométricos . Contudo , características biométricas podem mudar com o tempo , o que pode afetar o desempenho de reconhecimento devido a uma referência biométrica desatualizada . Esse efeito pode ser chamado de template ageing na área de sistemas biométricos adaptativos ou de mudança de conceito em aprendizado de máquina . Isso levanta a necessidade de adaptar automaticamente a referência biométrica com o tempo , uma tarefa executada por sistemas biométricos adaptativos . Esta tese estudou sistemas biométricos adaptativos considerando biometria em um contexto de fluxo de dados . Neste contexto , o teste é executado em um fluxo de dados biométrico , em que as amostras de consulta são apresentadas uma após a outra para o sistema biométrico . Um sistema biométrico adaptativo deve então classificar cada consulta e adaptar a referência biométrica . A decisão de executar a adaptação é tomada pelo sistema biométrico . Dentre as modalidades biométricas , esta tese foca em biometria comportamental , em particular em dinâmica da digitação e em biometria por acelerômetro . Modalidades comportamentais tendem a ser sujeitas a mudanças mais rápidas do que modalidades físicas . Entretanto , havia poucos_estudos  lidando com sistemas biométricos adaptativos para modalidades comportamentais , destacando uma lacuna para ser explorada . Ao longo da tese , diversos aspectos para aprimorar o projeto de sistemas biométricos adaptativos para modalidades comportamentais em um contexto de fluxo de dados foram discutidos : proposta de estratégias de adaptação para o algoritmo de classificação imunológico Self-Detector , combinação de modelos genuíno e impostor no framework do Enhanced Template Update e aplicação de normalização de scores em sistemas biométricos adaptativos . Com base na investigação desses aspectos , foi observado que a melhor escolha para cada aspecto estudado dos sistemas biométricos adaptativos pode ser diferente dependendo do conjunto de dados e , além disso , dependendo dos usuários no conjunto de dados . As diferentes características dos usuários , incluindo a forma como as características biométricas mudam com o tempo , sugerem que as estratégias de adaptação deveriam ser escolhidas por usuário . Isso motivou a proposta de um sistema biométrico adaptativo modular , chamado ModBioS , que pode escolher cada um desses aspectos por usuário . O ModBioS é capaz de generalizar diversos sistemas baseline e propostas apresentadas nesta tese em um framework modular , juntamente com a possibilidade de atribuir estratégias de adaptação diferentes por usuário . Resultados experimentais mostraram que o sistema biométrico adaptativo modular pode superar diversos sistemas baseline , enquanto que abre um grande número de oportunidades para trabalhos futuros 
 Uma etapa de suma importância na análise automática de imagens é a segmentação , que procura dividir uma imagem em regiões cujos pixels exibem um certo grau de similaridade . Uma característica que provê similaridade entre pixels de uma mesma região é a textura , formada geralmente pela combinação aleatória de suas intensidades . Muitos trabalhos vêm_sendo  realizados com o intuito de estudar técnicas não-supervisionadas de segmentação de imagens por modelos estocásticos , definindo texturas como campos aleatórios de Markov . Um método com esta abordagem que se destaca é o EM/MPM , um algoritmo iterativo que combina a técnica EM para realizar uma estimação de parâmetros por máxima_verossimilhança  com a MPM , utilizada para segmentação pela minimização do número de 
 erroneamente classificados 

 Este trabalho desenvolveu um estudo sobre a modelagem e a implementação do algoritmo EM/MPM , juntamente com sua abordagem multiresolução . Foram propostas uma estimação inicial de parâmetros por limiarização e uma combinação com o algoritmo de Annealing . Foi feito também um estudo acerca da validação de classes , ou seja , a busca pelo número de regiões diferentes na imagem , mostrando as principais técnicas encontradas na literatura e propondo uma nova abordagem , baseada na distribuição dos níveis de cinza das classes . Por fim , foi desenvolvida uma extensão do modelo para a segmentação de malhas em duas e três dimensões 
 Técnicas de Aprendizado de Máquina não-simbólicas , como Redes_Neurais  Artificiais , Máquinas de Vetores de Suporte e combinação de classificadores têm mostrado um bom_desempenho  quando utilizadas para análise de dados . A grande limitação dessas técnicas é a falta de compreensibilidade do conhecimento armazenado em suas estruturas internas . Esta Tese apresenta uma pesquisa realizada sobre métodos de extração de representações compreensíveis do conhecimento armazenado nas estruturas internas dessas técnicas não-simbólicas , aqui chamadas de caixa preta , durante seu processo de aprendizado . A principal_contribuição  desse trabalho é a proposta de um novo método pedagógico para extração de regras que expliquem o processo de classificação seguido por técnicas não-simbólicas . Esse novo método é baseado na otimização ( maximização ) da similaridade entre rankings de classificação produzidos por técnicas de Aprendizado de Máquina simbólicas e não simbólicas ( de onde o conhecimento interno esta sendo extraído ) . Experimentos foram realizados com vários conjuntos de dados e os resultados obtidos sugerem um bom potencial para o método 
 Máquinas de Estados_Finitos  , além de suas inúmeras aplicações , são amplamente utilizadas na Engenharia de Software para modelar especificações de sistemas . Nesses modelos , projetistas podem inserir , inadvertidamente , estados redundantes , ou seja , que exibem o mesmo comportamento . A eliminação desses estados traz diversos benefícios para as atividades que utilizam o modelo , como menor complexidade e menos recursos físicos para implementação . O processo de eliminação desses estados é denominado minimização , e pode ser realizado em tempo polinomial para máquinas completamente especificadas . Por outro_lado  , a minimização de máquinas parciais , cuja especificação não cobre todo o domínio de entrada , somente pode ser obtida em tempo polinomial com o uso de abordagens não determinísticas , ou seja , trata-se de um problema NP-Completo . Este trabalho apresenta uma estratégia para a minimização de máquinas de estados finitos parciais que faz o uso de heurísticas e otimizações para tornar o processo mais eficiente . Visando mensurar tal ganho de eficiência , foram realizados_experimentos  , nos quais os tempos de execução de uma implementação do método proposto foram medidos , juntamente com os tempos de implementações de dois outros métodos conhecidos . Os resultados mostraram vantagens significativas de performance para o novo método em relação aos métodos 
 A esteganografia em vídeos digitais possibilita o ocultamento de um grande volume de informações quando comparada a técnicas em imagens . Contudo , esta tarefa não é trivial quando aplicada a vídeos comprimidos , pois a inserção de informações ocultas pode adicionar ruído dificultando a perfeita recuperação das mesmas durante a decodificação . Este trabalho apresenta uma técnica para esteganografia em vídeos comprimidos , denominada MP4Stego , a qual explora estruturas e tecnologias do padrão de vídeo MPEG-4 de modo a proceder recuperação de informações sem perda e apresentar maior capacidade de inserção de dados ocultos . Entre os benefícios da técnica estão : A capacidade de ocultamento de grande volume de dados ; a capacidade de reprodução do vídeo contendo dados ocultos em players não especializados ( ajudando a dar a ilusão de que se trata de um vídeo comum ) , sua imunidade , até o momento , as técnicas de 
 Revisão de Crenças aborda o problema de como alterar estados epistêmicos , normalmente representados na literatura como conjuntos de sentenças lógicas . Resultados teóricos sólidos foram consolidados com o paradigma AGM , que lida com teorias ( conjuntos de sentenças logicamente fechados ) . Depois disso , a teoria foi estendida para bases de crenças , isto é , conjuntos arbitrários de sentenças . Apesar de todo esse arcabouço teórico , pesquisadores de IA enfrentam sérias dificuldades ao tentar implementar sistemas de revisão de crenças . Uma das maiores complicações é o fecho exigido pela teoria AGM , que não pode ser facilmente computado . Mesmo bases de crenças , que não exigem fechamento , parecem ser impróprias para fins práticos , pois suas alterações são geralmente muito rígidas ( dependentes de sintaxe ) . Algumas operações , conhecidas como pseudo-contrações , estão no meio do caminho entre mudanças para conjuntos de crenças e mudanças para bases de crenças . Nesse trabalho , propomos uma nova operação de pseudo-contração , estudamos suas propriedades e a caracterizamos . Também encontramos conexões entre esse operador e algumas outras pseudo-contrações 
 No contexto de veículos_autônomos  , a localização é um dos componentes fundamentais , pois possibilita tarefas como ultrapassagem , direção assistida e navegação_autônoma  . A presença de edifícios e o mau tempo interferem na recepção do sinal de GPS que consequentemente dificulta o uso de tal tecnologia para a localização de veículos dentro das cidades . Alternativamente , a localização com suporte aos mapas vem_sendo  empregada para estimar a posição sem a dependência do GPS . Nesta solução , a posição do veículo é dada pela região em que ocorre a melhor correspondência entre o mapa do ambiente e a leitura do sensor . Antes da criação dos mapas , características dos ambientes devem ser extraídas a partir das leituras dos sensores . Dessa forma , guias e sinalizações horizontais têm sido largamente utilizados para o mapeamento . Entretanto , métodos de mapeamento urbano geralmente necessitam de repetidas leituras do mesmo lugar para compensar as oclusões . A construção de representações precisas dos ambientes é essencial para uma adequada associação dos dados dos sensores como mapa durante a localização . De forma a evitar a necessidade de um processo manual para remover obstáculos que causam oclusão e áreas não observadas , propõe-se um método de localização de veículos com suporte aos mapas construídos a partir de observações parciais do ambiente . No sistema de localização proposto , os mapas são construídos a partir de guias e sinalizações horizontais extraídas a partir de leituras de um sensor multicamadas . As guias podem ser detectadas mesmo na presença de veículos que obstruem a percepção das ruas , por meio do uso de regressão robusta . Na detecção de sinalizações horizontais é empregado o método de limiarização por Otsu que analisa dados de reflexão infravermelho , o que torna o método insensível à variação de luminosidade . Dois tipos de mapas são empregados para a representação das guias e das sinalizações horizontais : mapa de grade de ocupação ( OGM ) e mapa de ocupação por processo Gaussiano ( GPOM ) . O OGM é uma estrutura que representa o ambiente por meio de uma grade reticulada . OGPOM é uma representação contínua que possibilita a estimação de áreas não observadas . O método de localização por Monte_Carlo  ( MCL ) foi adaptado para suportar os mapas construídos . Dessa forma , a localização de veículos foi testada em MCL com suporte ao OGM e MCL com suporte ao GPOM . No caso do MCL baseado em GPOM , um novo modelo de verossimilhança baseado em função densidade probabilidade de distribuição multi-normal é proposto . Experimentos foram realizados em ambientes urbanos reais . Mapas do ambiente foram gerados a partir de dados de laser esparsos de forma a verificar a reconstrução de áreas não observadas . O sistema de localização foi avaliado por meio da comparação das posições estimadas comum GPS de alta precisão . Comparou-se também o MCL baseado em OGM com o MCL baseado em GPOM , de forma a verificar qual abordagem apresenta melhores_resultados  
 Teoricamente , o Alinhamento Estratégico pode ser obtido com um bom Gerenciamento de Processos de Negócio ( BPM na sigla em inglês ) , porém a relação entre esses temas é , na maioria das vezes , assumida implicitamente e pouco elaborada , tornando-se difícil a visualização e vínculo entre as estratégias e os modelos de processos detalhados , sendo necessário empreender pesquisas na direção de uma união efetiva dos dois temas . O presente_trabalho  desenvolveu um metamodelo multinível , para ligar os objetivos estratégicos com os processos de negócio da organização através da modelagem de processos de negócio . O trabalho conceitual foi acrescido por uma ferramenta de suporte à modelagem que permite a geração de modelos com base no metamodelo multinível e também a representação dos modelos em diferentes formas de abstração . A abordagem foi aplicada em uma grande multinacional e posteriormente foi conduzida uma avaliação_experimental  no qual foram realizadas entrevistas com funcionários de diferentes níveis hierárquicos para validar a eficiência e eficácia do trabalho desenvolvido . Como resultado foi possível constatar não apenas a efetividade em alcançar os objetivos propostos como também a facilidade na utilização e compreensão da abordagem em níveis devido a simplicidade na representação dos elementos e seus possíveis 
 O atrito em mancais hidrodinâmicos é uma fonte importante de perdas em motores de combustão ( [ 69 ] ) . As condições extremas de carga induzem contato entre as superfícies dos mancais . Em tais condições não somente a macro-geometria do mancal é relevante , mas também são as escalas menores da superfície as que determinam o desempenho do mancal . A possibilidade de fabricar superfícies com detalhes na escala do micrometro ( [ 57 ] ) deixou em aberto a questão de se o atrito pode ser reduzido por meio de micro-texturas , até agora com resultados mistos . Este trabalho centra-se no desenvolvimento de métodos_numéricos  eficientes para resolver problemas de lubrificação na escala da rugosidade das superfícies . Devido às altas velocidades e a forma convergente-divergente dos mancais hidrodinâmicos o fluido cavita . Para tratar o fenômeno de cavitação empregamos o modelo de Elrod-Adams , um modelo conservativo que tem demonstrado em cuidadosos testes numéricos ( [ 12 ] ) e experimentais ( [ 119 ] ) ser essencial para obter resultados físicos significativos . Outro aspecto revelante do modelado é que os efeitos inerciais do mancal são considerados , o que é necessário para simular corretamente texturas em movimento . Como aplicação , os efeitos de micro-texturizar a superfície móvel do mancal foram estudados . Valores realistas são assumidos nos parâmetros físicos que definem o problema . Foram realizados extensivos estudos no regime de lubrificação hidrodinâmica . Também foram executadas simulações convergidas em malha , levando em conta a topografia real de superfícies medidas , e as hipóteses de lubrificação para superfícies rugosas foram avaliadas 
 Nos últimos_anos  , o estudo de métodos de animação de escoamento de fluidos tem sido uma área de intensa pesquisa em Computação_Gráfica  . O principal objetivo desse projeto é desenvolver novas técnicas em GPGPU baseadas na arquitetura CUDA para simular o escoamento de fluidos não-newtonianos , tais como fluidos viscoplásticos e viscoelásticos . Ao invés dos tradicionais métodos com malha diferenças_finitas  e elementos finitos , essas técnicas são baseadas em uma discretização lagrangeana das equações de governo desses fluidos através do método sem malha conhecido como SPH ( Smoothed Particle Hydrodynamics 
 A Web apresenta um conteúdo extenso de informações disponibilizado a uma população diversificada de pessoas , as quais podem apresentar as mais diferentes habilidades e exigências . Dessa maneira , garantir a acessibilidade a todo usuário é uma tarefa_difícil  , mesmo existindo um conjunto extenso de recomendações disponibilizadas pelo World_Wide  Web Consortium ( W3C ) . Assim , são propostas diferentes ferramentas para avaliação de acessibilidade , que contrapõem os artefatos às diretrizes com a finalidade de obter resultados automatizados , produzindo testes e gerando vários dados , como a localização do problema no código e as falhas especificadas . Para facilitar o processamento desses dados , mediante a disponibilização de uma linguagem comum , o W3C desenvolveu a Evaluation and Report Language ( EARL ) . Dados os problemas acerca das dificuldades em se garantir a acessibilidade para os diferentes perfis de usuários e a necessidade da interpretação dos relatórios em EARL , como colaboração para a avaliação manual , indispensável no contexto de testes de acessibilidade , neste trabalho é proposto um apoio por meio de um Ambiente para Análise de Avaliações de Acessibilidade e Usabilidade na Web ( A4U ) . A partir do estudo de caso realizado , pôde-se validar o ambiente A4U desenvolvido , o qual inclui relatórios de avaliações semiautomáticas , para que o desenvolvedor possa interpretá-los e prosseguir com a avaliação manual de acessibilidade e usabilidade . No âmbito do apoio desenvolvido , foram considerados os avanços em acessibilidade , usabilidade , a correlação entre os dois conceitos e a colaboração no desenvolvimento de uma ferramenta denominada AccessibilityUtil1 , a qual se destina ser uma fonte de práticas de acessibilidade advindas de experiências de desenvolvedores , mediante colaboração em um ambiente Web , relacionando-as com as diretrizes de acessibilidade do W3C . A presente pesquisa contribuiu para a consolidação das questões de acessibilidade e usabilidade , a partir do desenvolvimento do A4U , que viabiliza a avaliação humana de acessibilidade e usabilidade , e a inserção de resultados de avaliações gerados por ferramentas semiautomáticas , conduzindo o avaliador a produzir melhorias em ambas as 
 A necessidade de evolução de sistemas legados tem aumentado significativamente com o surgimento de novas tecnologias . Para apoiar essa tendência , diversos métodos de reengenharia têm sido_propostos  . No entanto , poucos possuem apoio computacional efetivo , alguns utilizam padrões de projeto ou padrões específicos de reengenharia , e nenhum utiliza framework baseado em linguagem de padrões . Este trabalho está inserido no domínio de Sistemas de Informação . Propõe a elaboração de um arcabouço de reengenharia ágil baseado em framework , que realiza a engenharia_reversa  do sistema legado com o apoio de linguagem de padrões de análise , fornecendo entendimento e documentação necessários para instanciar o framework . O entendimento do sistema legado também é apoiado pela sua execução , por meio de casos de teste . Esses casos de teste são utilizados posteriormente para validar o sistema alvo . O framework , cuja construção é baseada em linguagem de padrões , é utilizado para obter o projeto e a implementação do sistema alvo . Para permitir a reengenharia com o apoio do arcabouço definido , um processo ágil de reengenharia foi criado . Como no desenvolvimento de software , grande parte do tempo da reengenharia é despendido com atividades de VV & T . Para minimizar esse problema , uma abordagem de reúso de teste é proposta . Essa abordagem agrega recursos de teste aos padrões da linguagem de padrões de análise , permitindo o reúso , não somente das soluções de análise , como também dos recursos de testes associados . O uso de framework na reengenharia de software colabora para a sua evolução , pois o domínio ao qual pertence pode evoluir , já que nem todos os requisitos do domínio do framework podem ter sido elicitados durante o seu desenvolvimento . Assim , nesta tese é proposto também um processo de evolução de frameworks de aplicação . Os processos e a abordagem propostos são associados ao arcabouço definido para apoiar sua efetividade . Além_disso  , para avaliar o processo ágil de reengenharia , que fornece reúso em diversos níveis de abstração , um pacote de experimentação também é parcialmente definido . Estudos de caso e exemplos de uso foram conduzidos com os produtos definidos . Ressalta-se que outros estudos devem ser conduzidos para permitir a determinação de resultados com significância estatística 
 A popularização das redes de computadores , o aumento da capacidade computacional e sua utilização para produção musical despertam o interesse na utilização de computadores para comunicação síncrona de conteúdo musical . Esta comunicação pode permitir um novo nível de interatividade entre máquinas e pessoas nos processos de produção musical , incluindo a distribuição de atividades , pessoas e recursos em um ambiente computacional em rede . Neste contexto , este trabalho apresenta uma solução para comunicação síncrona de fluxos de áudio e MIDI em redes de computadores . Além de permitir a comunicação , a solução proposta simplifica a conexão de recursos musicais e permite a integração de sistemas heterogêneos , como diferentes sistemas operacionais , arquiteturas de áudio e formatos de codificação , de forma transparente em um ambiente distribuído . Como meio para alcançar esta solução , mapeamos requisitos e características desejáveis para este domínio de aplicação , a partir da interação com músicos e da análise de ferramentas relacionadas . Com base nestes requisitos e características projetamos uma arquitetura de sistema para o domínio específico de comunicação síncrona de conteúdo musical . Utilizando esta arquitetura como referência , implementamos uma biblioteca que compreende as funcionalidades essenciais para este domínio específico . A fim de integrar esta biblioteca com diferentes bibliotecas de áudio e MIDI , desenvolvemos um conjunto de ferramentas que correspondem aos requisitos propostos e que permite aos usuários a utilização de conexões de rede em diversas ferramentas musicais 
 Os efeitos dos dados defeituosos sobre os resultados dos processos analíticos são notórios . Aprimorar a qualidade dos dados exige estabelecer alternativas a partir de vários métodos , técnicas e procedimentos disponíveis . O processo de Avaliação da Qualidade dos Dados - pAQD - provê relevantes insumos na definição da alternativa mais adequada por meio do mapeamento dos defeitos nos dados . Relevantes abordagens computacionais apoiam esse processo . Tais abordagens utilizam métodos quantitativos ou baseados em asserções que usualmente restringem o papel humano a interpretação dos seus resultados . Porém , o pAQD depende do conhecimento do contexto dos dados visto que é impossível confirmar ou refutar a presença de defeitos baseado exclusivamente nos dados . Logo , a supervisão humana é essencial para esse processo . Sistemas de visualização pertencem a uma classe de abordagens supervisionadas que podem tornar visíveis as estruturas dos defeitos nos dados . Apesar do considerável conhecimento sobre o projeto desses sistemas , pouco existe para o domínio da avaliação visual da qualidade dos dados . Isto posto , este trabalho apresenta duas contribuições . A primeira reporta uma taxonomia que descreve os defeitos relacionados aos critérios de qualidade da acuracidade , completude e consistência para dados estruturados e atemporais . Essa taxonomia seguiu uma metodologia que proporcionou a cobertura sistemática e a descrição aprimorada dos defeitos em relação ao estado-da-arte das taxonomias . A segunda contribuição reporta relacionamentos entre propriedades-defeitos que estabelecem que certas propriedades visuais-interativas são mais adequadas para a avaliação visual de certos defeitos em dadas resoluções de dados . Revelados por um estudo de caso múltiplo e exploratório , esses relacionamentos oferecem indicações que reduzem a subjetividade durante o projeto de sistemas de visualização de apoio a avaliação visual da qualidade dos dados 
 Nesta tese são investigados três problemas básicos em aprendizado_supervisionado  : seleção de atributos , composição de atributos e combinação de classificadores simbólicos 
 A seleção de atributos é uma atividade de pré-processamento de dados que seleciona um subconjunto de atributos do conjunto original de exemplos . Existem , basicamente , três abordagens que são empregadas para a seleção de atributos : embutida , filtro e wrapper ; as duas últimas pesquisadas neste trabalho . Os experimentos_realizados  , utilizando diversos indutores e conjuntos de exemplos , para avaliar as abordagens filtro e wrapper nos permitem concluir que o uso de filtros deve ser considerado antes de se cogitar a utilização de wrappers , no caso de existirem muitos atributos para descrever os exemplos . Sob a perspectiva de compreensibilidade do conhecimento induzido , a análise sobre o impacto da seleção de atributos em um classificador simbólico mostrou um aumento do número de regras e do número de condições por regra 
 A composição de atributos , também conhecida como indução construtiva , é outra atividade de pré-processamento de dados . Dentre as várias abordagens de composição de atributos ( guiada por dados , por hipótese , por conhecimento e multi-estratégia ) , nesta tese é proposta uma metodologia para composição de atributos guiada pelo conhecimento . Os resultados dos experimentos_realizados  utilizando a metodologia_proposta  mostram que , mesmo com o auxílio do usuário/especialista , é difícil construir atributos derivados que sejam realmente relevantes para aprender o conceito embutido nos conjuntos de exemplos analisados de repositórios ( naturais ) , os quais , muitas_vezes  , já foram pré-processados . Esse fato foi confirmado , por um trabalho posterior , com dados do mundo_real  , no qual a metodologia_proposta  mostrou seu potencial 
 A combinação de classificadores , simbólicos ou não , é uma atividade de mineração de dados . Na realidade , uma das preocupações do Aprendizado de Máquina simbólico é que os classificadores induzidos devem ser fáceis de serem compreendidos pelos seres_humanos  . Para isso , deve-se escolher o indutor com bias mais adequado para cada tipo de situação , já que pesquisas mostraram que não existe o 'melhor ' indutor para todos os domínios . Aliada a essa escolha , é possível fazer uso de vários classificadores , combinando-os num único classificador final , formando um ensemble . Os ensembles possuem a tendência de melhorar o desempenho na classificação de exemplos não vistos durante o processo de aprendizado . Entretanto , o emprego de ensembles dificulta a compreensão humana sobre o comportamento do classificador final , já que ele deixa de ser simbólico , mesmo assumindo que cada classificador individual que o compõe seja simbólico . Na realidade , a combinação de classificadores simbólicos - provenientes de diferentes indutores - em um classificador final também simbólico é um tópico novo de pesquisa , ainda com poucos resultados divulgados . Com o objetivo de preencher essa lacuna , é proposto e desenvolvido neste trabalho o sistema Xruler . Para isso , inicialmente foi definido o formato padrão de regras PBM , o qual fornece uma perspectiva unificada sob a qual todo classificador simbólico pode ser convertido e analisado 
 Dentre outros componentes , o sistema Xruler possui um algoritmo de cobertura que pode ser aplicado ao conjunto de regras induzidas por diversos indutores para se obter um classificador simbólico final . Nos experimentos_realizados  com o sistema Xruler os resultados obtidos mostraram aumento da precisão e redução do número de regras . Sob o aspecto sintático das regras , isso pode ser considerado um avanço no sentido de uma maior compreensibilidade por seres_humanos  do conjunto final de regras 
 Muitas relações min-max em otimização combinatória podem ser demonstradas através de total dual integralidade de sistemas_lineares  . O conceito algébrico de bases de Hilbert foi originalmente introduzido com o objetivo de melhor compreender a estrutura geral dos sistemas totalmente dual integrais . Resultados apresentados posteriormente mostraram que bases de Hilbert também são relevantes para a otimização combinatória em geral e para a caracterização de certas classes de objetos discretos . Entre tais resultados , foram provadas , a partir dessas bases , versões do teorema de Carathéodory para programação inteira . Nesta dissertação , estudamos aspectos estruturais e computacionais de bases de Hilbert e relações destas com programação inteira e otimização combinatória . Em particular , consideramos versões inteiras do teorema de Carathéodory e conjecturas relacionadas 
 Durante o processo de desenvolvimento de software , uma grande quantidade de documentos é gerada com o propósito de registrar as experiências e as decisões relacionadas ao projeto de software . Apesar do esforço empregado na documentação de tais informações , muitas_vezes  esses documentos não contêm informações suficientes e necessárias para o completo entendimento do software , para a reutilização das experiências adquiridas e a recuperação do processo de tomada de decisão . De maneira geral , apenas as decisões finais a respeito do projeto são documentadas . O Design Rationale ( DR ) consiste das informações adicionais aos documentos padrões em um processo de desenvolvimento de software , facilitando sua compreensão , manutenção e reuso . Na literatura , muitas pesquisas referem-se aos problemas relacionados à atividade de captura de DR , principalmente no que diz_respeito  à sobrecarga de trabalho durante o momento de design . O desenvolvimento de mecanismos que facilitem a captura de Design Rationale durante a elaboração de artefatos de software é ainda um desafio . No contexto de ferramentas CASE ( Computer Aided Software Engineering ) , cuja utilização enfrenta grande resistência por parte de seus usuários ( desenvolvedores ) , torna-se imprescindível a aplicação de técnicas para garantir a máxima usabilidade dessas ferramentas , de forma a minimizar a resistência à sua utilização . O paradigma da computação ubíqua trouxe grandes mudanças ao desenvolvimento de aplicações da Ciência da Computação , visto que estas aplicações são transparentes , apresentam um comportamento contínuo e ciente de contexto , e visam tornar a interação com o usuário a mais natural possível . Diante desse contexto , a adoção de mecanismos de computação ubíqua na atividade de captura de DR torna-se uma abordagem de interesse científico . O uso de mecanismos de computação ubíqua faz com que a captura das informações e decisões relacionadas ao projeto de software seja realizada de forma mais natural , reduzindo a sobrecarga do uso de uma ferramenta que necessite de tempo adicional para o armazenamento do DR , seja durante o processo de tomada de decisões ou depois do mesmo . Assim sendo , o trabalho realizado neste projeto de mestrado consistiu na reengenharia de uma ferramenta de suporte a DR e à sua integração com um editor gráfico que permite a escrita manual e oferece um serviço de reconhecimento de escrita , de modo a prover uma maneira mais flexí ? vel para a entrada de dados e que pode ser utilizada em dispositivos com diferentes tamanhos e características , tais como Tablet PCs e lousas 
 Diariamente , novos sistemas ubíquos são desenvolvidos e integrados ao nosso cotidiano , cuja presença às vezes não é percebida pelos seus usuários . O objetivo deste trabalho é investigar as questões de segurança e privacidade existentes nos paradigmas de computação ubíqua e ciente de contexto . Utilizando um protótipo de TV interativa - TV-I ( Goularte , 2003 ) - que possui características de mobilidade e ciência de contexto , este trabalho define um conjunto de requisitos mínimos para a garantia de segurança e privacidade em sistemas ubíquos . É também objetivo deste trabalho utilizar informações contextuais para gerenciamento dinâmico de requisitos de segurança e privacidade por meio de políticas expressas em linguagem EPAL . Para validar os resultados desta pesquisa , foi implementado o Gerente de Segurança como um serviço do protótipo TV-I . A principal característica desse gerente é controlar o acesso a informações pessoais por meio de informações de contexto fornecidas pelo Gerente de Contexto , de autoria de Santos ( 2004 
 Sistemas Gerenciadores de Bases de Dados Relacionais ( SGBDR ) são capazes de lidar com um alto volume de dados . As consultas nestes sistemas são realizados a partir da relação de ordem total , domínio sob o qual estão definidos dados simples como números ou strings , por exemplo . No caso de dados complexos , como imagens médicas , áudio ou séries-temporais financeiras que não obedecem as propriedade da relação acima citada e necessária uma abordagem que seja capaz de realizar a recuperação por conteúdo destes dados em tempo hábil e com semântica adequada . Nesse sentido , a literatura nos apresenta , como paradigma consolidado , as consultas por similaridade . Esse paradigma e a base para o funcionamento de muitos aplicativos de auxílio a tomada de decisão pelo especialista como Recuperação de Imagens Médicas por Conteúdo ( CBMIR ) e Recuperação de Áudio por Conteúdo ( CBAR ) e inclui diversas sub-áreas de pesquisa tais como extratores de características , funções de distância e métodos de acesso métrico . O desenvolvimento de novos métodos extratores de características e novas funções de distância são de fundamental_importância  para a diminuição do gap semântico entre os aplicativos e usuários , enquanto os métodos de acesso métricos são os reponsáveis diretos pela rápida resposta dos sistemas . Integrar todas essas funcionalidades em um framework de suporte a consultas por similaridade dentro de um SGBDR permanece um grande desafio . Esse trabalho objetiva estender uma proposta inicial dos recursos disponíveis no SIREN , inserindo novos extratores de características e funções de distância para imagens médicas e séries-temporais financeiras transformando-o em um framework , de forma que seus componentes possam ser utilizados via comandos Structured Query Language ( SQL ) . Os resultados poderão ser diretamente utilizados por aplicativos de auxílio a tomada de decisão pelo 
 O aumento na disponibilidade de dados referentes a interação entre pessoas online tornou possível o estudo o processo de propagação de informações em redes_sociais  com volumes de dado antes jamais pensados . Neste trabalho , utilizamos dados do site de micro-blogging Twitter juntamente com conceitos de redes complexas para entender , caracterizar e classificar processos de difusão de informação observados nessa plataforma e em redes_sociais  em geral . Apresentamos importantes medidas para caracterização de cascatas de informação , bem como algoritmos eficientes para o seu cálculo . Com o auxilio dessas , mostramos que é possível quantificar a influência da rede social no processo de propagação de informação . Em seguida , constatamos que a informação tende a propagar por caminhos mínimos nessa rede . Por fim , mostramos que é possível utilizar apenas a topologia da rede social , sem nenhuma informação semântica , para agrupar tópicos , e que a topologia da rede social é fortemente influenciada pelos assuntos falados nela . Apesar de nosso trabalho possuir como base um único dataset , os métodos e medidas desenvolvidos são gerais e podem ser aplicados a qualquer processo de difusão de informação e a qualquer rede complexa 
 Este trabalho apresenta um sistema computacional que utiliza técnicas de Aprendizado de Máquina ( AM ) para auxiliar o diagnóstico oftalmológico . Trata-se de um sistema de medidas objetivas e automáticas dos principais vícios de refração ocular , astigmatismo , hipermetropia e miopia . O sistema funcional desenvolvido aplica técnicas convencionais de processamento a imagens do olho humano fornecidas por uma técnica de aquisição chamada Hartmann-Shack ( HS ) , ou Shack-Hartmann ( SH ) , com o objetivo de extrair e enquadrar a região de interesse e remover ruídos . Em seguida , vetores de características são extraídos dessas imagens pela técnica de transformada wavelet de Gabor e , posteriormente , analisados por técnicas de AM para diagnosticar os possíveis vícios refrativos presentes no globo ocular representado . Os resultados obtidos indicam a potencialidade dessa abordagem para a interpretação de imagens de HS de forma que , futuramente , outros problemas oculares possam ser detectados e medidos a partir dessas imagens . Além da implementação de uma nova abordagem para a medição dos vícios refrativos e da introdução de técnicas de AM na análise de imagens oftalmológicas , o trabalho contribui para a investigação da utilização de Máquinas de Vetores Suporte e Redes_Neurais  Artificiais em sistemas de Entendimento/Interpretação de Imagens ( Image Understanding ) . O desenvolvimento deste sistema permite verificar criticamente a adequação e limitações dessas técnicas para a execução de tarefas no campo do Entendimento/Interpretação de Imagens em problemas reais 
 Nesta tese , introduzimos o conceito de sequência convergente de permutações e provamos a existência de um objeto limite para tais sequências . Introduzimos ainda um novo modelo de permutação aleatória baseado em tais objetos e introduzimos um conceito novo de distância entre permutações . Provamos então que sequências de permutações aleatórias são convergentes e provamos a equivalência entre esta noção de convergência e convergência nesta nova distância . Obtemos ainda resultados de amostragem e quase-aleatoriedade para permutações . Provamos também uma caracterização para parâmetros testáveis de permutações 
 Neste trabalho , formulações implícitas são adaptadas à metodologia GENSMAC , para a solução_numérica  de escoamentos bidimensionais , transientes , newtonianos e incompressíveis . Usando a formulação velocidade-pressão e variações do método de projeção , o ambiente de simulação FreeFlow2D ê utilizado para resolver numericamente as equações de conservação , no contexto de diferenças_finitas  . Os termos difusivos nas equações de Navier-Stokes são tratados implicitamente por meio das formulações Implícita Regressiva , Crank-Nicolson e Adams-Bashforth/Crank-Nicolson . Os termos convectivos são tratados explicitamente por um esquema upwind de alta ordem limitado . Para escoamentos em canais e jatos com superfícies_livres  a baixos números de Reynolds , as formulações são robustas e possibilitam um aumento considerável no tamanho do passo temporal . Resultados_numéricos  que comparam muito bem com soluções analíticas e experimentais são apresentados 
 Ao contrário das linguagens procedimentais , em que os programas são escritos como uma sequência de instruções , as linguagens de programação funcional , tais como SML ( Standard Meta Language ) , Haskell e Lisp , enfatizam regras e casamento de padrões . Os programas em linguagens funcionais podem conter erros pela falta de entendimento de suas propriedades . Assim , a atividade de teste é de grande_importância  na identificação desses erros , podendo fornecer evidências da qualidade do produto em teste . No entanto , existem poucas iniciativas para o teste de programas funcionais bem como no desenvolvimento de ferramentas de apoio a essa atividade . Além_disso  , os trabalhos existentes não possuem uma medida de cobertura da atividade de teste . Assim , motiva-se a investigação de novas formas de se realizarem testes em programas funcionais . Um critério de teste que fornece uma maneira de auxiliar na geração e na avaliação de um conjunto de casos de teste é o Teste de Mutação . Neste trabalho , estabelecem-se subsídios para a investigação da aplicabilidade do Teste de Mutação para o teste programas funcionais , escritos em SML . Devido ao grande volume de informações que estão envolvidas na aplicação do Teste de Mutação , é essencial a existência de ferramentas de apoio para o uso desse critério . A fim de viabilizar a aplicação do Teste de Mutação para SML , foi desenvolvida a ferramenta web PROTEUM/SML , que implementa os operadores de mutação definidos neste trabalho . Um exemplo é fornecido para ilustrar os conceitos e a ferramenta PROTEUM/SML 
 A segmentação é um passo importante em praticamente todas as tarefas que envolvem processamento de imagens_digitais  . Devido à variedade de imagens e diferentes necessidades da segmentação , a automação da segmentação não é uma tarefa trivial . Em muitas situações , abordagens interativas , nas quais o usuário pode intervir para guiar o processo de segmentação , são bastante úteis . Abordagens baseadas na transformação watershed mostram-se adequadas para a segmentação interativa de imagens : o watershed a partir de marcadores possibilita que o usuário marque as regiões de interesse na imagem ; o watershed hierárquico gera uma hierarquia de partições da imagem sendo analisada , hierarquia na qual o usuário pode navegar facilmente e selecionar uma particular partição ( segmentação ) . Em um trabalho prévio , propomos um método que integra as duas_abordagens  de forma que o usuário possa combinar os pontos fortes dessas duas formas de interação intercaladamente . Apesar da versatilidade obtida ao se integrar as duas_abordagens  , as hierarquias construídas dificilmente contêm partições interessantes e o esforço de interação necessário para se obter um resultado desejado pode ser muito elevado . Nesta tese propomos um método , baseado em aprendizagem computacional , que utiliza imagens previamente segmentadas para tentar adaptar uma dada hierarquia de forma que esta contenha partições mais próximas de uma partição de interesse . Na formulação de aprendizagem computacional , diferentes características da imagem são associadas a possíveis contornos de regiões , e esses são classificados como contornos que devem ou não estar presentes na partição final por uma máquina de suporte vetorial previamente treinada . A hierarquia dada é adaptada de forma a conter uma partição que seja consistente com a classificação obtida . Essa abordagem é particularmente interessante em cenários nos quais lotes de imagens similares ou sequências de imagens , como frames em sequências de vídeo ou cortes produzidas por exames de diagnóstico por imagem , precisam ser segmentadas . Nesses casos , é esperado que , a cada nova imagem a ser segmentada , o esforço de interação necessário para se obter a segmentação desejada seja reduzido em relação ao esforço que seria necessário com o uso da hierarquia original . Para não dependermos de experimentos com usuários na avaliação da redução no esforço de interação , propomos e utilizamos um modelo de interação que simula usuários humanos no contexto de segmentação hierárquica . Simulações deste modelo foram comparadas com sequências de interação observadas em experimentos com usuários humanos . Experimentos com diferentes lotes e sequências de imagens mostram que o método é capaz de reduzir o esforço de interação 
 A Identificação por radiofrequência ( Radio Frequency Identification - RFID ) tem revolucionado a forma de identificar objetos , sendo usada desde aplicações de controle de estoques até o processo automatizado de pagamentos . Sua ampla aceitação e aplicabilidade tem estimulado pesquisadores a criar cada vez mais aplicações . Um problema chave da RFID são as colisões que ocorrem na identificação por meio dos protocolos de acesso múltiplo . Como , na prática , um leitor precisa identificar várias etiquetas em sua área de cobertura , algumas etiquetas podem responder ao mesmo tempo o que gera colisões e desperdício de recursos . Por este motivo , torna-se de grande valor um estudo abrangente sobre como melhorar a identificação das etiquetas de modo a reduzir o número de colisões . Além_disso  , aspectos como consumo de energia e tempo necessário para identificação também devem ser levados em consideração , uma vez que a utilização cada vez maior de dispositivos alimentados à bateria tem sido observada na prática . Esta tese investiga a categoria de protocolos anticolisão denominada Frame Slotted Aloha - FSA , pois é a categoria que possui maior potencial de utilização prática em sistemas RFID . Além_disso  , as diferentes métricas de análise de desempenho são também analisadas e categorizadas , uma vez que identificou-se que um conjunto de métricas devem ser observadas com o intuito de realizarem-se comparações justas com as propostas da literatura . Descobriu-se que a maioria das propostas não levam em consideração os aspectos chave de tempo e energia , assim como a característica de ser fácil de implementar e baixa complexidade . Esta tese propõe quatro algoritmos que visam diminuir o consumo de energia e o tempo do processo de identificação das etiquetas mantendo-se as características de baixa complexidade e similaridade com o padrão atual EpcGlobal Classe 1 Geração 2 ( C1G2 ) . O primeiro mecanismo visa diminuir a quantidade de respostas desnecessárias em cenários de localização e rastreamento . Os demais consistem em três propostas de algoritmos anticolisão para sistemas RFID . Os dois primeiros diferem na forma como o tamanho inicial de quadro é definido e como as colisões são tratadas , representando evoluções progressivas em direção a um melhor desempenho . O terceiro considera a ocorrência do efeito captura , o que traz a necessidade de mudanças no funcionamento do algoritmo anterior . Resultados de simulação mostram que os quatro mecanismos podem melhorar propostas existentes sem aumento de complexidade , resultando consequentemente em diminuição de recursos desperdiçados . Além_disso  também foram desenvolvidos dois softwares de apoio aos mecanismos propostos : nsRFIDsim e jRFIDsim . O primeiro trata-se de um módulo para o simulador ns-2 que simula um sistema RFID passivo . O segundo implementa uma proposta de benchmark para avaliação de desempenho de algoritmos anticolisão para RFID , visando fornecer para a comunidade_científica  uma forma padronizada de avaliar este tipo de algoritmo 
 Os ambientes virtuais de aprendizagem estão cada vez mais populares e estão recebendo cada vez mais atenção das pesquisas acadêmicas devido ao avanço tecnológico na última_década  e na modernização das escolas e ambientes de ensino . Diversos estudos apontam que alunos que utilizam sistemas educacionais melhoram o seu desempenho e aprendem mais . Entretanto , problemas importantes que dificultam sua utilização ainda precisam ser investigados . Dentre eles , um dos principais problemas encontrados é o uso inadequado destes sistemas por parte dos alunos . O tédio , desinteresse , monotonia , a falta de motivação , entre outros fatores , acabam por fazer com que o aluno se comporte inadequadamente ao interagir com o sistema . O comportamento inadequado mais conhecido é trapacear o sistema ( do Inglês `` gaming the system '' ) . Ao externalizar este comportamento , o aluno tenta identificar formas de resolver os exercícios mecanicamente , sem levar em consideração o que precisa ser aprendido . Nesse contexto , esse trabalho tem por objetivo estudar e definir uma alternativa para diminuir o comportamento indesejável nos sistemas por meio do uso de técnicas de Gamificação . Essas técnicas permitem `` persuadir '' o aluno a interagir de forma correta com o sistema . Este trabalho apresenta o desenvolvimento de um sistema educacional gamificado ( E-Game ) a fim de comprovar se os elementos de jogos auxiliam na diminuição de comportamentos como gaming the system nos alunos , aumentando a motivação deles durante a atividade . O sistema educacional foi submetido a experimentos empíricos em ambientes reais de aprendizagem para que fosse possível_obter  diferentes_tipos  de dados para análise . Dessa forma , um outro sistema educacional foi desenvolvido , sem técnicas de gamificação , a fim de obter dados comparativos . Um experimento foi realizado numa escola estadual de São Carlos com o total de 60 alunos . Observou-se , a partir do experimento , que há uma diferença estatisticamente significativa quando se usa os sistemas gamificado e não gamificado . Empiricamente , os resultados comprovam que há uma diminuição do comportamento externalizado gaming the system com o uso de sistemas educacionais gamificados . Além_disso  , foi observada uma diferença nos resultados relacionada aos gênero dos alunos , até então desconhecida pela literatura . Foi detectado que o gênero masculino externalizou menos trapaças durante o uso do sistema educacional gamificado , em relação ao gênero feminino . Por outro_lado  , o gênero feminino externalizou menos trapaças no ambiente não gamificado , em comparação ao ambiente gamificado 
 A classificação Multi-Rótulo consiste em aprender uma função que seja capaz de mapear um objeto para um conjunto de rótulos relevantes . Ela possui aplicações como associação de genes com funções biológicas , classificação semântica de cenas e categorização de texto . A classificação tradicional , de rótulo único é , portanto , um caso particular da Classificação Multi-Rótulo , onde cada objeto está associado com exatamente um rótulo . Uma abordagem bem sucedida para classificação é obter um modelo probabilístico da relação entre atributos do objeto e rótulos . Esse modelo pode então ser usado para classificar objetos , encon- trando a predição mais provável por meio da probabilidade marginal ou a explicação mais provavél dos rótulos dados os atributos . Dependendo da família de modelos_probabilísticos  escolhidos , tais inferências podem ser intratáveis quando o número de rótulos é grande . As redes Soma-Produto ( SPN , do inglês Sum Product Network ) são modelos_probabilísticos  profundos , que permitem inferência marginal tratável . No entanto , como em muitos outros modelos_probabilísticos  , a inferência da explicação mais provavél é NP-difícil . Embora SPNs já tenham sido usadas com sucesso para tarefas de classificação tradicionais , não existe investigação aprofundada no uso de SPNs para classificação Multi-Rótulo . Neste trabalho , investigamos o uso de SPNs para classificação Multi-Rótulo . Comparamos vários algoritmos de aprendizado de SPNs combinados com diferentes abordagens propostos para classi- ficação . Mostramos que os classificadores Multi-Rótulos baseados em SPN são competitivos contra classificadores estado-da-arte , como Random k-Labelsets usando Máquinas de Suporte Vetorial e inferência exata da explicação mais provavél em CutNets , em uma coleção de conjuntos de dados de referência 
 Algoritmos de aprendizado semissupervisionado baseado em grafos foram amplamente estudados nos últimos_anos  . A maioria desses algoritmos foi projetada a partir de problemas de otimização sem restrições usando um termo regularizador Laplaciano como funcional de suavidade numa tentativa de refletir a estrutura geométrica intrínsica da distribuição marginal dos dados . Apesar de vários artigos científicos recentes continuarem focando em métodos sem restrição para aprendizado semissupervisionado em grafos , uma análise estatística recente mostrou que muitos desses algoritmos podem ser instáveis em regressão transdutiva . Logo , nós focamos em propor novos métodos com restrições para aprendizado semissupervisionado em grafos . Nós começamos analisando o framework de regularização de métodos sem restrições existentes . Então , nós incorporamos duas restrições de normalização no problema de otimização de três desses métodos . Mostramos que os problemas de otimização propostos possuem solução de forma fechada . Ao generalizar uma dessas restrições para qualquer distribuição , provemos métodos generalizados para aprendizado semissupervisionado restrito baseado em grafos . Os métodos propostos possuem um framework de regularização mais flexível que os métodos sem restrições correspondentes . Mais precisamente , nossos métodos podem lidar com qualquer Laplaciano em grafos e usar regularização de ordem elevada , a qual é efetiva em tarefas de aprendizado semissupervisionado em geral . Para mostrar a efetividade dos métodos propostos , nós provemos análises experimentais robustas . Especificamente , nossos experimentos são subdivididos em duas partes . Na primeira parte , avaliamos algoritmos de aprendizado semissupervisionado em grafos existentes em dados de séries_temporais  para encontrar possíveis fraquezas desses métodos . Na segunda parte , avaliamos os métodos restritos propostos contra seis algoritmos de aprendizado semissupervisionado baseado em grafos do estado da arte em conjuntos de dados benchmark . Como a amplamente usada análise de melhor caso pode esconder informações relevantes sobre o desempenho dos algoritmos de aprendizado semissupervisionado com respeito à seleção de parâmetros , nós usamos modelos de avaliação empírica recentemente propostos para avaliar os nossos resultados . Nossos resultados mostram que os nossos métodos superam os demais métodos na maioria das configurações de parâmetro e métodos de construção de grafos . Entretanto , encontramos algumas configurações experimentais nas quais nossos métodos mostraram baixo desempenho . Para facilitar a reprodução dos nossos resultados , os códigos fonte , conjuntos de dados e resultados experimentais estão disponíveis gratuitamente 
 Os problemas de corte e empacotamento de itens_irregulares  , polígonos convexos e não convexos , são encontrado em diversas indústrias , tais como a metal-mecânica , a têxtil , a de calçados , a moveleira e outras . Nesta tese estudamos a versão bidimensional destes problemas , na qual desejamos alocar um conjunto de itens , sem sobreposição , no interior de um ou mais recipientes , limitados ou ilimitados , de modo a otimizar uma função objetivo . Neste trabalho estudamos o problema da mochila , o problema do assentamento , o problema empacotamento em faixa , o problema de corte de estoque e o problema de empacotamento de contêineres . Para estes problemas , os métodos heurísticos e modelos de programação matemática propostos e apresentam resultados muito promissores , ultrapassando em muitos_casos  os melhores_resultados  da literatura especializada . Esta tese esta organizada da seguinte maneira . No Capítulo 1 , apresentamos uma revisão dos problemas estudados , a proposta de valor deste doutorado com as principais_contribuições  e ideias . No Capítulo 2 , propomos uma meta-heurística para o problema de empacotamento em faixa para itens_irregulares  e círculos . Em seguida , no Capítulo 3 apresentamos uma heurística genérica para a alocação de itens_irregulares  que podem ser fracamente ou fortemente heterogêneos e serão alocados em um recipiente ( problema de maximização de saída ) ou de múltiplos recipientes ( problemas de minimização de entrada ) . O Capítulo 4 propõem um método de solução para o problema de corte de estoque com demanda conhecida e demanda estocástica . Nos Capítulos 5 e 6 apresentamos modelos de programação matemática para o problema de corte de itens_irregulares  em faixa . Finalmente , no Capítulo 7 , apresentamos a conclusão e uma sucinta direção para os trabalhos futuros 
 Neste trabalho estudamos aspectos algorítmicos do Problema da Cobertura por Sensores . Em linhas gerais , este problema a entrada consiste em uma região a ser monitorada por um conjunto de sensores previamente posicionados , cada qual dotado de bateria com duração limitada , e o objetivo é atribuir a cada sensor um tempo de início , de modo que toda a região seja coberta o maior tempo possível . Focamos nosso estudo no caso unidimensional do problema , chamado Problema da Cobertura de Faixa Restrita , no qual a região a ser monitorada é um intervalo ( da reta real ) . Estudamos diversas variantes , de acordo com os subintervalos que os sensores cobrem ( se de tamanhos fixos ou variados ) , e de acordo com a duração das baterias ( se uniformes ou não ) . Estudamos também o caso preemptivo : quando os sensores podem ser ligados mais de uma vez . Para este último caso , projetamos um algoritmo polinomial bem simples . O Problema da Cobertura de Faixa Restrita é NP-difícil no caso não-preemptivo em que os sensores têm bateria de duração variável . Para este caso , em 2009 Gibson e Varadarajan apresentaram um algoritmo polinomial que provaram ser uma 5-aproximação . Provamos que este algoritmo tem fator de aproximação 4 , e mostramos que este fator é justo . Apresentamos também formulações lineares inteiras para este caso , e os resultados computacionais obtidos 
 Este trabalho apresenta meios alternativos para a obtenção de imagens aéreas e sua utilização na agricultura . Imagens aéreas representam uma ferramenta importante na avaliação de diversos aspectos da atividade agrícola moderna , principalmente aqueles que não podem ser avaliados ao nível do solo . Três tipos de sistemas são definidos com níveis crescentes de complexidade e funcionalidade . Diversos critérios são propostos para a avaliação do sistema e para a determinação da sua adequação às principais aplicações na agricultura . Foi construido um protótipo de sistema do Tipo I baseado em aeromodelos que foi utilizado na avaliação da tecnologia e validação do projeto . Foram utilizados componentes facilmente encontrados no mercado , existindo potencial para a evolução técnica do sistema construído em cada um dos seus componentes básicos : a aeronave , o método de controle e o meio de captação das imagens . Os resultados obtidos_mostram  que o sistema pode substituir , a baixo custo , os meios convencionais utilizados até então para a obtenção dessas imagens 
 O objetivo principal deste trabalho é o desenvolvimento e avaliação de algoritmos numéricos paralelos e sua execução em máquinas_paralelas  ( máquinas multiprocessadas , máquinas vetoriais e máquinas_paralelas  virtuais ) . Os algoritmos desenvolvidos foram executados em diferentes condições tanto em termos de plataformas utilizadas como em termos de tamanho da aplicação considerada . Os resultados obtidos na implementação dos algoritmos numéricos são analisados baseando-se em algumas métricas ( tempo de execução e operações em ponto flutuante ) comuns aos resultados apresentados nos principais benchmarks estudados . Através dos resultados obtidos , o desempenho das bibliotecas de passagem de mensagem MPI e PVM , o desempenho das arquiteturas consideradas e da implementação dos algoritmos numéricos são analisados 
 O avanço de algumas áreas como computação e comunicação de dados , bem como a busca incessante pelo domínio das informações , contribui para aumentar cada vez mais as pesquisas relacionadas com aquisição de conhecimento , tema central da área de Inteligência_Artificial  . A aquisição implícita de conhecimento é realizada utilizando-se algoritmos de Aprendizado de Máquina . No caso de algoritmos simbólicos supervisionados , o conhecimento adquirido é representado em estruturas lógicas , tais como regras do tipo se então , que são entendíveis pelo ser humano . Quando o número de regras é elevado , ou as regras consideram muitas condições no seu corpo , torna-se difícil , ao ser humano , a análise desse conhecimento . Uma solução para esta questão é o desenvolvimento de boas medidas de avaliação de regras . Independentemente da quantidade de regras , essas medidas ajudam a selecionar aquelas que são mais úteis e interessantes , pois parte do conhecimento adquirido dos exemplos pode ser muito óbvio ou irrelevante . Neste trabalho são discutidas algumas medidas propostas na literatura , com a finalidade de auxiliar o usuário no entendimento e utilização proveitosa do conhecimento adquirido . Com base nos estudos realizados foi projetado e implementado um sistema computacional , denominado 7.0 system , para auxiliar na avaliação dessas regras de conhecimento . 0 RQsystem foi desenvolvido na linguagem de programação lógica Prolog e consiste de dois módulos principais . O primeiro é responsável pelo pré-processamento dos dados de entrada . O segundo módulo é responsável por fornecer diversar informações pré-definidas no sistema ou construídas e formuladas pelo usuário . o TZQsystern está descrito neste trabalho utilizando um pequeno conjunto de dados do mundo_real  e as regras geradas pelos algoritmos de Aprendizado de Máquina C.111-2 e C4.5 . Esse sistema tem características interessantes que lhe conferem uma boa utilidade tanto na avaliação de regras quanto no estudo de outras questões relacionadas com as regras . Extensões futuras do sistema poderão ser particularmente úteis em Data_Mining  
 O reconhecimento automático de faces é um dos problemas mais desafiadores no campo da visão_computacional  e de reconhecimento . Métodos seguros e robustos podem ser usados em uma grande variedade de aplicações , como em sistemas de identificação de suspeitos pela polícia . A identificação de suspeitos com auxílio de testemunhas ou vítimas é muito usada na solução de crimes . Um dos métodos mais usados na identificação é o álbum de fotografia , que é desgastante , toma tempo , e confimde a testemunha que examina milhares de fotos . Pode-se , entretanto , desenvolver um sistema que combine a descrição da testemunha com as descrições contidas numa base de dados , ordenando as fotos a serem apresentadas para a vitima em ordem de similaridade , tomando menos tediosa e mais rápida a identificação correta do suspeito pela testemunha . Este trabalho diz_respeito  à seleção de técnicas e à criação de um sistema de catinstramento multimidia de suspeitos , com utilização de um algoritmo de reconhecimento de fotos para apoio à ordenação da base de fotografias policiais . Entre os algoritmos existentes para reconhecimento de faces , foi selecionado o PCA ( Principal Component Analisys ) . Uma base de dados para cadastramento foi criada , incluindo fotografias , e o algoritmo de reconhecimento foi implementado para completar a funcionalidade do sistema . Testes sugerem bom grau de reconhecimento e boa adaptação à tarefa de ordenação do conjunto de fotos 
 Este trabalho apresenta um protótipo de ferramenta , a Html2Hip , que proporciona um ambiente de importação e adaptação de documentos descritos segundo o padrão HTML ( HyperTexi Markup Language ) para a representação interna do SASHE ( Sistema de Autoria e Suporte Hipermiclia para Ensino ) , que se baseia na estruturação de objetos multimidia , segundo a hierarquia de classes proposta pelo MCA ( Modelo de Contextos Aninhados ) . Além_disso  , este trabalho estendeu a capacidade do editor de nós de informação do tipo texto do protótipo anterior no que concerne ao processamento de arquivos-texto descritos pelo padrão RTF ( Rich Text Formai ) . Dessa forma , o SASHE tornou-se capaz de processar e organizar materiais instrucionais preparados em seu próprio ambiente , no ambiente NX/NTAT/ ( World-Wide Web ) , bem como em processadores de texto comuns 
 O armazenamento e a recuperação de imagens podem fazer-se pela indexação de características extraidas , automaticamente , por algum procedimento computacional . Estas caracteristicas são muitas_vezes  constituídas por valores numéricos sob a forma de vetores , representando coordenadas de pontos no espaço cuja dimensão é determinada pela quantidade destes valores . Diversos métodos de indexação voltados para este tipo de dados têm sido estudados e desenvolvidos no contexto de Sistema Gerenciadores de Banco de Dados . tidos como Métodos de Indexação Espacial , com o intuito de melhorar o desempenho do acesso às informações . O presente_trabalho  se inicia pela exposição de como o Modelo de Dados SIRIUS define `` Imagem- como tipo de dados abstrato , visando o desenvolvimento de aplicações na área médica . e como algoritmos de extração de características são associados a este tipo . Em seguida , fez-se o estudo sobre dados espaciais envolvendo : definições , propriedades , aplicações e métodos de indexação espacial , onde os métodos R-Tree e TV-Tree são descritos . Por fim , discutiu-se o resultado de testes feitos com esses métodos , utilizando conjuntos de pontos entre as dimensões 2 e 255 , o que aponta o segundo ( TV-Tree ) como o mais adequado a aplicações que trabalham com pontos em dimensões distintas e elevadas . Dessa forma , pôde-se definir qual o método mais preferencial a ser incorporado ao Gerenciador de Objetos do SIRIUS , afim de efetuar a indexação do tipo de dados -Imagem '' nesse ambiente 
 Este trabalho apresenta um estudo sobre as modificações necessárias no ASiA ( Ambiente de Simulação Automático ) para viabilizar a utilização de Redes de Petri para a representação de modelos . Esse estudo inicia-se com uma comparação entre Redes de Filas e Redes de Petri . utilizadas como técnicas para modelagem de sistemas computacionais , visando a avaliação desses sistemas através de simulação . O estudo da viabilidade analisa diversos aspectos tais como : possibilidade de utilização da linguagem SMPL ; avaliação das modificações necessárias no Editor Gráfico ; alterações necessárias no Gerador de Aplicações e no Gabarito utilizado pelo ASiA 
 Este trabalho apresenta um estudo sobre problemas de dimensionamento de lotes monoestágios , que consistem em determinar as quantidades de itens a serem produzidos em diferentes períodos de tempo , de modo a minimizar a soma dos custos de produção , preparação e estoque . A quantidade produzida em cada_período  deve ser capaz de atender as demandas dos itens , sem exceder a capacidade de máquina . Para retratar o consumo de recursos , são incluídos tempos de preparação e produção . Inicialmente , são apresentados alguns métodos básicos para resolução de modelos simplificados e , em seguida , apresenta-se dois métodos para resolução de importantes modelos da literatura de problemas monoestágios . O primeiro , foi desenvolvido por Trigeiro et ai . ( 1989 ) e consiste num método heurístico baseado em relaxação Lagrangiana , no método de otimização do subgradiente e em uma heurística de factibilização . O segundo método , desenvolvido por Diaby et_aL  ( 1992a ) , é um método exato , baseado num procedimento de enumeração implícita , onde os limitantes inferiores são gerados por relaxação Lagrangiana tendo como opção a utilização do método de otimização do subgradiente . O primeiro método foi implementado assim como uma versão modificada . Finalmente , são apresentados alguns experimentos computacionais comparando as duas versões 
 Este trabalho cobre as principais tecnologias de redes de comunicação de alto_desempenho  e suas aplicações . O enfoque principal é o estudo e a seleção de um padrão de rede de alta velocidade adequado para a implementação de sistemas computacionais paralelos de alto_desempenho  . Os resultados do estudo feito foram aplicados no SPP2 , uma arquitetura paralela baseada em computadores_pessoais  desenvolvida no LCAD-ICMC-USP . É proposta uma nova topologia de rede de alto_desempenho  para essa máquina baseada em componentes Myrinet . A nova rede de comunicações do SPP2 atende perfeitamente os requisitos iniciais do sistema . Os testes de desempenho realizados mostram um desempenho muito superior ao padrão de rede originalmente adotado no SPP2 e aos padrões de rede comumente utilizados em redes locais de estações de trabalho . A arquitetura SPP2 , com o novo sistema de comunicações , representa um grande passo na disseminação do processamento paralelo por apresentar características como baixo custo , alto_desempenho  , facilidade de construção e ampla disponibilidade de componentes para atualização do seu desempenho 
 Planejamento sob incerteza vem_sendo  cada vez mais requisitado em aplicações práticas de diversas_áreas  que eequerem soluções confiáveis para metas complexas . Em vista disso , nos últimos_anos  , algumas abordagens baseadas no uso de métodos formais para síntese automática de planos têm sido propostas na área de Planejamento em Inteligência_Artificial  . Entre essas abordagens , planejamento baseado em verificação de modelos tem se mostrado uma opção bastante promissora ; entretanto , conforme observamos , a maioria dos trabalhos dentro dessa abordagem baseia-se em CTL e trata apenas problemas de planejamento para metas de alcançabilidade simples ( como aquelas consideradas no planejamento clássico ) . Nessa tese , introduzimos uma classe de metas de planejamento mais expressivas ( metas de alcançabilidade estendidas ) e mostramos que , para essa classe de metas , a semântica de CTL não é adequada para formalizar algoritmos de síntese ( ou validação ) de planos . Como forma de contornar essa limitação , propomos uma nova versão de CTL , que denominamos alpha-CTL . Então , a partir da semântica dessa nova lógica , implementamos um verificador de modelos ( Vactl ) , com base no qual implementamos também um planejador ( Pactl ) capaz de resolver problemas de planejamento para metas de alcançabilidade estendidas , em ambientes não-determinísticos com observabilidade completa . Finalmente , discutimos como garantir a qualidade das soluções quando dispomos de um modelo de ambiente onde as probabilidades das transições causadas pela execução das ações são conhecidas 
 Linhas de produção são sistemas fabris para produção em larga_escala  , com grande_importância  no sistema industrial atual . Como se trata de um sistema já consolidado , é natural que existam métricas de desempenho e estratégias para otimização de eficiência para as diversas configurações de linhas de produção existentes . Este trabalho se concentra em linhas seriais não-ritmadas e estocásticas . Em particular , o interesse é no efeito de se desbalancear adequadamente uma linha de produção para melhorar sua produtividade , o que é denominado na literatura de fenômeno Tigela ( Bowl phenomenon ) . Uma revisão da literatura mostrou que esse fenômeno já foi estudado em diferentes cenários : linhas com e sem buffers , diversos perfis de carga ao longo da linha e diferentes distribuições probabilísticas regendo os tempos de execução das tarefas . Contudo , nenhum dos estudos considerou a indivisibilidade das tarefas , isto é , o fato de que cada tarefa deve ser executada em uma única estação . Essa é a principal lacuna explorada neste estudo . Para isso , são utilizadas instâncias recentemente propostas do Problema Simples de Balanceamento de Linhas de Produção ( SALBP , na sigla em inglês ) e instâncias do Problema de Designação de Trabalhadores e Balanceamento de Linhas de Produção ( ALWABP ) , que são solucionadas a partir de modelos de programação inteira_mista  que induzem soluções de acordo com o efeito Tigela . Essas soluções são utilizadas em um modelo de simulação estocástico , também apresentado neste trabalho . Em linhas gerais , os resultados mostram que as soluções do SALBP e ALWABP obtidas podem se beneficiar do efeito Tigela . Além_disso  , a variedade das instâncias utilizadas nos experimentos argumentam em favor de uma maior capacidade de generalização dos resultados para casos práticos se comparados aos casos simplificados previamente discutidos na literatura 
 A oferta de produtos , informação e serviços a partir de perfis de usuários tem tornado os sistemas de recomendação cada vez mais presentes na Web , aumentando a facilidade de escolha e de permanência dos usuários nestes sistemas . Entretanto , existem otimizações a serem feitas principalmente com relação à modelagem do perfil do usuário . Geralmente , suas preferências são modeladas de modo superficial , devido à escassez das informações coletadas , como notas ou comentários , ou devido a informações indutivas que estão suscetíveis a erros . Esta dissertação_propõe  uma ferramenta de recomendação baseado em interações multimodais , capaz de combinar informações de usuários processadas individualmente por algoritmos de recomendação tradicionais . Nesta ferramenta desenvolveram-se quatro técnicas de combinação afim fornecer aos sistemas de recomendação , subsídios para melhoria na qualidade das predições em diversos_domínios  
 Este trabalho apresenta uma especialização do Método Primal Simplex para resolver o Problema de Aproximação Linear no L1 e o Problema de Regressão Quantil , os quais são casos particulares de Problema de Programação Linear por Partes . No Problema de Regressão Quantil a função objetivo linear por partes depende de um parâmetro θ e , com pequenas adaptações da pós otimização clássica da Programação Linear , pode-se determinar o intervalo para θ onde a solução do problema fica invariante . Assim , este trabalho apresenta também uma maneira simples para realizar esta análise pós otimização . Além_disso  , este trabalho apresenta alguns resultados computacionais , utilizando-se de exemplos da literatura 
 O conceito de manufatura celular consiste em decompor um sistema de manufatura em subsistemas mais fáceis de gerenciar que o sistema global . As máquinas são agrupadas em células e as peças em famílias . Cada par família-célula constitui uma célula de manufatura . Nesta dissertação estudamos vários algoritmos e modelos para a obtenção de células de manufatura e propomos um método para resolver este problema baseado no algoritmo da máquina semente . Os resultados obtidos sobre vários exemplos da literatura são equivalentes ou melhores que aqueles existentes 
 O propósito deste trabalho é fazer uma análise_Bayesiana  conjugada e utilizar métodos amostrais na teoria de filas , em particular para os sistemas M/M/1 , M/M/1/k , M/M/c e M/M/ ∞ . Nosso maior interesse reside no estudo das chamadas medidas de desempenho : número de usuários no sistema e na fila , tempo de permanência no sistema e na fila e comprimento do período ocioso e ocupado , pois são essas medidas que nos fornecem o comportamento do sistema . Concentramos a atenção nas distribuições preditivas das medidas de desempenho . Na análise_Bayesiana  conjugada , mostramos que a escolha da priori é fundamental para que tenhamos distribuições preditivas com momentos . Mas esta escolha nem sempre é feita de maneira natural , e notamos que a análise_Bayesiana  conjugada pode se mostrar bastante complexa . Para evitarmos os problemas surgidos com a análise_Bayesiana  conjugada , sugerimos a utilização de métodos amostrais , através de uma técnica bastante original . Com o algoritmo Sampling-Importance-Resampling ( SIR ) simulamos as distribuições preditivas das medidas de desempenho . Com o histograma de Berger determinamos a informação a priori de p ( intensidade de tráfego ) , que pode ser feito via MINITAB . Para a utilização deste procedimento necessitamos somente da informação a priori da intensidade de tráfego 
 Esta dissertação apresenta um estudo de algoritmos de visualização em Computação_Gráfica  e descreve a implementação de alguns deles no módulo de visualização do ( SM ) 2 - Sistema de Modelagem de Sólidos Multirepresentacional , em desenvolvimento pelo grupo de Computação_Gráfica  e Processamento de Imagens do ICMSC/USP , e permite a descrição de objetos por instanciamento de primitiva , varreduras translacional e rotacional ou por semiespaços definidos implicitamente . A principal representação interna dos sólidos é uma estrutura B-Rep ( Boundary Representation ) . O estudo bibliográfico realizado engloba vários algoritmos de remoção de linhas/superfícies ocultas , uso de modelos de iluminação local e global e a geração de diferentes efeitos de iluminação , como técnicas de tonalização , fontes de luz , sombra , textura e transparência . Como parte deste trabalho , foi implementado um módulo de visualização para o ( SM ) 2 . Este módulo possui uma interface para a manipulação de parâmetros da visualização e permite visualizar os sólidos B-Rep na representação fio-de-arame e na forma iluminada . A visualização iluminada utiliza uma versão otimizada do algoritmo seanline para a remoção de superfícies ocultas . Diferentes técnicas de tonalização podem ser empregadas ( Flat , Gouraud e Phong ) . Além_disso  , o usuário pode definir várias fontes de luz , bem como definir diferentes propriedades de materiais , como cor , coeficientes de reflexão e transparência . Este trabalho permitiu a visualização eficiente de sólidos 
 O teste de programas concorrentes tem sido objeto de preocupação nos últimos_anos  ; mesmo assim , as iniciativas tomadas nesta área têm sido tímidas , com um número pequeno de publicações . A maioria das propostas para teste de programas concorrentes procura usar ao máximo o que já se tem feito para o teste de programas seqüenciais , procurando tratar com mais atenção aquelas construções que são específicas para programas concorrentes . Nesta dissertação estudam-se o teste_estrutural  de programas seqüenciais e a ferramenta POKE-TOOL , que apóia a aplicação dos critérios Potenciais Usos ; discutem-se propostas de teste de programas concorrentes , objetivando a configuração da POKE-TOOL para o teste de programas concorrentes . Dentro deste escopo , propõe-se uma linguagem intermediária para programas concorrentes ( LIConc ) , procurando considerar vários aspectos de linguagens de programação concorrente do tipo procedimental e implementa-se o módulo que mapeia programas escritos em Occam para programas escritos na LIConc . Essa abordagem possibilita que a extensão da POKETOOL para apoiar o teste de programas concorrentes mantenha a característica de multilinguagem já apresentada pela POKE-TOOL para programas seqüenciais 
 O alinhamento de seqüências biológicas é uma operação básica em Bioinformática , já que serve como base para outros processos como , por exemplo , a determinação da estrutura tridimensional das proteínas . Dada a grande quantidade de dados presentes nas seqüencias , são usadas técnicas matemáticas e de computação para realizar esta tarefa . Tradicionalmente , o Problema de Alinhamento de Seqüências Biológicas é formulado como um problema de otimização de objetivo simples , onde alinhamento de maior semelhança , conforme um esquema de pontuação , é procurado 
 A Otimização Multi-Objetivo aborda os problemas de otimização que possuem vários critérios a serem atingidos . Para este tipo de problema , existe um conjunto de soluções que representam um `` compromiso '' entre os objetivos . Uma técnica que se aplica com sucesso neste contexto são os Algoritmos_Evolutivos  , inspirados na Teoria da Evolução de Darwin , que trabalham com uma população de soluções que vão evoluindo até atingirem um critério de convergência ou de parada 
 Este trabalho formula o Problema de Alinhamento de Seqüências Biológicas como um Problema de Otimização Multi-Objetivo , para encontrar um conjunto de soluções que representem um compromisso entre a extensão e a qualidade das soluções . Aplicou-se vários modelos de Algoritmos_Evolutivos  para Otimização Multi-Objetivo . O desempenho de cada modelo foi avaliado por métricas de performance encontradas na literatura 
 Dentro da bioinformática uma das atividades mais realizadas é o alinhamento de seqüências biológicas [ 1 ] . Seus resultados são utilizados em várias atividades que desdobram-se em áreas de pesquisa interdisciplinares com geração de diversos subprodutos . Sendo uma das primeiras etapas de tais tarefas , o multialinhamento é então importante para garantir a qualidade dos resultados obtidos em vários estudos do material genético . Para este trabalho espera-se a reprodução dos resultados já publicados na área [ 2 ] ; [ 3 ] ; [ 4 ] ; [ 5 ] ; [ 6 ] ) . A implementação de um programa de multialinhamento global de seqüências biológicas utilizando uma abordagem iterativa estocástica por algoritmo genético , uma forma relativamente recente [ 7 ] de se atacar tal problema . Obtenção de um panorama sobre as soluções alternativas 
 Neste trabalho desenvolvemos um novo algoritmo para clustering para dados de expressão_gênica  . As abordagens tradicionais utilizam um conjunto de dados na forma de uma tabela de duas dimensões , onde as linhas são os genes e as colunas são as condições experimentais . Nós utilizamos uma estrutura de três dimensões , acrescentando fatias de tempo . Implementamos nosso algoritmo e testamos com conjuntos de dados sintéticos e dados reais , usando índices de validação para comparar os resultados obtidos pelo nosso algoritmo com os resultados produzidos pelo algoritmo TriCluster . Os resultados mostraram que o nosso algoritmo é bom para dados de expressão_gênica  em três dimensões e pode ser aplicado a dados de outros 
 Em Aprendizado de Máquina e Mineração de Dados , muitos dos trabalhos de classificação reportados na literatura envolvem classificação plana ( flat classification ) , em que cada exemplo é associado a uma dentre um conjunto finito ( e normalmente pequeno ) de classes , todas em um mesmo nível . Entretanto , existem problemas de classificação mais complexos em que as classes a serem preditas podem ser dispostas em uma estrutura hierárquica . Para esses problemas , a utilização de técnicas e conceitos de classificação_hierárquica  tem se mostrado útil . Uma das linhas de pesquisa com grande potencial para a utilização de tais técnicas é a Bioinformática . Dessa forma , esta dissertação apresenta um estudo envolvendo técnicas de classificação_hierárquica  aplicadas à predição de classes funcionais de proteínas . No total foram investigados doze algoritmos hierárquicos diferentes , sendo onze deles representantes da abordagem Top-Down , que foi o enfoque da investigação realizada . O outro algoritmo investigado foi o HC4.5 , um algoritmo baseado na abordagem Big- Bang . Parte dos algoritmos estudados foram desenvolvidos com base em uma variação da abordagem Top-Down , denominada de Top-Down Ensemble , que foi proposta neste estudo . Alguns do algoritmos baseados nessa nova abordagem apresentaram resultados promissores , superando os resultados dos demais algoritmos . Para avaliação dos resultados , foi utilizada uma medida específica para problemas hierárquicos , denominada taxa de acerto dependente da profundidade . Além dessa , outras três medidas de avaliação foram utilizadas , de modo a comparar os resultados reportados por diferentes 
 É rara a preocupação com a qualidade de implementação de software . Pode-se observar que as estruturas internas dos softwares são freqüentemente complexas e desorganizadas , especialmente no caso dos softwares que são ditos orientados a objetos . Essa complexidade afeta diretamente a manutenibilidade e a susceptibilidade a erros , dificultando a alteração e a adição de novas_funcionalidades  aos softwares . As próprias alterações , inerentes aos softwares , os tornam mais complexos , o que agrava o problema . Neste contexto , acredita-se que o controle da complexidade pode_levar  a produtos de software de melhor qualidade . Assim , trata-se neste trabalho da manutenção preventiva , implementada por meio de inspeções , refatorações e análise de métricas . São estudadas falhas de manutenibilidade em uma amostra de programas orientados a objetos e , a partir dos resultados , são propostos artefatos de apoio para um processo de inspeção de software e modelos para os produtos de trabalho gerados nesse processo . Propõe-se o uso da técnica de leitura PBR ( Leitura Baseada em Perspectivas ) como uma maneira de se melhorar a detecção de falhas de manutenibilidade . Finalmente , a proposta deste trabalho foi validada por meio de um estudo de 
 Este trabalho verifica a possibilidade de se aplicar Algoritmos_Evolutivos  no aprendizado on-line de jogos . Alguns autores concordam que Algoritmos_Evolutivos  não são aplicáveis na prática para se atingir o objetivo em questão . É com a intenção de contestar a veracidade desta afirmação que foi desenvolvido o presente_trabalho  . Para atingir o objetivo proposto , foi desenvolvido um jogo de computador , no qual o algoritmo de aprendizado gera estratégias inteligentes e adaptativas para os caracteres não controlados pelo jogador através de um algoritmo_evolutivo  . Desta forma , a função do algoritmo_evolutivo  é fazer com que a estratégia utilizada pelo computador se adapte à estratégia utilizada pelo usuário a cada vez que joga . É apresentada uma revisão bibliográfica a respeito de Computação Evolutiva e as técnicas utilizadas para implementar comportamentos inteligentes para os caracteres controlados por computador nos jogos atuais , esclarecendo suas vantagens , desvantagens e algumas possíveis aplicações . São também explicados o jogo e os algoritmos implementados , assim como os experimentos_realizados  e seus resultados . Por fim , é feita uma comparação do algoritmo_evolutivo  final com uma outra técnica de adaptação , chamada Dynamic Scripting . Assim , este trabalho oferece contribuições para o campo de Computação Evolutiva e Inteligência_Artificial  aplicada a 
 O ganho crescen te de desempenho nos computadores modernos tem impulsionado os trabalhos científicos nas áreas de simulação computacional . Muitos autores utilizam em suas pesquisas ferramentas comerciais que limitam seus trabalhos ao esconder os algoritmos internos destas ferramentas e dificultam a adição de dados in-vivo nestes trabalhos . Este trabalho explora esta lacuna deixada por aqueles autores . Elaboramos um arcabouço computacional capaz de reproduzir os fenômenos óticos e fisiológicos do sistema visual . Construímos com superfícies quádricas os modelos esquemáticos do olho humano e propomos um algoritmo de traçado de raio realístico . Então realizamos um estudo nos modelos esquemáticos e a partir deles mais a adição de dados in-vivo obtidos de um topógrafo de córnea extraímos informações óticas destes modelos . Calculamos os coeficientes e Zernike dos modelos para tamanhos diversos de pupila e obtivemos medidas de aberração do olho humano . Os resultados encontrados estão de acordo com os trabalhos_relacionados  e as simulações com dados in-vivo estão consoantes com as produzidas por um aparelho de frente de onda comerciais . Este trabalho é um esforço em aproveitar as informações adquiridas pelos equipamentos modernos de oftalmologia , além de auxiliar o entendimento de sistemas visuais biológicos acabam também em auxiliar a elaboração de sistemas de visão artificial e os projetistas de sistemas 
 Técnicas para geração de malhas volumétricas não estruturadas podem ser divididas em três categorias principais : técnicas baseadas em Octree , técnica de avanço de fronteira e técnicas Delaunay . Este trabalho faz_uso  de uma proposta para geração de malhas volumétricas não estruturadas baseada em uma técnica que utiliza uma triangulação adaptativa que contém similaridades com os métodos de geração de malhas baseados em Octree . Esta triangulação , além de decompor o domínio , oferece suporte para a construção de funções implícitas que aproximam a superfície do objeto de interesse . As funções implícitas são obtidas a partir de nuvens de pontos pelo método de Partição da Unidade Implícita - PUI . Um processo de poligonalização discretiza a isosuperfície fornecida pela função implícita gerando uma malha de triângulos superficial . Uma malha volumétrica é gerada a partir da malha superficial utilizando os tetraedros que decompõem o domínio criados pela triangulação . Apresentamos nesta tese um método original para a poligonalização de isosuperfícies obtidas_através  de nuvens de pontos . Apresentamos também uma abordagem para o melhoramento de malhas volumétricas baseadas na aplicação do conceito de molas virtuais . O foco_principal  é gerar malhas de qualidade para serem usadas em simulações de fluidos em artérias do corpo 
 Arquiteturas de referência são construídas por combinarem as melhores praticas , padrões , plataformas e componentes para a construção e padronização de sistemas de software de um determinado domínio . De fato , diversas arquiteturas de referência podem ser encontradas para o domínio de sistemas embarcados , motivadas principalmente pela importância e complexidade crescentes que esses sistemas de software vêm apresentando . Dentre as atividades para elaboração de uma arquitetura de referência , a descrição apropriada dessa arquitetura é essencial para permitir que ela seja de fato utilizada . Contudo , não há na literatura um consenso sobre qual a melhor maneira de descrever arquiteturas de referência do domínio de sistemas embarcados , os tipos de informação que devem ser capturados ou ainda o conjunto de pontos de vista que pode ser construído . Visando sistematizar e padronizar a representação de arquiteturas de referência de sistemas embarcados , este trabalho propõe o método ProSA-Re . O método baseia-se nos resultados de uma revisão_sistemática  conduzida sobre o assunto e estabelece um conjunto de atividades e diretrizes para a construção da representação de arquiteturas de referência de sistemas embarcados . O método também esclarece o ciclo de vida de arquiteturas de referência , de modo a auxiliar na manutenção e na evolução das representações construídas com o seu apoio . Para ilustrar o ProSA-Re , uma representação da arquitetura de referência de sistemas multiagentes locais foi elaborada . Em seguida , a realização de uma avaliação com especialistas da área de arquitetura de software e um estudo de caso com usuários dessa representação permitiram a identicação de vantagens e limitações desse método . Espera-se que os resultados alcancados nesta dissertação possam contribuir para o reúso do conhecimento arquitetural e o desenvolvimento mais eficiente de sistemas de software do domínio de sistemas 
 Hermes é um arcabouço para a programação de aplicações P2P . Com ele , pode-se criar diversos tipos de aplicações distribuídas , sem se preocupar com a camada de comunicação . O Hermes não é uma implementação de uma rede de sobreposição P2P , e sim uma camada acima das implementações já existentes . O desenvolvedor da aplicação fica isolado da implementação da rede de sobreposição utilizada . Esse isolamento é feito de forma tal que não há limitações quanto à arquitetura de rede utilizada pela implementação , seja ela centralizada , descentralizada , distribuída estruturada ou distribuída não-estruturada . Entre os serviços oferecidos pelo Hermes estão : troca de mensagens , busca , comunicação em grupo e armazenamento distribuído . Geralmente , no início do desenvolvimento de uma aplicação distribuída , tem-se poucas informações sobre o seu tamanho final ou perfil de utilização . O Hermes possibilita ao desenvolvedor da aplicação adiar , até o momento da efetiva implantação do sistema , a decisão sobre qual arquitetura de rede ou qual implementação de rede de sobreposição são as mais apropriadas para suas necessidades . Possibilita também , quando o perfil de utilização muda com o tempo , a troca da implementação utilizada por uma outra que se adeque mais ao novo perfil sem alterações no código da aplicação 
 Este trabalho propõe políticas para servidores_Web  baseados em sessões , visando ao oferecimento de garantias de qualidade de serviço ( Quality of Service - QoS ) a seus usuários . Para o fornecimento de QoS , dois tipos de políticas são consideradas : as responsáveis pela diferenciação de serviços e as responsáveis pelo controle de admissão . Alguns algoritmos de escalonamento foram desenvolvidos com o objetivo de oferecer melhores serviços para as sessões prioritárias : o algoritmo de escalonamento baseado em sessão ( SBSA ) e o algoritmo de escalonamento baseado em sessão com diferenciação de serviços ( SBSA-DS ) . Quanto ao controle de admissão , foram consideradas duas novas políticas , as quais utilizam diferentes parâmetros para a tomada de decisão e têm como objetivo garantir a finalização do maior número de sessões prioritárias . A partir dos resultados obtidos nos experimentos_realizados  por meio de simulação , constatou-se que o emprego dos algoritmos e das políticas desenvolvidos melhora o atendimento das sessões para as quais deve ser oferecido um serviço preferencial . Portanto , em servidores_Web  preocupados com a garantia de QoS para seus usuários , os quais interagem com o sistema por meio de sessões , o uso dessas novas técnicas se mostra 
 Um Data Warehouse ( DW ) é um banco de dados centralizado , orientado por assunto , integrado , não volátil e histórico , criado com o objetivo de dar_apoio  ao processo de tomada de decisão e que estrutura os dados em uma arquitetura analítica bastante distinta da arquitetura relacional utilizada nos bancos de dados transacionais . Construir um DW é um projeto de engenharia bastante complexo pois envolve muitas tecnologias e muitas pessoas , de diferentes equipes , em um grande esforço conjunto para construir esta base central de informações corporativas . O processo tradicional de construção de um DW não utiliza conceitos ágeis e , pelo escopo de desenvolvimento ser grande , pode_levar  muito tempo até que funcionalidades sejam entregues aos clientes . Os métodos ágeis de engenharia de software são muito usados como uma alternativa aos métodos tradicionais de desenvolvimento e têm diferenciais que trazem muito valor a projetos grandes pois , além de buscar desenvolver versões funcionais em prazos curtos , defendem que todos os sistemas têm a constante necessidade de se adaptar a mudanças . Neste trabalho são aplicadas práticas ágeis no processo tradicional de engenharia de DW para que o desenvolvimento seja realizado em ciclos iterativos curtos , tornando possível o desenvolvimento rápido e evolutivo de um DW com entregas constantes de novas_funcionalidades  . A contínua evolução deste complexo ambiente analítico é apoiada por conceitos de banco de dados evolutivos e também por fundamentos de métodos ágeis 
 Nas últimas_décadas  , diversas propostas de técnicas e de processos visando aumentar a sustentabilidade da agricultura ganharam evidência . Tais propostas geram novos modelos de planejamento em que devem ser considerados aspectos técnicos e ecológicos de produção , bem como o acesso de pequenos agricultores familiares ao mercado consumidor . Neste tipo de planejamento da produção , a rotação de culturas desempenha um papel fundamental , pois contribui para a manutenção dos recursos produtivos , para a minimização do uso de recursos não-renováveis e para o controle biológico da população de herbívoros , patógenos e plantas espontâneas . Nesta tese abordamos dois problemas de Programação de Rotação de Culturas ( PRC ) focados na produção de base sustentável de hortaliças : o problema de PRC com restrições de Adjacências ( PRC-A ) e o problema de PRC com atendimento da Demanda ( PRC-D ) . O planejamento da produção de hortaliças é complexo pois envolve , em geral , um grande número de culturas com limitações específicas quanto à época de plantio e com períodos de cultivo e produtividades muito variáveis . A programação de rotação de culturas para as áreas de plantio é formulada como um modelo de otimização 01 e , para os dois problemas , em cada programação considera se tanto aspectos técnicos ( época de plantio e colheita etc . ) quanto ecológicos ( adubação verde , pousio etc. ) . No problema PRC-A o objetivo é a maximização da ocupação das áreas produtivas em que as restrições de plantio são estendidas às áreas adjacentes . Como a formulação_matemática  para o problema tem , em geral , um número muito grande de restrições e variáveis , com matriz de restrições esparsa e bloco-diagonal , o modelo é reformulado com a Decomposição DantzigWolfe , o que permitiu sua resolução por procedimentos baseados em geração de colunas , heurísticos e exatos . No problema PRC-D desejase suprir a demanda de um conjunto de hortaliças tendo-se disponível um conjunto de áreas heterogêneas . As culturas passíveis de plantio , bem como as suas produtividades , dependem da área considerada . O problema foi formulado como um modelo de otimização_linear  em que cada variável está associada a uma programação de rotação de culturas . O modelo contém potencialmente um número grande de programações de rotação e é resolvido por geração de colunas . Experimentos computacionais usando instâncias baseadas em dados reais confirmam a eficácia dos modelos e das metodologias propostos para os 
 A busca de palavras em uma grande coleção de documentos é um problema muito recorrente nos dias de hoje , como a própria utilização dos conhecidos `` motores de busca '' revela . Para que as buscas sejam realizadas em tempo que independa do tamanho da coleção , é necessário que a coleção seja indexada uma única vez . O tamanho destes índices é tipicamente linear no tamanho da coleção de documentos . A compressão de dados é outro recurso bastante utilizado para lidar com o tamanho sempre crescente da coleção de documentos . A intenção deste estudo é aliar a indexação utilizada nas buscas à compressão de dados , verificando alternativas às soluções já propostas e visando melhorias no tempo de resposta das buscas e no consumo de memória utilizada nos índices . A análise das estruturas de índice com os algoritmos de compressão mostra que arquivo invertido por blocos em conjuntos com compressão Huffman por palavras é uma ótima opção para sistemas com restrição de consumo de memória , pois proporciona acesso aleatório e busca comprimida . Neste trabalho também são propostas novas codificações livres de prefixo a fim de melhorar a compressão obtida e capaz de gerar códigos auto-sincronizados , ou seja , com acesso aleatório realmente viável . A vantagem destas novas codificações é que elas eliminam a necessidade de gerar a árvore de codificação Huffman através dos mapeamentos propostos , o que se traduz em economia de memória , codificação mais compacta e menor tempo de processamento . Os resultados obtidos_mostram  redução de 7 % e 9 % do tamanho dos arquivos comprimidos com tempos de compressão e descompressão melhores e menor consumo de memória 
 Esta dissertação de mestrado apresenta o projeto , implementação e teste de um gerador de carga de trabalho para servidores_Web  que permite a caracterização de sessões e que representa uma carga real . O gerador desenvolvido gera carga visando à alimentar programas que simulam o comportamento de servidores_Web  . Para a definição dos parâmetros utilizados no gerador de carga sintética desenvolvido , diversos estudos sobre a carga em servidores_Web  encontrados na literatura , logs de servidores_Web  reais e benchmarks para servidores_Web  , foram utilizados . Vários algoritmos de controle de admissão e de escalonamento de requisições descritos na literatura foram implementados para teste e validação do gerador desenvolvido . Visando a utilização de sessões , é proposto nesta dissertação um algoritmo de controle de admissão , RED-SB ( Random Early Detection - Session Based ) que se baseia em um algoritmo ( RED ) que não considera sessão . Esses algoritmos foram executados utilizando-se um modelo de simulação de servidor Web com suporte a diferenciação de serviços , chamado SWDS ( Servidor Web com Diferenciação de Serviços 
 Este trabalho de pesquisa é dedicado ao desenvolvimento , análise e implementação de um novo esquema upwind de alta_resolução  ( denominada PFDPUS ) para a aproximação de termos convectivos em leis de conservação e problemas relacionados em mecânica dos fluídos . Usando variáveis normalizadas de Leonard , o equema PFDPUS é baseado em uma função polinomial por partes que satisfaz os critérios de estabilidade CBC e TVD . O desempenho do esquema PEDPUS é investigado na solução das equações de advecção de escalares , Burgers , Euler e MHD . O novo esquema é então aplicado para simular_escoamentos  incompressíveis envolvendo superfícies_livres  móveis . Para tanto , o esquema PFDPUS é implementado dentro do software CLAWPACK para problemas compressíveis , e no código Freeflow para poblemas incompressíveis . Os resultados numéricos são comparados com dados experimentais , numéricos e 
 O principal objetivo deste trabalho é desenvolver procedimentos inferências em uma perspectiva bayesiana para modelos de sobrevivência com ( ou sem ) fração de cura baseada na distribuição exponencial por partes . A metodologia bayesiana é baseada em métodos de Monte_Carlo  via Cadeias de Markov ( MCMC ) . Para detectar observações influentes nos modelos considerados foi usado o método bayesiano de análise de influência caso a caso ( Cho et al. , 2009 ) , baseados na divergência de Kullback-Leibler . Além_disso  , propomos o modelo destrutivo binomial negativo com fração de cura . O modelo proposto é mais geral que os modelos de sobrevivência com fração de cura , já que permitem estimar a probabilidade do número de causas que não foram eliminadas por um tratamento 
 No presente_trabalho  , estudamos alguns conceitos relacionados ao desenvolvimento de programas paralelos , algumas formas de aplicar computação_paralela  em métodos de otimização contínua e dois métodos que envolvem o uso de otimização . O primeiro método que apresentamos , chamado PUMA ( Pointwise Unconstrained Minimization Approach ) , recupera constantes óticas e espessuras de filmes finos a partir de valores de transmitância . O problema de recuperação é modelado como um problema inverso e resolvido com auxílio de um método de otimização . Através da paralelização do PUMA viabilizamos a recuperação empírica de constantes e espessuras de sistemas compostos por até dois filmes sobrepostos . Relatamos aqui os resultados obtidos e discutimos o desempenho da versão paralela e a qualidade dos resultados obtidos . O segundo método estudado tem o objetivo de obter configurações iniciais de moléculas para simulações de dinâmica molecular e é chamado PACKMOL . O problema de obter uma configuração inicial de moléculas é modelado como um problema de empacotamento e resolvido com o auxílio de um método de otimização . Construímos uma versão paralela do PACKMOL e mostramos os ganhos de desempenho obtidos com a paralelização 
 Este trabalho apresenta o desenvolvimento de um método_numérico  para simular_escoamentos  viscoelásticos com superfícies_livres  de um fluido governado pelo modelo de Oldroyd-B . As equações governantes para um fluido Oldroyd-B são consideradas . A derivada temporal é aproximada por um método de segunda_ordem  . Uma formulação para o cálculo do tensor de tensão extra nos contornos rígidos é apresentada . As equações governantes são resolvidas pelo método de diferenças_finitas  numa malha_deslocada  utilizando variáveis primitivas . O método_numérico  descrito neste trabalho foi implementado no ambiente de simulação Freeflow-2D . Resultados_numéricos  demonstrando que o método_numérico  empregado neste trabalho aplicado a vários escoamentos bidimensionais de um fluido Oldroyd-B são apresentados 
 O estudo da acústica de salas data do início do século XX e , até então , esta atividade era vista quase como uma arte e não como ciência . Foram desenvolvidas teorias , como a acústica geométrica de salas , que tornou o estudo do fenômeno acústico mais facilmente inteligível . Com o advento dos computadores digitais , os pesquisadores da área de acústica , como Asbjørn Krokstad e Manfred Robert Schroeder desenvolveram , em torno de 1960 , os primeiros algoritmos para simular o comportamento do campo sonoro dentro de salas . Desde então , vários métodos de simulação acústica foram criados [ QIK+08 ] . Nesta dissertação descrevemos uma implementação funcional do algoritmo de Traçado de Raios na linguagem Java . Explicamos em nosso trabalho os vários métodos de simulação acústica existentes nos dias de hoje e descrevemos os passos necessários para a correta implementação de um algoritmo de traçado de raios . Nossa implementação foi realizada como um módulo do sistema AcMus , uma ferramenta de software_livre  que reúne , em um único ambiente integrado , medição , análise e , agora , simulação acústica de salas , tornando-se o único software_livre  existente a integrar essas três funcionalidades em um único ambiente . O módulo de simulação foi implementado como um arcabouço orientado a objetos de forma que outros pesquisadores possam estendê-lo incluindo , com pouco esforço , novos algoritmos de simulação e novas_funcionalidades  
 O modelo de programação baseado em atores é frequentemente utilizado para o desenvolvimento de grandes aplicações e sistemas . Podemos citar como exemplo o serviço de bate-papo do Facebook ou ainda o WhatsApp . Estes sistemas dão suporte a milhares de usuários conectados simultaneamente levando em conta estritas restrições de desempenho e interatividade . Tais sistemas normalmente são amparados por infraestruturas de hardware com processadores de múltiplos núcleos . Normalmente , máquinas deste porte são baseadas em uma estrutura de memória_compartilhada  hierarquicamente ( NUMA - Non-Uniform Memory Access ) . Nossa análise dos atuais ambientes de execução para atores e a pesquisa na literatura mostram que poucos_estudos  sobre a adequação deste ambientes a essas plataformas hierárquicas foram conduzidos . Estes ambientes de execução normalmente assumem que o espaço de memória é uniforme o que pode causar sérios problemas de desempenho . Nesta tese nós estudamos os desafios enfrentados por um ambiente de execução para atores quando da sua execução nestas plataformas . Estudamos particularmente os problemas de gerenciamento de memória , de escalonamento e de balanceamento de carga . Neste documento nós também analisamos e caracterizamos as aplicações baseadas no modelo de atores . Tal análise nos permitiu evidenciar o fato de que a execução de benchmarks e aplicações criam estruturas de comunicação peculiares entre os atores . Tais peculiaridades podem , então , ser utilizadas pelos ambientes de execução para otimizar o seu desempenho . A avaliação dos grafos de comunicação e a implementação da prova de conceito foram feitas utilizando um ambiente de execução real , a máquina virtual da linguagem Erlang . A linguagem Erlang utiliza o modelo de atores para concorrência com uma sintaxe clara e consistente . As modificações que nós efetuamos nesta máquina virtual permitiram uma melhora significativa no desempenho de certas aplicações através de uma melhor afinidade de comunicação entre os atores . O escalonamento e o balanceamento de carga também foram melhorados graças à utilização do conhecimento sobre o comportamento da aplicação e sobre a plataforma de hardware 
 Esta tese apresenta um conjunto de avaliações experimentais com o objetivo de identificar como as camadas de hardware e software podem ser determinantes na obtenção de desempenho em máquinas virtuais em nuvem . A computação em nuvem apresenta benefícios no fornecimento de serviços de infraestrutura de forma conveniente e sob_demanda  . Essa oferta de serviços , sob_demanda  , pode conduzir os clientes à contratação de serviços virtualizados que atendam às suas necessidades com bom_desempenho  . Quando os serviços de infraestrutura são vendidos ou alugados com a descrição apenas das características das máquinas virtuais , o desempenho obtido pode não ser o desejado devido às restrições dos recursos físicos . Sendo_assim  , é proposto um levantamento sobre quais informações são fornecidas pelos provedores de nuvem . De posse das informações coletadas , um conjunto de experimentos demonstra a importância da camada de virtualização para os serviços de infraestrutura . Os resultados apresentados nesta tese demonstram que apesar de os provedores terem tipos de máquinas virtuais semelhantes nos recursos virtuais , os recursos físicos são determinantes na obtenção do desempenho . Além dos recursos físicos , os recursos de software também são determinantes na obtenção do desempenho , devido principalmente à marcante influência do virtualizador considerado 
 Em um programa de ensaio de prociência ( EP ) conduzido pelo Grupo de Motores , um grupo de onze laboratórios da área de temperatura realizaram medições em cinco pontos da escala de um termopar . Neste trabalho , propomos um modelo de regressão com variável explicativa X ( aleatória ) representando o termopar padrão que denominaremos por artefato e a variável dependente Y representando as medições dos laboratórios . O procedimento para a realização da comparação é simples , ambos termopares são colocados no forno e as diferenças entre as medições são registradas . Para a análise dos dados , vamos trabalhar com a diferença entre a diferença das medições do equipamento do laboratório e o artefato , e o valor de referência ( que é determinado por 2 laboratórios que pertencem a Rede Brasileira de Calibração ( RBC ) ) . O erro de medição tem variância determinada por calibração , isto é , conhecida . Assim , vamos encontrar aproximações para as estimativas de máxima_verossimilhança  para os parâmetros do modelo via algoritmo EM . Além_disso  , propomos uma estratégia para avaliar a consistência dos laboratórios participantes do programa de 
 Otimizar simultaneamente os interesses dos usuários , anunciantes e publicadores é um grande desafio na área de publicidade computacional . Mais precisamente , a ordenação de anúncios , ou ad ranking , desempenha um papel central nesse desafio . Por outro_lado  , nem mesmo as melhores fórmulas ou algoritmos de ordenação são capazes de manter seu status por um longo tempo em um ambiente que está em constante mudança . Neste trabalho , apresentamos uma análise orientada a dados que mostra a importância de combinar diferentes dimensões de publicidade computacional por meio de uma abordagem evolutiva para ordenação de anúncios afim de responder a mudanças de forma mais eficaz . Nós avaliamos as dimensões de valor comercial , desempenho histórico de cliques , interesses dos usuários e a similaridade textual entre o anúncio e a página . Nessa avaliação , nós averiguamos o desempenho e a correlação das diferentes dimensões . Como consequência , nós desenvolvemos uma abordagem evolucionária para combinar essas dimensões . Essa abordagem é composta por três partes : um repositório de configurações para facilitar a implantação e avaliação de experimentos de ordenação ; um componente evolucionário de avaliação orientado a dados ; e um motor de programação genética para evoluir fórmulas de ordenação de anúncios . Nossa abordagem foi implementada com sucesso em um sistema real de publicidade computacional responsável por processar mais de quatorze bilhões de requisições de anúncio por mês . De acordo com nossos resultados , essas dimensões se complementam e nenhuma delas deve ser neglicenciada . Além_disso  , nós mostramos que a combinação evolucionária dessas dimensões não só é capaz de superar cada uma individualmente , como também conseguiu alcançar melhores_resultados  do que métodos estáticos de ordenação de anúncios 
 O estado_emocional  dos usuários influencia na tomada de decisão e é essencial para o conhecimento e explicação do comportamento dos usuários com aplicações computacionais . Sistemas computacionais fazem parte da vida cotidiana , influenciando o comportamento humano e estimulando mudanças nos estados emocionais . A avaliação das emoções dos usuários , durante a interação com aplicações computacionais , pode ajudar a fornecer interfaces flexíveis e melhores sistemas de recomendação . No entanto , as emoções são complexas e difíceis de identificar ou avaliar . Pesquisas anteriores demonstraram que o uso de sensores individuais , em um cenário do mundo_real  , não fornece uma avaliação emocional precisa . Acredita-se que somente uma visão holística pode permitir tirar conclusões significativas sobre o estado_emocional  dos usuários . Assim , nesta Tese , propõe-se uma abordagem chamada UserSense , que considera vários sensores para estimar os estados emocionais dos usuários em tempo de interação . A abordagem proposta , a partir de múltiplos sensores , considera várias entradas de usuários ( como a fala , movimentos faciais , frequência cardíaca e atividades ) e utiliza métodos de inteligência_artificial  para mapear essas diferentes respostas em um ou mais estados emocionais . Para a validação , os dados coletados pelos sensores durante a interação do usuário foram confrontados com os dos especialistas ( psicólogos ) . A Teoria Componencial das Emoções e o Espaço Emocional Semântico de Scherer são utilizados na fundamentação teórica da abordagem UserSense que , baseada no Middleware Adaptativo OpenCom , incorpora funcionalidades para carregar e descarregar novos recursos , necessários a infraestrutura proposta . Os resultados experimentais_mostram  que a combinação de resultados gerados por vários sensores , fornece uma avaliação mais precisa dos estados emocionais do que considerar sensores individualmente 
 As Interfaces Cérebro-Computador ( do inglês Brain-Computer Interfaces BCI ) são sistemas que visam permitir a interação entre usuários e máquinas por meio do monitoramento das atividades cerebrais . Sistemas de BCI são considerados como uma alternativa para que pessoas com perda severa ou total do controle motor , tais como as que sofrem de Esclerose Lateral Amiotrófica , possam contar com algum controle sobre o ambiente externo . Para mapear intenções individuais em operações de máquina , os sistemas de BCI empregam um conjunto de etapas que envolvem a captura e pré-processamento dos sinais cerebrais , a extração e seleção de suas características mais relevantes e a classificação das intenções . O projeto e a implementação de sistemas de BCI viáveis ainda são questões em aberto devido aos grandes desafios encontrados em cada uma de suas etapas . Esta lacuna motivou este trabalho de mestrado o qual apresenta uma avaliação dos principais extratores de características utilizados para classificar ensaios de imagética motora , cujos dados foram obtidos por meio de eletroencefalografia ( EEG ) e apresentam influências de artefatos , mais precisamente daqueles produzidos por interferências provenientes de atividades oculares ( monitoradas por eletrooculografia EOG ) . Foram considerados sinais coletados pela BCI Competition IV-2b , os quais contêm informações sobre três canais de EEG e três outros de EOG . Como primeira etapa , foi realizado o pré-processamento desses canais utilizando a técnica de Análise de Componentes Independentes ( ICA ) em conjunto com um limiar de correlação para a remoção de componentes associados a artefatos oculares . Posteriormente , foram avaliadas diferentes abordagens para a extração de características , a mencionar : i ) Árvore Diádica de Bandas de Frequências ( ADBF ) ; ii ) Padrões Espaciais Comuns ( CSP ) ; iii ) Padrões Espectro-Espaciais Comuns ( CSSP ) ; iv ) Padrões Esparsos Espectro-Espaciais Comuns ( CSSSP ) ; v ) CSP com banco de filtros ( FBCSP ) ; vi ) CSSP com banco de filtros ( FBCSSP ) ; e , finalmente , vii ) CSSSP com banco de filtros ( FBCSSSP ) . Contudo , como essas técnicas podem produzir espaços de exemplos com alta dimensionalidade , considerou-se , também , a técnica de Seleção de Características baseada em Informação Mútua ( MIFS ) para escolher os atributos mais relevantes para o conjunto de dados adotado na etapa de classificação . Finalmente , as Máquinas de Vetores de Suporte ( SVM ) foram utilizadas para a classificação das intenções de usuários . Experimentos permitem concluir que os resultados do CSSSP e FBCSSSP são equiparáveis àqueles produzidos pelo estado da arte , considerando o teste de significância estatística de Wilcoxon bilateral com confiança de 0 , 95 . Apesar disso o CSSSP tem sido negligenciado pela área devido ao fato de sua parametrização ser considerada complexa , algo que foi automatizado neste trabalho . Essa automatização reduziu custos computacionais envolvidos na adaptação das abordagens para indivíduos específicos . Ademais , conclui-se que os extratores de características FBCSP , CSSP , CSSSP , FBCSSP e FBCSSSP não necessitam da etapa de remoção de artefatos oculares , pois efetuam filtragens por meio de modelos autoregressivos 
 Neste trabalho estudamos problemas relacionados com a busca de pontos e segmentos em janelas retangulares com os lados paralelos aos eixos . É dado um conjunto de segmentos ( ou pontos ) no plano . Em uma primeira fase estes segmentos são organizados em estruturas de dados de tal forma a tornar buscas por aqueles que estão contidos em janelas retangulares mais eficiente . Na segunda fase são dadas as janelas de maneira online . Várias destas estruturas de dados são baseadas em árvores balanceadas , tais como , árvore limite , árvore de busca com prioridade , árvore de intervalos e árvore de segmentos . Na dissertação mostramos detalhadamente estas estruturas de dados e os algoritmos para resolver este problema para conjuntos de pontos ( versão unidimensional do problema ) e para segmentos no plano , tanto horizontais e verticais como com qualquer orientação ( sem cruzamentos ) . Os algoritmos são analisados de forma rigorosa quanto ao seu uso de espaço e de tempo . Implementamos também os vários algoritmos estudados , construindo uma biblioteca destas estruturas de dados . Apresentamos , finalmente os resultados de experimentos computacionais com instâncias do problema 
 Atividades sistemáticas e rigorosas no processo de desenvolvimento são fundamentais para a obtenção de produtos de software de qualidade , entre elas a utilização de técnicas formais . Apesar do rigor estabelecido pelo uso dessas técnicas nas fases iniciais de desenvolvimento , a qualidade da especificação pode não ser satisfatória . A condução de atividades de W & T Verificação , Validação e Teste , paralelamente às atividades de desenvolvimento , visa a assegurar a qualidade adequada a cada passo do processo . Este trabalho tem como objetivo contribuir para o teste de especificações SDL . SDL é uma técnica formal que surgiu da necessidade de se desenvolver uma linguagem padrão de especificação para a área de telecomunicações . Mais recentemente , SDL tem sido utilizada para a descrição de arquiteturas de software . Diversas técnicas têm sido propostas e utilizadas para garantir a qualidade de especificações SDL , como simulação e model checking ; no entanto , não propiciam a análise quantitativa da atividade de validação , como por exemplo , a análise de cobertura , um dos aspectos fundamentais reconhecidos por diversos pesquisadores para avaliar a qualidade das atividades de teste e validação . Neste trabalho foram propostos critérios de teste baseados em fluxo de controle , em fluxo de dados e em mutação , além de estratégias de teste associadas para o teste dos processos de especificações SDL . Também foi definida uma família de critérios de fluxo de controle , FCCSDL , considerando o comportamento de todo o sistema , representado pela Arvore de Alcançabilidade para SDL proposta no escopo deste trabalho . Também foram definidas facilidades para a depuração de especificações SDL , com base nas informações de cobertura de teste . As ferramentas CATSDL e SmartDsDL foram desenvolvidas para apoiar as atividades de teste e depuração de especificações SDL . Com base no estudo de caso conduzido , foram fornecidas evidências da viabilidade da aplicação dos critérios de teste definidos e estabelecida uma estratégia de teste para guiar a atividade de teste de especificações SDL 
 Num cenário mundial de rápida expansão das relações interculturais e da transmissão de conhecimento científico e tecnológico , a língua muitas_vezes  representa uma barreira para a comunicação . Ferramentas computacionais , como os tradutores automáticos , podem acelerar e facilitar a comunicação escrita e oral entre povos de línguas e culturas diversas . Algoritmos e técnicas de alinhamento de textos paralelos têm recebido muita atenção nesse cenário . Neste projeto foram implementadas algumas técnicas de alinhamento lexical de textos paralelos para um corpus em português do Brasil e inglês , que foram avaliadas segundo as métricas de revocação e precisão , e seus resultados foram analisados 
 A definição de abordagens de teste de software no contexto de sistemas embarcados_críticos  é de fundamental_importância  para a melhoria de qualidade desses sistemas . Este projeto de mestrado concentra-se essencialmente na área de robôs_móveis  que são sistemas críticos . O mapeamento das técnicas de teste dos programas tradicionais para sistemas robóticos móveis não é trivial , pois é necessário considerar as características inerentes a esses sistemas , as quais incluem comunicação , sincronização , não determinismo e paralelismo . Esses sistemas são formados por diferentes componentes , como sensores , atuadores e softwares de controle , os quais se comunicam , em geral , por meio de troca de mensagens . Nesse sentido , este projeto visa aproveitar a experiência adquirida pelo grupo de pesquisa do ICMC/USP com a proposição de critérios e ferramentas de teste para programas concorrentes , para o contexto de teste de integração de sistemas robóticos móveis . Assim , neste estudo é apresentado um comparativo sobre as similaridades e diferenças presentes em programas concorrentes e sistemas robóticos móveis com objetivo de auxiliar na adaptação dos critérios de teste . Dois estudos de caso foram conduzidos nos quais os critérios propostos para programas concorrentes foram aplicados a sistemas robóticos desenvolvidos em ROS . Como resultados , observou-se os critérios de teste são capazes de testar adequadamente aspectos de comunicação e sincronização de sistemas robóticos , contribuindo com a qualidade desses sistemas 
 Nesta dissertação é apresentado um método para o projeto de células de manufatura baseado em coloração em grafos . O método proposto calcula as dissimilaridades entre as peças e organiza o sistema de produção em famílias de peças e grupos de máquinas . Um grafo correspondente ao sistema de produção é gerado e um algoritmo de coloração acionado para obter um número de cores igual ao número desejado de células . O programa correspondente foi escrito em linguagem Delphi e está implementado em microcomputador . Os resultados obtidos sobre vários exemplos da literatura são equivalentes ou melhores do que aqueles propostos até o presente momento , em termos do número de movimentos inter-células e da dimensão das células 
 A exigência por produtos de software com qualidade tem-se mostrado recorrente no mercado dos dias atuais . A fim de que essa qualidade possa se manifestar , vários processos que concorrem para a `` fabricação '' do software necessitam ser muito bem definidos , executados e gerenciados . É , pois , imperativo , que se tenha uma abordagem para Garantia de Qualidade de Software , bem planejada e sistemática , para ser utilizada na avaliação dos produtos e dos processos envolvidos no desenvolvimento de software , e que permita e assegure o uso dos resultados dessa avaliação pelos gerentes e desenvolvedores ao longo de todo o ciclo de desenvolvimento . O que o presente_trabalho  propõe , então , é a abordagem GQS-AE ( uma abordagem evolucionista para garantir a qualidade do software ) , que descreve a forma como manipular , organizar e gerenciar os recursos - técnicos e humanos - disponíveis em uma empresa de software , sob uma base pré-estabelecida , a fim de que sejam alcançados os objetivos de melhoria da qualidade de tal software . Ressalta-se , porém , que essa abordagem foi concebida de forma simplificada e presa às restrições que normalmente apresenta uma pequena empresa , incapacitada , na grande maioria das vezes , de adaptar modelos já existentes por serem eles mais adequados à realidade de empresas de maior porte 
 Este trabalho apresenta uma avaliação de meta-heurísticas para a resolução do Problema de Designação Quadrática ( Quadratic Assignment Problem ) . O Problema de Designação Quadrática consiste em posicionar-se um determinado conjunto de instalações em um determinado conjunto de localidades , dados os fluxos entre as instalações e as distâncias entre as localidades , com o objetivo de minimizar um somatório que vincula os fluxos às distâncias de acordo com o posicionamento . Foram implementadas heurísticas que utilizam Busca Tabu , Algoritmos Genéticos e Algoritmos Meméticos , e uma avaliação com exemplos numéricos encontrados na literatura foi realizada 
 Esta dissertação de mestrado tem por objetivo analisar os conjuntos-K , uma hierarquia de redes neurais biologicamente mais plausíveis , e aplicá-los ao problema de classificação de imagética motora através do eletroencefalograma ( EEG ) . A imagética motora consiste no ato de processar um movimento motor da memória humana de longo tempo para a memória de curto prazo . A imagética motora deixa um rastro no sinal do EEG que torna possível a identificação e classificação dos diferentes movimentos motores . A tarefa de classificação de imagética motora através do EEG é reconhecida como complexa devido à não linearidade e quantidade de ruído da série_temporal  do EEG e da pequena quantidade de dados disponíveis para aprendizagem . Os conjuntos-K são um modelo conexionista que simula o comportamento dinâmico e caótico de populações de neurônios do cérebro e foram modelados com base em observações do sistema olfatório feitas por Walter Freeman . Os conjuntos-K já foram aplicados em diversos_domínios  de classificação diferentes , incluindo EEG , tendo demonstrado bons_resultados  . Devido às características da classificação de imagética motora , levantou-se a hipótese de que a aplicação dos conjuntos-K na tarefa pudesse prover bons_resultados  . Um simulador para os conjuntos-K foi construído para a realização dos experimentos . Não foi possível validar a hipótese levantada no trabalho , dado que os resultados dos experimentos_realizados  com conjuntos-K e imagética motora não apresentaram melhorias significativas para a tarefa nas comparações realizadas 
 Com o desenvolvimento e barateamento dos equipamentos de aquisição de imagens , principalmente na área médica , tem sido geradas muitas imagens , as quais devem ser analisadas pelos especialistas . Esta tarefa pode ser muitas_vezes  cansativa e demorada , levando a possíveis erros no diagnóstico , pois a leitura das imagens depende da experiência e do estado físico e emocional do médico . Assim , sistemas de auxílio ao diagnóstico por computador ( Computer-aided diagnosis - CAD ) têm se tornado grandes aliados no processo de diagnóstico , realizando uma segunda leitura da imagem , servindo como uma segunda opinião ao especialista . Por isso , é necessário o desenvolvimento de técnicas de mineração de imagens para o aumento da precisão e da velocidade da análise das imagens . Assim , o objetivo deste trabalho foi desenvolver métodos de representação de imagens e de classificação associativa para aumentar a precisão da classificação de sistemas de auxílio ao diagnóstico médico por imagens . Para a representação de imagens foram desenvolvidas técnicas para reduzir a lacuna que há entre a representação numérica das imagens e seu significado semântico , a qual é chamada de ` gap semântico ' . Para isso , foi usada a teoria das redes complexas para modelar as imagens em redes livres de escala , e os descritores das imagens foram compostos pelas medidas topológicas extraídas rede modelada . Os vetores de características gerados foram bem compactos , o que possibilitou também evitar o problema da ` maldição da alta dimensionalidade ' . Para a classificação , foi desenvolvido o classificador associativo SACMiner , por meio do uso de regras de associação estatísticas , o qual evita a fase de discretização de dados , lidando diretamente com dados contínuos . Este foi um passo importante , já que a discretização pode causar a perda de informações e gerar inconsistência na base de dados . Além do SACMiner , foi desenvolvido o classificador MinSAR , o qual , além de não demandar a fase de discretização , também evita que o usuário tenha que fornecer parâmetros de entrada ao algoritmo responsável por gerar as regras . As técnicas até aqui listadas foram aplicadas em um sistema de auxílio ao diagnóstico de mama e comparadas com técnicas descritas na literatura , e os resultados mostram que as técnicas aqui propostas sobrepujaram as atuais da literatura . E por fim , foram sugeridas novas medidas para caracterizar imagens de pacientes com epilepsia no lobo temporal mesial , por meio do uso de medidas de espessura cortical , as quais melhoraram a precisão do sistema para este tipo de 
 A recuperação de informação é ainda um assunto essencial a melhorar nos diferentes_tipos  de sistemas web . Um tipo de sistema web que é muito utilizado na atualidade , é o sistema colaborativo . Estes sistemas permitem que os usuários estejam mais envolvidos , seja contribuindo com a inserção de textos , imagens ou dados , assim como utilizando etiquetas ( tags ) para identificar aos elementos existentes no sistema e que serão compartilhados com outros usuários . Nesta dissertação utilizamos um sistema colaborativo de compartilhamento de imagens arquitetônicas , onde os usuários podem inserir títulos e tags livremente para descrever uma imagem . Contudo as tags podem ter um significado ambíguo , resultando em imagens recuperadas que não são relevantes , quando são utilizadas técnicas tradicionais , como por exemplo busca booleana ou por palavra-chave . Além_disso  , os usuários podem utilizar consultas mais complexas utilizando uma linguagem livre , e utilizando as técnicas mencionadas podem recuperar informação não relevante . Assim , esta pesquisa aborda , a construção de uma ontologia no domínio arquitetônico denominada OntoArq , baseada no vocabulário controlado da USP e no tesauro experimental de arquitetura brasileira , a qual possibilitou fortalecer a relação entre as tags e os conceitos estruturados da ontologia , por meio de uso de hierarquias de classes e relações semânticas existentes entre as classes . A ontologia também ajudou a melhorar a recuperação de documentos para consultas complexas que utilizam uma linguagem livre , por meio da adição de termos arquitetônicos relacionados à consulta original dada pelo usuário . E quando a consulta expandida é utilizada em conjunto com o modelo de espaço vetorial existente no sistema de recuperação , auxilia na recuperação de imagens mais relevantes . A avaliação de nossa abordagem foi realizada através de experimentos que utilizaram os dados do sistema Arquigrafia , dois conjuntos de consultas e medidas de avaliação como precisão , cobertura e medida-F. Os conjuntos eram compostos por 11 consultas dada por especialistas da área de arquitetura e 9 consultas aleatórias extraídas do log de busca do Google Analytics do sistema Arquigrafia , tendo um total de 20 consultas . Para nossos experimentos utilizamos as 20 consultas que pertenciam aos dois conjuntos de consultas mencionados , dentre os quais obtivemos resultados positivos para 16 consultas , considerando um valor de precisão , cobertura e medida-F maior do que 50 % , com nossa abordagem . Em comparação a outra abordagem , que usa a técnica de busca boolena , obteve-se 1 consulta com resultado positivo , também considerando precisão , cobertura e medida-F maior do que 50 % . Assim , podemos concluir que nossa abordagem obteve melhores_resultados  . Além_disso  , pelos resultados obtidos , consideramos que nossa abordagem , ao utilizar uma ontologia , pode ser um inicio de como empregar as ontologias como ferramenta de apoio para dar um maior significado semântico às tags que existem num sistema colaborativo e como as ontologias permitem a adição de termos na consulta , sendo estes termos relacionados a uma área do conhecimento , que para nosso caso , a área da arquitetura . Desta maneira podemos recuperar os documentos associados às imagens , os quais serão mais relevantes para consulta feita pelo usuário 
 Sistemas multi-agentes ( SMAs ) podem ser usados para modelar fenômenos que podem ser decompostos em diversos agentes que interagem entre si dentro de um ambiente . Em particular , eles podem ser usados para modelar sociedades humanas e animais , com a finalidade de se analisar as suas propriedades computacionalmente . Esta tese trata da análise automatizada de um tipo particular de tais modelos sociais , a saber , aqueles baseados em princípios behavioristas , o que contrasta com as abordagens cognitivas mais dominante na literatura de SMAs . A principal característica das teorias behaviorista é a ênfase na descrição do comportamento em termos da interação entre agentes e seu ambiente . Desta forma , não apenas ações refl exivas , mas também de aprendizado , motivações , e as emoções podem ser definidas . Mais especificamente , nesta tese apresentamos uma arquitetura de agentes formal ( especificada através da Notação Z ) baseada na teoria da Análise do Comportamento de B. F. Skinner , e fornecemos uma noção adequada e formal de ambiente ( com base na álgebra de processos pi-calculus ) para colocar tais agentes juntos em um SMA . Simulações são freqüentemente utilizadas para se analisar SMAs . As técnicas envolvidas tipicamente consistem em simular um SMA diversas vezes , seja para coletar estatísticas , seja para observar o que acontece através de animações . Contudo , simulações podem ser usadas de forma a pertmitir a realização de verificações automatizadas do SMA caso sejam entendidas como explorações de grandes espaços-de-estados . Nesta tese propomos uma técnica de verificação baseada nessa observação , que consiste em simular um SMA de uma forma guiada , a fim de se determinar se uma dada hipótese sobre ele é verdadeira ou não . Para tal fim , tiramos proveito da importância que os ambientes têm nesta tese : a especificação formal do ambiente de um SMA serve para calcular as evoluções possíveis do SMA como um sistema de transição , estabelecendo assim o espaço-de-estados a ser investigado . Neste cálculo , os agentes são levados em conta simulando-os , a fim de determinar , em cada estado do ambiente , quais são suas ações . Cada execução da simulação é uma seqüência de estados nesse espaço-de-estados , que é calculado em tempo de execução , conforme a simulação progride . A hipótese a ser investigada , por sua vez , é dada como um outro sistema de transição , chamado propósito de simulação , o qual define as simulações desejáveis e indesejáveis ( e.g. , `` sempre que o agente fizer X , ele fará Y depois '' ) . Em seguida , é possível verificar se o SMA satisfaz o propósito de simulação de acordo com uma série de relações de satisfatibilidade precisamente definidas . Algoritmicamente , isso corresponde a construir um produto síncrono desses dois sistemas de transições ( i.e. , o do SMA e o do propósito de simulação ) em tempo de execução e usá-lo para operar um simulador . Ou seja , o propósito de simulação é usado para guiar o simulador , de modo que somente os estados relevantes sejam efetivamente simulados . Ao terminar , um tal algoritmo pode fornecer um veredito conclusivo ou inconclusivo . Se conclusivo , descobre-se se o SMA satisfaz ou não o propósito de simulação com relação às observações feitas durante as simulações . Se inconclusivo , é possível realizar alguns ajustes e tentar novamente . em resumo , portanto , nesta tese propomos quatro novos elementos : ( i ) uma arquitetura de agente , ( ii ) uma especificação formal do ambiente desses agentes , de modo que possam ser compostos em um SMA , ( iii ) uma estrutura para descrever a propriedade de interesse , a qual chamamos de propósito de simulação , e ( iv ) uma técnica para se analisar formalmente o SMA resultante com relação a um propósito de simulação . Esses elementos estão implementados em uma ferramenta , denominada Simulador Formalmente Guiado ( FGS , do inglês Formally Guided Simulator ) . Estudos de caso executáveis no FGS são fornecidos para ilustrar a abordagem 
 As emoções são objeto de estudo não apenas da psicologia , mas também de diversas_áreas  como filosofia , psiquiatria , biologia , neurociências e , a partir da segunda metade do século XX , das ciências cognitivas . Várias teorias e modelos emocionais foram propostos , mas não existe consenso quanto à escolha de uma ou outra teoria ou modelo . Neste sentido , diversos pesquisadores argumentam que existe um conjunto de emoções básicas que foram preservadas durante o processo evolutivo , pois servem a propósitos específicos . Porém , quantas e quais são as emoções básicas aceitas ainda é um tópico em discussão . De modo geral , o modelo de emoções básicas mais difundido é o proposto por Paul Ekman , que afirma a existência de seis emoções : alegria , tristeza , medo , raiva , aversão e surpresa . Estudos também indicam que existe um pequeno conjunto de expressões faciais universais capaz de representar as seis emoções básicas . No contexto das interações homem-máquina , o relacionamento entre ambos vem se tornando progressivamente natural e social . Desta forma , à medida que as interfaces evoluem , a capacidade de interpretar sinais emocionais de interlocutores e reagir de acordo com eles de maneira apropriada é um desafio a ser superado . Embora os seres_humanos  utilizem diferentes maneiras para expressar emoções , existem evidências de que estas são mais precisamente descritas por expressões faciais . Assim , visando obter interfaces que propiciem interações mais realísticas e naturais , nesta tese foi desenvolvida uma modelagem computacional , baseada em princípios psicológicos e biológicos , que simula o sistema de reconhecimento emocional existente nos seres_humanos  . Diferentes etapas são utilizadas para identificar o estado_emocional  : a utilização de um mecanismo de pré-atenção visual , que rapidamente interpreta as prováveis emoções , a detecção das características faciais mais relevantes para o reconhecimento das expressões emocionais identificadas , e a análise de características geométricas da face para determinar o estado_emocional  final . Vários experimentos demonstraram que a modelagem proposta apresenta taxas de acerto elevadas , boa capacidade de generalização , e permite a interpretabilidade das características faciais encontradas 
 Neste trabalho é apresentada a implementação do Sistema de Simulação Distribuída ( SSD ) , uma extensão funcional para simulação distribuída na linguagem de programação concorrente Occam2 . O sistema proposto foi desenvolvido utilizando-se a estrutura básica da extensão funcional para simulação SMPL , implementada em C e que permite a execução de simulação seqüencial . Um dos pontos críticos da simulação distribuída é a utilização de um protocolo para garantir o sincronismo da simulação . No sistema desenvolvido foi adotado o método CMB , proposto por Chandy , Misra e Bryant . Este método é assíncrono e conservativo , isto é , garante que o sincronismo será sempre verificado . A implementação e testes do sistema proposto foram efetuados utilizando-se uma máquina com arquitetura MIMD baseada em Transputers . Os resultados obtidos_mostram  que a utilização do sistema implementado pode apresentar ganhos de performance significativos . São analisados os tempos de execução variando diversos parâmetros , tais como : granularidade e número de iterações 
 A crescente utilização de sistemas baseados em computação em praticamente todas as áreas da atividade humana provoca uma crescente demanda por qualidade e produtividade , tanto do ponto de vista do processo de produção como do ponto de vista dos produtos de software gerados . Nessa perspectiva , atividades agregadas sob o nome de Garantia de Qualidade de Software têm sido introduzidas ao longo de todo o processo de desenvolvimento de software . Dentre essas atividades destacam-se as atividades de Teste e Revisão , ambas com o objetivo principal de minimizar a introdução de erros durante o processo de desenvolvimento nos produtos de software gerados . A atividade de Teste constitui um dos elementos para fornecer evidências da confiabilidade do software em complemento a outras atividades , como por exemplo , o uso de revisões e de técnicas formais e rigorosas de especificação e de verificação . A atividade de Revisão , por sua vez , é um 'filtro ' eficiente para o processo de engenharia de software , pois favorece a identificação e a eliminação de erros antes do passo seguinte do processo de desenvolvimento . Atualmente , pesquisas estão sendo realizadas com objetivo de determinar qual técnica , Revisão ou Teste , é mais adequada e efetiva , em determinadas circunstâncias , para descobrir determinadas classes de erros ; e de forma mais ampla , como as técnicas podem ser aplicadas de forma complementar para melhoria da qualidade de software . Ainda que a atividade de teste seja indispensável no processo de desenvolvimento , investigar o aspecto complementar dessas técnicas é de grande interesse , pois em muitas situações tem-se observado que as revisões são tão ou mais efetivas quanto os testes . Nessa perspectiva , este trabalho tem como objetivo realizar um estudo comparativo , por meio da replicação de experimentos , entre Técnicas de Teste e Técnicas de Revisão no que se refere à detecção de erros em produtos de software ( código fonte e documento de especificação de requisitos ) . Para realizar esse estudo são utilizados critérios de teste das técnicas funcional ( particionamento em classes de equivalência e análise do valor limite ) , estrutural ( todos-nós , todos-arcos , todos-usos , todos-potenciais-usos ) , baseada em erros ( análise de mutantes ) , bem como , técnicas de leitura ( stepwise abstraction e perspective based reading ) e técnicas de inspeção ( ad hoc e checklist ) . Além de comparar a efetividade e a eficiência das técnicas em detectar erros em produtos de software , este trabalho objetivo ainda utilizar os conhecimentos específicos relacionados a critérios de teste para reavaliar as técnicas utilizadas nos experimentos de Basili & Selby , Kamsties & Lott e Basili 
 Utilizou-se neste trabalho um sistema de filas contendo três servidores exponenciais , heterogêneos , operando em paralelo . Trocas entre filas são permitidas após o usuário analisar dois aspectos : a diferença entre o tamanho das filas envolvidas na troca e o grau de vizinhança entre elas . O jockeying não é obrigatório , podendo os usuários optar por ele com uma probabilidade de ocorrência de acordo com os aspectos citados . Como resultado deste estudo foi obtida uma equação geral que representa o sistema . O sistema M/ ( M/1 ) 3 com jockeying probabilístico tem uma ociosidade bem menor que o tradiconal M/Mi/3 , alimentado por fila única . Outras características foram analisadas 
 Devido à facilidade com que informações biomédicas em língua natural são registras e armazenadas no formato digital , a recuperação de informações a partir de registros de pacientes nesse formato não estruturado apresenta diversos problemas a serem solucionados . Assim , a extração de informações estruturadas ( por exemplo , no formato_atributo-valor  ) a partir de registros não estruturados é um importante problema de pesquisa . Além_disso  , a representação de registros médicos não estruturados no formato_atributo-valor  , permite a aplicação de uma grande variedade de métodos de extração de padrões . Para mapear registros médicos não estruturados no formato_atributo-valor  , propomos uma metodologia que pode ser utilizada para automaticamente ( ou semi-automaticamente , com a ajuda de um especialista do domínio ) mapear informações médicas de interesse armazenadas nos registros médicos e descritas em linguagem_natural  em um formato estruturado . Essa metodologia foi implementada em um sistema computacional chamado TP-DISCOVER , o qual gera uma tabela no formato_atributo-valor  a partir de um conjunto de registros de pacientes ( documentos ) . De modo a identificar entidades importantes no conjunto de documentos , assim como relacionamentos significantes entre essas entidades , propomos uma abordagem de extração de terminologia híbrida ( lingüística/estatística ) a qual seleciona palavras e frases que aparecem com freqüência acima de um dado limiar por meio da aplicação de medidas estatísticas . A idéia geral dessa abordagem híbrida de extração de terminologia é que documentos especializados são caracterizados por repetir o uso de certas unidades léxicas ou construções morfo-sintáticas . Nosso objetivo é reduzir o esforço despendido na modelagem manual por meio da observação de regularidades no texto e o mapeamento dessas regularidades como nomes de atributos na representação atributo-valor . A metodologia_proposta  foi avaliada realizando a estruturação automática de uma coleção de 6000 documentos com informações de resultados de exames de Endoscopia Digestiva Alta descritos em língua natural . Os resultados experimentais , os quais podem ser considerados os piores resultados , uma vez que esses resultados poderiam ser muito melhores caso a metodologia for utilizada semi-automaticamente junto com um especialista do domínio , mostram que a metodologia_proposta  é adequada e permite reduzir o tempo usado pelo especialista para analisar grande quantidade de registros 
 Uma das questões mais importantes nas linguagens e arcabouços orientados a aspectos atuais é a expressividade da linguagem ou mecanismo de definição de pointcuts . A expressividade de uma linguagem de pointcuts impacta diretamente a qualidade dos pointcuts , uma propriedade que pode ser decisiva para a eficácia das implementações de aspectos . Neste trabalho , propomos os seletores de pontos de junção como um mecanismo de extensão simples para enriquecer linguagens de pointcut atuais com elementos que fazem o papel de `` novos pointcuts primitivos '' . Os seletores de pontos de junção permitem a criação de pointcuts com maior valor semântico . Apesar de existirem mecanismos similares em algumas abordagens existentes , o conceito subjacente não foi claramente definido ou completamente explorado . Apresentamos também uma arquitetura simples para a adição de seletores de pontos de junção a um arcabouço orientado a aspectos existente , e mostramos exemplos do uso de seletores para melhorar a qualidade de pointcuts e facilitar o desenvolvimento de aspectos 
 A Programação Concorrente tornou-se uma forma popular de desenvolvimento de software . Este paradigma de desenvolvimento e essencial para construir aplicações com o intuito de reduzir o tempo computacional em muitos domínios como , por exemplo , previsão tempo , processamento de imagem , entre outros . Estes programas têm novas características como a comunicação , a sincronização e o não determinismo , que precisam ser considerados durante a atividade de teste . O teste de software e uma atividade que busca garantir a qualidade por meio da identificação de falhas no produto . O Teste de Mutação e um critério de teste que se baseia nos enganos que podem ser cometidos pelos desenvolvedores de software . Porém , o teste de mutação não pode ser aplicado em programas concorrentes da mesma maneira como e aplicado em programas sequenciais por causa das particularidades presentes nos programas concorrentes . Um problema de aplicar o teste de mutação nesse contexto e o comportamento não determinístico das aplicações . Este trabalho_investiga  a definição do teste de mutação para programas concorrentes implementados em MPI ( Message Passing Interface ) , os quais realizam comunicação e sincronização por meio de troca de mensagens . Para isso , defeitos típicos nesse domínio foram considerados , buscando modelar operadores de mutação para tratar os aspectos de comunicação e sincronização dessas aplicações . Também foi proposto um procedimento para dar suporte a análise comportamental dos mutantes . As idéias foram implementadas em uma ferramenta de teste chamada ValiMPI 
 A navegação_autônoma  é um dos problemas fundamentais na área da robótica_móvel  . Algoritmos capazes de conduzir um robô até o seu destino de maneira segura e eficiente são um pré-requisito para que robôs_móveis  possam executar as mais diversas tarefas que são atribuídas a eles com sucesso . Dependendo da complexidade do ambiente e da tarefa que deve ser executada , a programação de algoritmos de navegação não é um problema de solução trivial . Esta tese trata do desenvolvimento de sistemas de navegação_autônoma  baseados em técnicas de aprendizado_supervisionado  . Mais especificamente , foram abordados dois problemas distintos : a navegação de robôs/- veículos em ambientes urbanos e a navegação de robôs em ambientes não estruturados . No primeiro caso , o robô/veículo deve evitar obstáculos e se manter na via navegável , a partir de exemplos fornecidos por um motorista humano . No segundo caso , o robô deve identificar e evitar áreas irregulares ( maior vibração ) , reduzindo o consumo de energia . Nesse caso , o aprendizado foi realizado a partir de informações obtidas por sensores . Em ambos os casos , algoritmos de aprendizado_supervisionado  foram capazes de permitir que os robôs navegassem de maneira segura e eficiente durante os testes experimentais 
 Com a popularização da Web 2.0 e RIA - Rich Internet Applications , as aplicações web cada vez mais utilizam-se da linguagem JavaScript , para implementar recursos de interação sofisticados e complexos na plataforma da Web , visando atrair os usuários com experiências que agradem e atendam suas expectativas . Uma vez que esses recursos de RIA , muitas_vezes  , fornecem feedback visual de mudanças realizadas na interface , usuários que interagem com a Web por meio de Tecnologias Assistivas , como leitores de tela , não são capazes de identificar e interagir corretamente com os componentes de interface . Assim , a WAI - Web Accessibility Initiative propôs a especificação ARIA - Accessible Rich Internet Applications que determina um conjunto de propriedades que atribuem semântica aos elementos de um componente de interface ( Widget ) , permitindo que as Tecnologias Assistivas identifiquem previamente o comportamento dos componentes de interface e informem o usuário sobre as alterações que possam ser realizadas na estrutura da página web . Nesse contexto , esta tese teve como objetivo elaborar estratégias de avaliação automática dos requisitos de acessibilidade da especificação ARIA . Foram elaboradas três diferentes abordagens para analisar os requisitos da especificação ARIA , utilizando a metodologia de pesquisa-ação com a condução de três ciclos das atividades de planejar , agir , descrever e avaliar . As abordagens foram desenvolvidas com base em Testes de Aceitação e verificações de características tecnológicas das aplicações web , considerando especificamente o modelo de interação de usuários deficientes visuais que utilizam leitores de tela . Cada uma das abordagens foi validada separadamente e os resultados apresentam tendências de que as estratégias são capazes de avaliar corretamente o comportamento esperado de uma aplicação rica de Internet acessível , segundo as recomendações ARIA para usuÁ¡rios deficientes visuais . As abordagens também apresentaram como contribuições : a inclusão do modelo de interação do usuário no processo de avaliação e levantamentos sobre os níveis de conformidade de aplicações web e bibliotecas JavaScript com a especificação ARIA . Os resultados obtidos a partir das abordagens propostas nesta tese contribuem para o processo de Engenharia Web de aplicações ricas de Internet 
 O teste de software engloba diferentes técnicas , métodos e conceitos capazes de garantir a qualidade dos mais variados tipos de sistemas . Dentre tais técnicas , encontra-se o teste baseado em Máquinas de Estados_Finitos  ( MEFs ) , que visa a garantir a conformidade entre a implementação e a especificação de um software . Com esse propósito , diversos métodos foram propostos para a geração de seqüências de verificação que garantam cobertura total das possíveis falhas existentes em uma implementação . A maioria dos métodos conhecidos são baseados na utilização de seqüências de distinção . Esse recurso , porem , não existe para toda MEF . Alguns métodos buscam a geração de seqüências de verificação baseados em recursos alternativos as seqüências de distinção , contudo , as seqüências geradas são exponencialmente longas . Este trabalho apresenta um método para geração de seqüências de verificação que visa a reduzir o tamanho das seqüências geradas para o domínio de MEFs que não dispõem de seqüência de distinção . Para isso , o método proposto baseia-se na utilização de conjuntos de distinção . Uma avaliação_experimental  foi realizada afim de mensurar a redução proporcionada pelo método proposto em relação aos principais métodos existentes na literatura . Com esse intuito , foram geradas MEFs aleatórias sob a perspectiva diferentes fatores . Em relação a variação do número de estados , os resultados indicaram reduções acima de 99 ; 5 % em comparação com os métodos existentes , quando analisadas 75 % das MEFs 
 A mineração de grandes coleções de textos , imagens e outros_tipos  de documentos tem se mostrado uma forma efetiva para exploração e interação com grandes quantidades de informações disponíveis , principalmente na World_Wide  Web . Neste contexto , diversos trabalhos têm tratado de mineração tanto de coleções estáticas quanto de coleções dinâmicas de objetos . Adicionalmente , técnicas de visualização têm sido propostas para auxiliar o processo de entendimento e de exploração dessas coleções , permitindo que a interação do usuário melhore o processo de mineração ( user in the loop ) . No caso específico de dados dinâmicos , foi desenvolvido por Roberto Pinho e colegas uma técnica incremental ( IncBoard ) com o objetivo de visualizar coleções dinâmicas de elementos . Tal técnica posiciona os elementos em um grid bidimensional baseado na similaridade de conteúdo entre os elementos . Procura-se manter elementos similares próximos no grid . A técnica foi avaliada em um processo que simulava a chegada de novos dados , apresentando iterativamente novos elementos a serem posicionados no mapa corrente . Observa-se , entretanto , que um aspecto importante de tal ferramenta seria a possibilidade de novos elementos - a serem exibidos no mapa , mantendo coerência com o mapa corrente - serem selecionados a partir do interesse demonstrado pelo usuário . Realimentação de relevância tem se mostrado muito efetiva na melhoria da acurácia do processo de recuperação . Entretanto , um problema ainda em aberto é como utilizar técnicas de realimentação de relevância em conjunto com exploração visual no processo de recuperação de informação . Neste trabalho , é investigado o desenvolvimento de técnicas de exploração visual utilizando realimentação de relevância para sistemas de recuperação de informação de domínio específico . O Amuzi , um sistema de busca de músicas , foi desenvolvido como uma prova de conceito para a abordagem investigada . Dados coletados da utilização do Amuzi , por usuários , sugerem que a combinação de tais técnicas oferece vantagens , quando utilizadas em determinados domínios . Nesta dissertação , a recuperação de informação com realimentação de relevância apoiada em visualização , bem como o sistema Amuzi são descritos . Também são analisados os registros de utilização dos 
 Sistemas-de-Sistemas ( SoS , do inglês Systems-of-Systems ) realizam um importante e até essencial papel na sociedade . Referem-se a complexos sistemas intensivos em software , resultado da interoperabilidade de sistemas constituintes independentes que trabalham juntos para realizar missões mais complexas . SoS têm emergido especialmente em domínios de aplicação crítica , portanto , um alto nível de qualidade deve ser garantido durante seu desenvolvimento e evolução . Entretanto , lidar com qualidade em SoS ainda apresenta grandes desafios , uma vez que possuem um conjunto de características únicas que podem diretamente afetar a qualidade desses sistemas . Além_disso  , não existem modelos abrangentes para o suporte à avaliação de qualidade de SoS . Motivado por este cenário , a principal_contribuição  deste projeto de mestrado é apresentar um modelo de avaliação para SoS , especialmente destinado ao domínio de gerenciamento de crises e emergências . Este modelo foi construído no contexto de um grande projeto de pesquisa internacional , e cobre as mais importantes atividades de avaliação , considerando as principais características e desafios de SoS geralmente não abordados por outros modelos . Este modelo foi aplicado na avaliação de um SoS de gerenciamento de crises e emergência , e nossos resultados têm mostrado sua viabilidade para o efetivo gerenciamento da qualidade de SoS 
 Mineração de Dados é um processo de natureza iterativa e interativa responsável por identificar padrões em grandes_conjuntos  de dados , objetivando extrair conhecimento válido , útil e inovador a partir desses . Em Mineração de Dados , Regras de Associação é uma técnica que consiste na identificação de padrões intrínsecos ao conjunto de dados . Essa técnica tem despertado grande interesse nos pesquisadores de Mineração de Dados e nas organizações , entretanto , a mesma possui o inconveniente de gerar grande volume de conhecimento no formato de regras , dificultando a análise e interpretação dos resultados pelo usuário 
 Nesse contexto , este trabalho tem como objetivo principal generalizar e eliminar Regras de Associação não interessantes e/ou redundantes , facilitando , dessa maneira , a análise das regras obtidas com relação à compreensibilidade e tamanho do conjunto de regras . A generalização das Regras de Associação é realizada com o uso de taxonomias 
 Entre os principais resultados deste trabalho destacam-se a proposta e a implementação do algoritmo GART e do módulo computacional RulEE-GAR . O algoritmo GART ( Generalization of Association Rules using Taxonomies - Generalização de Regras de Associação usando Taxonomias ) utiliza taxonomias para generalizar Regras de Associação . Já o módulo RulEE-GAR , além de facilitar o uso do algoritmo GART durante a identificação de taxonomias e generalização de regras , provê funcionalidades para analisar as Regras de Associação generalizadas 
 Os experimentos_realizados  , neste trabalho , mostraram que o uso de taxonomias na generalização de Regras de Associação pode reduzir o volume de um conjunto de regras 
 A teoria das redes complexas tem se tornado cada vez mais em uma poderosa teoria computacional capaz de representar , caracterizar e examinar sistemas com estrutura não trivial , revelando características intrínsecas locais e globais que facilitam a compreensão do comportamento e da dinâmica de tais sistemas . Nesta tese são exploradas as vantagens das redes complexas na resolução de problemas relacionados com tarefas do âmbito musical , especificamente , são estudadas três abordagens : reconhecimento de padrões , mineração e síntese de músicas . A primeira abordagem é desempenhada através do desenvolvimento de um método para a extração do padrão rítmico de uma peça musical de caráter popular . Nesse tipo de peças coexistem diferentes espécies de padrões rítmicos , os quais configuram uma hierarquia que é determinada por aspectos funcionais dentro da base rítmica . Os padrões rítmicos principais são caracterizados por sua maior incidência dentro do discurso musical , propriedade que é refletida na formação de comunidades dentro da rede . Técnicas de detecção de comunidades são aplicadas na extração dos padrões rítmicos , e uma medida para diferenciar os padrões principais dos secundários é proposta . Os resultados mostram que a qualidade da extração é sensível ao algoritmo de detecção , ao modo de representação do ritmo e ao tratamento dado às linhas de percussão na hora de gerar a rede . Uma fase de mineração foi desempenhada usando medidas topológicas sobre a rede obtida após a remoção dos padrões secundários . Técnicas de aprendizado_supervisionado  e não-supervisionado foram aplicadas para discriminar o gênero musical segundo os atributos calculados na fase de mineração . Os resultados revelam a eficiência da metodologia_proposta  , a qual foi constatada através de um teste de significância estatística . A última abordagem foi tratada mediante o desenvolvimento de modelos para a composição de melodias através de duas perspectivas , na primeira perspectiva é usada uma caminhada controlada por critérios sobre redes complexas predefinidas e na segunda redes neurais recorrentes e sistemas dinâmicos caóticos . Nesta última perspectiva , o modelo é treinado para compor uma melodia com um valor preestabelecido de alguma característica tonal subjetiva através de uma estratégia de controle proporcional que modifica a complexidade de uma melodia caótica , melodia que atua como entrada de inspiração da rede 
 Técnicas de agrupamento de dados usualmente assumem que o conjunto de dados é de tamanho fixo e pode ser alocado na memória . Neste contexto , um desafio consiste em aplicar técnicas de agrupamento em bases de dados de tamanho ilimitado , com dados gerados continuamente e em ambientes dinâmicos . Dados gerados nessas condições originam o que se convencionou chamar de Fluxo Contínuo de Dados ( FCD ) . Em aplicações de FCD , operações de acesso aos dados são restritas a apenas uma leitura ou a um pequeno número de acessos aos dados , com limitações de memória e de tempo de processamento . Além_disso  , a distribuição dos dados gerados por essas fontes pode ser não estacionária , ou seja , podem ocorrer mudanças ao longo do tempo , denominadas de mudanças de conceito . Nesse sentido , algumas técnicas de agrupamento em FCD foram propostas na literatura . Muitas dessas técnicas são baseadas no algoritmo das k-Médias . Uma das limitações do algoritmo das k-Médias consiste na definição prévia do número de grupos . Ao se assumir que o número de grupos é desconhecido a priori e que deveria ser estimado a partir dos dados , percorrer o grande espaço de soluções possíveis ( tanto em relação ao número de grupos , k , quanto em relação às partições possíveis para um determinado k ) torna desafiadora a tarefa de agrupamento de dados - ainda mais sob a limitação de tempo e armazenamento imposta em aplicações de FCD . Neste contexto , essa tese tem como principais_contribuições  : ( i ) adaptar algoritmos que têm sido usados com sucesso em aplicações de Fluxo Contínuo de Dados ( FCD ) nas quais k é conhecido para cenários em que se deseja estimar o número de grupos ; ( ii ) propor novos algoritmos para agrupamento que estimem k automaticamente a partir do FCD ; ( iii ) avaliar sistematicamente , e de maneira quantitativa , os algoritmos propostos de acordo com as características específicas dos cenários de FCD . Foram desenvolvidos 14 algoritmos de agrupamento para FCD capazes de estimar o número de grupos a partir dos dados . Tais algoritmos foram avaliados em seis bases de dados artificiais e duas bases de dados reais amplamente utilizada na literatura . Os algoritmos desenvolvidos podem auxiliar em diversas_áreas  da Mineração em FCD . Os algoritmos_evolutivos  desenvolvidos mostraram a melhor relação de custo-benefício entre eficiência computacional e qualidade das partições obtidas 
 Com a grande variedade de produtos e serviços disponíveis na Web , os usuários possuem , em geral , muita liberdade de escolha , o que poderia ser considerado uma vantagem se não fosse pela dificuldade encontrada em escolher o produto ou serviço que mais atenda a suas necessidades dentro do vasto conjunto de opções disponíveis . Sistemas de recomendação são sistemas que têm como objetivo auxiliar esses usuários a identificarem itens de interesse em um conjunto de opções . A maioria das abordagens de sistemas de recomendação foca em recomendar itens mais relevantes para usuários individuais , não levando em consideração o contexto dos usuários . Porém , em muitas aplicações é importante também considerar informações contextuais para fazer as recomendações . Por exemplo , um usuário pode desejar assistir um filme com a sua namorada no sábado à noite ou com os seus amigos durante um dia de semana , e uma locadora de filmes na Web pode recomendar diferentes_tipos  de filmes para este usuário dependendo do contexto no qual este se encontra . Um grande desafio para o uso de sistemas de recomendação sensíveis ao contexto é a falta de métodos para aquisição automática de informação contextual para estes sistemas . Diante desse cenário , neste trabalho é proposto um método para extrair_informações  contextuais do conteúdo de páginas_Web  que consiste em construir hierarquias de tópicos do conteúdo textual das páginas considerando , além da bag-of-words tradicional ( informação técnica ) , também informações mais valiosas dos textos como entidades nomeadas e termos do domínio ( informação privilegiada ) . Os tópicos extraídos das hierarquias das páginas_Web  são utilizados como informações de contexto em sistemas de recomendação sensíveis ao contexto . Neste trabalho foram realizados_experimentos  para avaliação do contexto extraído pelo método proposto em que foram considerados dois baselines : um sistema de recomendação que não considera informação de contexto e um método da literatura de extração de contexto implementado e adaptado para este mestrado . Além_disso  , foram utilizadas duas bases de dados . Os resultados obtidos foram , de forma geral , muito bons apresentando ganhos significativos sobre o baseline sem contexto . Com relação ao baseline que extrai informação contextual , o método proposto se mostrou equivalente ou melhor que o mesmo 
 A tecnologia atual não oferece prevenção contra cópia , adulteração ou plágio de uma imagem estática em meio digital sem autorização do verdadeiro autor . Dado que tais mal feitos não podem ser evitados , resta ao criador da obra original lutar a posteriori por seus direitos nos fóruns adequados ( no tribunal , por exemplo ) . Na época da fotografia analógica com filme , o negativo poderia ser utilizado como prova . Hoje este recurso raramente está disponível e se faz necessária uma solução alternativa . A técnica de Marca d´Água é uma das possibilidades criptográficas existentes para apoiar o autor em sua defesa . O principio da Marca d´Água é o encapsulamento de informações relevantes , preferencialmente de forma imperceptível , na imagem a ser protegida . Tais informações , quando extraídas da imagem marcada , devem revelar o verdadeiro autor num processo de disputa . Soluções de Marca d´Água combinada com Criptografia Visual são encontradas na literatura . A principal vantagem deste caminho é a propriedade Imperceptível por segurança perfeita que a Marca d´Água assume quando tratada por Criptografia Visual . O segredo ( neste caso , a Marca d´Água ) é segmentado via Criptografia Visual em 2 transparências : uma delas é encapsulada na imagem a ser protegida e a outra é mantida pelo verdadeiro autor . Basta a sobreposição de tais transparências para que a Marca d´Água seja revelada . Nesta pesquisa propomos um novo método , denominado MACV , que combina Marca d´Água , Criptografia Visual e um algoritmo de hashing . O MACV apresenta , entre outras , as seguintes propriedades desejáveis de Marca d´Água : imperceptível por segurança perfeita , alta entropia , armazenamento na própria imagem e sem ambiguidade . Veremos em nossa pesquisa bibliográfica que há uma lacuna de soluções que apresentem , simultaneamente , todas estas propriedades . Esta lacuna torna o MACV único em sua categoria 
 A computacao paralela , empregada no meio cientifico para resolucao de problemas que de- mandam grande poder_computacional  , teve nos ultimos anos o surgimento de um novo tipo de comunicacao entre instancias do paralelismo . Trata-se da Comunicacao Unilateral ( CUL ) , onde somente uma instancia realiza a operacao de transferencia de informacoes , e esta ocorre em segundo plano , ao contrario da Comunicacao Bilateral ( CBL ) , onde uma instancia envia a informacao e a outra recebe . Neste contexto se buscou analisar os beneficios que a CUL agrega ao paralelismo de um programa que se utiliza de uma grade nao estruturada em me- moria . Duas formas de apoio ao paralelismo foram utilizadas : uma biblioteca , a `` Message Passing Interface '' ( MPI ) ( especificamente a sua parte que descreve a CUL ) , e uma extensao a linguagem Fortran , o Coarray Fortran ( CAF ) . A semantica do MPI CUL e mais complexa que a do CAF , mas a do CAF e mais restritiva . Para analisar a semantica e desempenho da CUL foi realizada uma ambientacao utilizando MPI CUL e CAF no paralelismo de um programa simples , denominado jogo da Vida ( Game of Life ) , com grade estruturada e com otimo desempenho paralelo atraves do MPI CBL . Na programacao o MPI CUL se mostrou verborragico ( aumento do numero de linhas de codigo ) e complexo , principalmente quando se utiliza um controle refinado de sincronismo entre as imagens . Ja o CAF reduziu o nu- mero de linhas de codigo ( entre 20 % e 40 % ) , e o sincronismo e muito mais simples . Os resultados mostraram uma piora no desempenho no caso do MPI CUL , mas para o CAF o desempenho absoluto foi melhor que a implementacao original ate o numero de nucleos de processamento que compartilham a mesma memoria . Para grades nao estruturadas se utilizou o Ocean Land Atmospheric Model ( OLAM ) , um modelo de simulacao do sistema terrestre com grade baseada em prismas triangulares , paralelizado atraves de MPI CBL . A implementacao da comunicacao por MPI CUL na estrutura do paralelismo existente mos- trou que esta semantica possui alguns pontos que podem prejudicar a programacao , como o tratamento da exposicao de memoria ( cada instancia tem uma memoria exposta de tamanho diferente ) e como e realizado o sincronismo entre as instancias . Em termos de desempenho as curvas de speed-ups mostraram que o MPI CUL prejudicou o OLAM independentemente da implementacao das bibliotecas ou do equipamento utilizado , com reducao de pelo menos 20 % no speed-up para sete ou mais processadores . Assim como no jogo da Vida o MPI com comunicacao unilateral penalizou o desempenho 
 A realização de atividades de teste é indispensável para a garantia da qualidade de um produto e para a identificação de defeitos , diminuindo custos de manutenção e evitando ao máximo o risco do cliente encontrar esses defeitos . Nessa linha , testes baseados em modelos têm se mostrado atrativos , pois o custo de geração de casos de testes e de correção de defeitos tende a ser menor . Devido à sua simplicidade conceitual e expressividade na descrição do comportamento de um sistema , um dos modelos mais usados e pesquisados na área de teste baseado em modelos são as Máquinas de Estados_Finitos  ( MEFs ) . Por meio de MEFs e com apoio de ferramentas apropriadas , a geração de casos de testes para avaliar os comportamentos esperados de um sistema é automatizada , reduzindo tanto o custo da geração e da manutenção quanto as falhas humanas . Desta forma , a aplicabilidade de métodos de geração de casos de teste baseados em modelos no contexto de sistemas embarcados vem_sendo  investigada . O objetivo deste trabalho de mestrado consiste em investigar a aplicabilidade dos métodos de geração em cenários de teste reais , com foco em sistemas embarcados , identificando as difi- culdades e limitações do processo , bem como os requisitos essenciais para a adequação dos métodos de geração propostos na literatura e de ferramentas de apoio à atividade de teste . O foco_principal  do projeto é a implementação de mecanismos que atendam aos requisitos levantados , visando a usabilidade , segurança e portabilidade da 
 Esta dissertação de mestrado aborda a Composição de Web_services  baseada em atributos de Qualidade de Serviço ( CWSbQ ) . Foi selecionado o domínio de sistemas de e-commerce , por se tratar de uma área em franca expansão , tanto no cenário nacional como internacional . Além_disso  , os sistemas de e-commerce podem se beneficiar da CWSbQ devido à sua interoperabilidade e atendimento aos requisitos de qualidade de serviço de seus clientes . Foram implementados dez algoritmos para resolver esse problema , usando várias técnicas diferentes , sendo elas : busca aleatória , busca exaustiva , busca heurística , busca meta-heurística e um algoritmo híbrido . Foram realizados_experimentos  de avaliação de desempenho , considerando tamanhos de espaço de busca diferentes e dois deadlines estabelecidos . As principais_contribuições  deste trabalho foram o desenvolvimento de três novos algoritmos : Heurística Gulosa ( HG ) , Heurística Gulosa 2 ( HG2 ) e Algoritmo Genético Duplamente Híbrido ( AGDH ) ; o uso de planejamento de experimentos e avaliação de desempenho para aperfeiçoar Algoritmos Genéticos ( AGs ) usados no problema de CWSbQ ; e o estudo de adequação dos algoritmos de CWSbQ para diferentes 
 Este trabalho faz parte de um projeto maior , o Annotation and Image Markup Project , que tem o objetivo de criar uma base de conhecimento médico sobre imagens radiológicas para identificação , acompanhamento e reasoning acerca de lesões tumorais em pesquisas sobre câncer e consultórios médicos . Esse projeto está sendo desenvolvido em conjunto com o Radiological Sciences Laboratory da Stanford University . O problema específico , que será abordado nesse trabalho , é que a maior_parte  das informações_semânticas  sobre imagens radiológicas não são capturados e relacionados às mesmas usando termos de ontologias biomédicas e padrões , como o RadLex e DICOM , o que impossibilita a sua avaliação automática por computadores , busca em arquivos médicos em hospitais , etc . Para tratar isso , os radiologistas precisam de uma solução computacional fácil , intuitiva e acessível para adicionar essas informações . Nesse trabalho foi desenvolvida uma solução Web para inclusão dessas anotações , o sistema ePAD . O aplicativo permite a recuperação de imagens médicas , como as imagens disponíveis em sistemas de informação hospitalares ( PACS ) , o delineamento dos contornos de lesões tumorais , a associação de termos ontológicos a esses contornos e o armazenamento desses termos em uma base de conhecimento . Os principais desafios desse trabalho envolveram a aplicação de interfaces intuitivas baseadas em Rich Internet Applications e sua operação a partir de um navegador Web padrão . O primeiro protótipo funcional do ePAD atingiu seus objetivos ao demonstrar sua viabilidade técnica , sendo capaz de executar o mesmo trabalho básico de anotação de aplicações Desktop , como o OsiriX-iPad , sem o mesmo overhead . Também mostrou a sua utilidade a comunidade médica o que gerou o interesse de usuários 
 Agrupamento de dados é um dos problemas centrais na áea de mineração de dados , o qual consiste basicamente em particionar os dados em grupos de objetos mais similares ( ou relacionados ) entre si do que aos objetos dos demais grupos . Entretanto , as abordagens tradicionais pressupõem que cada objeto pertence exclusivamente a um único grupo . Essa hipótese não é realista em várias aplicações práticas , em que grupos de objetos apresentam distribuições estatísticas que possuem algum grau de sobreposição . Algoritmos de agrupamento_fuzzy  podem lidar naturalmente com problemas dessa natureza . A literatura sobre agrupamento_fuzzy  de dados é extensa , muitos algoritmos existem atualmente e são mais ( ou menos ) apropriados para determinados cenários , por exemplo , na procura por grupos que apresentam diferentes formatos ou ao operar sobre dados descritos por conjuntos de atributos de tipos diferentes . Adicionalmente , existem cenários em que os dados podem_estar  distribuídos em diferentes locais ( sítios de dados ) . Nesses cenários o objetivo de um algoritmo de agrupamento de dados consiste em encontrar uma estrutura que represente os dados existentes nos diferentes sítios sem a necessidade de transmissão e armazenamento/processamento centralizado desses dados . Tais algoritmos são denominados algoritmos de agrupamento distribuído de dados . O presente_trabalho  visa o estudo e aperfeiçoamento de algoritmos de agrupamento_fuzzy  centralizados e distribuídos existentes na literatura , buscando identificar as principais características , vantagens , desvantagens e cenários mais apropriados para a aplicação de cada um deles , incluindo análises de complexidade de tempo , espaço e de comunicação para os algoritmos 
 Com o surgimento da criptografia de chave_pública  , muito esforço foi feito para a criação de protocolos de criptografia e de assinatura que fossem comprovadamente seguros contra indivíduos maliciosos . Existem várias definições de segurança , tanto para protocolos de criptografia como para protocolos de assinatura , e também existem vários modelos de adversários , que simulam um indivíduo malicioso tentando corromper o protocolo . A família de protocolos de assinatura Rabin possui os recordes de velocidade de vericação da assinatura , chegando a ser até 100 vezes mais rápida do que o RSA . Este trabalho apresenta uma redução eficiente de segurança no modelo do oráculo aleatório para uma variante do protocolo de assinatura Rabin descrito por Bernstein , onde não é necessário o uso de nenhuma função para geração de bits pseudo-aleatórios , o que torna o protocolo mais robusto . A redução apresentada é uma redução polinomial e eficiente do problema da fatoração de inteiros para o problema de quebrar o protocolo Principal Rabin-Williams B = 0 
 O tema central deste trabalho é o estudo de problemas sobre caminhos mais longos em grafos , de pontos de vista tanto estrutural como algorítmico . A primeira parte tem como foco o estudo de problemas motivados pela seguinte questão levantada por T. Gallai em 1966 : é verdade que em todo grafo conexo existe um vértice comum a todos os seus caminhos mais longos ? Hoje , já se conhecem diversos grafos conexos cuja intersecção de todos os seus caminhos mais longos é vazia . Entretanto , existem classes de grafos para as quais a resposta à pergunta de Gallai é afirmativa . Nessa linha , apresentamos alguns resultados da literatura e duas novas classes que obtivemos : os grafos exoplanares e as 2-árvores . Motivado por esse problema , nos anos 80 , T. Zamfirescu formulou a seguinte pergunta que permanece em aberto : é verdade que em todo grafo conexo existe um vértice comum a quaisquer três de seus caminhos mais longos ? Apresentamos , além de alguns resultados conhecidos , uma prova de que a resposta é afirmativa para grafos em que todo bloco não trivial é hamiltoniano . Notamos que esse último resultado e o acima mencionado para grafos exoplanares generalizam um teorema de M. Axenovich ( 2009 ) que afirma que quaisquer três caminhos mais longos em um grafo exoplanar têm um vértice em comum . Finalmente , mencionamos alguns outros resultados da literatura relacionados com o tema . Na segunda parte , investigamos o problema de encontrar um caminho mais longo em um grafo . Este problema é NP-difícil para grafos arbitrários . Isto motiva investigações em duas linhas a respeito da busca de tais caminhos . Pode-se procurar classes especiais de grafos para as quais existem algoritmos polinomiais , ou pode-se abrir mão da busca de um caminho mais longo , e projetar um algoritmo eficiente que encontra um caminho cujo comprimento esteja próximo do comprimento de um mais longo . Nesse trabalho estudamos ambas as abordagens e apresentamos alguns resultados da literatura 
 Esta tese apresenta a caracterização de desempenho dos serviços executados em um ambiente em nuvem , quando são consideradas rajadas de diferentes origens , intensidades e variabilidades nas cargas de trabalho . Os resultados mostram que a presença de rajadas no processo de chegada das requisições e/ou nas demandas de serviço , ocasiona uma considerável degradação no desempenho dos serviços e , portanto , devem ser consideradas nos modelos de cargas de trabalhos e nas atividades voltadas para avaliação de desempenho em computação em nuvem . Considerando-se a grande influência das rajadas , é proposta e validada uma metodologia que permite monitorar uma carga de trabalho e determinar a ocorrência de rajadas tanto nas taxas de chegadas de requisições quanto nas demandas de serviços . A metodologia utilizada na condução deste trabalho consta de diferentes modelos de cargas de trabalho com rajadas de diferentes variabilidades e intensidades , desenvolvidos e integrados à arquitetura CloudSim-BEQoS proposta nesta tese . Utilizando-se essa arquitetura é possível executar um conjunto de experimentos que possibilitam a obtenção dos resultados que caracterizam o desempenho dos serviços quando são criadas condições de rajadas nas cargas de trabalho submetidas à nuvem 
 Desde a década de 70 , há um crescente_interesse  em simulações em computador de fenômenos físicos visto sua diversidade de aplicações . Dentre esses fenômenos , podem ser destacados a interação entre corpos rígidos , elásticos , plásticos , quebráveis e também fluidos . Neste trabalho realizamos a simulação de um desses fenômenos , o escoamento de fluidos , por um método conhecido como Smoothed Particles Hydrodynamics , uma abordagem lagrangeana baseada em partículas para resolução das equações que modelam o movimento do fluido . Várias são as vantagens de métodos lagrangeanos usando partículas sobre os que usam malhas , por exemplo , as propriedades do material transladam com as partículas como função do tempo , além da capacidade de lidar com grandes deformações . Dentre as desvantagem , destacamos uma deficiência relacionada ao ganho de energia total do sistema e estabilidade das partículas . Para lidar com isso , utilizamos uma abordagem baseada na lei da conservação da energia : em um sistema isolado a energia total se mantém constante e ela não pode ser criada ou destruida . Dessa forma , alterando o integrador temporal nós restringimos o aumento arbitrário de energia , tornando a simulação mais tolerante às condições 
 As redes de sensores sem fio ( RSSF ) são compostas por pequenos dispositivos distribuídos em uma região geográfica com a finalidade de monitorar ou interagir com o ambiente . Esse tipo de rede tem sido alvo de grande atenção da comunidade acadêmica e empresarial , dados os avanços das produções científicas e aplicações comerciais . Além_disso  , há grande potencial para esse modelo de rede , pois há diversos benefícios em se ter muitos dispositivos de baixo custo trabalhando em cooperação e que ainda podem interagir com o mundo_real  . As RSSFs apresentam novos desafios , até então inexistentes na maioria das redes modernas . A baixa capacidade de processamento dos dispositivos , a limitação do tamanho de um pacote , a baixa taxa de transferência de pacotes , a baixa capacidade da bateria de um dispositivo e o alcance limitado do rádio dificultam ou até inviabilizam muitas implementações de segurança . Neste sentido , há uma grande variedade de protocolos de segurança , os quais tentam fornecer o máximo de propriedades desejadas , como por exemplo autenticidade e confidencialidade de mensagens . Nesta dissertação , analisamos e comparamos dois pares de protocolos de segurança que possuem grande atenção da comunidade . Os protocolos foram analisados com base em seus mecanismos criptográficos e propriedades oferecidas . Além_disso  , com o uso de um simulador de RSSFs , realizamos experimentos que ajudam a entender o comportamento de dois protocolos , principalmente no que se relaciona com o consumo de energia dos dispositivos sensores 
 A recuperação de dados armazenados em Bancos de Dado em geral é feita utilizando estruturas de indexação , que permitem fazer a recuperação dos dados muito mais rapidamente do que se a busca fosse feita sequencialmente . No entanto , as estruturas de indexação que podem ser utilizadas dependem das propriedades dos domínios de dados indexados e do tipo de consultas que devem ser respondidas . Tradicionalmente , os gerenciadores de bancos de dados suportam bem dados de domínios que possuem a propriedade de relação de ordem total , tais como números e textos com a relação de ordem lexicográfica permitindo consultas por igualdade e consultas envolvendo relações de ordem tais como > , < ou = , > , < ou = etc . Além_disso  , as estruturas de indexação comumente utilizadas em sistemas gerenciadores de bases de dados são construídas para serem armazenadas em disco , particionando o conjunto de dados em registros de tamanho fixo . O exemplo mais comum desse arranjo é o das árvores de indexação , quando os registros são então chamados `` nós '' . Aplicações mais sofisticadas frequentemente apresentam dados em outros domínios , com outros_tipos  de consulta . Quando as aplicações lidam com dados em domínio métrico , além dos próprios elementos de dados , é definida uma função de similaridade ( ou de distância ) entre pares de elementos , e essa função é a única maneira de comparação entre dois elementos de dados do conjunto . Existem diversas estruturas de indexação criadas para dados em domínios métricos . Entretanto essas estruturas ou são estáticas ( impedindo que novos elementos sejam acrescentados depois que a estrutura foi criada ) , ou são para armazenamento em disco . Neste trabalho foi desenvolvida uma nova estrutura métrica dinâmica para dados métricos totalmente armazenada em memória principal . Uma outra propriedade interessante dessa estrutura é que a execução de uma consulta por existência ( point query ) percorre um único caminho de busca . Essa característica é muito interessante , pois todas as outras árvores dinâmicas existentes requerem que a navegação seja feita não apenas em profundidade , mas também em largura . A estrutura proposta permite a navegação apenas em profundidade para a consulta por existência 
 Os sistemas de integração de dados que usam a arquitetura de data warehouse ( DW ) têm se tornado cada vez maiores e mais difíceis de gerenciar devido à crescente heterogeneidade das fontes de dados envolvidas . Apesar dos avanços tecnológicos e científicos , os projetos de DW ainda são muito lentos na geração de resultados pragmáticos . Este trabalho busca responder à seguinte questão : como pode ser reduzida a complexidade do desenvolvimento de sistemas de DW que integram dados provenientes de sistemas transacionais heterogêneos ? Para isso , apresenta duas contribuições : 1 ) A criação de diretrizes metodológicas baseadas em ciclos de modelagem conceitual e análise de dados para guiar a construção de um sistema modular de integração de dados . Essas diretrizes foram fundamentais para reduzir a complexidade do desenvolvimento do projeto internacional Retrovirus Epidemiology Donor Study-II ( REDS-II ) , se mostrando adequadas para serem aplicadas em sistemas reais . 2 ) O desenvolvimento de um método de validação de lotes de dados candidatos a serem incorporados a um sistema integrador , que toma decisões baseado no perfil estatístico desses lotes , e de um projeto de sistema que viabiliza o uso desse método no contexto de sistemas de DW 
 No Ensino à Distância via Internet , o professor não tem contato físico com seus alunos e , por isso , perde consideravelmente a percepção da interação destes em relação ao material didático . Além_disso  , pode-se atingir um número muito grande de usuários caso o meio de ensino seja a Web . Então , é importante dar ao professor ferramentas que o ajudem a conhecer seus alunos e a planejar sua atuação de forma a atender melhor um grande número de alunos . Sabe-se que a Web é um meio que pode ser ricamente instrumentado . Em princípio , cada clique num hyperlink , cada visita e outros dados de atividade online podem ser capturados e armazenados para futura análise . Entretanto , a quantidade de dados que se obtém pode ser imensa , tornando sua análise trabalhosa e demorada . Surge , então , o problema de analisar esses dados a fim de se extrair_informações  úteis . Pesquisas na área de Mineração de Dados fornecem ferramentas úteis para tratar este problema , sendo que métodos de agrupamento são particularmente interessantes . Uma das dificuldades encontradas nesta nova área , chamada de Web Usage Mining ( WUM ) , é lidar simultaneamente com dados categóricos e contínuos . Neste trabalho desenvolveu-se um novo método de agrupamento , o LogiCluster , baseado no Modelo de Regressão Logística , o qual é adequado para dados categóricos e contínuos , tanto em separado quanto em conjunto 
 Vários autores tem construído estimadores de Bayes nâo-paramétricos para a função de distribuição acumulada . A distribuição à priori tem , por exemplo , sido processos de Dirichlet , processos neutral to the right . Neste trabalho nós estudamos o problema de achar estimadores de Bayes nâo-paramétricos para a taxa de falha acumulada de um processo pontuai baseado no modelo de intensidade multiplicativo de Aalen . Desta forma nós consideramos uma classe conjugada de processos de Levy chamados de processos beta e apresentamos fórmulas para obter um processo posterior . 0 estimador de Bayes é comparado com dois outros estimadores não-paramétricos , Kaplan-Meier e Nelson-Aalen e um estimador paramétrico , a taxa de falha acumulada de uma distribuição Weibull 
 O monitoramento biológico de determinadas espécies de aves representa uma fonte importante de informações referentes à qualidade do meio ambiente . O censo , que consiste na contagem dos indivíduos de uma população , é uma das principais variáveis deste processo . Em virtude deste contexto , o trabalho proposto nesta dissertação tem como objetivo a implementação e aplicação de técnicas de segmentação de imagens para a contagem de indivíduos pertencentes a uma determinada população de aves , e de seus respectivos ninhos , em imagens aéreas de seu habitat ( os ninhais ) . Devido às características dessas imagens , a segmentação baseada em textura , é adotada , como abordagem principal do projeto . O método escolhido para este estudo faz parte do conjunto de métodos baseados em modelos estocásticos para segmentação por textura . Em particular , ( este método utiliza como modelo fundamental um Campo Aleatório de Markov ( Markov Random Field ) . Esta dissertação apresenta os fundamentos estatísticos da técnica , adotada. , bem como as etapas de sua implementação e alguns melhoramentos incorporados ao método padrão , afim de torná-lo mais robusto . Por fim , são ilustrados alguns resultados de segmentação de imagens de ninhais , bem como o de algumas imagens sintéticas e imagens de modalidade médica . As imagens aéreas são adquiridas em parceria , com outro projeto cm curso no ICMC-USP : o P r o j e t o A R A R A ( Aeronaves de Reconhecimento Assistidas por Rádio e Autónomas ) 
 Tratar grandes quantidades de dados é uma exigência dos modernos algoritmos de mineração de texto . Para algumas aplicações , documentos são constantemente publicados , o que demanda alto custo de armazenamento em longo prazo . Então , é necessário criar métodos de fácil adaptação para uma abordagem que considere documentos em fluxo , e que analise os dados em apenas um passo sem requerer alto custo de armazenamento . Outra exigência é a de que essa abordagem possa explorar heurísticas a fim de melhorar a qualidade dos resultados . Diversos modelos para a extração automática das informações latentes de uma coleção de documentos foram propostas na literatura , dentre eles destacando-se os modelos_probabilísticos  de tópicos . Modelos probabilísticos de tópicos apresentaram bons_resultados  práticos , sendo estendidos para diversos modelos com diversos tipos de informações inclusas . Entretanto , descrever corretamente esses modelos , derivá-los e em seguida obter o apropriado algoritmo de inferência são tarefas difíceis , exigindo um tratamento matemático rigoroso para as descrições das operações efetuadas no processo de descoberta das dimensões latentes . Assim , para a elaboração de um método simples e eficiente para resolver o problema da descoberta das dimensões latentes , é necessário uma apropriada representação dos dados . A hipótese desta tese é a de que , usando a representação de documentos em grafos bipartidos , é possível endereçar problemas de aprendizado de máquinas , para a descoberta de padrões latentes em relações entre objetos , por exemplo nas relações entre documentos e palavras , de forma simples e intuitiva . Para validar essa hipótese , foi desenvolvido um arcabouço baseado no algoritmo de propagação de rótulos utilizando a representação em grafos bipartidos . O arcabouço , denominado PBG ( Propagation in Bipartite Graph ) , foi aplicado inicialmente para o contexto não supervisionado , considerando uma coleção estática de documentos . Em seguida , foi proposta uma versão semissupervisionada , que considera uma pequena quantidade de documentos rotulados para a tarefa de classificação transdutiva . E por fim , foi aplicado no contexto dinâmico , onde se considerou fluxo de documentos textuais . Análises comparativas foram realizadas , sendo que os resultados indicaram que o PBG é uma alternativa viável e competitiva para tarefas nos contextos não supervisionado e semissupervisionado 
 Desastres naturais são fenômenos que causam grandes danos às pessoas em áreas urbanas e rurais , motivando medidas de prevenção e reação . Se múltiplas fontes de informação são consideradas , tais medidas podem ser mais assertivas e efetivas . Porém , a integração de dados heterogêneos ainda impõe desafios devido às diferenças em suas estruturas . Para preencher essa lacuna , esta pesquisa apresenta uma arquitetura orientada a serviços , como parte de uma plataforma geoespacial para gestão de risco de desastres , a qual visa apoiar a integração de dados de sensores e de voluntários relacionados a inundações . Como resultado , a composição dos serviços descritos pelos componentes arquiteturais possibilita a integração entre dados de sensores e voluntários usando algoritmos diferentes de forma flexível e automática 
 O desenvolvimento e a utilização de ambientes computacionais como apoio ao ensino e aprendizagem , aliados à evolução da computação móvel , têm contribuído significativamente para o estabelecimento de uma nova modalidade de ensino conhecida como aprendizagem móvel ou mobile learning ( m-learning ) . Nesse cenário emergente , os ambientes educacionais existentes , mesmo possuindo diversos benefícios e facilidades no que diz_respeito  ao ensino e aprendizagem , apresentam problemas e desafios que precisam ser explorados . Um dos aspectos relevantes a ser investigado refere-se ao estabelecimento e adoção de padrões arquiteturais . De fato , grande parte desses ambientes é construída de forma isolada , possuindo arquiteturas e estruturas próprias , o que pode impactar negativamente a capacidade de padronização . O presente_trabalho  de pesquisa insere-se neste contexto , tendo como principal objetivo investigar e definir uma arquitetura de referência orientada a serviço voltada para ambientes de aprendizagem móvel . Tal arquitetura , denominada Ref-mLearning , visa contribuir para a evolução , reúso e interoperabilidade desses ambientes , possibilitando ainda um aumento na qualidade e redução de custos durante o seu desenvolvimento . Resultados alcançados por meio de avaliações e condução de um estudo de caso demonstram que a Ref-mLearning é uma arquitetura viável possuindo uma boa estrutura e organização para o desenvolvimento de ambientes de aprendizagem móvel orientados a serviço 
 Os métodos incrementais pertencem a uma classe de métodos iterativos que divide o conjunto de dados em subconjuntos ordenados , e que atualiza a imagem ao processar cada subconjunto ( sub-iterações ) . Isso acelera a convergência das reconstruções , e imagens de qualidade são obtidas em menos iterações . No entanto , a cada sub-iteração é necessário calcular os operadores de projeção e retroprojeção , resultando no custo_computacional  de ordem O ( n3 ) para a reconstrução de imagens de dimensão × . Por outro_lado  , algumas alternativas baseadas na interpolação em uma grade regular no espaço de Fourier ou em transformadas rápidas não-uniformes , dentre outras ideias , foram desenvolvidas a fim de aliviar esse custo_computacional  . Além_disso  , diversas abordagens foram bem sucedidas em acelerar o cálculo das iterações de algoritmos clássicos , mas nenhuma havia sido utilizada em conjunto com os métodos incrementais . Neste trabalho é proposta uma nova abordagem em que a técnica de transformada rápida de Fourier não uniforme ( NFFT ) é utilizada nas sub-iterações de métodos incrementais com o objetivo de efetuar de forma eficiente os cálculos numericamente mais intensos : a projeção e a retroprojeção , resultando em métodos incrementais com complexidade O ( n2 log n ) . Os métodos propostos são aplicados à tomografia por radiação síncrotron e os resultados da pesquisa mostram um bom_desempenho  
 Sistemas-de-Sistemas ( do inglês , Systems-of-Systems ou simplesmente SoS ) representam um campo emergente de pesquisa na Engenharia de Software . Em particular , SoS referem-se a sistemas que possibilitam a interoperabilidade de sistemas complexos , distribuídos , cooperando entre si para atingir uma missão comum . Diversos SoS têm sido desenvolvidos e utilizados , mas não há um consenso sobre os diversos termos e conceitos nesse campo , o que pode dificultar a comunicação entre os diferentes interessados envolvidos no desenvolvimento e evolução dos SoS , além da falta de padronização e entendimento comum entre pesquisadores e profissionais . Este projeto de Mestrado estabeleceu a OntoSoS , uma ontologia para formalizar termos e conceitos no campo de SoS , explicitando e permitindo o compartilhamento e reúso do conhecimento contido na ontologia . Como resultado , este projeto pretende contribuir para o campo de SoS , auxiliando também nas atividades relacionadas à Engenharia de SoS . É também esperado que essa ontologia possa servir como um material de ensino em cursos relacionados à Engenharia de SoS 
 Conteúdo Gerado por Usuário ( CGU ) é a denominação dada ao conteúdo criado de forma espontânea por indivíduos comuns , sem vínculos com meios de comunicação . Esse tipo de conteúdo carrega informações valiosas e pode ser explorado por diversas_áreas  do conhecimento . Muito do CGU é disponibilizado em forma de textos avaliações de produtos , comentários em fóruns sobre filmes e discussões em redes_sociais  são exemplos . No entanto , a linguagem utilizada em textos de CGU diverge , de várias maneiras , da norma culta da língua , dificultando seu processamento por técnicas de PLN . A linguagem de CGU é fortemente ligada à língua utilizada no cotidiano , contendo , assim , uma grande quantidade de ruídos . Erros ortográficos , abreviações , gírias , ausência ou mau uso de pontuação e de capitalização são alguns ruídos que dificultam o processamento desses textos . Diversos trabalhos relatam perda considerável de desempenho ao testar ferramentas do estado-daarte de PLN em textos de CGU . A Normalização Textual é o processo de transformar palavras ruidosas em palavras consideradas corretas e pode ser utilizada para melhorar a qualidade de textos de CGU . Este trabalho relata o desenvolvimento de métodos e sistemas que visam a ( a ) identificar palavras ruidosas em textos de CGU , ( b ) encontrar palavras candidatas a sua substituição , e ( c ) ranquear os candidatos para realizar a normalização . Para a identificação de ruídos , foram propostos métodos baseados em léxicos e em aprendizado de máquina , com redes neurais profundas . A identificação automática apresentou resultados comparáveis ao uso de léxicos , comprovando que este processo pode ser feito com baixa dependência de recursos . Para a geração e ranqueamento de candidatos , foram investigadas técnicas baseadas em similaridade lexical e word embeddings . Concluiu-se que o uso de word embeddings é altamente adequado para normalização , tendo atingido os melhores_resultados  . Todos os métodos propostos foram avaliados com base em um córpus de CGU anotado no decorrer do projeto , contendo textos de diferentes origens : fóruns de discussão , reviews de produtos e publicações no Twitter . Um sistema , Enelvo , combinando todos os métodos foi implementado e comparado a um outro sistema normalizador existente , o UGCNormal . Os resultados obtidos pelo sistema Enelvo foram consideravelmente superiores , com taxa de correção entre 67 % e 97 % para diferentes_tipos  de ruído , com menos dependência de recursos e maior flexibilidade na normalização 
 Dentre as técnicas de mineração de dados encontra-se a associação , a qual identifica todas as associações intrínsecas contidas na base de dados . Entretanto , essa característica , vantajosa por um lado , faz com que um grande número de padrões seja gerado , sendo que muito deles , mesmo sendo estatisticamente aceitos , são triviais , falsos , ou irrelevantes à aplicação . Além_disso  , a técnica de associação tradicional gera padrões compostos apenas por itens contidos na base de dados , o que leva à extração , em geral , de um conhecimento muito específico . Essa especificidade dificulta a obtenção de uma visão geral do domínio pelos usuários finais , que visam a utilização/exploração de conhecimentos úteis e compreensíveis . Assim , o pós-processamento das regras descobertas se torna um importante tópico , uma vez que há a necessidade de se validar as regras obtidas . Diante do exposto , este trabalho apresenta uma abordagem de pós-processamento de regras de associação que utiliza conhecimento de domínio , expresso via taxonomias , para obter um conjunto de regras de associação generalizadas compacto e representativo . Além_disso  , a fim de avaliar a representatividade de padrões generalizados , é apresentado também neste trabalho um estudo referente à utilização de medidas de interesse objetivas quando aplicadas a regras de associação generalizadas . Nesse estudo , a semântica da generalização é levada em consideração , já que cada uma delas fornece uma visão distinta do domínio . Como resultados desta tese , foi possível observar que : um conjunto de regras de associação pode ser compactado na presença de um conjunto de taxonomias ; para cada uma das semânticas de generalização existe um conjunto de medidas mais apropriado para ser utilizado na avaliação de regras 
 Atualmente , a Onion-tree [ Carélo et al. , 2009 ] é o método de acesso métrico baseado em memória primária mais eficiente para pesquisa por similaridade disponível na literatura . Ela indexa dados complexos por meio da divisão do espaço métrico em regiões ( ou seja , subespaços ) disjuntas , usando para isso dois pivôs por nó . Para prover uma boa divisão do espaço métrico , a Onion-tree introduz as seguintes características principais : ( i ) procedimento de expansão , o qual inclui um método de particionamento que controla o número de subespaços disjuntos gerados em cada nó ; ( ii ) técnica de substituição , a qual pode alterar os pivôs de um nó durante operações de inserção baseado em uma política de substituição que garante uma melhor divisão do espaço métrico , independente da ordem de inserção dos elementos ; e ( iii ) algoritmos para a execução de consultas por abrangência e aos k-vizinhos mais próximos , de forma que esses tipos de consulta possam explorar eficientemente o método de particionamento da Onion-tree . Entretanto , a Onion-tree apenas oferece funcionalidades voltadas à inserção dos dados um-a-um em sua estrutura . Ela não oferece , portanto , uma operação de bulk-loading que construa o índice considerando todos os elementos do conjunto de dados de uma única vez . A principal vantagem dessa operação é analisar os dados antecipadamente para garantir melhor particionamento possível do espaço métrico . Com isto , a carga inicial de grandes volumes de dados pode ser melhor realizada usando a operação de bulk-loading . Este projeto de mestrado visa suprir a falta da operação de bulk-loading para a Onion-tree , por meio da proposta de algoritmos que exploram as características intrínsecas desse método de acesso métrico . No total , são propostos três algoritmos de bulk-loading , denominados GreedyBL , SampleBL e HeightBL , os quais utilizam respectivamente as seguintes abordagens : gulosa , amostragem e de estimativa da altura do índice . Testes experimentais realizados sobre conjuntos de dados com volume variando de 2.536 a 102.240 imagens e com dimensionalidade variando de 32 a 117 dimensões mostraram que os algoritmos propostos introduziram vantagens em relação à estrutura criada pelo algoritmo de inserção um-a-um da Onion-tree . Comparado com a inserção um-a-um , o tamanho do índice foi reduzido de 9 % até 88 % . Em consultas por abrangência , houve redução de 16 % até 99 % no número de cálculos de distância e de 9 % a 99 % no tempo gasto em relação à inserção . Em consultas aos k-vizinhos mais próximos , houve redução de 13 % a 86 % em número de cálculos de distância e de 9 % até 63 % no tempo 
 Os hiperdocumentos manipulados pelos sistemas hipertexto clássicos não podem ser utilizados em outro sistema ou plataforma . O padrão HyTime , uma extensão de SGML , constitui um esforço para permitir que documentos hipermídia e multimídia possam ser descritos em termos de sua estrutura e conteúdo e , desse modo , ser utilizados por quaisquer sistemas conformantes . Esse trabalho_investiga  os recursos para especificação de hiperdocumentos hipermídia de HyTime , em particular , aqueles providos por um Minimal hyperlinking HyTime document , uma classe de documentos definida pelo padrão 
 Como resultado deste trabalho , algumas estruturas tradicionais da literatura sobre hipertexto foram especificadas em um DTD através de construtores HyTime , um sistema básico hipermídia para autoria e apresentação dessas estruturas foi modelado , e a codificação da apresentação dessas estruturas através de applets Java foi implementada para que elas possam ser utilizadas no ambiente da WWW 
 Aprendizado múltipla instância ( MIL ) é um paradigma de aprendizado de máquina que tem o objetivo de classificar um conjunto ( bags ) de objetos ( instâncias ) , atribuindo rótulos só para os bags . Em MIL apenas os rótulos dos bags estão disponíveis para treinamento , enquanto os rótulos das instâncias são desconhecidos . Este problema é frequentemente abordado através da seleção de uma instância para representar cada bag , transformando um problema MIL em um problema de aprendizado_supervisionado  padrão . No entanto , não se conhecem abordagens que apoiem o usuário na realização desse processo . Neste trabalho , propomos uma visualização baseada em árvore multi-escala chamada MILTree que ajuda os usuários na realização de tarefas relacionadas com MIL , e também dois novos métodos de seleção de instâncias , chamados MILTree-SI e MILTree-Med , para melhorar os modelos MIL . MILTree é um layout de árvore de dois_níveis  , sendo que o primeiro projeta os bags , e o segundo nível projeta as instâncias pertencentes a cada bag , permitindo que o usuário explore e analise os dados multi-instância de uma forma intuitiva . Já os métodos de seleção de instãncias objetivam definir uma instância protótipo para cada bag , etapa crucial para a obtenção de uma alta precisão na classificação de dados multi-instância . Ambos os métodos utilizam o layout MILTree para atualizar visualmente as instâncias protótipo , e são capazes de lidar com conjuntos de dados binários e multi-classe . Para realizar a classificação dos bags , usamos um classificador SVM ( Support Vector Machine ) . Além_disso  , com o apoio do layout MILTree também pode-se atualizar os modelos de classificação , alterando o conjunto de treinamento , a fim de obter uma melhor classificação . Os resultados experimentais validam a eficácia da nossa abordagem , mostrando que a mineração visual através da MILTree pode ajudar os usuários em cenários de classificação multi-instância 
 Sistemas-de-sistemas ou SoS ( do inglês , `` Systems-of-Systems '' ) , são sistemas complexos de larga_escala  e , algumas vezes , críticos e intensivos a software que têm se mostrado uma classe de sistemas promissora em vários domínios de aplicação . Em paralelo , arquiteturas de software têm um papel_importante  no desenvolvimento de sistemas intensivos a software , tratando requisitos funcionais e não-funcionais . Processos sistemáticos para o design de arquiteturas de software de SoS podem lidar com desafios do desenvolvimento desses sistemas , incluindo a promoção da colaboração de sistemas constituintes independentes , envolvendo diferentes proprietários , missões e interesses . Embora SoS intensivos a software sejam relevantes e necessários em diversos_domínios  de aplicação , a maior_parte  de suas arquiteturas tem sido desenvolvidas de forma ad hoc . Há uma ausência de processos estruturados para arquitetar SoS , dificultando a adoção segura de SoS , reduzindo possibilidades de compartilhamento de soluções arquiteturais para problemas comuns e impactando negativamente no sucesso desses sistemas . Esta tese apresenta um processo geral para SoS reconhecidos chamado SOAR ( do inglês , `` General Process for Acknowledged SoS Software Architectures '' ) que dá suporte ao estabelecimento de instâncias de processos para o design arquitetural desses sistemas . Concebido para prover diferentes níveis de suporte de acordo com o contexto de desenvolvimento de cada SoS , o SOAR é constituído por um kernel de alto nível que descreve o que precisa ser feito para arquitetar SoS e também por três práticas que descrevem atividades e produtos de trabalho para guiar como conduzir a análise , a síntese e a avaliação arquitetural . Na avaliação do SOAR , foram realizados três surveys , um estudo de viabilidade e um experimento . Os resultados obtidos indicam que o SOAR pode oferecer um suporte positivo na instanciação de processos para o design de SoS reconhecidos e , como consequência , contribuir para o desenvolvimento e a evolução destes sistemas complexos intensivos a software 
 Aplicações de dados intensivos precisam acessar uma grande quantidade de dados armazenados em memórias off-chip . Devido ao dispendioso tempo de acesso , muitos autores propõem arquiteturas que implementam sistemas tais como cache e Scratch-pad Memories ( SPM ) para otimizar a reutilização de dados . Algumas vezes a SPM pode ser mais adequada que a memória cache , especialmente quando o elemento de processamento é customizado para a aplicação . No entanto , é necessário definir qual é o melhor tamanho da memória e qual e quando um conjunto de dados deve ser carregado na memória . Neste contexto , é desejável desenvolver uma técnica que possa gerar estes valores automaticamente . Este trabalho apresenta uma técnica para minimizar o número de acessos off-chip e o tamanho da memória on-chip ( SPM ) usada em aplicações baseadas em FPGA . A técnica desenvolvida é baseada em algoritmos genéticos devido à sua flexibilidade para explorar todo o espaço de projeto e fornecer um conjunto de soluções válidas . Ao usar a abordagem , é possível gerar soluções válidas para loops regulares e função de endereçamento afim ( affine ) . Por exemplo , para a aplicação Sobel nossa técnica foi capaz de encontrar um padrão de acesso que forneceu 83,3 % de redução aos acessos à memória off-chip usando menos de 1 KB de memória on-chip . 
 Com o excesso de informação disponível online , a Sumarização Automática tornouse uma área de bastante interesse na grande área da Inteligência_Artificial  . Alguns autores tentaram caracterizar o processo de sumarização para compreender melhor como sumarizadores o realizam . O alinhamento de um sumário e seus textos fonte pode ser encarado como uma caracterização desse processo . Com relação à sumarização_automática  , a técnica de alinhamento consiste em obter relações entre segmentos de um ou vários textos e seu sumário e , da forma que o conteúdo de um segmento esteja contido no outro . Uma vez obtidas essas relações , tornase possível ( i ) aprender como sumarizadores profissionais realizam a sumarização , ( ii ) explicitar regras e modelos para a sumarização , e ( iii ) criar métodos automatizados utilizando as regras e modelos explicitados , o que traz uma contribuição à Sumarização Automática . Neste trabalho , foram propostas três abordagens dentro das abordagens superficiais e profundas do Processamento de Língua Natural para realizar os alinhamentos de forma automática . A primeira utiliza três métodos superficiais , sendo eles Word overlap , tamanho relativo e posição relativa . A segunda caracterizase em uma técnica de alinhamento com mais conhecimento linguístico , pois nela foi utilizada uma teoria discursiva , a CST ( CrossDocument Structure Theory ) . A terceira utiliza Aprendizado de Máquina , caracterizando uma abordagem híbrida dada a característica de seus atributos superficiais e profundos , relativo à primeira e à segunda abordagem . Uma avaliação comparativa entre elas , e também entre um trabalho da literatura , foi realizada . Quando os dados do aprendizado de máquina eram balanceados , foi atingido o valor de 97,2 % de medidaF , maior valor encontrado . O método superficial Word overlap também obteve um bom resultado , sendo ele 66,2 % de medidaF 
 Na atualidade , o interesse pelo uso de ontologias tem sido incrementado . No entanto , o processo de construção pode ser custoso em termos de tempo . Para uma ontologia ser construída , precisa-se de um especialista com conhecimentos de um editor de ontologias . Com a finalidade de reduzir tal processo de construção pelo especialista , analisamos e propomos um método para realizar aprendizado de ontologias ( AO ) de forma supervisionada . O presente_trabalho  consiste em uma abordagem combinada de diferentes técnicas no AO . Primeiro , usamos uma técnica estatística chamada C/NC-values , acompanhada da ferramenta Cogroo , para extrair os termos mais representativos do texto . Esses termos são considerados por sua vez como conceitos . Projetamos também uma gramática de restrições ( GR ) , com base na informação linguística do Português , com o objetivo de reconhecer e estabelecer relações entre conceitos . Para poder enriquecer a informação na ontologia , usamos a análise de conceitos formais ( ACF ) com o objetivo de identificar possíveis superconceitos entre dois conceitos . Finalmente , extraímos ontologias para os textos de três temas , submetendo-as à avaliação dos especialistas na área . Um web site foi feito para tornar o processo de avaliação mais amigável para os avaliadores e usamos o questionário de marcos de características proposto pelo método OntoMetrics . Os resultados mostram que nosso método provê um ponto de partida aceitável para a construção de ontologias 
 A presente tese de doutorado insere-se na área de interação pelo olhar . A interação pelo olhar é uma forma de comunicação com o computador utilizando os movimentos oculares do usuário . Pessoas com deficiência física , que não conseguem usar dispositivos convencionais como um teclado e um mouse de computador , podem se beneficiar da interação pelo olhar para se comunicarem e se inserirem na sociedade . Para isso a entrada de texto ou digitação pelo olhar é um recurso importante e assunto principal dessa tese . O instrumento mais comum para entrada de texto pelo olhar consiste de um teclado virtual onde os caracteres são selecionados por tempo de latência . Essa forma de interação , embora simples , sofre de seleções involuntárias ( problema conhecido como toque de Midas ) se o tempo de latência for curto ( menos de 500 ms ) . Já para tempos de latência mais longos , a interação se torna lenta . Alternativas para entrada de texto pelo olhar são os gestos discretos ou contínuos do olhar . O uso de gestos discretos permite reduzir o toque de Midas , porém o desempenho é inferior ao tempo de latência . Já nos métodos baseados em gestos contínuos , o olhar está sempre preso ao controle da interface . Uma técnica de interação proposta recentemente , chamada de `` alternância entre contextos '' , permite reduzir o efeito do toque de Midas , utilizando apenas uma sacada para cada seleção . Além_disso  , essa técnica permite aos usuários manterem o ritmo de interação sem ajustar nenhum parâmetro na interface . A presente tese de doutorado visa melhorar a usabilidade e experiência dos usuários na interação pelo olhar com teclados virtuais . Os objetivos específicos são : investigar a relação entre a manipulação do contraste dos estímulos visuais e o tempo de reação sacádico para facilitar os movimentos oculares e tornar a interação mais rápida e agradável ; propor e investigar novas extensões e aplicações da alternância entre contextos , visando reduzir o toque de Midas e ao mesmo tempo generalizar o método para outras tarefas de navegação e seleção de objetos pelo olhar ; e desenvolver novos métodos de entrada de texto pelo olhar para melhorar a velocidade de digitação dos usuários , sem incrementar a carga de trabalho e mantendo a interação simples e fácil de aprender . A avaliação dos novos métodos e modelos propostos foi feita por meio de vários estudos com usuários . Os dados coletados nos estudos , tanto quantitativos quanto qualitativos , foram analisados com métodos estatísticos utilizados na área de interação homem-computador . As contribuições originais apresentadas na presente tese são : a proposta e a avaliação do efeito gap gradiente como feedback visual para facilitar a execução de movimentos sacádicos durante a interação pelo olhar ; a proposta e investigação de contextos dinâmicos como extensão da alternância entre contextos , para permitir um melhor aproveitamento da área útil do monitor com uma baixa taxa de erros de seleção , assim como de meta-keys para navegação e execução de comandos de forma geral ; e a proposta e a avaliação de AugFix , um novo modelo de feedback visual que melhora a velocidade e a experiência dos usuários na entrada de texto pelo olhar , com aplicação em teclados virtuais baseados nos paradigmas do tempo de latência e a alternância entre contextos 
 Nos últimos_anos  , as redes_sociais  na Web 2.0 vêm ganhando cada vez mais importância para trabalhar e compartilhar ideias . As redes_sociais  armazenam informações do usuário , como preferências , experiência profissional , dados pessoais e com quem o usuário interage . Essas informações são úteis para diversos fins , como oferecer produtos e serviços personalizados . Com a aparição de cada vez mais redes_sociais  , surgem problemas como a duplicação de perfis de usuários . Atualmente há algumas técnicas para interoperar as redes_sociais  , como serviços de autenticação única ou representação padrão para compartilhamento de dados . O objetivo deste trabalho foi realizar um estudo dessas técnicas e tecnologias disponíveis , implementá-las por meio de componentes do Groupware Workbench , e implantar e avaliar os componentes desenvolvidos na rede social Arquigrafia . A avaliação dos componentes foi realizada por meio dos aspectos e questões propostos pelo projeto DataPortability . A avaliação mostrou que as questões diretamente relacionadas com a interoperabilidade técnica e semântica foram respondidas 
 Os W-operadores são operadores invariantes por translação e localmente definidos dentro de uma janela W. Devido a sua grande utilidade em análise de imagens , estes operadores foram extensamente pesquisados , sendo que uma abordagem para o seu estudo é a partir da Morfologia Matemática . Uma propriedade interessante de W-operadores é que eles possuem uma sup-decomposição , ou seja , um W-operador pode ser decomposto em termos de uma família de operadores sup-geradores que , por sua vez , são parametrizados por elementos da base desse $ W $ -operador . No entanto , a sup-decomposição tem uma estrutura intrinsecamente paralela que não permite uma implementação eficiente em máquinas de processamento sequencial . Em um trabalho publicado em 2001 , Hashimoto e Barrera formalizaram o problema de transformar a sup-decomposição em decomposições puramente sequenciais como um problema de encontrar soluções discretas de uma equação . Neste texto , estendemos o trabalho desenvolvido por eles . Estudamos e exploramos as propriedades matemáticas do problema , e desenvolvemos estratégias heurísticas para encontrar uma decomposição sequencial de um $ W $ -operador a partir de sua base que seja eficiente ao ser executado 
 Em um problema de natureza hierárquica , o nível mais influente toma certas decisões que afetam o comportamento dos níveis inferiores . Cada decisão do nível mais influente é considerada como fixa pelos níveis inferiores , que , com tais informações , tomam decisões que maximizam seus objetivos . Essas decisões podem influenciar os resultados obtidos pelo nível superior , que , por sua vez , também anseia pela decisão ótima . Em programação matemática , este problema é modelado como um problema de programação em níveis . Neste trabalho , consideramos uma classe particular de problemas de programação em níveis : os problemas de programação matemática em dois_níveis  . Estudamos uma técnica de resolução que consiste em substituir o problema do nível inferior por suas condições necessárias de primeira_ordem  , que podem ser formuladas de diversas maneiras , conforme as restrições de complementaridade são modificadas . O novo problema torna-se um problema de programação não linear e pode ser resolvido com algoritmos clássicos de otimização . Com o auxílio de condições de otimalidade de primeira e segunda_ordem  mostramos as relações entre o problema original e o problema reformulado . Aplicamos a técnica a problemas encontrados na literatura , analisamos o seu comportamento e apresentamos estratégias para eliminar certos inconvenientes encontrados 
 O computador tem sido empregado na educação praticamente desde seu surgimento e a literatura tem apontado como vantagens de sua incorporação no processo de ensino-aprendizagem , seu potencial de promoção de interatividade e de resposta rápida ( retroação ) . Mais recentemente , com a grande popularização da Web , o uso de Sistemas Gerenciadores de Cursos ( SGC ) passou a ser ferramenta necessária para personalizar o aprendizado do aluno e para sintetizar informações ao professor . Além_disso  os SGC viabilizaram o surgimento de uma grande quantidade de cursos na modalidade de Educação a Distância ( EAD ) . Dentre os SGC um que merece destaque é o Moodle , em função de ser um sistema livre e por adotar uma arquitetura modular que permite a incorporação de novas ferramentas . Entretanto , nota-se atualmente uma carência no Moodle de ferramentas que proporcionem aprendizado interativo e que eventualmente permita retroação imediata . Deste modo , a proposta deste trabalho é o enriquecimento do Moodle , apresentando um novo pacote , o iTarefa , que possibilita o gerenciamento de atividades interativas . O iTarefa possibilita a incorporação de qualquer applet Java , desde que este esteja na forma de um Módulo de Aprendizagem Interativa ( iMA ) . Neste trabalho serão apresentados , além do pacote iTarefa , os resultados de seu uso em disciplina de graduação e alguns minicursos para professores e alunos do ensino fundamental e médio 
 A extração de metadados semânticos de vídeos digitais para uso em serviços de personalização é importante , já que o conteúdo é adaptado segundo as preferências de cada usuário . Entretanto , apesar de serem encontradas várias propostas na literatura , as técnicas de indexação automática são capazes de gerar informações_semânticas  apenas quando o domínio do conteúdo é restrito . Alternativamente , existem técnicas para a criação manual dessas informações por profissionais , contudo , são dispendiosas e suscetíveis a erros . Uma possível solução seria explorar anotações colaborativas dos usuários , mas tal estratégia provoca a perda de individualidade dos dados , impedindo a extração de preferências do indivíduo a partir da interação . Este trabalho tem como objetivo propor uma arquitetura de personalização que permite a indexação multimídia de modo irrestrito e barato , utilizando anotações colaborativas , mas mantendo-se a individualidade dos dados para complementar o perfil de interesses do usuário com conceitos relevantes . A multimodalidade de metadados e de preferências também é explorada na presente tese , fornecendo maior robustez na extração dessas informações , e obtendo-se uma maior carga semântica que traz benefícios às aplicações . Como prova de conceito , este trabalho apresenta dois serviços de personalização que exploram a arquitetura proposta , avaliando os resultados por meio de comparações com abordagens previamente propostas na 
 Os Métodos Ágeis surgiram no final da década de 90 , como uma alternativa aos métodos prescritivos de desenvolvimento de software . Eles propõem uma nova abordagem de desenvolvimento , eliminando gastos com documentação excessiva e burocrática , enfatizando a interação entre as pessoas e as atividades que efetivamente trazem valor ao cliente . Nos últimos_anos  , diversos princípios e práticas baseados na indústria de manufatura foram incorporadas pelos Métodos Ágeis de desenvolvimento de software . Um dos princípios absorvidos é o de melhorar a eficácia de uma organização através de melhorias globais . Embora este princípio seja bem difundido nos Métodos Ágeis , utilizá-lo não é uma tarefa fácil . Nem sempre é fácil ter uma visão global do processo de desenvolvimento . Além_disso  , para realizar melhorias globais é necessário descobrir a causa para possíveis problemas , o que também pode ser uma tarefa_difícil  . Esse trabalho_investiga  duas_abordagens  da indústria de manufatura que enxergam uma organização como um sistema no qual todas as partes são inter-relacionadas . Com base nelas , três abordagens de desenvolvimento de software existentes são analisadas . Finalmente , um estudo comparativo foi feito para avaliar as principais características dos métodos de desenvolvimento estudados . Esse estudo estende o trabalho feito por Abrahamssom et al. , no livro Agile Software Development : Current Research and Future Directions , avaliando o desempenho dos métodos seguindo o arcabouço proposto pelos mesmos autores 
 Artigos científicos são o principal mecanismo que pesquisadores usam para reportar suas descobertas científicas , e uma coleção de artigos em uma área de pesquisa pode revelar muito sobre sua evolução ao longo do tempo , como a emergência de novos tópicos e a evolução dos mesmos quanto ao seu conteúdo . No entanto , dada uma ampla coleção de artigos é geralmente muito difícil extrair_informações  importantes que possam ajudar leitores a interpretar globalmente , navegar e então eventualmente focar em itens relevantes para sua tarefa . Mapas de documentos baseados em conteúdo são representações_visuais  criadas para avaliar a similaridade entre documentos , e têm se mostrado úteis em auxiliar tarefas exploratórias neste cenário . Documentos são representados por marcadores visuais projetados em um espaço bidimensional de forma que documentos com conteúdo similar permaneçam próximos . Apesar de estes mapas permitirem a identificação visual de grupos de documentos relacionados e de fronteiras entre esses grupos , eles não transmitem explicitamente a evolução_temporal  de uma coleção . Nesta tese , propomos e validamos um mapa de documentos dinâmico interativo para coleções de artigos científicos capaz de evidenciar o comportamento temporal para apoiar tarefas de análise , preservando ao mesmo tempo a acurácia local do mapa e o contexto do usuário . As mudanças nas relações de similaridade , evidenciadas ao longo do tempo nesse mapa , oferecem suporte para detecção da evolução_temporal  dos tópicos . Essa evolução é caracterizada por meio de eventos de transição entre grupos , como a emergência de novos grupos e tópicos em momentos específicos e a especialização de um grupo , e pela detecção de mudanças no vocabulário dos tópicos , utilizando técnicas que extraem os termos mais relevantes ( tópicos ) em cada grupo , em diferentes 
 Imagens digitais são utilizadas para diversas finalidades , variando de uma simples foto com os amigos até a identificação de doenças em exames médicos . Por mais que as tecnologias de captura de imagens tenham evoluído , toda imagem adquirida digitalmente possui um ruído intrínseco a ela que normalmente é adquirido durante os processo de captura ou transmissão da imagem . O grande desafio neste tipo de problema consiste em recuperar a imagem perdendo o mínimo possível de características importantes da imagem , como cantos , bordas e texturas . Este trabalho propõe uma abordagem baseada em um Algoritmo Genético Híbrido ( AGH ) para lidar com este tipo de problema . O AGH combina um algoritmo genético com alguns dos melhores métodos de supressão de ruídos em imagens encontrados na literatura , utilizando-os como operadores de busca local . O AGH foi testado em imagens normalmente utilizadas como benchmark corrompidas com um ruído branco aditivo Gaussiano ( N ; 0 ) , com diversos níveis de desvio padrão para o ruído . Seus resultados , medidos pelas métricas PSNR e SSIM , são comparados com os resultados obtidos por diferentes métodos . O AGH também foi testado para recuperar imagens SAR ( Synthetic Aperture Radar ) , corrompidas com um ruído Speckle multiplicativo , e também teve seus resultados comparados com métodos especializados em recuperar imagens SAR . Através dessa abordagem híbrida , o AGH foi capaz de obter resultados competitivos em ambos os tipos de testes , chegando inclusive a obter melhores_resultados  em diversos casos em relação aos métodos da literatura 
 Esta dissertação descreve a criação e o desenvolvimento de um sistema interativo de modelagem e visualização de fractais determinísticos em 3-D. O domínio do sistema abrange inicialmente , estruturas fractais obtidas a partir de superficies equipotenciais de Julia e Mandelbrot , bem como quaternions aplicados aos mesmos conjuntos . A técnica de representação/visualização escolhida foi uma modificação de ray-tracing conhecida como `` boundary tracking '' , com o auxílio de um z-buffer . O texto visa também introduzir a Geometria Fractal como uma técnica alternativa de modelagem e visualização tridimensional de objetos no contexto da Computação_Gráfica  
 Este trabalho consiste na avaliação empírica da adequação e do custo de aplicação dos critérios Potenciais Usos no teste de programas COBOL , com o apoio de uma ferramenta de teste denominada POKE-TOOL . Os critérios Potenciais Usos são critérios estruturais de teste baseados em fluxo de dados : informação de fluxo de dados ( definição e uso de variáveis do programa ) é utilizada para derivar os requisitos de teste . Utiliza-se , para a realização deste experimento , uma estratégia de teste que combina técnicas funcionais e estruturais , reconhecido o aspecto complementar dessas técnicas . Para a geração do conjunto inicial de casos de teste são aplicados os critérios funcionais Análise de Valores Limites e Particionamento em Classes de Equivalência . Utiliza-se a ferramenta POKE-TOOL para a análise de adequação do conjunto inicial de casos de teste em relação aos critérios Potenciais Usos e novos casos de teste são gerados visando a cobrir todos os elementos requeridos . A análise dos resultados obtidos indica que o uso dos critérios Potenciais Usos no teste de programas COBOL é factível , pois um pequeno número de casos de teste é requerido para satisfazer estes critérios . Vários modelos de estimativas para a previsão do número de casos de teste requeridos e do número de caminhos não executáveis gerados para esta classe de programas são analisados . São caracterizadas também as principais causas de não executabilidade nos programas do `` benchmark '' utilizado , uma vez que a existência desses caminhos é um problema inerente ao teste_estrutural  
 Neste trabalho , desenvolvemos uma análise para modelos de componentes de variância . Assumindo diferentes densidades a priori para os parâmetros do modelo de componentes de variância com 2 ou 3 componentes de variância , exploramos o uso de métodos de aproximação de Laplace para obter as quantidades a posteriori de interesse . Também estudamos a importância de uma boa parametrização para a obtenção de resultados precisos . Além_disso  , também consideramos distribuições não-normais para os efeitos aleatórios e desenvolvemos um estudo comparativo considerando diferentes conjuntos de dados 
 A busca por tecnologias que possibilitem ganhos substanciais de produtividade no desenvolvimento de sistemas de software é um dos desafios da engenharia de software . Ao longo dos anos , diversas tecnologias surgiram com o objetivo de aumentar a produtividade . Reutilização de software é uma tecnologia emergente que pode ajudar a aumentar o índice de produtividade no desenvolvimento de software . Um sistema de reutilização deve fornecer mecanismos que permitam ao desenvolvedor de software identificar , catalogar , recuperar e integrar componentes de software . A identificação de componentes de software é tarefa complexa , pois responder o que reutilizar , como reutilizar e quando reutilizar não é simples . Para auxiliar nas respostas às questões citadas , a análise de domínio surge como uma abordagem sistemática para identificação , registro e elaboração de diretrizes do uso dos componentes de software existentes em um domínio de aplicação . Este trabalho descreve uma experiência prática de aplicação de conceitos e técnicas de análise de domínios na busca de abstrações reutilizáveis de software existentes nas aplicações do domínio . Apresenta como resultado da análise : um esquema de classificação dos componentes reutilizáveis de software que permite a criação de uma biblioteca de componentes , apresenta o modelo de domínio através de uma arquitetura padrão de construção de aplicações e uma linguagem de domínio , baseada no jargão utilizado no domínio e que permite especificar novas aplicações . O trabalho mostra , também , um exemplo do uso dos resultados da análise de domínio através da implementação de uma ferramenta automatizada de software para gerencia de uma biblioteca de componentes de software , bem com descreve uma proposta para construção de um gerador de aplicação , ambos como parte de um sistema de reutilização 
 Sistemas Hipermídia são programas capazes de armazenar e recuperar informações não-lineares , estabelecendo uma estrutura complexa e flexível representada por nós interligados . À medida em que aumenta o espaço de navegação , tal como acontece na World_Wide  Web ( WWW ou Web ) , a possibilidade de desorientação do usuário no espaço de navegação torna-se maior . Assim , a Hipermídia Adaptativa investiga métodos e técnicas para a adaptação automática de conteúdos e/ou ligações para características , interesses ou objetivos individuais . Trabalhos recentes em Hipermídia Adaptativa sugerem o uso de técnicas de Aprendizado de Máquina e Modelagem de Usuários . Este trabalho_investiga  o uso de técnicas de Aprendizado de Máquina para a adaptação de estruturas ( ligações ) em um ambiente Hipermídia , em especial a World_Wide  Web . Para tanto , avalia-se o desempenho de diferentes algoritmos de Aprendizado de Máquina para a adaptção de ligações em ambiente WWW . Os resultados experimentais obtidos sugerem o potencial do emprego de técnicas de Aprendizado de Máquina 
 Métodos de visualização fazem_uso  de representações gráficas interativas embutidas em uma área de exibição para exploração e análise de dados . Esses recursos visuais usam primitivas geométricas para representar dados ou compor representações mais sofisticadas que facilitem a extração visual de informações . Uma das tarefas mais desafiadoras é determinar um layout ótimo visando explorar suas capacidades para transmitir informação dentro de uma determinada visualização . Os algoritmos existentes para construir layouts a partir de primitivas geométricas são tipicamente projetados para lidar com requisitos como alinhamento ortogonal , remoção de sobreposição , área usada , organização hierárquica , atualização dinâmica entre outros . No entanto , a maioria das técnicas são capazes de lidar com apenas alguns desses requerimentos simultaneamente , prejudicando sua utilização e flexibilidade . Nesta tese , propomos um conjunto de abordagens para construir layouts a partir de primitivas geométricas que simultaneamente lidam com uma gama mais ampla de requerimentos . Baseando-se em projeções_multidimensionais  e formulações de otimização , os nossos métodos organizam objetos geométricos no espaço_visual  para gerar layouts bem estruturados que preservam a relação semântica entre objetos enquanto ainda fazem um uso eficiente da área de exibição . Um conjunto detalhado de comparações quantitativas com métodos existentes para a geração de layouts e aplicações em visualização de conjunto de dados de texto , imagem e vídeo comprova a eficácia das técnicas propostas 
 Os principais objetos de estudo neste trabalho são os números de Ramsey para circuitos e o lema da regularidade de Szemerédi . Dados grafos $ L_1 , \ldots , L_k $ , o número de Ramsey $ R ( L_1 , \ldots , L_k ) $ é o menor inteiro $ N $ tal que , para qualquer coloração com $ k $ cores das arestas do grafo completo com $ N $ vértices , existe uma cor $ i $ para a qual a classe de cor correspondente contém $ L_i $ como um subgrafo . Estaremos especialmente interessados no caso em que os grafos $ L_i $ são circuitos . Obtemos um resultado original solucionando o caso em que $ k=3 $ e $ L_i $ são circuitos pares de mesmo tamanho 
 Em anos recentes , muitos algoritmos bio-inspirados têm surgido para resolver problemas de classificação . Em confirmação a isso , a revista Nature , em 2002 , publicou um artigo que já apontava para o ano de 2003 o uso comercial de Sistemas Imunológicos Artificiais para detecção de fraude em instituições financeiras por uma empresa britânica . Apesar disso , não observamos , a luz de nosso conhecimento , nenhuma publicação científica com resultados promissores desde então . Nosso trabalho tratou de aplicar Sistemas Imunológicos Artificiais ( AIS ) para detecção de fraude em cartões de crédito . Comparamos AIS com os métodos de Árvore de Decisão ( DT ) , Redes_Neurais  ( NN ) , Redes Bayesianas ( BN ) e Naive Bayes ( NB ) . Para uma comparação mais justa entre os métodos , busca exaustiva e algoritmo genético ( GA ) foram utilizados para selecionar um conjunto paramétrico otimizado , no sentido de minimizar o custo de fraude na base de dados de cartões de crédito cedida por um emissor de cartões de crédito brasileiro . Em adição à essa otimização , fizemos também uma análise e busca por parâmetros mais robustos via multi-resolução , estes parâmetros são apresentados neste trabalho . Especificidades de bases de fraude como desbalanceamento de dados e o diferente custo entre falso positivo e negativo foram levadas em conta . Todas as execuções foram realizadas no Weka , um software público e Open Source , e sempre foram utilizadas bases de teste para validação dos classificadores . Os resultados obtidos são consistentes com Maes et_al  . que mostra que BN são melhores que NN e , embora NN seja um dos métodos mais utilizados hoje , para nossa base de dados e nossas implementações , encontra-se entre os piores métodos . Apesar do resultado pobre usando parâmetros default , AIS obteve o melhor resultado com os parâmetros otimizados pelo GA , o que levou DT e AIS a apresentarem os melhores e mais robustos resultados entre todos os métodos testados 
 Neste projeto analisamos jogos de formação de redes que são variantes do problema da floresta de Steiner , nos quais indivíduos desejam conectar conjuntos de vértices terminais em um grafo de forma a minimizar seus custos , podendo dividir o custo das arestas com os demais participantes . Estudamos como o método de divisão de custos influencia na existência e na qualidade dos equilíbrios desses jogos em comparação com o valor da solução ótima centralizada 
 Um dos principais problemas em testes de hipóteses para a homogeneidade de curvas de sobrevivência ocorre quando as taxas de falha ( ou funções de intensidade ) não são proporcionais . Apesar do teste de Log-rank ser o teste não paramétrico mais utilizado para se comparar duas ou mais populações sujeitas a dados censurados , este teste apresentada duas restrições . Primeiro , toda a teoria assintótica envolvida com o teste de Log-rank , tem como hipótese o fato das populações envolvidas terem distribuições contínuas ou no máximo mistas . Segundo , o teste de Log-rank não apresenta bom comportamento quando as funções intensidade cruzam . O ponto inicial para análise consiste em assumir que os dados são contínuos e neste caso processos Gaussianos apropriados podem ser utilizados para testar a hipótese de homogeneidade . Aqui , citamos o teste de Renyi e Cramér-von Mises para dados contínuos ( CCVM ) , ver Klein e Moeschberger ( 1997 ) [ 15 ] . Apesar destes testes não paramétricos apresentar bons_resultados  para dados contínuos , esses podem ter problemas para dados discretos ou arredondados . Neste trabalho , fazemos um estudo simulação da estatística de Cramér von-Mises ( CVM ) proposto por Leão e Ohashi [ 16 ] , que nos permite detectar taxas de falha não proporcionais ( cruzamento das taxas de falha ) sujeitas a censuras arbitrárias para dados discretos ou arredondados . Propomos também , uma modificação no teste de Log-rank clássico para dados dispostos em uma tabela de contingência . Ao aplicarmos as estatísticas propostas neste trabalho para dados discretos ou arredondados , o teste desenvolvido apresenta uma função poder melhor do que os testes 
 O objetivo deste trabalho é estudar a performance de alguns métodos de comparações múltiplas ( MCMs ) que ajustam o valor-p quando as estatísticas empregadas nos testes são a log-rank e a Cramér-von Mises , ambas não paramétricas e com estrutura de dependência . A vantagem dos MCMs que ajustam o valor-p é que eles controlam as taxas de erro tipo I e tipo II para cada hipótese , afim de atingir um poder estatístico elevado , mantendo a taxa de erro da família dos testes ( FWER ) menor ou igual ao nível de significância escolhido . Trabalhamos com o procedimento clássico de Bonferroni e com outros métodos vistos como seu melhoramento , com especial atenção a certos procedimentos derivados do método de Simes que permitem realizar inferências sob as hipóteses individuais . Foi verificado teoricamente que a estatística log-rank pertence à classe multivariada totalmente positiva de ordem 2 ( 'MTP IND . 2 ' ) , uma vez que o método de Simes garante o controle da FWER quando as estatísticas dependentes assumem esta condição . O controle da FWER empregando a estatística de Cramér-von Mises foi observado apenas por meio de simulações . Os MCMs foram analisados através de estudos computacionais em modelos discretos e contínuos sob censura com foco no problema de comparar um tratamento versus 
 Algumas abordagens para o problema de Planejamento Ótimo da Operação de Sistemas Hidrotérmicos ( POOSH ) utilizam modelos estocásticos para representar as vazões afluentes dos reservatórios do sistema . Essas abordagens utilizam , em geral , técnicas de Programação Dinâmica Estocástica ( PDE ) para resolver o POOSH . Por outro_lado  , muitos autores têm defendido o uso dos modelos determinísticos ou , particularmente , a Programação Dinâmica Determinística ( PDD ) por representar de forma individualizada a interação entre as usinas hidroelétricas do sistema . Nesse contexto , esta dissertação tem por objetivo comparar o desempenho da solução do POOSH obtida via PDD com a solução obtida pela PDE , que emprega um modelo Markoviano periódico , com distribuição condicional Log-Normal Truncada para representar as vazões . Além_disso  , é realizada a análise com abordagem_bayesiana  , no modelo de vazões , para estimação dos parâmetros e previsões das vazões afluentes . Comparamos as performances simulando a operação das usinas hidroelétricas de Furnas e Sobradinho , considerando séries de vazões geradas 
 Neste trabalho , propomos um método de pontos_interiores  para minimização com restrições_lineares  de grande porte . Este método explora a linearidade das restrições , partindo de um ponto viável e preservando a viabilidade dos iterandos . Apresentamos os principais resultados de convergência global , além de uma descrição rica em detalhes de uma implementação prática de todos os passos do método . Para atestar a implementação do método , exibimos uma ampla experimentação numérica , e uma análise comparativa com métodos bem difundidos na comunidade de otimização contínua 
 O Teste Baseado em Modelos ( TBM ) emergiu como uma estratégia promissora para minimizar problemas relacionados à falta de tempo e recursos em teste de software e visa verificar se a implementação sob teste está em conformidade com sua especificação . Casos de teste são gerados automaticamente a partir de modelos comportamentais produzidos durante o ciclo de desenvolvimento de software . Entre as técnicas de modelagem existentes , Sistemas de Transição com Entrada/Saída ( do inglês , Input/Output Transition Systems - IOTSs ) , são modelos amplamente_utilizados  no TBM por serem mais expressivos do que Máquinas de Estado Finito ( MEFs ) . Apesar dos métodos existentes para geração de testes a partir de IOTSs , o problema da seleção de casos de testes é um tópico difícil e importante . Os métodos existentes para IOTS são não-determinísticos , ao contrário da teoria existente para MEFs , que fornece garantia de cobertura completa com base em um modelo de defeitos . Esta tese investiga a aplicação de modelos de defeitos em métodos determinísticos de geração de testes a partir de IOTSs . Foi proposto um método para geração de conjuntos de teste com base no método W para MEFs . O método gera conjuntos de teste de forma determinística além de satisfazer condições de suficiência de cobertura da especificação e de todos os defeitos do domínio de defeitos definido . Estudos empíricos avaliaram a aplicabilidade e eficácia do método proposto : resultados experimentais para analisar o custo de geração de conjuntos de teste utilizando IOTSs gerados aleatoriamente e um estudo de caso com especificações da indústria mostram a efetividade dos conjuntos gerados em relação ao método tradicional de Tretmans 
 Os avanços na área de comunicações foram indiscutívelmente essenciais para a obtenção de sistemas e aplicações modernos como os o atuais . A computação ubíqua se tornou realidade , permitindo que sistemas embarcados especializados ganhassem espaço e cada vez mais autonomia . Esse é notavelmente o caso de veículos não tripulados que têm sido criativamente explorados em aplicações inovadoras e avançadas . Entretanto , para o funcionamento eficiente desses veículos e sistemas não tripulados , além de melhorias de comunicação , é altamente desejável que as necessidades relevantes co-relacionadas a comunica¸cao sejam cuidadosamente observadas , levando a uma facilitação na inserção de veículos não tripulados em espaços públicos . Além_disso  , ao abordar essas demandas de modo integrado , as chances de produzir melhores_resultados  é maior . Esta tese apresenta a HAMSTER , uma arquitetura de comunicação de dados baseada em mobilidade e segurança para veículos não tripulados , que aborda três tipos principais de comunicação : máquina-para- máquina , máquina-para-infraestrutura e comunicações internas . Quatro elementos adicionais co-relacionados são fornecidos juntamente com a arquitetura HAMSTER de modo a prover abordagens mais precisas em relação a aspectos de segurança física e da informação ( plataforma SPHERE ) , análise de criticalidade ( índice NCI ) , eficiência energética ( plataforma NP ) e comunicações ad hoc e infraestruturadas orientadas a mobilidade ( plataforma NIMBLE ) . Além_disso  , são fornecidas três versões especializadas : para veículos aéreos não tripulados ( Flying HAMSTER ) , veículos terrestres não tripulados ( Running HAMSTER ) e veículos submarinos e de superfície não tripulados ( Swimming HAMSTER ) . A validação da arquitetura é obtida por meio de estudos de caso sobre cada recurso abordado , levando a diretrizes sobre o desenvolvimento de veículos mais preparados para atender a requisitos de certificação , comunicação mais eficiente e segura , abordagens assertivas sobre criticidade e abordagens verdes nas comunicações internas . Por fim , os resultados comprovaram a eficiência da arquitetura HAMSTER e os elementos com ela providos , bem como a flexibilidade em realizar experimentos focados em vários aspectos de comunicação , auxiliando na obtenção de comunicações seguras em veículos_autônomos  
 Em Aprendizado de Máquina - AM não existe um único algoritmo que é sempre melhor para todos os domínios de aplicação . Na prática , diversas pesquisas mostram que Redes_Neurais  Artificiais - RNAs têm um 'bias ' indutivo apropriado para diversos_domínios  . Em razão disso , RNAs têm sido aplicadas na resolução de vários problemas com desempenho satisfatório 

 Sistemas de AM simbólico possuem um 'bias ' indutivo menos flexível do que as RNAs . Enquanto que as RNAs são capazes de aprender qualquer função , sistemas de AM simbólico geralmente aprendem conceitos que podem ser descritos na forma de hiperplanos . Por outro_lado  , sistemas de AM simbólico representam o conceito induzido por meio de estruturas simbólicas , as quais são geralmente compreensíveis pelos seres_humanos  . Assim , sistemas de AM simbólico são preferíveis quando é essencial a compreensibilidade do conceito induzido . RNAs carecem da capacidade de explicar suas decisões , uma vez que o conhecimento é codificado na forma de valores de seus pesos e 'thresholds ' . Essa codificação é difícil de ser interpretada por seres_humanos  

 Em diversos_domínios  de aplicação , tal como aprovação de crédito e diagnóstico médico , prover uma explicação sobre a classificação dada a um determinado caso é de crucial importância . De um modo similar , diversos usuários de sistemas de AM simbólico desejam validar o conhecimento induzido , com o objetivo de assegurar que a generalização feita pelo algoritmo é correta . Para que RNAs sejam aplicadas em um maior número de domínios , diversos pesquisadores têm proposto métodos para extrair conhecimento compreensível de RNAs 

 As principais_contribuições  desta tese são dois métodos que extraem conhecimento simbólico de RNAs . Os métodos propostos possuem diversas vantagens sobre outros métodos propostos previamente , tal como ser aplicáveis a qualquer arquitetura ou algoritmo de aprendizado de RNAs supervisionadas . O primeiro método proposto utiliza sistemas de AM simbólico para extrair conhecimento de RNAs , e o segundo método proposto estende o primeiro , combinado o conhecimento induzido por diversos sistemas de AM simbólico por meio de um Algoritmo Genético - AG 

 Os métodos propostos são analisados experimentalmente em diversos_domínios  de aplicação . Ambos os métodos são capazes de extrair conhecimento simbólico com alta fidelidade em relação à RNA treinada . Os métodos propostos são comparados com o método TREPAN , apresentando resultados promissores . TREPAN é um método bastante conhecido para extrair conhecimento de RNAs 

 Um sistema de software que apresente problemas em sua execução pode gerar conseqüências desde um simples incômodo ao usuário , até desastres como a perda de uma sonda da NASA em Marte . As atividades de teste visam identificar erros nos sistemas de software , prevenindo estas conseqüências indesejáveis . Porém , os testes podem envolver entre 30 % e 40 % do esforço de desenvolvimento do sistema , e em sistemas críticos , seu custo pode ser de 3 a 5 vezes maior do que o custo combinado das demais atividades . Para tentar reduzir estes custos podemos automatizar parte das atividades . No presente caso , pretende-se minimizar os casos de teste gerados manualmente , utilizando uma técnica denominada verificação de modelos . Esta técnica consiste em verificar propriedades definidas formalmente através de expressões matemáticas , utilizando uma ferramenta de verificação que simula a execução do código . Além_disso  , um sistema que utilize um tratamento de condições excepcionais eficiente , tem sua manutenibilidade , robustez e confiabilidade melhoradas . Por isso , definimos propriedades relacionadas ao tratamento de exceções , como ponto de entrada para a verificação de modelos . Apresentamos um ambiente de testes criado para permitir a verificação destas propriedades com o verificador Java PathFinder e a exibição das estatísticas de cobertura de testes de acordo com o critério selecionado . Este ambiente facilita a execução dos testes , pois apresenta uma interface_gráfica  com o usuário que permite a configuração e execução dos testes sem que seja necessária a escrita de código pelo testador . Apresentamos também o resultado do uso deste ambiente para o teste de vários programas exemplo , utilizando desde código concorrente até diferentes estratégias de tratamento de exceção e discutimos as características , cuidados no uso e limitações das ferramentas utilizadas 
 Acrescente capacidade de armazenamento introduziu novos desafios no contexto de exploração de grandes bases de dados de músicas . Esse trabalho consiste em investigar técnicas de comparação de músicas representadas por sinais polifônicos , com o objetivo de encontrar similaridades , permitindo a identificação de músicas cover em grandes bases de dados . Técnicas de extração de características a partir de sinais musicais foram estudas , como também métricas de comparação a partir das características obtidas . Os resultados mostraram que é possível encontrar um novo método de identificação de covers com um menor_custo  computacional do que os existentes , mantendo uma boa 
 A Mineração de Textos ( MT ) visa descobrir conhecimento inovador nos textos não estruturados . A extração dos termos que representam os textos de um domínio é um dos passos mais importantes da MT , uma vez que os resultados de todo o processo da MT dependerão , em grande parte , da qualidade dos termos obtidos . Nesta tese , considera-se como termos as unidades lexicais realizadas para designar conceitos em um cenário tematicamente restrito . Para a extração dos termos , pode-se fazer uso de abordagens como : estatística , linguística ou híbrida . Normalmente , para a Mineração de Textos , são utilizados métodos estatísticos . A aplicação desses métodos é computacionalmente menos custosa que a dos métodos linguísticos , entretanto seus resultados são geralmente menos interpretáveis . Ambos métodos , muitas_vezes  , não são capazes de identificar diferenças entre termos e não-termos , por exemplo , os estatísticos podem não identificar termos raros ou que têm a mesma frequência de não-termos e os linguísticos podem não distinguir entre termos que seguem os mesmo padrões linguísticos dos não-termos . Uma solução para esse problema é utilizar métodos híbridos , de forma a combinar as estratégias dos métodos linguísticos e estatísticos , visando atenuar os problemas inerentes a cada um deles . Considerando as características dos métodos de extração de termos , nesta tese , foram investigados métodos estatísticos , formas de obtenção de conhecimento linguístico e métodos híbridos para a extração de termos simples - aqueles constituídos de somente um radical , com ou sem afixos - na língua_portuguesa  do Brasil . Quatro medidas estatísticas ( tvq , tv , tc e comGram ) , originalmente utilizadas em outras tarefas , foram avaliadas na extração de termos simples , sendo que duas delas ( tvq e tv ) foram consideradas relevantes para essa tarefa . Quatro novas medidas híbridas ( n_subst. , n_adj. , n_po e n_verbo ) foram propostas , sendo que três delas ( n_subst , . n_adj. , e n_po ) auxiliaram na extração de termos . Normalmente os métodos de extração de termos selecionam candidatos a termos com base em algum conhecimento linguístico . Depois disso , eles aplicam a esses candidatos medidas ou combinação de medidas ( e/ou heurísticas ) para gerar um ranking com tais candidatos . Quanto mais ao topo desse ranking os candidatos estão , maior a chance de que eles sejam termos . A escolha do liminar a ser considerado nesse ranking é feita , em geral de forma_manual  ou semiautomática por especialistas do domínio e/ou terminólogos . Automatizar a forma de escolha dos candidatos a termos é a primeira motivação da extração de termos realizada nesta pesquisa . A segunda motivação desta pesquisa é minimizar o elevado número de candidatos a termos presente na extração de termos . Esse alto número , causado pela grande quantidade de palavras contidas em um corpus , pode aumentar a complexidade de tempo e os recursos_computacionais  utilizados para se extrair os termos . A terceira motivação considerada nesta pesquisa é melhorar o estado da arte da extração automática de termos simples da língua_portuguesa  do Brasil , uma vez que os resultados dessa extração ( medida_F  = 16 % ) ainda são inferiores se comparados com a extração de termos em línguas como a inglesa ( medida_F  = 92 % ) e a espanhola ( medida_F  = 68 % ) . Considerando essas motivações , nesta tese , foi proposto o método MATE-ML ( Automatic Term Extraction based on Machine Learning ) que visa extrair automaticamente termos utilizando técnicas da área de aprendizado de máquina . No método MATE-ML , é sugerido o uso de filtros para reduzir o elevado número de candidatos a termos durante a extração de termos sem prejudicar a representação do domínio em questão . Com isso , acredita-se que os extratores de termos podem gerar listas menores de candidatos extraídos , demandando , assim , menos tempo dos especialistas para avaliar esses candidatos . Ainda , o método MATE-ML foi instanciado em duas_abordagens  : ( i ) ILATE ( Inductive Learning for Automatic Term Extraction ) , que utiliza a classificação supervisionada indutiva para rotular os candidatos a termos em termos e não termos , e ( ii ) TLATE ( Transductive Learning for Automatic Term Extraction ) , que faz_uso  da classificação semissupervisionada transdutiva para propagar os rótulos dos candidatos rotulados para os não rotulados . A aplicação do aprendizado transdutivo na extração de termos e a aplicação ao mesmo tempo de um conjunto rico de características de candidatos pertencentes a diferentes níveis de conhecimento - linguístico , estatístico e híbrido também são consideradas contribuições desta tese . Nesta tese , são discutidas as vantagens e limitações dessas duas_abordagens  propostas , ILATE e TLATE . Ressalta-se que o uso dessas abordagens alcança geralmente resultados mais altos de precisão ( os melhores casos alcançam mais de 81 % ) , altos resultados de cobertura ( os melhores casos atingem mai de 87 % ) e bons valores de medida_F  ( máximo de 41 % ) em relação aos métodos e medidas comparados nas avaliações experimentais realizadas considerando três corpora de diferentes domínios na língua_portuguesa  do 
 Estruturas de tratamento de exceção são extremamente comuns em softwares desenvolvidos em linguagens modernas , como Java , e afetam de forma contundente o comportamento de um software quando exercitadas . Apesar destas duas características , as principais técnicas de verificação , teste de software e verificação formal , e as ferramentas a elas vinculadas , tendem a negligenciar o comportamento excepcional . Alguns dos fatores que levam a esta negligência são a não especificação do comportamento excepcional em termos de projeto e a consequente implementação das estruturas de tratamento com base no julgamento individual de cada programador . Isto resulta na não consideração de partes expressivas do código em termos de verificação e , consequentemente , a possibilidade de não serem detectados erros relativos tanto às próprias estruturas de tratamento quanto às estruturas de código vinculadas a estas . A fim de abordar este problema , propomos uma técnica , baseada em model checking , que automatiza o processo de exercício de caminhos excepcionais . Isto permite que seja observado o comportamento de um software quando da ocorrência de uma exceção . Pretendemos , com esta técnica , dar suporte para que seja aplicado aos caminhos que representam o comportamento excepcional de um software as mesmas técnicas de detecção de erros que são aplicadas aos caminhos que representam o comportamento normal e , com isso , agregar um aumento na qualidade do desenvolvimento de software 
 Neste trabalho estudou-se diversas arquiteturas de dispositivos FPGAs presentes no mercado , visando a utilização desta tecnologia em arquiteturas de computação reconfigurável . Especificamente foram investigados recursos e técnicas de reconfigurabilidade dinâmica destes dispositivos 
 A possibilidade de reconfigurar dinamicamente o hardware cria diversas expectativas de superação das arquiteturas de computação tradicional . Surge o conceito de hardware virtual , assim como inúmeras dificuldades em utilizar efetivamente esta tecnologia . Outra característica promissora que a tecnologia FPGA oferece é a possibilidade de realizar upgrades remotos do hardware , sem a necessidade de substituição física de equipamentos ou parte deles 
 Em adicional , foi implementado um sistema multi-FPGAs com dispositivos reconfiguráveis individualmente de forma local ou remota . Este sistema poderá ser a base para uma arquitetura de computação reconfigurável , sendo ela dinâmica ou não 
 Os dispositivos lógicos programáveis pertencentes à família APEX 20K , são configurados no momento da inicialização do sistema com dados armazenados em dispositivos especificamente desenvolvidos para esse fim . Esta família de FPGAs possui uma interface otimizada , permitindo também que microprocessadores os configure de maneira serial ou paralela , síncrona ou assíncronamente . Depois de configurados , estes FPGAs podem ser reconfigurados em tempo real com novos dados de configuração . A reconfiguração em tempo real conduz a inovadoras aplicações de computação reconfigurável 
 Os dispositivos de configuração disponíveis comercialmente , limitam-se a configurar os FPGAs apenas no momento da inicialização do sistema e sempre com o mesmo arquivo de configuração . Este trabalho apresenta a implementação de um controlador de configuração capaz de gerenciar a configuração e reconfiguração de múltiplos FPGAs , a partir de vários arquivos distintos de configuração 
 Todo o projeto é desenvolvido , testado e validado através da ferramenta EDA Quartus II , que propicia um ambiente de desenvolvimento integrado de projeto , compilação e síntese lógica , simulação e análise de tempo 
 O objetivo principal deste trabalho é o estudo das funções univalentes e de suas propriedades . Este estudo é direcionado principalmente aos polinômios univalentes e à investigação de prolemas extremos envolvendo seus coeficientes , seus zeros e suas propriedades geométricas . Encontramos uma relação interessante entre os polinômios univalentes e os polinômios univalentes definida por Suffridge . Geramos várias funções univalentes estreladas e convexas através de suas propriedades geométricas e da localização de seus zeros 
 Recentemente , várias abordagens estão sendo propostas na área de modelagem de processos de negócio . Dentre elas estão as linguagens BPEL e NPDL . BPEL é uma linguagem de representação e execução de processos de negócio que se mostrou bastante expressiva e uma forte candidata a padrão de mercado . NPDL é uma linguagem de definição de processos de negócio baseada em uma extensão de álgebra de processos chamada ACP . NPDL possui uma ferramenta capaz de interpretar e controlar a execução de processos de negócio chamada de NavigationPlanTool . A tradução de processos BPEL para expressões NPDL tem como objetivo fornecer aos processos descritos em BPEL um ambiente de controle e execução baseado em um formalismo algébrico . Entretanto , isso não é uma tarefa fácil . A presença de conceitos em BPEL que não são mapeáveis para NPDL faz com que grande parte da expressividade de BPEL se perca na tradução . Essa perda se dá pela limitação da própria ACP , na qual NPDL se baseia . Para sanar essa dificuldade , surgiu a idéia de estender ou trocar a base algébrica da NPDL . Substituindo a ACP por outro arcabouço algébrico ou incorporando idéias de outras álgebras , seria possível tornar a NPDL mais próxima de BPEL , facilitando , assim , o trabalho de mapeamento . Dentre os arcabouços formais disponíveis , LOTOS tem se mostrado uma interessante alternativa à ACP como base para a NPDL . Para comprovar os benefícios da utilização de conceitos de LOTOS na NPDL ou , até mesmo , de uma troca da base algébrica da NPDL de ACP para LOTOS , este trabalho faz um estudo comparativo entre esses dois formalismos algébricos , buscando encontrar a álgebra com maior expressividade e que represente melhor os conceitos presentes em BPEL . Para essa comparação , serão utilizados os principais conceitos existentes na linguagem BPEL , bem como os Padrões de Controle de Fluxo de Workflow . Não pertence ao escopo deste trabalho a implementação da NPDL usando LOTOS como base formal 
 Nos ultimos anos o uso de tecnicas de aprendizado computacional tornou se uma das tarefas comumente realizadas , pois tem inumeras aplicacoes de reconhecimento de padroes , tais como : reco- nhecimento de voz , classificacao de texto , reconhecimento facial , diagnostico por imagens medicas , entre outras . Dessa forma , um grande numero de tecnicas que lidam com este tipo de problema tem sido desenvolvido ate o momento . Neste trabalho apresentamos uma alternativa para melhorar a taxa acerto de classificacao do classificador binario SLS , que apresentou resultados comparaveis com as SVMs . Nesse metodo , o Gradiente Descendente e utilizado para otimizar a posicao final dos conjuntos de segmentos de reta que representarao cada classe . Embora convirja rapidamente a um valor otimo , muitas_vezes  e possivel o algoritmo parar em uma regiao de otimos locais , que nao representa o minimo global . Dado esse problema , foram utilizados diferentes algoritmos_evolutivos  em combinacao com o Gradiente Descendente a fim de melhorar a acuracia do classificador SLS . Adicionalmente a aplicacao de algoritmos_evolutivos  na fase de treinamento do classificador SLS , foram exploradas duas propostas : ( i ) explorar o uso de diferente numero de segmentos de reta para representar a distribuicao de dados de cada classe . Dado que no algoritmo original do metodo SLS o numero de segmentos de reta e igual para cada classe , o qual pode significar alguma perda de acuracia ou sobreposicao dos segmentos de reta ; ( ii ) estimar a melhor combinacao de segmentos de reta a serem usados para cada classe . O uso de diferentes quantidades de segmentos de reta por classe pode ser de ajuda na obtencao de melhores porcentagens de acerto , mas determinar uma quantidade otima que permita representar cada classe , e um trabalho dificil . Assim , usamos o algoritmo X-Means , que e um algoritmo de agrupamento , para estimar o numero de segmentos de reta . As propostas exibiram bons_resultados  que possibilitam a aplicacao do classificador SLS , com um algoritmo de treinamento hibrido , em problemas reais 
 Os Sistemas de Gerenciamento de Bases de Dados Relacionais ( SGBDR ) foram concebidos para o armazenamento e recuperação de grandes volumes de dados . Tradicionalmente , estes sistemas suportam números , pequenas cadeias de caracteres e datas ( que podem ser comparados por identidade ou por relações de ordem { RO ) , porém vem se tornando necessário organizar , armazenar e recuperar dados mais complexos , como por exemplo dados multimídia ( imagens , áudio e vídeo ) , séries_temporais  etc . Quando se trata de dados complexos há uma mudança de paradigma , pois as comparações entre elementos são feitas por similaridade em vez das RO utilizadas tradicionalmente , tendo como mais frequentemente utilizados os operadores de comparação por abrangência ( Rq ) e por k-vizinhos mais próximos ( k-NN ) . Embora muitos estudos estejam sendo feitos nessa área , quando lidando com consultas por similaridade grande parte do esforço é direcionado para criar as estruturas de indexação e dar suporte às operações necessárias para executar apenas o aspecto da consulta que trata da similaridade , sem focar em realizar uma integração homogênea das consultas que envolvam ambos os tipos de operadores simultaneamente nos ambientes dos SGDBRs . Um dos principais problemas nessa integração é lidar com as peculiaridades do operador de busca por k-NN . Todos os operadores de comparação por identidade e por RO são comutativos e associativos entre si . No entanto o operador de busca por k-NN não atende a nenhuma dessas propriedades . Com isso , a expressão de consultas em SQL , que usualmente pode ser feita sem que a expressão da ordem entre os predicados seja importante , precisa passar a considerar a ordem . Além_disso  , consultas que utilizam comparações por k-NN podem gerar múltiplos empates , e a falta de uma metodologia para resolvê-los pode_levar  a um processo de desempate arbitrário ou insensível ao contexto da consulta , onde usuários não tem poder para intervir de maneira significativa . Em alguns_casos  , isso pode_levar  a uma mesma consulta a retornar resultados distintos em casos onde a estrutura interna dos dados estiver sujeita a modificações , como por exemplo em casos de transações concorrentes em um SGBDR . Este trabalho aborda os problemas gerados pela inserção de operadores de busca por similaridade nos SGBDR , mais especificamente o k-NN , e propõe novas maneiras de representação de consultas com múltiplos predicados , por similaridade ou RO , assim como novos operadores derivados do k-NN que são mais adequados para um ambiente relacional que permita consultas híbridas , e permitem também controle sobre o tratamento de empates 
 Mídias sociais como o Twitter e o Facebook atuam , em diversas situações , como canais de iniciativas que buscam ampliar as ações de cidadania . Por outro_lado  , certas ações e manifestações na mídia convencional por parte de instituições governamentais , ou de jornalistas e políticos como deputados e senadores , tendem a repercutir nas mídias_sociais  . Como resultado , gerase uma enorme_quantidade  de dados em formato textual que podem ser muito informativos sobre ações e políticas governamentais . No entanto , o público-alvo continua carente de boas ferramentas que ajudem a levantar , correlacionar e interpretar as informações potencialmente úteis associadas a esses textos . Neste contexto , este trabalho apresenta dois sistemas orientados à análise de dados governamentais e de mídias_sociais  . Um dos sistemas introduz uma nova visualização , baseada na metáfora do rio , para análise temporal da evolução de tópicos no Twitter em conexão com debates políticos . Para tanto , o problema foi inicialmente modelado como um problema de clusterização e um método de segmentação de texto independente de domínio foi adaptado para associar ( por clusterização ) tweets com discursos parlamentares . Uma versão do algorimo MONIC para detecção de transições entre agrupamentos foi empregada para rastrear a evolução_temporal  de debates ( ou agrupamentos ) e produzir um conjunto de agrupamentos com informação de tempo . O outro sistema , chamado ATR-Vis , combina técnicas de visualização com estratégias de recuperação ativa para envolver o usuário na recuperação de tweets relacionados a debates políticos e associa-os ao debate correspondente . O arcabouço proposto introduz quatro estratégias de recuperação ativa que utilizam informação estrutural do Twitter melhorando a acurácia do processo de recuperação e simultaneamente minimizando o número de pedidos de rotulação apresentados ao usuário . Avaliações por meio de casos de uso e experimentos quantitativos , assim como uma análise qualitativa conduzida com três especialistas ilustram a efetividade do ATR-Vis na recuperação de tweets relevantes . Para a avaliação , foram coletados dois conjuntos de tweets relacionados a debates parlamentares ocorridos no Brasil e no Canadá , e outro formado por um conjunto de notícias que receberam grande atenção da mídia no período da coleta 
 O impacto dos serviços de mídia social em nossa sociedade é crescente . Indivíduos frequentemente utilizam mídias_sociais  para obter notícias , decidir quais os produtos comprar ou para se comunicar com amigos . Como consequência da adoção generalizada de mídias_sociais  , um grande volume de dados sobre como os usuários se comportam é gerado diariamente e armazenado em grandes bancos de dados . Aprender a analisar e extrair conhecimentos úteis a partir destes dados tem uma série de potenciais aplicações . Por exemplo , um entendimento mais detalhado sobre como usuários legítimos interagem com serviços de mídia social poderia ser explorado para projetar métodos mais precisos de detecção de spam e fraude . Esta pesquisa de doutorado baseia-se na seguinte hipótese : dados gerados por usuários de mídia social apresentam padrões que podem ser explorados para melhorar a eficácia de tarefas como previsão e modelagem no domínio das mídias_sociais  . Para validar esta hipótese , foram projetados métodos de mineração de dados adaptados aos dados de mídia social . As principais_contribuições  desta pesquisa de doutorado podem ser divididas em três partes . Primeiro , foi desenvolvido o Act-M , um modelo matemático que descreve o tempo das ações dos usuários . O autor demonstrou que o Act-M pode ser usado para detectar automaticamente bots entre usuários de mídia social com base apenas nos dados de tempo . A segunda contribuição desta tese é o VnC ( Vote-and- Comment ) , um modelo que explica como o volume de diferentes_tipos  de interações de usuário evolui ao longo do tempo quando um conteúdo é submetido a um serviço de mídia social . Além de descrever precisamente os dados reais , o VnC é útil , pois pode ser empregado para prever o número de interações recebidas por determinado conteúdo de mídia social . Por fim , nossa terceira contribuição é o método MFS-Map . O MFS-Map fornece automaticamente anotações textuais para imagens de mídias_sociais  , combinando eficientemente características visuais e de metadados das imagens . As contribuições deste doutorado foram validadas utilizando dados reais de diversos serviços de mídia social . Os experimentos mostraram que os modelos Act-M e VnC forneceram um ajuste mais preciso aos dados quando comparados , respectivamente , a modelos existentes para dinâmica de comunicação e difusão de informação . O MFS-Map obteve precisão superior e tempo de execução reduzido quando comparado com outros métodos amplamente_utilizados  para anotação de imagens 
 Em aplicações distribuídas baseadas em agentes_móveis  , a coordenação das ações dos agentes_móveis  é uma tarefa complexa . A maior dificuldade é devido ao fato que agentes_móveis  podem mudar de endereço dinamicamente 
 Nesta dissertação , apresentamos o projeto e a implementação de um mecanismo de coordenação de agentes_móveis  que contorna este problema . Este mecanismo , que chamamos de Canal de Broadcast , está baseado na difusão de mensagens e possibilita que os membros de um grupo de agentes_móveis  interajam entre si , independentemente de suas localizações correntes 
 Modelos de coordenação existentes oferecem formas de interação entre agentes_móveis  , mas todos eles impõem alguma exigência . Ou os agentes_móveis  devem conhecer a localização de outros agentes , ou devem estar localizados no mesmo lugar ou devem migrar para um lugar específico 
 A principal vantagem deste mecanismo de coordenação está na total transparência de localização : as mensagens podem ser endereçadas a um conjunto de agentes_móveis  independentemente de sua localização corrente 
 Este mecanismo foi implementado no ASDK ( Aglets Software Development Kit ) da IBM e a sua utilidade foi demonstrada usando dois problemas típicos de coordenação em Sistemas_Distribuídos  : a Exclusão Mútua e o protocolo Manager-Workers . Testamos o desempenho do mecanismo e identificamos que o custo do Canal de Broadcast não é tão alto comparado aos benefícios que proporciona 
 Através deste mecanismo , os agentes_móveis  poderão executar as suas tarefas e interagir entre sí com o propósito da coordenação sem as exigências impostas por outros modelos de coordenação 

 A modelagem matemática por meio de equações_diferenciais  é uma importante ferramenta para prever o comportamento de baterias redox de vanádio , pois ela pode contribuir para o aperfeiçoamento do produto e melhor entendimento dos princípios da sua operação . Os estudos de modelagem podem ser aliados à análise assintótica no intuito de promover reduções ou simplificações que tornem os modelos menos complexos , isso é feito a partir da observação da importância que cada termo exerce sobre as equações . Tais simplificações são úteis neste contexto , visto que os modelos geralmente abordam uma célula apenas - a menor unidade operacional da bateria - enquanto aplicações reais exigem o uso de dezenas ou centenas delas implicando em uma maximização do uso de recursos_computacionais  . Neste trabalho , foram investigadas múltiplas formas de reduções assintóticas que empregadas na construção dos modelos puderam acelerar o tempo de processamento em até 2,46 vezes ou reduzir os requisitos de memória principal em até 11,39 % . As simulações computacionais foram executadas pelo software COMSOL Multiphysics v. 4.4 , e também por scripts desenvolvidos em ambiente de programação MATLAB . A validação dos resultados foi feita comparando-os a dados experimentais presentes na literatura . Tal abordagem permitiu também validar as rotinas implementadas para a simulação dos modelos comparando suas soluções com aquelas providas pelo COMSOL 
 Esta dissertação de mestrado tem como objetivo avaliar a integração de políticas de tolerância a falhas em uma arquitetura de Web_Services  com múltiplos módulos . A arquitetura utilizada é denominada WSARCH , e foi desenvolvida para o estudo das relações e interoperabilidade entre serviçcos . Os mecanismos de tolerência a falhas foram integrados aos módulos da arquitetura , testados , comparados e avaliados . A avaliação de desempenho mostrou que os mecanismos de tolerância a falhas introduzidos foram eficientes e apresentaram resultados adequados . As técnicas de reputação utilizadas na seleção de serviço atuaram satisfatoriamente e foram consideradas um importante avanço nos mecanismos da 
 Este trabalho apresenta uma nova_técnica  de visualização que aproveita as vantagens do rendering volumétrico direto e do rendering de superfícies em um ambiente híbrido . O método faz_uso  de uma pré-visualização sobre o bordo do volume que viabiliza uma interação em tempo real com objetos volumétricos modelados por meio de malhas não estruturadas . Além_disso  , essa nova abordagem de visualização é paralelizável e pode se acelerada com placas gráficas comuns 
 Neste trabalho é apresentado o Método de Diferenças Finitas Geralizadas ( MDFG ) , como uma alternativa de método meshless para solucionar equações_diferenciais  parciais . Este método está baseado na aproximação local por mínimos quadrados a partir de nós espalhados sobre o domínio . E proposto um critério de seleção dos nós utilizados na aproximação , garantindo que o erro de truncamento gerado pelo MDFG é baixo . Demonstra-se teoricamente a convergência do método em certos problemas elípticos e parabólicos , e alguns exemplos são apresentados para comprovar este fato . A aplicação do método na solução de EDP em domínios de alta complexidade é mostrada num exemplo de escoamento de fluido numa artéria 
 A atual evolução tecnológica permite gerar , coletar e armazenar uma enorme_quantidade  de dados , resultando em um grande acúmulo de informações . Isto possibilita vislumbrar a extração de conhecimento útil por meio de um processo próprio denominado Descoberta de Conhecimento em Bases de Dados e , como passo deste , mineração de dados . Técnicas de Visualização podem ser utilizadas neste contexto , fornecendo representações_visuais  e mecanismos de interação que exploram características próprias do ser humano para apoiar este processo . Da união destes dois aspectos , mineração de dados e Visualização , surge um novo tema de estudo que vem recebendo destaque no meio académico : Mineração Visual de Dados . Dependendo do domínio em que esta é aplicada , diversas particularidades sobressaem . Dados de domínio geográfico apresentam características espaciais e temporais que tornam a descoberta de conhecimento difícil de ser realizada e lhe conferem propriedades únicas diante dos demais domínios . O presente_trabalho  aborda o uso de técnicas de Visualização em atividades de exploração e análise de dados com atributos espaciais e temporais , enfatizando as técnicas de interação inerentes à Visualização . O trabalho tem enfoque na aplicação de técnicas de visualização e interação para promover o tratamento dos dados contidos numa volumosa base de dados pluviométricos 
 Nas últimos décadas , estudos em neurobiologia evidenciam a existência de comportamentos caóticos no cérebro humano e animal , tanto a nível microscópico ( neurônio ) quanto a nível macroscópico ( atividade global de cérebro ) . Essas evidencias motivam a exploração de sistemas caóticos em redes neurais_artificiais  . Diante deste contexto , o presente_trabalho  tem como objetivo estudar as redes neurais caóticas existentes e desenvolver novas redes neurais caóticas para reconhecimento de padrões multi-valorados . Dois novos modelos são propostos , cujos funcionamentos são divididos em duas fases : fase de armazenamento e fase de reconhecimento . Na primeira fase , um conjunto de padrões é armazenado em pontos fixos pelo algoritmo de aprendizado de matriz pseudo-inversa . Na segunda fase , a dinâmica periódica e caótica que existem em mapas caóticos são utilizadas . Em ambos modelos propostos , a órbita periódica representa um padrão recuperado , enquanto a órbita caótica oferece um mecanismo de busca eficiente . Uma das vantagens dos modelos propostos em relação às redes neurais caóticas existentes é que os primeiros não só podem reconhecer padrões binários , mas também podem reconhecer padrões multi-valorados , o que é uma característica importante em aplicações práticas 
 Um problema relevante na modelagem de escoamentos com movimento de corpo rígido consiste na consideração de forças externas bem como forças que o fluido exerce sobre o próprio corpo rígido . Um outro problema importante refere-se à descrição da trajetória dos corpos rígidos . Essas duas questões são o objeto de estudo deste trabalho . No sentido de solucioná-las , foram implementadas duas extensões ao modelador de movimentos do sistema Freeflow-3D . A primeira incorpora um tipo de movimento definido por forças externas e a segunda um tipo de movimento definido por interpolação linear por partes 
 O presente_trabalho  explora a integração dos processos de data warehousing e data mining em um sistema de comércio_eletrônico  multi-agente ( especificamente o projeto DEEPSIA - Dynamic on-linE IntErnet Purchasing System based on Intelligent Agents ) através da implementação de um protótipo que interliga diferentes fontes de dados procedentes dos distintos subsistemas presentes no projeto . Como resultado deste trabalho , é apresentado um protótipo para a análise da navegação dos usuários utilizando a exploração de regras de associação e análise de conglomerados ( clustering ) 
 A sumarização multidocumento consiste na tarefa de produzir automaticamente um único sumário a partir de um conjunto de textos derivados de um mesmo assunto . É imprescindível que seja feito o tratamento de fenômenos que ocorrem neste cenário , tais como : ( i ) a redundância , a complementaridade e a contradição de informações ; ( ii ) a uniformização de estilos de escrita ; ( iii ) tratamento de expressões referenciais ; ( iv ) a manutenção de focos e perspectivas diferentes nos textos ; ( v ) e a ordenação temporal das informações no sumário . O tratamento de tais fenômenos contribui significativamente para que seja produzido ao final um sumário informativo e coerente , características difíceis de serem garantidas ainda que por um humano . Um tipo particular de coerência estudado nesta tese é a coerência local , a qual é definida por meio de relações entre enunciados ( unidades menores ) em uma sequência de sentenças , de modo a garantir que os relacionamentos contribuirão para a construção do sentido do texto em sua totalidade . Partindo do pressuposto de que o uso de conhecimento discursivo pode melhorar a avaliação da coerência local , o presente_trabalho  propõe-se a investigar o uso de relações discursivas para elaborar modelos de coerência local , os quais são capazes de distinguir automaticamente sumários coerentes dos incoerentes . Além_disso  , um estudo sobre os erros que afetam a Qualidade Linguística dos sumários foi realizado com o propósito de verificar quais são os erros que afetam a coerência local dos sumários , se os modelos de coerência podem identificar tais erros e se há alguma relação entre os modelos de coerência e a informatividade dos sumários . Para a realização desta pesquisa foi necessário fazer o uso das informações semântico-discursivas dos modelos CST ( Cross-document Structure Theory ) e RST ( Rhetorical Structure Theory ) anotadas no córpus , de ferramentas automáticas , como o parser Palavras e de algoritmos que extraíram informações do córpus . Os resultados mostraram que o uso de informações semântico-discursivas foi bem sucedido na distinção dos sumários coerentes dos incoerentes e que os modelos de coerência implementados nesta tese podem ser usados na identificação de erros da qualidade linguística que afetam a coerência local 
 Controle de Acesso ( CA ) é um dos principais pilares da segurança da informação . Em resumo , CA permite assegurar que somente usuários habilitados terão acesso aos recursos de um sistema , e somente o acesso necessário para a realização de uma dada tarefa será disponibilizado . Neste contexto , o controle de acesso baseado em papel ( do inglês , Role Based Access Control - RBAC ) tem se estabelecido como um dos mais importante paradigmas de controle de acesso . Em uma organização , usuários recebem responsabilidades por meio de cargos e papéis que eles exercem e , em sistemas RBAC , permissões são distribuídas por meio de papéis atribuídos aos usuários . Apesar da aparente simplicidade , enganos podem ocorrer no desenvolvimento de sistemas RBAC e gerar falhas ou até mesmo brechas de segurança . Dessa forma , processos de verificação e validação tornam-se necessários . Teste de CA visa identificar divergências entre a especificação e o comportamento apresentado por um mecanismo de CA . Teste Baseado em Modelos ( TBM ) é uma variante de teste de software que se baseia em modelos explícitos de especificação para automatizar a geração de casos testes . TBM tem sido aplicado com sucesso no teste funcional , entretanto , ainda existem lacunas de pesquisa no TBM de requisitos não funcionais , tais como controle de acesso , especialmente de critérios de teste . Nesta dissertação de mestrado , dois aspectos do TBM de RBAC são investigados : métodos de geração de teste baseados em Máquinas de Estados_Finitos  ( MEF ) para RBAC ; e priorização de testes para RBAC . Inicialmente , dois métodos tradicionais de geração de teste , W e HSI , foram comparados ao método de teste mais recente , SPY , em um experimento usando políticas RBAC especificadas como MEFs . As características ( número de resets , comprimento médio dos casos de teste e comprimento do conjunto de teste ) e a efetividade dos conjuntos de teste gerados por cada método para cinco políticas RBAC foram analisadas . Posteriormente , três métodos de priorização de testes foram comparados usando os conjuntos de teste gerados no experimento anterior . Neste caso , um critério baseado em similaridade RBAC foi proposto e comparado com a priorização aleatória e baseada em similaridade simples . Os resultados obtidos mostraram que o método SPY conseguiu superar os métodos W e HSI no teste de sistemas RBAC . A similaridade RBAC também alcançou uma detecção de defeitos superior 
 O projeto de mestrado , denominado de forma abreviada como GPUServices , se insere no contexto da pesquisa e do desenvolvimento de métodos de processamento de dados de sensores tridimensionais aplicados a robótica_móvel  . Tais métodos serão chamados de serviços neste projeto e incluem algoritmos de pré-processamento de nuvens de pontos 3D com segmentação dos dados , a separação e identificação de zonas planares ( chão , vias ) , e detecção de elementos de interesse ( bordas , obstáculos ) . Devido à grande quantidade de dados a serem tratados em um curto espaço de tempo , esses serviços utilizam processamento paralelo por GPU para realizar o processamento parcial ou completo destes dados . A área de aplicação em foco neste projeto visa prover serviços para um sistema ADAS : veículos_autônomos  e inteligentes , forçando-os a se aproximarem de um sistema de processamento em tempo real devido ao contexto de direção autônoma . Os serviços são divididos em etapas de acordo com a metodologia do projeto , mas sempre buscando a aceleração com o uso de paralelismo inerente : O pré-projeto consiste de organizar um ambiente que seja capaz de coordenar todas as tecnologias utilizadas e que explore o paralelismo ; O primeiro serviço tem a responsabilidade de extrair inteligentemente os dados do sensor que foi usado pelo projeto ( Sensor laser Velodyne de múltiplos feixes ) , que se mostra necessário devido à diversos erros de leitura e ao formato de recebimento , fornecendo os dados em uma estrutura matricial ; O segundo serviço em cooperação com o anterior corrige a desestabilidade espacial do sensor devido à base de fixação não estar perfeitamente paralela ao chão e devido aos amortecimentos do veículo ; O terceiro serviço separa as zonas semânticas do ambiente , como plano do chão , regiões abaixo e acima do chão ; O quarto serviço , similar ao anterior , realiza uma pré-segmentação das guias da rua ; O quinto serviço realiza uma segmentação de objetos do ambiente , separando-os em blobs ; E o sexto serviço utiliza de todos os anteriores para a detecção e segmentação das guias da rua . Os dados recebidos pelo sensor são na forma de uma nuvem de pontos 3D com grande potencial de exploração do paralelismo baseado na localidade das informações . Porém , sua grande dificuldade é a grande taxa de dados recebidos do sensor ( em torno de 700.000 pontos/seg . ) , sendo esta a motivação deste projeto : usar todo o potencial do sensor de forma eficiente ao usar o paralelismo de programação GPU , disponibilizando assim ao usuário serviços de tratamento destes dados 
 O advento contínuo de novas tecnologias tem criado um tipo rico e crescente de fontes de informação disponíveis para análise e investigação . Neste contexto , a análise de dados multidi- mensional é consideravelmente importante quando se lida com grandes e complexos conjuntos de dados . Dentre as possibilidades ao analisar esses tipos de dados , a aplicação de técnicas de visualização pode auxiliar o usuário a encontrar e entender os padrões , tendências e estabelecer novas metas . Alguns exemplos de aplicações de visualização de análise de dados multidimen- sionais vão de classificação de imagens , nuvens semântica de palavras , e análise de grupos de coleção de documentos , à exploração de conteúdo multimídia . Esta tese apresenta vários métodos de visualização para explorar de forma interativa conjuntos de dados multidimensionais que visam de usuários especializados aos casuais , fazendo_uso  de ambas representações estáticas e dinâmicas criadas por projeções_multidimensionais  . Primeiramente , apresentamos uma técnica de projeção_multidimensional  que preserva fielmente distância e que pode lidar com qualquer tipo de dados com alta-dimensionalidade , demonstrando cenários de aplicações em ambos os casos de multimídia e coleções de documentos de texto . Em seguida , abordamos a tarefa de interpretar as projeções em 2D , calculando erros de vizinhança . Posteriormente , apresentamos um conjunto de visualizações interativas que visam ajudar os usuários com essas tarefas , revelando a qualidade de uma projeção em 3D , aplicadas em diferentes cenários de alta dimensionalidade . Na parte final , discutimos duas_abordagens  diferentes para obter percepções sobre dados multimídia , em particular vídeos de futebol . Enquanto a primeira abordagem utiliza projeções_multidimensionais  , a segunda faz_uso  de uma eficiente metáfora visual para auxiliar usuários não especialistas em navegar e obter conhecimento em partidas de futebol 
 Este trabalho descreve um método para expandir a aplicabilidade do sistema FreeFlow-2D para problemas de escoamentos de fluidos multifásicos . Este método permite a simulação de escoamento de fluido multifásico , incompressível e com superfície_livre  . Um número arbitrário de fases com propriedades diferentes pode ser utilizado . Tensões superficial e interfacial são também consideradas 
 A técnica numérica utilizada baseia-se no GENSMAC ( Generalized-Simplified-Marker-and-Cell ) e consideram-se propriedades variáveis de acordo com a posição da interface durante o escoamento . O campo de velocidade é computado utilizando-se às equações de Navier-Stokes discretizadas por esquema de diferença finita numa malha_deslocada  
 O método foi implementado em três_módulos  : Modelador , Simulador e Visualizador . A validação foi efetuada comparando-se os resultados numéricos com resultados analíticos e experimentais . O método mostrou-se robusto e computacionalmente eficiente para os problemas considerados 
 Problemas com a escrita podem afetar o desempenho de profissionais de maneira marcante , principalmente no caso de cientistas e acadêmicos que precisam escrever com proficiência e desembaraço não somente na língua materna , mas principalmente em inglês . Durante os últimos_anos  , ferramentas de suporte à escrita , algumas com enfoque em textos científicos , como o AMADEUS e o SciPo foram desenvolvidas e têm auxiliado pesquisadores na divulgação de suas pesquisas . Entretanto , a criação dessas ferramentas é baseada em córpus , sendo muito custosa , pois implica em selecionar textos bem escritos , além de segmentá-los de acordo com sua estrutura esquemática . Nesse mestrado estudamos , avaliamos e implementamos métodos de detecção automática da estrutura esquemática e de avaliação automática da qualidade de escrita de resumos científicos em inglês . Investigamos o uso de tais métodos para possibilitar o desenvolvimento de dois tipos de ferramentas : de detecção de bons resumos e de crítica . Nossa abordagem é baseada em córpus e em aprendizado de máquina supervisionado . Desenvolvemos um detector automático da estrutura esquemática , que chamamos de AZEA , com taxa de acerto de 80,4 % eKappa de 0,73 , superiores ao estado da arte ( acerto de 73 % , Kappa de 0,65 ) . Experimentamos várias combinações de algoritmos , atributos e diferentes seções de um artigo científicos . Utilizamos o AZEA na implementação de duas dimensões de uma rubrica para o gênero científico , composta de 7 dimensões , e construímos e disponibilizamos uma ferramenta de crítica da estrutura de um resumo . Um detector de erros de uso de artigo também foi desenvolvido , com precisão é de 83,7 % ( Kappa de 0,63 ) para a tarefa de decidir entre omitir ou não um artigo , com enfoque no feedback ao usuário e como parte da implementação da dimensão de erros gramaticais da rubrica . Na tarefa de detectar bons resumos , utilizamos métodos usados com sucesso na avaliação automática da qualidade de escrita de redações com as implementações da rubrica e realizamos experimentos iniciais , ainda com resultados fracos , próximos à baseline . Embora não tenhamos construído um bom avaliador automático da qualidade de escrita , acreditamos que este trabalho indica direções para atingir esta meta , e forneça algumas das ferramentas 
 Interfaces multimodais processam vários_tipos  de entrada do usuário , tais como voz , gestos e interação com caneta , de uma maneira combinada e coordenada com a saída multimídia do sistema . Aplicações que suportam a multimodalidade provêem um modo mais natural e flexível para a execução de tarefas em computadores , uma vez que permitem que usuários com diferentes níveis de habilidades escolham o modo de interação que melhor se adequa às suas necessidades . O uso de interfaces que fogem do estilo convencional de interação baseado em teclado e mouse vai de encontro ao conceito de computação ubíqua , que tem se estabelecido como uma área de pesquisa que estuda os aspectos tecnológicos e sociais decorrentes da integração de sistemas e dispositivos computacionais à ambientes . Nesse contexto , o trabalho aqui reportado visou investigar a implementação de interfaces multimodais em aplicações de computação ubíqua , por meio da construção de um framework de software para integração de modalidades de escrita e 
 O enchimento de reservatórios hidrelétricos provoca impactos ambientais , interferindo nos ecossistemas e nos modos de vida das populações envolvidas , pois durante este processo a biomassa terrestre é decomposta lançando substâncias que se concentram nos volumes dos diferentes compartimentos do reservatório . Nestas condições , algumas regiões do reservatório passam por períodos em que os teores de oxigênio dissolvido e a concentração de matéria orgânica comprometem o equilíbrio da flora e fauna locais , afetando tanto a qualidade das águas como também os ecótonos vizinhos . Neste trabalho , será tratado o problema do transporte das concentrações das espécies constituintes , da temperatura e da turbulência , 2D e 2DH , durante o enchimento de compartimentos de 
 A complexidade inerente ao processo de produção de energia apresenta um desafio aos especialistas quando estes se deparam com o dimensionamento e operação de sistemas de recursos hídricos . A produção energética de um sistema hidroelétrico depende fundamentalmente das séries de vazões afluentes às diversas usinas hidrelétricas do sistema . No entanto , a incerteza das vazões futuras e sua aleatoriedade são obstáculos que dificultam todo o planejamento da operação do sistema energético brasileiro . A inexistência de um software específico para análise de séries de vazões ocorridas nas usinas hidrelétricas , associada à importância desse tipo de dado no contexto energético , motivou a concepção de uma ferramenta gráafica para visualização e previsão desses dados . Acredita-se que a visualização desses dados por meio de representações apropriadas e altamente interativas possa promover hipóteses e revelar novas informações dos fenômenos associados a essas quantidades , melhorando a qualidade das decisões de planejamento do sistema energético . Este trabalho de mestrado apresenta em detalhes o sistema desenvolvido , chamado Power Map Explorer , e das técnicas nele 
 Com a popularização da Internet , a disponibilização online de documentos de qualquer espécie tornou-se extremamente rápida . Utilizando-se de ferramentas de busca , pode-se ter acesso a quase todos os tipos de informação em questão de segundos . Porém , a quantidade de sites que proporcionam alguma informação importante é , em geral , muito pequena , se comparada ao número total de páginas que é fornecido pela ferramenta de busca . Isso ocorre , basicamente , pelo fato de que as páginas retornadas são ordenadas , por exemplo , de acordo com a quantidade de acessos à página ou à quantidade de links que levam a ela . Isso significa que uma página contendo a informação que o usuário deseja , mas que esteja no final da lista , dificilmente será lida se existir uma grande , quantidade de páginas antes dela . Assim , seria de grande ajuda uma ferramenta capaz de : a ) recuperar um conjunto apropriado de documentos de acordo com palavras-chave fornecidas pelo usuário ; b ) analisar o conteúdo dos links encontrados extraindo informações relevantes dos textos e decidir se o documento pode ser importante para o usuário ; c ) fazer um clustering ( agrupamento por similaridade ) desses documentos relevantes e d ) exibir um mapa no qual documentos similares estejam próximos entre si e distantes daqueles relacionados com outra área . Essa ferramenta está sendo desenvolvida no LABIC/ICMC-USP e recebeu o nome de FIP ( Ferramenta Inteligente de Apoio à Pesquisa ) . Este trabalho visa investigar técnicas de clustering , principalmente , as aplicadas a documentos e decidir por aquela que melhor atenda os requisitos da FIP em termos de qualidade dos clusters , tempo de processamento e consumo de memória , visto que é tratada uma grande quantidade de documentos na ferramenta . Neste trabalho são testadas técnicas de clustering aglomerativo hierárquico , de particionamento e de mapa auto-organizável em corpus de artigos científicos , jornalísticos e de fórums de discussão : são discutidas as vantagens e desvantagens de cada uma ; e indicadas , no caso particular da ferramenta FIP , as abordagens apropriadas 
 Um importante problema na área de reconhecimento de padrões é o reconhecimento de textos manuscritos . O problema de reconhecimento de expressões matemáticas manuscritas é um caso particular , que vem_sendo  tratado por décadas . Esse problema é considerado desafiador devido à grande quantidade de possíveis tipos de símbolos , às variações intrínsecas da escrita , e ao complexo arranjo bidimensional dos símbolos na expressão . Neste trabalho adotamos o problema de reconhecimento de símbolos matemáticos manuscritos para realizar um estudo empírico sobre o comportamento de classificadores multi-classes . Examinamos métodos básicos de aprendizado para classificação multi-classe , especialmente as abordagens um-contra-todos e todos-contra-todos de decomposição de um problema multi-classe em problemas de classificação binária . Para decompor o problema em subproblemas menores , propomos também uma abordagem que utiliza uma árvore de decisão para dividir hierarquicamente o conjunto de dados , de modo que cada subconjunto resultante corresponda a um problema mais simples de classificação . Esses métodos são examinados usando-se como classificador base os modelos de classificação vizinhos-mais-próximos e máquinas de suporte vetorial ( usando a abordagem um-contra-todos para combinar os classificadores binários ) . Para classificação , os símbolos são representados por um conjunto de características conhecido na literatura por HBF49 e que foi proposto recentemente especificamente para problemas de reconhecimento de símbolos on-line . Experimentos foram realizados para avaliar a acurácia dos classificadores , o desempenho dos classificadores para número crescente de classes , tempos de treinamento e teste , e uso de diferentes sub-conjuntos de características . Este trabalho inclui uma descrição dos fundamentos utilizados , detalhes do pré-processamento e extração de características para representação dos símbolos , e uma exposição e discussão_sobre  o estudo empírico realizado . Os dados adicionais que foram coletados para os experimentos serão publicamente disponibilizados 
 Neste trabalho abordamos o problema de programação de tarefas em um ambiente flow shop permutacional com mais de duas máquinas . Restringimos o estudo para o caso em que todas as tarefas têm uma data de entrega comum e restritiva , e onde o objetivo é minimizar a soma total dos adiantamentos e atrasos das tarefas em relação a tal data de entrega . É assumido também um ambiente estático e determinístico . Havendo soluções com o mesmo custo , preferimos aquelas que envolvem menos tempo de espera no buffer entre cada máquina . Devido à dificuldade de resolver o problema , mesmo para instâncias pequenas ( o problema pertence à classe NP-difícil ) , apresentamos uma abordagem heurística para lidar com ele , a qual está baseada em busca local e faz_uso  de um algoritmo linear para atribuir datas de conclusão às tarefas na última máquina . Este algoritmo baseia-se em algumas propriedades analíticas inerentes às soluções_ótimas  . Além_disso  , foi desenvolvida uma formulação_matemática  do problema em programação linear_inteira  mista ( PLIM ) que vai permitir validar a eficácia da abordagem . Examinamos também o desempenho das heurísticas com testes padrões ( benchmarks ) e comparamos nossos resultados com outros obtidos na literatura 
 Árvore de componentes é uma representação completa de imagens que utiliza componentes conexos dos conjuntos de níveis de uma imagem e a relação de inclusão entre esses componentes . Essas informações possibilitam diversas aplicações em processamento de imagens e visão_computacional  , e.g . filtros conexos , segmentação , extração de características entre outras . Aplicações que utilizam árvore de componentes geralmente computam atributos que descrevem os componentes conexos representados pelos nós da árvore . Entre esses atributos estão a área , o perímetro e o número de Euler , que podem ser utilizados diretamente ou indiretamente ( para o cálculo de outros atributos ) . Os `` bit-quads '' são padrões de tamanho 2x2 binários que são agrupados em determinados conjuntos e contados em imagens binárias . Embora o uso de `` bit-quads '' resulte em um método rápido para calcular atributos em imagens binárias , o mesmo não ocorre para o cálculo de atributos dos nós de uma árvore de componentes , porque os padrões contados em um nó podem se repetir nos conjuntos de níveis da imagem e serem contados mais de uma vez . A literatura recente propõe uma adaptação dos bit-quads para o cálculo incremental e eficiente do número de buracos na árvore de componentes . Essa adaptação utiliza o fato de cada nó da árvore de componentes representar um único componente conexo e uma das definições do número de Euler para o cálculo do número de buracos . Embora essa adaptação possa calcular o número de Euler , os outros atributos ( área e perímetro ) não podem ser computados . Neste trabalho é apresentada uma extensão dessa adaptação de bit-quads que permite a contagem de todos os agrupamentos de bit-quads de maneira incremental e eficiente na árvore de componentes . De forma que o método proposto possa calcular todos os atributos que podem ser obtidos pelos bit-quads ( além do número de buracos ) em imagens binárias na árvore de componentes de maneira incremental 
 Esta Tese apresenta um framework para análise exploratória de dados via técnicas de agrupamento . O objetivo é facilitar o trabalho dos especialistas no domínio dos dados . O ponto central do framework é um algoritmo de ensemble multi-objetivo , o algoritmo MOCLE , complementado por um método para a visualização integrada de um conjunto de partições . Pela aplicação conjunta das idéias de ensemble de agrupamentos e agrupamento multi-objetivo , o MOCLE efetua atomaticamente importantes passos da análise de agrupamento : executa vários algoritmos conceitualmente diferentes com várias configurações de parâmetros , combina as partições resultantes desses algoritmos e seleciona as partições com os melhores compromissos de diferentes medidas de validação . MOCLE é uma abordagem robusta para lidar com diferentes_tipos  de estrutura que podem_estar  presentes em um conjunto de dados . Ele resulta em um conjunto conciso e estável de estruturas alternativas de alta qualidade , sem a necessidade de conhecimento prévio sobre os dados e nem conhecimento profundo em análise de agrupamento . Além_disso  , para facilitar a descoberta de estruturas mais complexas , o MOCLE permite a integração automática de conhecimento prévio de uma estrutura simples por meio das suas funções objetivo . Finalmente , o método de visualização proposto permite a observação simultânea de um conjunto de partições . Isso ajuda na análise dos resultados do MOCLE 
 Diversas políticas de escalonamento para aplicações paralelas voltadas a ambientes computacionais_distribuídos  têm sido propostas . Embora tais políticas apresentem bons_resultados  , elas são , geralmente , avaliadas em cenários específicos . Quando o cenário muda , com diferentes ambientes distribuídos e condições de carga , essas políticas podem ter seu desempenho deteriorado . Nesse contexto , este trabalho apresenta um estudo comparativo envolvendo dez políticas de escalonamento avaliadas em diferentes cenários . Cada uma das políticas foi submetida a uma combinação de quatro cargas de trabalho de ocupação da UCP e três variações da taxa de comunicação média entre os processos , utilizando a rede . Foram considerados ainda três sistemas distribuídos distintos : dois clusters , com diferentes quantidades de nós , e um grid computacional . Foi utilizada a simulação com ambientes próximos ao real e cargas de trabalho obtidas de modelos realísticos . Os resultados demonstraram que , embora as políticas sejam voltadas a ambientes computacionais paralelos e distribuídos , quando o cenário muda , o desempenho cai e a ordem de classificação entre as políticas se altera . Os resultados permitiram ainda demonstrar a necessidade de se considerar a comunicação entre os processos durante o escalonamento em grids computacionais 
 As técnicas de projeção_multidimensional  tornaram-se uma ferramenta de análise importante . Elas buscam mapear dados de um espaço multidimensional para um espaço_visual  , de menor dimensão , preservando as estruturas de distância ou de vizinhança no mapa visual produzido . Apesar dos recentes avanços , as técnicas existentes ainda apresentam deficiências que prejudicam a sua utilização como ferramentas exploratórias em certos domínios . Um exemplo está nos cenários streaming , nos quais os dados são produzidos e/ou coletados de forma contínua . Como a maioria das técnicas de projeção necessitam percorrer os dados mais de uma vez para produzir um layout final , e fluxos normalmente não podem ser carregados por completo em memória principal , a aplicação direta ou mesmo a adaptação das técnicas existentes em tais cenários é inviável . Nessa tese de doutorado é apresentado um novo modelo de projeção , chamado de Xtreaming , no qual as instâncias de dados são visitadas apenas uma vez durante o processo de projeção . Esse modelo é capaz de se adaptar a mudanças nos dados conforme eles são recebidos , atualizando o mapa visual para refletir as novas estruturas que surgem ao longo do tempo . Os resultados dos testes mostram que o Xtreaming é muito competitivo em termos de preservação de distâncias e tempo de execução se comparado com técnicas do estado-da-arte . Também é apresentada uma nova_técnica  de projeção_multidimensional  , chamada de User-assisted Projection Technique for Distance Information ( UPDis ) , que foi projetada para permitir a intervenção do usuário exigindo apenas informações de distância entre as instâncias , e que é utilizada como parte do Xtreaming . Os resultados também mostram que a UPDis é tão rápida , precisa e flexível quanto as técnicas do estado-da-arte 
 Este trabalho detalha a motivação para a avaliação de desempenho não estacionária de sistemas computacionais e introduz uma abordagem para a modelagem dinâmica de desempenho baseada na análise de resposta em frequência . A modelagem dinâmica é uma abordagem essencial que se desenvolveu como recurso importante na engenharia e algumas ciências naturais por muitas décadas e conta com uma coleção de ferramentas matemáticas para descrever o comportamento dinâmico de sistemas . Seja por tradição ou pela dificuldade de aplicação o uso de modelos dinâmicos em avaliação de desempenho de sistemas computacionais é recente e consideravelmente menos explorada que em outros domínios . A contribuição proposta por esta pesquisa , é a formulação de um arcabouço para avaliação de desempenho não estacionário de sistemas computacionais . O propósito desse arcabouço é produzir um modelo analítico dinâmico experimentalmente construído , que represente a dinâmica que governa o desempenho do sistema . A abordagem é composta por : modelo_conceitual  para formulação de métricas de desempenho transientes ; um método empírico para a obtenção do modelo dinâmico ; uma metodologia de análise baseada em resposta de frequência . Usos práticos são ilustrados através de estudos de caso em que os resultados demonstram a aplicabilidade da abordagem 
 Nesta dissertação , propomos um algoritmo dual para resolver o problema de roteamento de dados . Este problema pode ser formulado como um problema de multifluxo a critério convexo . Nosso algoritmo explora a decomposição por arcos do lagrangeano . No primeiro capitulo relembramos alguns conceitos , definições e propriedades da teoria dos grafos . No segundo capitulo apresentamos dois métodos para a resolução de problemas de fluxo de custo mínimo com função linear . No terceiro capitulo apresentamos o método do particionamento e o método da decomposição pela atribuição do recurso , que são utilizados para resolver problemas de multifluxo a critério linear . O quarto capitulo é dedicado a apresentação do método Dual para a resolução do problema de roteamento de dados , bem como a análise e apresentação dos resultados computacionais . Na última parte apresentamos a conclusão e perspectivas de trabalhos futuros 
 Esta dissertação apresenta o desenvolvimento de um método para o particionamento de conjuntos através de coloração em grafos . Uma aplicação deste método para a determinação de um horário escolar é igualmente apresentada . O programa computacional desenvolvido foi escrito em linguagem Pascal e testado utilizando-se um microcomputador IBM-PC compatível . Os resultados obtidos são de boa_qualidade  apesar do caráter não polinomial do problema estudado 
 Nesta dissertação é apresentado um algoritmo de coloração em grafos para o projeto de células de manufatura em Tecnologia de Grupo . O algoritmo proposto computa as dissimilaridades entre as peças e estabelece uma partição de peças e máquinas por meio da construção e coloração de uma árvore de peso máximo em um número de cores igual ao número requerido de células . Este algoritmo permite a resolução eficiente de exemplos da literatura , apesar do caráter não polinomial do problema estudado . O programa correspondente foi escrito em Turbo-Pascal e está implantado em um microcomputador 
 Nesta dissertação é apresentado um método composto de duas fases para efetuar a seleção de máquinas e o carregamento dos produtos sobre as máquinas selecionadas . Na primeira fase é identificado um conjunto de soluções factíveis de um problema da mochila através de programação_dinâmica  ou de um procedimento heurístico . Na segunda fase , resolve-se um problema de atribuição usando o algoritmo de enumeração implícita ou o algoritmo guloso . O método proposto foi implementado em linguagem Turbo-Pascal 7.0 
 A identificação de plantas invasoras é de extrema importância em diversos procedimentos utilizados na agricultura . Apesar de ser uma tarefa computacionalmente difícil , esta identificação tem se tornado muito importante no contexto da agricultura de precisão 
 A agricultura de precisão substitui os tratos culturais de grandes áreas da cultura , feitos pela média do nível dos problemas encontrados nessas áreas , por tratamento específicos e pontuais . As pricipais vantagens são o aumento de produtividade , relacionado com a diminuição da variabilidade na produção , a economia de insumos e a preservação do meio ambiente 
 Este trabalho enfoca o reconhecimento de plantas invasoras em tempo real . Para manter o requisito de tempo real , são utilizadas redes neurais_artificiais  como meio para o reconhecimento de padrões 
 Entre as diversas plantas invasoras de ocorrência freqüente no cerrado brasileiro , foi selecionado o picão preto para a avaliação das técnicas adotadas . Uma arquitetura modular de reconhecimento é proposta , com o uso de processamento paralelo , facilitando a inclusão de módulos de reconhecimento de outras plantas invasoras sem a deterioração do desempenho do sistema 
 Os resultados obtidos são amplamente satisfatórios , demonstrando a possibilidade do desenvolvimento de um sistema embarcado completo de identificação de plantas invasoras em tempo real . Este sistema , apoiado pelo sistema de posicionamento global GPS , pode servir de base para uma série de máquinas agrícolas inteligentes , como pulverizadores de herbicidas e outros defensivos utilizados na agricultura 
 Planejamento em inteligência_artificial  é a tarefa de determinar ações que satisfaçam um dado objetivo . Nos problemas de planejamento sob incerteza , as ações podem ter efeitos probabilísticos . Esses problemas são modelados como Processos de Decisão Markovianos ( Markov Decision Processes - MDPs ) , modelos que permitem o cálculo de soluções_ótimas  considerando o valor esperado de cada ação em cada estado . Contudo , resolver problemas grandes de planejamento probabilístico , i.e. , com um grande número de estados e ações , é um enorme desafio . MDPs grandes podem ser reduzidos através da computação de bissimulações estocásticas , i.e. , relações de equivalência sobre o conjunto de estados do MDP original . A partir das bissimulações estocásticas , que podem ser exatas ou aproximadas , é possível_obter  um modelo abstrato reduzido que pode ser mais fácil de resolver do que o MDP original . No entanto , para problemas de alguns domínios , a computação da bissimulação estocástica sobre todo o espaço de estados é inviável . Os algoritmos propostos neste trabalho estendem os algoritmos usados para a computação de bissimulações estocásticas para MDPs de forma que elas sejam computadas sobre o conjunto de estados alcançáveis a partir de um dado estado inicial , que pode ser muito menor do que o conjunto de estados completo . Os resultados experimentais_mostram  que é possível resolver problemas grandes de planejamento probabilístico com desempenho_superior  às técnicas conhecidas de bissimulação estocástica 
 Atualmente , muitas redes com características dinâmicas estão em funcionamento ( por exemplo MANETs , DTNs , redes oportunistas , etc ) . Neste trabalho , estudamos um modelo para estas redes chamado de Grafos Evolutivos , que permite expressar a dinamicidade das conexões entre nós por meio de uma simples extensão da estrutura comum de grafos . Esta modelagem é utilizada no arcabouço proposto por Casteigts et_al  . para definir algoritmos distribuídos em redes dinâmicas , que utiliza grafos evolutivos para representar a topologia da rede e renomeação de rótulos para expressar a comunicação entre os nós . Utilizamos esta abordagem para estudar o problema da exclusão mútua distribuída em redes dinâmicas e diversos algoritmos propostos para ele , a fim de definir e validar suas condições necessárias e suficientes de conectividade em redes dinâmicas . Além da formalização de algoritmos , o modelo de grafos evolutivos também pode ser utilizado para analisar redes dinâmicas . Rastros de redes dinâmicas reais são amplamente_utilizados  na literatura para estudos de algoritmos pois estes geram resultados mais realísticos do que redes simuladas com padrões de movimento . A partir dos detalhes de cada conexão entre nós de um destes rastros , é possível construir um grafo evolutivo , do qual se pode extrair dados como jornadas ótimas entre nós , variação da conectividade no tempo , estabilidade , e periodicidade . Com as informações mencionadas , um pesquisador pode observar com maior precisão as características do rastro , o que facilita na escolha da rede mais apropriada para sua necessidade . Além_disso  , o conhecimento prévio de tais características de uma rede auxilia no estudo do comportamento de algoritmos executados sobre ela e provém uma validação para suposições geralmente feitas pelos pesquisadores . Para fornecer estas informações , desenvolvemos uma ferramenta Web que analisa rastros de redes dinâmicas e agrega os dados em um formato de fácil visualização . Descrevemos , neste trabalho , a implementação e a utilidade de todos os serviços da ferramenta 
 Este trabalho aborda o problema multiestágio de planejamento e programação da produção em indústrias cervejeiras . O processo de fabricação de cerveja pode ser dividido em duas etapas principais : preparação do líquido e envase . A primeira etapa ocorre , na maior_parte  do tempo , dentro de tanques de fermentação e maturação . A segunda ocorre nas linhas de envase , podendo ter início assim que o líquido estiver pronto nos tanques . O tempo de preparação do líquido demora vários dias , enquanto que na maioria das indústrias de bebidas carbonatadas este tempo é de no máximo algumas horas . O objetivo deste estudo é obter planos de produção viáveis que visam otimizar as decisões de programação envolvidas nestes processos . Visitas a cervejarias no Brasil e em Portugal foram realizadas para uma maior familiaridade do processo de produção e dados foram coletados . Modelos de programação inteira_mista  para representar o problema foram desenvolvidos , baseados em abordagens CSLP ( The Continuous Setup Lot-Sizing Problem ) , GLSP ( General Lot Sizing and Scheduling Problem ) , SPL ( Simple Plant Location Problem ) e ATSP ( Asymmetric Travelling Salesman Problem ) . Os resultados mostram que os modelos são coerentes e representam adequadamente o problema , entretanto , mostram-se difíceis de serem resolvidos na otimalidade . Esta dificuldade de resolução dos modelos motivou o desenvolvimento de procedimentos MIP-heurísticos , como também de uma metaheurística GRASP ( Greedy Randomized Adaptive Search Procedure ) . As soluções obtidas pelos procedimentos heurísticos são de boa_qualidade  , quando comparadas ao melhor limitante inferior encontrado por meio da resolução dos modelos matemáticos . Os testes computacionais foram realizados utilizando instâncias geradas com base em dados reais 
 O planejamento da operação de sistemas hidrotérmicos mostra-se de grande interesse no Brasil , onde cerca de 90 % da energia elétrica é de geração hidráulica . Sendo_assim  , torna-se importante operar da melhor forma possível para minimizar o custo térmico e evitar vertimentos indesejados e outros_tipos  de perdas de energia hidroelétrica . Esta dissertação considera o problema de planejamento a longo prazo da operação de um sistema hidrotermico e sua formulação . Para se obter a solução de tal problema apresenta-se o Modelo Equivalente Certo ( EC ) , que utiliza uma técnica de otimização determinística acoplada a um modelo de previsão de vazões com atualização em tempo real ( base mensal ) . São realizadas comparações entre as soluções obtidas por esta abordagem e por Programação Dinâmica Estocástica ( PDE ) , considerando-se várias usinas do Sistema Sudeste Brasileiro , individualmente , e simulando-se a operação destas usinas para períodos de 2 anos do histórico de vazões . Os resultados obtidos_mostram  que a abordagem proposta consegue tratar as incertezas das vazões , no horizonte de planejamento de longo prazo , com a mesma potencialidade da PDE . Além_disso  , o Modelo Equivalente Certo não exige simplificações , tais como o uso do reservatório equivalente , para tratar sistemas com múltiplas usinas 
 Este trabalho propõe um processo de desenvolvimento de software orientado a objetos que utiliza a notação da Unified Modeling Language ( UML ) . Para cada fase do ciclo de vida de um software são apresentados os diagramas que auxiliam na organização das informações referentes ao software a ser desenvolvido e as diretrizes de uso desses diagramas . O processo utiliza conceitos e princípios utilizados em alguns métodos orientado a objetos , como por exemplo , o Fusion , o Objectory e o TeamFusion . O processo introduz uma abordagem de teste que fornece diretrizes para a geração de modelos de teste ao longo do desenvolvimento do software . Para cada fase são identificadas técnicas e critérios de teste passíveis de aplicação , sendo que para cada fase indica-se um critério de teste visando a cobertura dos requisitos mínimos de teste para garantir que o produto desenvolvido seja confiável . Para ilustrar o emprego do processo foi realizado um estudo de caso com o Sistema de Apoio à Escrita - SAPES , para gerenciamento das informações relativas às publicações de um pesquisador . A partir de um modelo de requisitos é feita toda a modelagem do SAPES , desde a fase de engenharia de requisitos até a fase de implementação . Somente parte do sistema foi implementada . A abordagem de teste foi aplicada para produzir os modelos de teste que foram utilizados para avaliar a confiabilidade do software obtido com a utilização do processo 
 Uma grande revolução tecnológica ocorreu nos últimos_anos  em diversas_áreas  relacionadas a ciência da computação . Um dos aspectos que mais influenciou esta revolução foi o armazenamento , o processamento e a análise de grandes quantidades de dados geradas por várias empresas e centros de pesquisas . Com isso , a incorporação de métodos e técnicas estatísticas para a aquisição de conhecimento de dados na área de Aprendizado de Máquina tem apresentado um grande crescimento . O propósito desse trabalho é investigar alguns algoritmos de Aprendizado de Máquina pertencente ao paradigma estatístico para a aquisição de conhecimento a partir de conjuntos de dados . Nessa investigação foram estudados os algoritmos estatísticos Naive Bayes , Auto Class , Auto Class Pro e K-Means . Dois estudos de casos ( um conjunto de plantas iris e um conjunto de domicílios de clientes ) foram realizados verificando , entre outros , o comportamento desses algoritmos , a relevância dos atributos dos conjuntos de dados e apresentando os clusters encontrados nas ferramentas de visualização 
 Inferência Bayesiana para pesquisa de mercado incluindo erros de resposta é estudado como uma mistura de duas distribuições de Bernoulli . Como a análise_Bayesiana  geralmente implica em cálculos complexos , o método de Monte Cano com dados ampliados é desenvolvido para obter os resumos marginais a posteriori . Variáveis latentes foram introduzidas para indicar qual componente da mistura gerou a informação com erro de classificação . Também , um procedimento Bayesiano baseado no conceito de `` p-value '' e na distância de variação total foi introduzida para medir o efeito do erro na distribuição marginal a posteriori . É também realizado , uma comparação entre o modelo misto proposto e o modelo exato introduzido por Gaba e VVinkler com o objetivo de verificar a eficiência da técnica aplicada . Uma ilustração com dados simulados é 
 Apresentamos neste trabalho o uso de meta-análise sob um enfoque Bayesiano . Metaanálise se refere a métodos quantitativos que combinam resultados de estudos independentes para tirar conclusões gerais . Freqüentemente resultados de diferentes estudos são combinados de forma inadequada , resultando em uma análise inferencial não confiável . Métodos Bayesianos para meta-análise são preferíveis em vista do pequeno número de estudos que prevalecem em meta-análise . Para exemplificar o uso desta técnica , utilizamos alguns conjuntos de dados disponíveis na literatura e um conjunto de dados simulado . A análise_Bayesiana  foi possível devido a utilização de métodos de simulação de Monte_Carlo  via Cadeias de Markov 
 Neste trabalho estudamos esquemas numéricos para resolver as formulações de valor de fronteira e de valor inicial para uma frente em movimento . Nosso objetivo é motivar e apresentar esquemas baseados nas relações existentes entre frentes em propagação , equações de Hamilton-Jacobi e leis de conservação hiperbólicas . Quando uma frente inicial evolui no tempo através de uma das formulações hiperbólicas , podem surgir singularidades , cúspides e mudanças em sua topologia e assim faz-se necessário a compreensão das técnicas de discretização de leis de conservação hiperbólicas para a obtenção de esquemas numéricos capazes de tratar e descrever corretamente esses problemas na geometria da frente . A solução_numérica  das leis de conservação inclui o desenvolvimento de esquemas numéricos capazes de resolver choques , descontinuidades e escolher a solução entrópica entre as muitas soluções fracas existentes . Para isso , analisamos esquemas na forma conservativa com propriedades especiais , tais como , esquemas Upwind , Monótonos , TVD , Entropia , Limitante de fluxo e Limitante de inclinação . Esses esquemas são acompanhados com uma coleção de implementações . Essa teoria pode ser empregada para o rastreainento da interface de escoamentos multifsicos , e uma aplicação futura que estamos interessados é a determinação da fronteira de um domínio a partir de seus pontos_interiores  para aplicação no simulador de escoamentos multifásicos na área de mecânica de fluidos 
 Esta dissertação aborda a solução de modelos de rede de filas para sistemas computacionais_distribuídos  , através de métodos analíticos e por simulação . Dessa forma , são discutidos detalhadamente os seguintes métodos analíticos : Análise de Valor Médio ( AVM ) , Rede de Jackson , Método de Gordon e Newell , Redes BCMP , Decomposição Hierárquica , Limites de Desempenho e Processo Nascimento-e-Morte . Esses métodos são aplicados em diversos modelos que representam elementos fundamentais de um sistema computacional distribuído . Os modelos considerados abrangem elementos de um sistema computacional distribuído , incluindo servidores de arquivos , rede de comunicação e estações de trabalho . Além dos métodos analíticos , considera-se também a simulação , implementados no Ambiente ASiA , que gera programas de simulação orientados a evento . Os resultados obtidos tanto analiticamente , como por simulação , são apresentados , discutidos e comparados , constatando-se uma equivalência . Esses resultados mostram que os diversos métodos analíticos estudados podem ser empregados com êxito na solução de modelos práticos da área de sistemas computacionais_distribuídos  
 Este trabalho apresenta uma avaliação de desempenhq das plataformas de portabilidade PVM e MPI quando exeCutadas em um sistema computacional distribuído e na máquina_paralela  D3M SP2 . Essa análise é feita através do desenvolvimento de vários algoritmos paralelos de ordenação . São utilizadas quatro implementações do MPI e do PVM : D3M MPI , D3M PVMe ( ambas executadas no SP2 ) ; MPICH e PVM ( ambas executadas no sistema computacional distribuído ) . Os algoritmos paralelos foram desenvolvidos e os resultados obtidos são usados para estabelecer uma comparação entre os diferentes ambientes e entre os diversos algoritmos de ordenação implementados . Os algoritmos seqüenciais foram também avaliados , permitindo analisar o `` speedup '' para a execução das aplicações paralelas implementadas em cada plataforma . Os resultados globais obtidos tornam possível verificar-se e demonstrar-se ( para o ambiente particular considerado ) que o PVM apresenta melhor desempenho em ambientes computacionais_distribuídos  e o MPI se comporta mais adequadamente em arquiteturas paralelas 
 A aplicação de defensivos químicos em áreas agrícolas é de primordial importância para o rendimento de lavouras . O uso de aeronaves é cada vez mais comum em tal tarefa , principalmente pelo fato de aumentar a agilidade na operação . Entretanto , características climáticas , como intensidade e orientação do vento , podem causar prejuízo aos produtores ( e.g . aplicação sobreposta dos defensivos e multas por aplicação de defensivos na borda exterior da lavoura ) . Essa lacuna motivou este trabalho de mestrado , que tem por objetivo avaliar um sistema integrado de veículos aéreos não tripulados ( VANTs ) e redes de sensores sem fio ( RSSFs ) para aumentar a eficiência da aplicação de defensivos químicos em campos agrícolas . Um VANT é utilizado para percorrer o campo enquanto aplica defensivos químicos . Uma RSSF com nós capazes de sensoriar a concentração de defensivos químicos é instalada no solo do campo . Os nós da RSSF e o VANT são equipados com módulos de rádio , possibilitando a troca de informação entre eles . Desse modo , o VANT consegue obter a distribuição de defensivos químicos aplicados no campo , podendo então tomar decisões para aumentar a eficiência da aplicação . Para análise desse sistema , dois experimentos foram realizados utilizando o simulador OMNeT++ e o framework MiXiM . Em um primeiro experimento é comparada a eficiência do sistema proposto com o sistema tradicional de pulverização agrícola , mostrando que é possível aumentar , em média , 16 % a quantidade de defensivos químicos aplicados dentro do campo agrícola . E , em alguns_casos  , é possível_obter  um aumento de até 118,25 % . No segundo experimento são analisados cinco diferentes protocolos de comunicação , mostrando que é possível otimizar o uso da bateria nos nós sensores sem fio em até 10 vezes , sem diminuir a eficiência do 
 Este trabalho considera o problema de dimensionamento de lotes em um sistema de produção multiestágio , onde cada estágio é composto por máquinas_paralelas  com capacidades limitadas . O problema consiste em determinar um plano de produção que atenda a demanda dos itens finais e de seus componentes em cada_período  de um horizonte finito de planejamento . Um tempo de preparação é considerado para começar a produção em qualquer máquina e período . O objetivo é determinar um plano de produção que minimize os custos de produção , preparação e de estoque . O problema é formulado como um programa inteiro misto e um método heurístico básico é proposto . A partir deste método básico , algumas heurísticas variantes foram desenvolvidas sendo que , algumas incorporam estratégias de Busca Tabu . A análise computacional foi feita com milhares de exemplos gerados aleatoriamente . Para os exemplos de dimensões pequenas , as soluções heurísticas foram comparadas com as soluções_ótimas  obtidas pelo pacote CPLEX 4.0 . Para os exemplos maiores , os resultados obtidos foram analisados considerando o limitante inferior obtido através da técnica da Relaxação Lagrangiana e do método do subgradiente . Uma proposta de resolução para um problema de dimensionamento e sequenciamento de lotes também é apresentado nesta tese . Este problema é uma extensão do problema de dimensionamento de lotes , pois integra o dimensionamento de lotes e o sequenciamento de itens nas máquinas e períodos . O objetivo consiste em determinar o quanto produzir dos itens , em cada_período  e máquina e , em que ordem estes itens devem ser produzidos . O procedimento de resolução é baseado no método básico proposto para o problema de dimensionamento de lotes . As seqüências de produção em cada máquina e período podem ser interpretadas como rotas do caixeiro viajante . No apêndice , uma notação matricial é introduzida , permitindo mostrar facilmente a equivalência entre formulações alternativas para o problema de dimensionamento de lotes multiestágio , bem como entre abordagens de solução 
 O problema da árvore geradora de comunicação ótima recebe um grafo com comprimentos não negativos nas arestas e um requerimento não negativo entre cada par de vértices ; sendo o objetivo encontrar uma árvore geradora do grafo que minimize o custo de comunicação , que é a soma sobre cada par de vértice da distância entre eles na árvore vezes o requerimento entre eles . Este problema é NP-difícil , assim como vários casos particulares dele . Neste trabalho estudamos algumas variantes deste problema , introduzimos novos casos particulares que são também NP-difíceis e propomos esquemas de aproximação polinomial para alguns deles 
 O problema de corte consiste no corte de objetos maiores para produção de peças menores , de modo que uma certa função objetivo seja otimizada , por exemplo , a perda seja minimizada . O problema de empacotamento pode também ser visto como um problema de corte , onde as peças menores são arranjadas dentro dos objetos . Uma abordagem em grafo E/OU para a resolução de problemas de corte e empacotamento foi proposta inicialmente por Morabito ( 1989 ) para problemas de corte bidimensionais e , mais tarde , estendida para problemas tridimensionais ( Morabito , 1992 ) . Nesta abordagem foi utilizada uma técnica de busca híbrida , onde se combinou a busca em profundidade primeiro com limite de profundidade e a busca hill-climbing , utilizando-se heurísticas baseadas nos limitantes superiores e inferiores . Experiências computacionais mostraram a viabilidade de uso na prática desta abordagem . Mais tarde , Arenales ( 1993 ) generalizou esta a abordagem em grafo E/OU mostrando como diferentes problemas de corte poderiam ser resolvidos , independentemente da dimensão , formas dos objetos e itens , baseado em simples hipóteses , sem realizar , entretanto , estudos computacionais . O presente_trabalho  tem por objetivo estender a abordagem em grafo E/OU para tratar outros casos não analisados pelos trabalhos anteriores , tais como situações envolvendo diferentes processos de corte , bem como a implementação computacional de métodos baseados na abordagem em grafo E/OU , mostrando , assim , a versatilidade da abordagem para tratar diversas situações práticas de problemas de corte e sua viabilidade computacional 
 Padrões , linguagens de padrões e frameworks são formas de reuso de software . A complexidade de frameworks , causada pela dificuldade tanto em construí-los quanto em utilizá-los , é um dos inibidores do uso dessa tecnologia . Um processo para facilitar a construção e instanciação de frameworks é apresentado . Uma linguagem de padrões é utilizada para apoiar todo o processo , desde a identificação da funcionalidade do framework , seu projeto , implementação , validação , até sua instanciação para sistemas específicos do domínio . O framework obtido é do tipo caixa-branca e sua instanciação é feita especializando-se suas classes para sistemas específicos . O processo inclui , também , a construção de uma ferramenta para automatizar a instanciação do framework , por meio da qual é possível_obter  um sistema específico fornecendo apenas informações sobre os padrões da linguagem utilizados na sua modelagem . O processo é ilustrado com a Linguagem de Padrões para Gestão de Recursos de Negócios ( GRN ) , que serviu de base para construção do framework GREN . Apresenta-se também o GREN-Wizard , uma ferramenta para instanciação automática do GREN para sistemas no domínio da GRN . A avaliação do processo é feita por meio de alguns experimentos e vários relatos de uso dos diversos sub-processos que compõem o processo geral , usando o GREN , a GRN e o GREN-Wizard 
 Na visão_computacional  , a detecção de objetos é uma tarefa que tem signifgificativa importância . Podemos verificar isto através da existência de inúmeros métodos propostos na literatura . Cada um destes métodos se apóia em algumas características presentes na imagem para alcançar um desempenho eficiente . Considerando ambientes que utilizam cores para determinação de objetos presentes em uma imagem , é possível utilizá-las como características que permitam detectar os objetos . Neste trabalho , são investigados dois classificadores de cores . O primeiro é baseado em limiarização no espaço HSV e o segundo é constituído de um mapa auto-organizável para classificação dos pixels no espaço RGB . Objetivando a construção de um sistema classificador de cores eficiiente , capaz de processar vídeo em tempo real , é proposta uma técnica que se baseia no conceito de quantização . Outro aspecto investigado foi a detecção de movimento para evitar o processamento de pontos indesejados . O desempenho do sistema de classificação de cores é avaliado em um ambiente de futebol de robôs da categoria Mirosot , que é um ambiente dinâmico e que exige que todo o processamento da imagem seja rápido de modo a detectar corretamente todos objetos presentes em cada quadro . Os resultados mostram que o classificador de cores é capaz de detectar todos objetos no ambiente de futebol de robôs , sendo cada quadro processado em menos de 30 milisegundos , tornando o sistema desenvolvido muito adequado ao processamento de 
 A reconstrução de superfícies a partir de nuvens de pontos faz parte de um novo paradigma de modelagem em que modelos computacionais para objetos reais são reconstruídos a partir de dados amostrados sobre a superfície dos mesmos . O principal problema que surge nesse contexto é o fato de que não são conhecidas relações de conectividade entre os pontos que compõe a amostra . Os objetivos do presente_trabalho  são estudar métodos implícitos para a reconstrução de superfícies e propor algumas melhorias pouco exploradas por métodos já existentes . O uso de funções implícitas no contexto da reconstrução conduz a métodos mais robustos em relação a ruídos , no entanto , uma das principais desvantagens de tais métodos está na dificuldade de capturar detalhes finos e sharp features . Nesse sentido , o presente_trabalho  propõe o uso de abordagens adaptativas , tanto na poligonalização de superfícies quanto na aproximação de superfícies . Além_disso  , questões relativas à robustez das soluções locais e à qualidade da malha também são abordadas . Por fim , o método desenvolvido é acoplado aumsoftware traçador de raios afimde se obterumamaneira de modelar cenas tridimensionais utilizando nuvens de pontos , além dos objetos gráficos tradicionais . Os resultados apresentados mostram que muitas das soluções propostas oferecem um incremento à qualidade dos métodos de reconstrução anteriormente 
 A taxonomia vegetal atualmente exige um grande esforço dos botânicos , desde o processo de aquisição do espécime até a morosa comparação com as amostras já catalogadas em um herbário . Nesse contexto , o projeto TreeVis surge como uma ferramenta para a identificação de vegetais por meio da análise de atributos foliares . Este trabalho é uma ramificação do projeto TreeVis e tem o objetivo de identificar vegetais por meio da análise do corte transversal de uma folha ampliado por um microscópio . Para tanto , foram extraídas assinaturas da cutícula , epiderme superior , parênquima paliçádico e parênquima lacunoso . Cada assinatura foi avaliada isoladamente por uma rede_neural  pelo método leave-one-out para verificar a sua capacidade de discriminar as amostras . Uma vez selecionados os vetores de características mais importantes , os mesmos foram combinados de duas maneiras . A primeira abordagem foi a simples concatenação dos vetores selecionados ; a segunda , mais elaborada , reduziu a dimensionalidade ( três atributos apenas ) de algumas das assinaturas componentes antes de fazer a concatenação . Os vetores finais obtidos pelas duas_abordagens  foram testados com rede_neural  via leave-one-out para medir a taxa de acertos alcançada pelo sinergismo das assinaturas das diferentes partes da folha . Os experimentos consitiram na identificação de oito espécies diferentes e na identificação da espécie Gochnatia polymorpha nos ambientes Cerrado e Mata Ciliar , nas estações Chuvosa e Seca , e sob condições de Sol e 
 Este trabalho propõe um Ambiente de Programação de Robôs Móveis direcionado especialmente à língua_portuguesa  , que consiste em um novo ambiente de programação gráfica e textual , capaz de atender a perfis diferenciados de usuários . O ambiente de programação contém três_módulos  para programação de robôs_móveis  : o modulo C ou editor C , o módulo Assembly ou editor Assembly , e o módulo blocos que é um módulo que contêm blocos de programação para auxiliar os programadores inexperientes a programarem robôs_móveis  por meio de uma meta linguagem desenvolvida que encapsula a linguagem C. O diferencial deste trabalho consiste na Gramática adaptativa criada para robôs_móveis  , que é uma derivação das gramáticas descendentes recursivas com um mecanismo de busca ( `` look ahead '' ) . A derivação encontra-se na definição formal de gramática que foi adaptada nessa proposta para permitir a categorização de terminais da gramática , adicionando um passo a mais na compilação , que é a checagem de categorias permitidas da linguagem . Nesta dissertação são relacionados alguns ambientes de programação de robôs disponíveis na literatura bem como uma discussão de suas características . Também são apresentados experimentos_realizados  com usuários não especializados em programação , principalmente crianças , e com o Laboratório de Robótica a Distância da Universidade de São Paulo em São Carlos , São 
 Dentre as diversas tarefas em que os algoritmos_evolutivos  têm sido empregados , a indução de regras e de árvores de decisão tem se mostrado uma abordagem bastante atrativa em diversos_domínios  de aplicação . Algoritmos de indução de árvores de decisão representam uma das técnicas mais populares em problemas de classificação . Entretanto , os algoritmos tradicionais de indução apresentam algumas limitações , pois , geralmente , usam uma estratégia gulosa , top down e com particionamento recursivo para a construção das árvores . Esses fatores degradam a qualidade dos dados , os quais podem gerar regras estatisticamente não significativas . Este trabalho propõe o algoritmo LEGAL-Tree , uma nova abordagem baseada em algoritmos genéticos para indução de árvores de decisão . O algoritmo proposto visa evitar a estratégia gulosa e a convergência para ótimos locais . Para isso , esse algoritmo adota uma abordagem multi-objetiva lexicográfica . Nos experimentos_realizados  sobre bases de dados de diversos problemas de classificação , a função de fitness de LEGAL-Tree considera as duas medidas mais comuns para avaliação das árvores de decisão : acurácia e tamanho da árvore . Os resultados obtidos mostraram que LEGAL-Tree teve um desempenho equivalente ao algoritmo SimpleCart ( implementação em Java do algoritmo CART ) e superou o tradicional algoritmo J48 ( implementação em Java do algoritmo C4.5 ) , além de ter superado também o algoritmo_evolutivo  GALE . A principal_contribuição  de LEGAL-Tree não foi gerar árvores com maior acurácia preditiva , mas sim gerar árvores menores e , portanto , mais compreensíveis ao usuário do que as outras abordagens , mantendo a acurácia preditiva equivalente . Isso mostra que LEGAL-Tree obteve sucesso na otimização lexicográfica de seus objetivos , uma vez que a idéia era justamente dar preferência às árvores menores ( em termos de número de nodos ) quando houvesse equivalência de 
 Neste trabalho apresenta-se um novo esquema prático tipo upwind de alta_resolução  , denominado EPUS ( Eight-degree Polynomial Upwind_Scheme  ) , para resolver numericamente equações de conservação TVD e é implementado no contexto do método das diferenças_finitas  . O desempenho do esquema é investigado na resolução de sistemas hiperbólicos de leis de conservação e escoamentos_incompressíveis  complexos com superfícies_livres  . Os resultados numéricos mostraram boa concordãncia com outros resultados numéricos e dados experimentais 
 Redes são representações poderosas para muitos sistemas complexos , onde vértices representam elementos do sistema e arestas representam conexões entre eles . Redes Complexas podem ser definidas como grafos de grande escala que possuem distribuição não trivial de conexões . Um tópico importante em redes complexas é a detecção de comunidades . Embora a detecção de comunidades tenha revelado bons_resultados  na análise de agrupamento de dados com grupos de diversos formatos , existem ainda algumas dificuldades na representação em rede de um conjunto de dados . Outro tópico recente é a caracterização de simplicidade em redes complexas . Existem poucos trabalhos nessa área , no entanto , o tema tem muita relevância , pois permite analisar a simplicidade da estrutura de conexões de uma região de vértices , ou de toda a rede . Além_disso  , mediante a análise de simplicidade de redes dinâmicas no tempo , é possível conhecer como vem se comportando a evolução da rede em termos de simplicidade . Considerando a rede como um sistema dinâmico de agentes acoplados , foi proposto neste trabalho uma medida de distância baseada no tempo de consenso na presença de um líder em uma rede acoplada . Utilizando essa medida de distância , foi proposto um método de detecção de comunidades para análise de agrupamento de dados , e um método de análise de simplicidade em redes complexas . Além_disso  , foi proposto uma técnica de construção de redes esparsas para agrupamento de dados . Os métodos têm sido testados com dados artificiais e reais , obtendo resultados 
 A diminuição no custo de computadores_pessoais  tem favorecido a construção de sistemas computacionais complexos , tais como aglomerados e grades . Devido ao grande número de recursos existentes nesses sistemas , a probabilidade de que faltas ocorram é alta . Uma abordagem que auxilia a tornar sistemas mais robustos na presença de faltas é a detecção de sua ocorrência , a fim de que processos possam ser reiniciados em estados seguros , ou paralisados em estados que não ofereçam riscos . Abordagens comumente adotadas para detecção seguem , basicamente , três tipos de estratégias : as baseadas em mensagens de controle , em estatística e em aprendizado de máquina . No entanto , elas tipicamente não consideram o comportamento de processos ao longo do tempo . Observando essa limitação nas pesquisas relacionadas , este trabalho apresenta uma abordagem para medir a variação no comportamento de processos ao longo do tempo , a fim de que mudanças inesperadas sejam detectadas . Essas mudanças são consideradas , no contexto deste trabalho , como faltas , as quais representam transições indesejadas entre estados de um processo e podem levá-lo a processamento incorreto , fora de sua especificação . A proposta baseia-se na estimação de cadeias de Markov que representam estados visitados por um processo durante sua execução . Variações nessas cadeias são utilizadas para identificar faltas . A abordagem proposta é comparada à técnica de aprendizado de máquina Support Vector Machines , bem como à técnica estatística Auto-Regressive Integrated Moving Average . Essas técnicas foram escolhidas para comparação por estarem entre as mais empregadas na literatura . Experimentos realizados mostraram que a abordagem proposta possui , com erro 'alfa ' = 1 % , um F-Measure maior do que duas vezes o alcançado pelas outras técnicas . Realizou-se também um estudo adicional de predição de faltas . Nesse sentido , foi proposta uma técnica preditiva baseada na reconstrução do comportamento observado do sistema . A avaliação da técnica mostrou que ela pode aumentar em até uma ordem de magnitude a disponibilidade ( em horas ) de um 
 O presente_trabalho  de pesquisa tem por objetivo estudar a paralelização de algoritmos voltados à solução de equações_diferenciais  parciais . Esses algoritmos são utilizados para gerar a solução_numérica  das equações de Navier-Stokes em um escoamento bidimensional incompressível de um fluido newtoniano . As derivadas espaciais são calculadas através de um método de diferenças_finitas  compactas com a utilização de aproximações de altas ordens de precisão . Uma vez que o cálculo de derivadas espaciais com alta ordem de precisão da forma compacta adotado no presente estudo requer a solução de sistemas_lineares  tridiagonais , é importante realizar estudos voltados a resolução desses sistemas , para se obter uma boa performance . Ressalta-se ainda que a solução de sistemas_lineares  também faz-se presente na solução_numérica  da equação de Poisson . Os resultados obtidos decorrentes da solução das equações_diferenciais  parciais são comparados com os resultados onde se conhece a solução analítica , de forma a verificar a precisão dos métodos implementados . Os resultados do código voltado à resolução das equações de Navier-Stokes paralelizado para simulação de escoamentos_incompressíveis  são comparados com resultados da teoria de estabilidade linear , para validação do código final . Verifica-se a performance e o speedup do código em questão , comparando-se o tempo total gasto em função do número de elementos de processamento 
 Neste trabalho está sendo proposto uma abordagem_bayesiana  para resolver o problema de inferência com restrição nos parâmetros para os modelos de Petterson , Prodan , Stofel e Curtis , utilizados para representar a relação hipsométrica em clones de Eucalyptus sp . Consideramos quatro diferentes densidades de probabilidade a priori , entre as quais , a densidade a priori não informativa de Jeffreys , a densidade a priori vaga normal flat , uma densidade a priori construída empiricamente e a densidade a priori potência . As estimativas bayesianas foram calculadas com a técnica de simulação de Monte_Carlo  em Cadeia de Markov ( MCMC ) . Os métodos propostos foram aplicados em vários conjuntos de dados reais e os resultados foram comparados aos obtidos com os estimadores de máxima_verossimilhança  . Os resultados obtidos com as densidades a priori não informativa e vaga foram semelhantes aos resultados encontrados com os estimadores de máxima_verossimilhança  , porém , para vários conjuntos de dados , as estimativas não apresentaram coerência biológica . Por sua vez , as densidades a priori informativas empírica e a priori potência sempre produziram resultados coerentes biologicamente , independentemente do comportamento dos dados na parcela , destacando a superioridade desta 
 Diversas áreas de pesquisa são dedicadas à compreensão de fenômenos que exigem a coleta ininterrupta de sequências de amostras , denominadas fluxos de dados . Esses fenômenos frequentemente apresentam comportamento variável e são estudados por meio de indução não supervisionada baseada em agrupamento de dados . Atualmente , o processo de agrupamento tem exibido sérias limitações em sua aplicação a fluxos de dados , devido às exigências impostas pelas variações comportamentais e pelo modo de coleta de dados . Embora tem-se desenvolvido algoritmos eficientes para agrupar fluxos de dados , há a necessidade de estudos sobre a influência de variações comportamentais nos parâmetros de algoritmos ( e.g. , taxas de aprendizado e limiares de proximidade ) , as quais interferem diretamente na compreensão de fenômenos . Essa lacuna motivou esta tese , cujo_objetivo  foi a proposta de uma abordagem para a adaptação do viés indutivo de algoritmos de agrupamento de fluxos de dados de acordo com variações comportamentais dos fenômenos em estudo . Para cumprir esse objetivo projetou-se : i ) uma abordagem baseada em uma nova arquitetura de rede_neural  artificial que permite avaliação de comportamento de fenômenos por meio da estimação de cadeias de Markov e entropia de Shannon ; ii ) uma abordagem para adaptar parâmetros de algoritmos de agrupamento tradicional de acordo com variações comportamentais em blocos sequenciais de dados ; e iii ) uma abordagem para adaptar parâmetros de agrupamento de acordo com a contínua avaliação da estabilidade de dados . Adicionalmente , apresenta-se nesta tese uma taxonomia de técnicas de detecção de variação comportamental de fenômenos e uma formalização para o problema de agrupamento de fluxos de 
 Este trabalho lida com o problema de balanceamento e sequenciamento de linhas de produção multi-modelo com trabalhadores deficientes , uma generalização de dois importantes problemas da literatura de linhas de produção : o Problema de Balanceamento de Linhas de Produção Multi-Modelo ( MALBP ) e o Problema de Balanceamento e Designação de Trabalhadores em Linhas de Produção ( ALWABP ) . O MALBP tem sido particularmente importante nas últimas_décadas  , onde , em um cenário de maior competividade , cresce a necessidade de produção em larga_escala  de produtos customizados . O ALWABP , por sua vez , é de grande_importância  em Centros de Trabalhadores com Deficiências ( CTDs ) , onde é necessário considerar as competências individuais de cada trabalhador , que se revelam nos diferentes tempos de execução de uma tarefa , segundo o trabalhador escolhido . Ao nosso conhecimento , nenhum estudo se dedicou a resolver estes dois problemas conjuntamente . Nesta dissertação , propomos modelos lineares para os problemas de balanceamento e sequenciamento de linhas de produção multi-modelo em CTDs . Para o problema de sequenciamento , limitantes inferiores e superiores e métodos heurísticos de resolução são desenvolvidos e discutidos . Testes computacionais foram efetuados e os resultados sugerem que os métodos desenvolvidos são 
 A indústria de fundição produz uma vasta gama de itens com presença na cadeia produtiva de vários setores da economia . São gerados pelo setor , desde itens simples de uso doméstico até itens sofisticados , utilizados por indústrias de base e pelo setor de autopeças . Por ser um segmento muito importante para a economia brasileira , estudos visando melhorar sua eficiência são fundamentais para assegurar sua competitividade em relação ao cenário mundial . Segundo a literatura , um dos principais fatores que influenciam sua produtividade é o planejamento da produção . Nos últimos_anos  , os investimentos nesta área vem aumentando e tem recebido mais atenção do ponto de vista acadêmico . Esta dissertação aborda o planejamento da produção em fundições de pequeno_porte  , cujas principais decisões são : decidir quais ligas fundir nos fornos disponíveis e quais itens devem ser vazados a partir das ligas fundidas . Uma questão ainda não considerada na literatura é a obrigatoriedade de completar a produção de todos os itens de um pedido para que este seja considerado atendido e entregue ao cliente . No entanto , em muitos_casos  reais , um pedido não pode ser atendido parcialmente , ou seja , os itens que compõem esse pedido podem fazer parte de um mesmo item final . Em outros casos , o pedido pode ser atendido parcialmente , mas incorre-se em custos adicionais de expedição . Esta dissertação busca preencher esta lacuna , para tanto , propomos : ( 1 ) um modelo matemático para tratar o problema de planejamento da produção de pedidos em fundições de pequeno_porte  ; ( 2 ) um modelo para tratar a possibilidade de múltiplas entregas para o problema estudado . Além_disso  , também é proposta uma heurística Relax-and Fix ( relaxe-e-fixe ) para a solução do problema 
 A busca por documentos relevantes ao usuário é um problema que se torna mais custoso conforme as bases de conhecimento crescem em seu ritmo acelerado . Este problema passou a resolvido por sistemas distribuídos , devido a sua escalabilidade e tolerância a falhas . O desenvolvimento de sistemas voltados a estas enormes bases de conhecimento -- e a maior de todas , a Internet -- é uma indústria que movimenta bilhões de dólares por ano no mundo inteiro e criou gigantes . Neste trabalho , são apresentadas e discutidas estruturas de dados e arquiteturas distribuídas que tratem o problema de indexar e buscar grandes coleções de documentos em sistemas distribuídos , alcançando grande desempenho e escalabilidade . Serão também discutidos alguns dos grandes sistemas de busca da atualidade , como o Google e o Apache Solr , além do planejamento de uma grande aplicação com protótipo em desenvolvimento . Um projeto próprio de sistema de busca distribuído foi implementado , baseado no Lucene , com idéias coletadas noutros trabalhos e outras novas . Em nossos experimentos , o sistema distribuído desenvolvido neste trabalho superou o Apache Solr com um vazão 37,4\ % superior e mostrou números muito superiores a soluções não-distribuídas em hardware de custo muito superior ao nosso cluster 
 O uso de veículos aéreos não tripulados ( VANTs ) tem se tornado cada vez mais comum , principalmente em aplicações de uso civil . No cenário militar , o uso de VANTs tem focado o cumprimento de missões específicas que podem ser divididas em duas grandes categorias : sensoriamento remoto e transporte de material de emprego militar . Este trabalho se concentra na categoria do sensoriamento remoto . O trabalho foca a definição de um modelo e uma arquitetura de referência para o desenvolvimento de sensores inteligentes orientados a missões específicas . O principal objetivo destas missões é a geração de mapas temáticos . Neste trabalho são investigados processos e mecanismos que possibilitem a geração desta categoria de mapas . Neste sentido , o conceito de MOSA ( Mission Oriented Sensor Array ) é proposto e modelado . Como estudos de caso dos conceitos apresentados são propostos dois sistemas de mapeamento automático de fontes sonoras , um para o caso civil e outro para o caso militar . Essas fontes podem ter origem no ruído gerado por grandes animais ( inclusive humanos ) , por motores de combustão interna de veículos ou por atividade de artilharia ( incluindo caçadores ) . Os MOSAs modelados para esta aplicação são baseados na integração de dados provenientes de um sensor de imageamento termal e uma rede de sensores acústicos em solo . A integração das informações de posicionamento providas pelos sensores utilizados , em uma base cartográfica única , é um dos aspectos importantes tratados neste trabalho . As principais_contribuições  do trabalho são a proposta de sistemas MOSA , incluindo conceitos , modelos , arquitetura e a implementação de referência representada pelo sistema de mapeamento automático de fontes sonoras 
 A possibilidade de recuperar e comparar imagens usando as suas características visuais intrínsecas é um recurso valioso para responder a consultas por similaridade em imagens médicas . Desse modo , a agregação desses recursos aos Sistemas de Arquivamento e Comunicação de Imagens ( Picture Archiving and Communication Systems - PACS ) vêm potencializar a utilidade e importância destes no contexto de atividades tais como ensino e treinamento de novos radiologistas , estudos de casos e auxílio ao diagnóstico de forma geral , uma vez que as consultas por similaridade permitem que casos parecidos possam ser facilmente recuperados . O trabalho apresentado nesta tese possui duas vertentes . Primeiro , ele apresenta novos métodos de extração e de características , com o objetivo de obter a essência das imagens , considerando um critério específico . Os atributos obtidos pelos algoritmos de extração são armazenados em vetores de características para posteriormente serem utilizados para indexar e recuperar as imagens baseando-se em seu conteúdo , para responder a consultas por similaridade . Há uma relação próxima entre os vetores de características e as funções de distância utilizadas para compará-los . Assim , a segunda parte deste trabalho trata da proposta , análise e comparação de novas famílias de funções de distância . As funções de distância propostas têm por objetivo tratar o problema do gap semântico , o qual representa o principal obstáculo das funções de distância tradicionais , derivadas da família Lp , quando processam consultas por similaridade . As principais_contribuições  desta tese incluem o desenvolvimento de novos métodos de extração e comparação de características de imagens , que operam sobre os três principais descritores de baixo_nível  de imagens : distribuição de cor , textura e forma . Os experimentos_realizados  mostraram que os ganhos em precisão são maiores para os métodos propostos , quando comparados com algoritmos tradicionais . No que diz_respeito  às famílias de funções de distância propostas ( WAID e SAID ) , pelos resultados iniciais obtidos , podemos afirmar que eles são bastante promissores no sentido de se aproximarem da expectativa do usuário , no momento de comparar imagens . Os resultados obtidos com esse trabalho podem ser futuramente integrados aos PACS . Particularmente , pretendemos acrescentar novos algoritmos e métodos ao cbPACS , que consiste em um sistema PACS em construção , desenvolvido em uma colaboração entre o Grupo de Bases de Dados e Imagens ( GBDI ) do Instituto de Ciências_Matemáticas  e de Computação - USP e o Centro de Ciências da Imagens e Física Médica ( CCIFM ) da Faculdade de Medicina de Ribeirão Preto - 
 A Plataforma Lattes é , hoje , a principal base de currículos dos pesquisadores brasileiros . Os currículos da Plataforma Lattes armazenam de forma padronizada dados profissionais , acadêmicos , de produções bibliográficas e outras informações dos pesquisadores . Através de uma base de Currículos Lattes , podem ser gerados vários_tipos  de relatórios consolidados . As ferramentas existentes da Plataforma Lattes não são capazes de detectar alguns problemas que aparecem na geração dos relatórios consolidados como duplicidades de citações ou produções bibliográficas classificadas de maneiras distintas por cada autor , gerando um número total de publicações errado . Esse problema faz com que os relatórios gerados necessitem ser revistos pelos pesquisadores e essas falhas deste processo são a principal inspiração deste projeto . Neste trabalho , utilizamos como fonte de informações currículos da Plataforma Lattes para popular uma ontologia e utilizá-la principalmente como uma base de dados a ser consultada para geração de relatórios . Analisamos todo o processo de extração de informações a partir de arquivos HTML e seu posterior processamento para inserí-las corretamente dentro da ontologia , de acordo com sua semântica . Com a ontologia corretamente populada , mostramos também algumas consultas que podem ser realizadas e fazemos uma análise dos métodos e abordagens utilizadas em todo processo , comentando seus pontos fracos e fortes , visando detalhar todas as dificuldades existentes no processo de população ( instanciação ) automática de uma ontologia 
 Paradigmas e técnicas de desenvolvimento como a programação Orientada a Objetos ( OO ) e a programação Orientada a Aspectos ( OA ) procuram melhorar os níveis de reuso e manutenibilidade na produção de software . Contudo , com a introdução de mecanismos com maior poder de expressividade e , consequentemente , a possível introdução de novos tipos de defeitos , a utilização de linguagens OO e OA pode se tornar um obstáculo ao invés de um auxílio ao desenvolvimento de software . Para lidar com esse problema , nesta dissertação é proposta uma abordagem de teste_estrutural  de integração para programas orientados a objetos e a aspectos implementados em Java e AspectJ . É definido um modelo de fluxo de controle e de dados baseado no bytecode Java { chamado Grafo Def-Uso Contextual ( ou Contextual Def-Use graph ) - que é uma abstração formada pela integração dos grafos Def-Uso Orientados a Aspectos ( AODU ) da unidade sob teste com todas as unidades que interagem direta ou indiretamente com ela até um nível de profundidade de interação máximo ou definido pelo testador . São defiidos três critérios de teste : todos-nós-integrados-Nd , todas-arestas-integradas-Nd e todos-usos-integrados-Nd . Para automatizar o uso do modelo e critérios propostos , a ferramenta JaBUTi/AJ foi estendida . Exemplos de usos são discutidos e , por meio de um estudo_experimental  , uma análise de aplicabilidade da abordagem proposta é 
 Diversas áreas da Computação ( Personalização e Adaptação de Conteúdo , Recuperação de Informação , entre outras ) se beneficiam da segmentação de vídeo em unidades menores de informação . A literatura apresenta diversos métodos e técnicas cujo_objetivo  é identificar essas unidades . Uma limitação é que tais técnicas não tratam o problema da detecção de cenas em segmentos semanticamente complexos , definidos como trechos de vídeo que apresentam mais de um assunto ou tema , e cuja semântica latente dificilmente pode ser determinada utilizando-se somente uma única mídia . Esses segmentos são muito relevantes , pois estão presentes em diversos_domínios  de vídeo , tais como filmes , noticiários e mesmo comerciais . A presente Dissertação de Mestrado propõe uma técnica de segmentação de vídeo capaz de identificar cenas em segmentos semanticamente complexos . Para isso utiliza a semântica latente alcançada com o uso de Bag of Visual Words para agrupar os segmentos de um vídeo . O agrupamento é baseado em multimodalidade , analisando-se características visuais e sonoras de cada vídeo e combinando-se os resultados por meio da estratégia fusão tardia . O presente_trabalho  demonstra a viabilidade técnica em reconhecer cenas em segmentos semanticamente 
 A Engenharia de Software é uma disciplina que tem entre seus objetivos melhorar a produtividade dos processos de desenvolvimento de software , assim como propiciar qualidade ao produto resultante desses processos . Para mensurar a qualidade dos produtos de software , foram criados modelos de qualidade , que recomendam métricas , processos e atividades que passaram a se tornar parte do dia-a-dia do desenvolvimento de projetos em empresas . Considerando outra perspectiva , a indústria de software tem adotado cada vez mais os métodos ágeis . Esses métodos foram desenvolvidos visando a entrega rápida do software , com ciclos curtos e adaptáveis de desenvolvimento , foco na comunicação direta e baixo volume de documentação . Considerando a importância do tema qualidade de software , e a baixa aderência dos modelos tradicionais de qualidade aos métodos ágeis , o objetivo deste projeto foi investigar o tema qualidade de software no contexto ágil , ou seja , estudar quais métricas de qualidade são empregadas nesse processo de desenvolvimento . Para isso foram realizados dois estudos empíricos , um estudo de caso e um survey , sobre atividades de garantia e controle de qualidade , métricas de qualidade de software , processos e ferramentas utilizadas no desenvolvimento de software . Os resultados obtidos guiaram a construção de uma ferramenta de apoio para avaliação da qualidade durante o desenvolvimento ágil de software . Os resultados dos estudos mostraram que a execução constante de atividades como revisão de código e refatoração , são fatores essenciais para garantia de qualidade nos métodos ágeis . Outro resultado encontrado foi o de que praticantes de métodos ágeis são entusiastas do processo de desenvolvimento utilizado . Eles conhecem o método e praticam com alta fidelidade os passos definidos pelo processo . É possível concluir que os métodos ágeis possuem diversas atividades como foco na garantia de qualidade de seu produto desde os estágios iniciais do desenvolvimento . A cultura ágil cria um ambiente propício para motivação e engajamento das equipes de desenvolvimento , fato que reflete positivamente na qualidade final dos 
 Alexander A. Razborov ( 2007 ) desenvolveu a teoria de álgebras de flags para calcular a densidade assintótica mínima de triângulos em um grafo em função de sua densidade de arestas . A teoria das álgebras de flags , contudo , pode ser usada para estudar densidades assintóticas de diversos objetos combinatórios . Nesta dissertação , apresentamos dois resultados originais obtidos na teoria de torneios através de técnicas de demonstração de álgebras de flags . O primeiro resultado compreende a minimização da densidade assintótica de torneios transitivos em uma sequência de torneios , a qual provamos ocorrer se e somente se a sequência é quase aleatória . Como subprodutos , obtemos também novas caracterizações de quase aleatoriedade e diversos outros elementos da álgebra de flags cuja densidade é minimizada se e somente se a sequência é quase aleatória . O segundo resultado compreende uma classe de propriedades equivalentes sobre uma sequência de torneios que chamamos de propriedades quase carrossel e que , de uma forma similar às propriedades quase aleatórias , forçam que a sequência convirja para um homomorfismo limite específico . Várias propriedades quase carrossel , quando comparadas às propriedades quase aleatórias , sugerem que sequências quase aleatórias e sequências quase carrossel estão o mais distantes possível umas das outras na classe de sequências quase balanceadas 
 Nos problemas de corte de itens_irregulares  , temos um conjunto de itens menores que devem ser alocados em objetos maiores ( recipientes ) de forma que estes estejam inteiramente contidos no recipiente e não se sobreponham . Neste trabalho , resolvemos um problema de corte e empacotamento de uma indústria que confecciona aventais e forros de luva , no qual deseja-se alocar uma lista de itens dentro de recipientes retangulares utilizando a menor quantidade de recipientes possível e minimizando o comprimento utilizado em cada recipiente . Para isto , utilizamos métodos exatos e heurísticos adaptados para o corte de aventais e forros de luva , com o objetivo de obter soluções de alta qualidade . Foram realizados_experimentos  computacionais que comprovaram a eficiência dos métodos de solução presentes neste trabalho 
 Cada vez mais observa-se a inserção de novas tecnologias em salas de aulas . Com o auxílio de políticas publicas , computadores ligados a internet tem estado presentes nas classes nos mais longinquos lugares do nosso país . Isto tem proporcionado que o conhecimento chegue de forma mais ampla e irrestrita a todas as crianças em fase de desenvolvimento . Na ultima decada , além de microcomputadores , tem-se observado a presença , em salas de aulas , de Ipads , celulares , cujos proprietários são os próprios alunos e até mesmo lousa eletrônica em escolas com poder aquisitivo maior . Aliado a isto , nota-se também a inserção de kits robóticos que tem motivado muito os alunos no aprendizado de raciocínio lógico e de programação , pois , eles experimentam o conceito : `` aprender por meio do fazer '' . O uso de todas estas tecnologias tem como objetivo principal cativar a atenção dos alunos , incentivar a pesquisa e o aprendizado interativo , uma vez que o ensino antes expositivo dá lugar ao ensino interativo , isto é , que conta com a participação mais ativa do estudante . Nesta direção , esta dissertação de Mestrado traz uma inovação no sentido que está sendo proposto um sistema que permite que um robô humanoide seja inserido em salas de aulas . Trata-se de um protótipo que permite que o robô reconheça figuras geométricas planas , que pode ser estendido para outros_tipos  de conteúdos . O objetivo é a integração de um sistema de visão_computacional  em um ambiente de controle de um robô humanoide para torná-lo capaz de reconhecer figuras geométricas planas , para ser utilizada como uma ferramenta de ensino . Este sistema de visão é baseado em técnicas de Atenção Visual e utiliza uma rede_neural  LEGION para segmentar os objetos mais salientes da imagem e uma rede_neural  do tipo Multicamadas ( MLP ) , para realizar a classificação desses objetos . Graças a este sistema de visão , o robô consegue discernir figuras sobrepostas independente do ambiente real no qual esteja inserido . Para avaliar o desempenho do sistema proposto , algumas aplicações foram desenvolvidas que envolveram a participação de crianças interagindo com o robô no reconhecimento de figuras geométricas . Embora sejam necessários uma maior numero de experimentos , os resultados obtidos indicam que o sistema proposto apresenta-se como uma ferramenta alternativa , promissora e interessante , tendo sida muito bem recebida por parte dos alunos e professores 
 Os veículos aéreos não tripulados desempenham diversas funções que vão desde tarefas de monitoramento e inspeção , em aplicações não militares , até tarefas de espionagem e detecção de alvos na área militar . Estes veículos têm como principal componente de controle um piloto automático capaz de manter a aeronave estabilizada e de conduzi-la através de uma rota selecionada . Atualmente , o desenvolvimento de veículos aéreos não tripulados , para aplicações civis , está sendo favorecido e facilitado pelo atual estágio de desenvolvimento tecnológico e , principalmente , pela redução do custo e do tamanho dos componentes eletrônicos . O projeto ARARA ( Aeronaves de Reconhecimento Assistidas por Rádio e Autônomas ) , visa a construção de um veículo aéreo não tripulado para monitoramento . Tem como principal objetivo a substituição de aeronaves convencionais utilizadas na obtenção de imagens aéreas para o monitoramento de plantações e áreas sob controle ecológico . O piloto automático para as aeronaves do projeto ARARA está dividido nos módulos Sistema de Navegação e Sistema de Controle . O módulo Sistema de Navegação mantém a aeronave na rota e corrige os desvios em seu curso . O Sistema de Controle mantém a aeronave estabilizada e executa as manobras solicitadas pelo Sistema de Navegação . O Sistema de Controle é o foco_principal  deste Trabalho . O Sistema de Controle nas aeronaves do projeto ARARA é o único módulo que atua diretamente nos servomecanismos existentes no avião , sendo dependente de suas características . As simulações foram realizadas no MATLAB Simulink utilizando módulos específicos para a simulação do modelo do avião e para o ajuste dos controladores 
 O grande crescimento da Internet ocorreu a partir da década de 1990 com o surgimento dos provedores comerciais de serviços , e resulta principalmente da boa aceitação e vasta disseminação do uso da Web . O grande problema que afeta a escalabilidade e o uso de tal serviço refere-se à organização e à classificação de seu conteúdo . Os engenhos de busca atuais possibilitam a localização de páginas na Web pela comparação léxica de conjuntos de palavras perante os conteúdos dos hipertextos . Tal mecanismo mostra-se ineficaz quando da necessidade pela localização de conteúdos que expressem conceitos ou objetos , a exemplo de produtos à venda oferecidos em sites de comércio_eletrônico  . A criação da Web Semântica foi anunciada no ano de 2000 para esse propósito , visando o estabelecimento de novos padrões para a representação formal de conteúdos nas páginas_Web  . Com sua implantação , cujo prazo inicialmente previsto foi de dez anos , será possível a expressão de conceitos nos conteúdos dos hipertextos , que representarão objetos classificados por uma ontologia , viabilizando assim o uso de sistemas , baseados em conhecimento , implementados por agentes inteligentes de software . O projeto DEEPSIA foi concebido como uma solução centrada no comprador , ao contrário dos atuais Market Places , para resolver o problema da localização de páginas_Web  com a descrição de produtos à venda , fazendo_uso  de métodos de classificação de textos , apoiados pelos algoritmos k-NN e C4.5 , no suporte ao processo decisório realizado por um agente previsto em sua arquitetura , o Crawler Agent . Os testes com o sistema em sites brasileiros denotaram a necessidade pela sua adaptação em diversos aspectos , incluindo-se o processo decisório envolvido , que foi abordado pelo presente_trabalho  . A solução para o problema envolveu a aplicação e a avaliação do método Support Vector Machines , e é descrita em detalhes 
 Este trabalho apresenta um estudo , implementação e validação em ambiente simulado de uma política de escalonamento de tempo real para provisão de QoS absoluto em serviço Web . Sintetizando características de escalonamento de tempo real , com baixa latência e de modelo re-alimentado , a política proposta permite um ajuste ponderado pela quantificação da exigência à qual o sistema está submetido por meio de suas classes . A meta é oferecer ações imediatas às requisições mais urgentes , sem , entretanto , degradar a qualidade do sistema como um todo . Verificou-se que a estratégia de escalonamento baseada em exigência ( EBS - Exigency-Based Scheduling ) é benéfica para o controle da qualidade de serviço oferecida . Escalonar de forma a evitar demasiado peso imposto ao sistema permite que o servidor tenha mais condições de cumprir os requisitos contratuais . Também foi alvo do estudo a criação de uma métrica de avaliação da satisfação de atendimento por parte dos usuários dos serviços . Os resultados alcançados com o emprego da política EBS sinalizam uma melhoria em termos de qualidade de serviço e melhor satisfação dos clientes de forma 
 O presente_trabalho  apresenta o resultado de um estudo comparando o uso de técnicas de aprendizado de máquina na adaptação dinâmica de vídeo . O sistema utiliza informações de contexto , como as características de um dispositivo , preferências do usuário e condições de rede , como entrada de um mecanismo de decisão responsável por encontrar parâmetros de codificação mais adequados ao contexto à geração de um novo fluxo de vídeo dinamicamente 
 As técnicas de teste podem ser divididas , num primeiro nível , naquelas baseadas no código ( caixa branca ) e naquelas baseadas na especificação ( caixa preta ou funcionais ) . Nenhuma delas é completa pois visam a identificar tipos diferentes de defeitos e a sua utilização em conjunto pode elevar o nível de confiabilidade das aplicações . Assim , tornam-se importantes estudos que contribuam para um melhor entendimento da relação existente entre técnicas funcionais e estruturais , como elas se complementam e como podem ser utilizadas em conjunto . Este trabalho está inserido no contexto do projeto PLAVIS ( Plataforma para Validação e Integração de Software em Aplicações Espaciais ) , e tem como objetivo realizar um estudo comparativo entre as técnicas de geração de casos de teste funcionais ( baseadas nas especificações formais ) e os critérios estruturais baseados em fluxo de controle e fluxo de dados , aplicados nas implementações . Num contexto específico , esse estudo deve fornecer dados de como se relacionam essas duas técnicas ( funcional e estrutural ) gerando subsídios para sua utilização em conjunto . Num contexto mais amplo - o do projeto PLAVIS - visa a estabelecer uma estratégia de teste baseada em critérios funcionais e estruturais e que possam , juntamente com as ferramentas que dão suporte a eles , compor um ambiente de teste disponível à utilização em aplicações espaciais dentro do 
 A comparação de sequências finitas é uma ferramenta que é utilizada para a solução de problemas em várias áreas . Comparamos sequências inferindo quais são as operações de edição de substituição , inserção e remoção de símbolos que transformam uma sequência em uma outra . As matrizes de pontuação são estruturas largamente utilizadas e que definem um custo para cada tipo de operação de edição . Uma matriz de pontuação G é indexada pelos símbolos do alfabeto . A entrada de G na linha A , coluna B mede o custo da operação de edição para substituir o símbolo A pelo símbolo B . As matrizes de pontuação induzem funções que atribuem uma pontuação para um conjunto de operações de edição . Algumas dessas funções para a comparação de duas e de várias sequências são estudadas nesta tese . Quando cada símbolo de cada sequência é editado exatamente uma vez para transformar uma sequência em outra , o conjunto de operações de edição pode ser representado por uma estrutura conhecida por alinhamento . Descrevemos uma estrutura para representar o conjunto de operações de edição que não pode ser representado por um alinhamento convencional e descrevemos um algoritmo para encontrar a pontuação de uma sequência ótima de operações de edição usando um algoritmo conhecido para encontrar a pontuação de um alinhamento convencional ótimo . Considerando três diferentes funções induzidas de pontuação , caracterizamos , para cada uma delas , a classe das matrizes para as quais as funções induzidas de pontuação são métricas nas sequências . Dadas duas matrizes de pontuação G e G ' , dizemos que elas são equivalentes para uma dada função que é induzida por uma matriz de pontuação e que avalia a qualidade de um alinhamento se , para quaisquer dois alinhamentos A e B , vale o seguinte : o alinhamento A é `` melhor '' do que o alinhamento B considerando a matriz G se e somente se A é `` melhor '' do que o alinhamento B considerando a matriz G ' . Neste trabalho , determinamos condições necessárias e suficientes para que duas matrizes de pontuação sejam equivalentes . Finalmente , definimos três novos critérios para pontuar alinhamentos de várias sequências . Todos os critérios consideram o comprimento do alinhamento além das operações de edição por ele representadas . Para cada um dos critérios definidos , propomos um algoritmo e o problema de decisão correspondente mostramos ser NP-completo 
 A equação de Poisson-Boltzmann tem uma vasta gama de aplicações , desde a ciência coloidal e microfluídica até bioquímica e biofísica . O potencial elétrico na dupla camada elétrica leva a um potencial de força , em termos das equações de Navier-Stokes que é então usado para simular o fluxo resultante . Em escoamentos bifásicos uma simplificação desta equação é usada para se obter o campo de pressão . O presente_trabalho  tem como principal objetivo estudar o problema de Poisson-Boltzmann com coeficiente constante e propor uma solução através da implementação do método das interfaces imersas utilizando diferenças_finitas  de altas ordens de precisão numérica 
 As atividades de validação , verificação e teste contribuem para melhorar a qualidade dos programas , independentemente do paradigma de programação utilizado . Erlang é um exemplo de linguagem funcional , e aspectos como : dados imutáveis , higher-order functions , lazy evaluation e pattern matching impõem restrições à atividade de teste_estrutural  de software , as quais requerem uma atenção especial do testador . A linguagem Erlang foi criada para o desenvolvimento de aplicações concorrentes , em tempo real e com tolerância a falhas . A aplicação da atividade de teste de software torna-se necessária para aplicações desenvolvidas em Erlang . Um mapeamento sistemático realizado identificou os trabalhos_relacionados  e também a identificação de falhas típicas encontradas em programas Erlang . O resultado do mapeamento evidenciou a falta de propostas que considerem as principais características de Erlang , incluindo uma ferramenta de teste que dê suporte à aplicação prática da atividade de teste . Esta lacuna foi considerada significativa . Foi proposto um conjunto de critérios de teste estruturais para verificar a cobertura de códigos em aplicações sequenciais e distribuídas de programas Erlang . Os critérios de teste exploram as possíveis falhas relacionadas à comunicação entre funções , comunicação entre processos , sincronização , concorrência , recursividade e tolerância a falhas . A definição dos critérios contou com o suporte de um modelo de teste para a obtenção das informações sobre o fluxo de controle , fluxo de dados e fluxo de comunicação de programas Erlang . O modelo estabeleceu 15 tipos de nós , 9 tipos de arestas e 5 tipos de usos de variáveis . Para apoiar a aplicação dos critérios , uma ferramenta de teste , chamada Valierlang , também foi implementada . A ValiErlang é composta por 5 módulos que realizam as seguintes etapas : análise estática , instrumentação do código fonte , geração do GFC , definição dos elementos requeridos , execução do código instrumentado , geração do rastro de execução e avaliação dos elementos cobertos e elementos não executáveis . Foi realizado um estudo_experimental  para verificar a aplicabilidade dos critérios de teste por meio da ValiErlang . Neste estudo foram utilizados seis programas com diferentes características , todas essenciais aos programas em Erlang . Com base nos resultados obtidos , foi possível comprovar que a abordagem contribui com o testador devido ao sucesso na aplicação dos critérios e também a eficácia dos critérios em revelar defeitos . E as etapas para o testador de software realizar a aplicação do teste_estrutural  , tem o apoio ferramental da ValiErlang 
 Sistemas embarcados , incluindo sistemas embarcados_críticos  ( SEC ) , estão cada vez mais presentes na sociedade moderna , provendo soluções específicas que variam de sistemas pequenos até sistemas grandes e complexos , como é possível encontrar em carros , aviões e equipamentos médicos . Falhas nesses sistemas podem resultar em danos à seres_humanos  e ao meio ambiente , ou então em uma perda financeira irrecuperável . Sendo_assim  , é muito importante garantir que os SEC sejam construídos e apresentem um nível adequado de qualidade . Para garantir que esses sistemas alcancem tal qualidade , é necessário considerar sua arquitetura de software , já que esta impacta de maneira significativa na qualidade do software enquanto artefato ou produto . Deste modo , o objetivo deste projeto de mestrado é de propor um modelo de qualidade que contém os atributos de qualidade mais importantes para SEC , servindo como artefato para apoiar a execução de atividades arquiteturais ( além de outras que possam se beneficiar ) tais como análise e avaliação , no contexto de SEC 
 A fim de se utilizar algoritmos de Aprendizado de Máquina para tarefas de classificação , é admitida a existência de um conjunto de exemplos_rotulados  , conhecido como conjunto de treinamento , o qual é utilizado para o treinamento do classificador 
 Entretanto , em casos reais , esse conjunto de treinamento pode não conter um número de exemplos suficientemente grande para se induzir um bom classificador 
 Recentemente , a comunidade_científica  tem mostrado um grande interesse em uma variação dessa abordagem de aprendizado_supervisionado  . Essa nova abordagem , conhecida como aprendizado semi-supervisionado , assume que , juntamente com o conjunto de treinamento , há um segundo conjunto , de exemplos não rotulados , também disponível durante o treinamento . Uma das metas do aprendizado semi-supervisionado é o treinamento de classificadores quando uma grande quantidade de exemplos não rotulados está disponível juntamente com um pequeno conjunto de exemplos_rotulados  
 A motivação para o aprendizado semi-supervisionado deve-se ao fato que , em muitas aplicações do mundo_real  , conjuntos de exemplos não rotulados são facilmente encontrados ou muito baratos para serem coletados , quando comparados aos conjuntos 
 exemplos_rotulados  . Um outro fator é que exemplos não rotulados podem ser coletados de forma automática enquanto os rotulados necessitam de especialistas ou outros custosos recursos de classificação 
 Os exemplos não rotulados podem ser utilizados de diversas maneiras . Neste trabalho é explorado um mecanismo no qual os exemplos não rotulados podem ser utilizados para melhorar tarefas de classificação e é proposto um algoritmo semi-supervisionado , denominado k-meanski , o qual viabiliza o uso de exemplos não 
 em aprendizado_supervisionado  
 A técnica utilizada pelo algoritmo proposto está baseada em duas premissas . A primeira delas é que os exemplos tendem a se agrupar naturalmente em clusters , ao invés de se distribuirem uniformemente no espaço de descrição dos exemplos . Além_disso  , cada exemplo do conjunto inicial de exemplos_rotulados  deve estar 
 perto do centro de um dos clusters existentes no espaço de descrição de exemplos . A segunda premissa diz que a maioria dos exemplos nos clusters pertencem a uma classe específica . Obviamente , a validade dessas premissas é dependente do conjunto de dados utilizado 
 O algoritmo k-meanski funciona bem nos casos em que os dados estão em 
 com ambas as premissas . Entretanto , caso elas sejam violadas , a performance do algoritmo não será boa . São mostrados experimentos utilizando conjuntos de dados do mundo_real  , escolhendo-se aleatoriamente exemplos desses conjuntos para atuarem como exemplos_rotulados  
 Software Livre é software fornecido com código fonte , e que pode ser livremente usado , modificado e redistribuído . Projetos de Software Livre são organizações virtuais formadas por indivíduos que trabalham juntos no desenvolvimento de um software_livre  específico . Estes indivíduos trabalham geograficamente dispersos , utilizando ferramentas simples para coordenar e comunicar seu trabalho através da Internet . Este trabalho analisa esses projetos do ponto de vista de seu processo de software ; em outras_palavras  , analisa as atividades que realizam para produzir , gerenciar e garantir a qualidade do seu software . Na parte inicial do trabalho é feita uma extensa revisão bibliográfica , comentando os principais trabalhos na área , e são detalhadas as características principais dos projetos de software_livre  . O conteúdo principal deste trabalho resulta de dois anos de participação ativa na comunidade , e de um levantamento realizado através de questionário , detalhando mais de quinhentos projetos diferentes . São apresentadas treze hipóteses experimentais , e os resultados do questionário são discutidos no contexto destas hipóteses . Dos projetos avaliados nesse levantamento , algumas características comuns foram avaliadas . As equipes da grande maioria dos projetos são pequenas , tendo menos de cinco participantes . Além_disso  , existe uma distribuição equilibrada entre algumas formas de organização descritas na literatura , incluindo o 'ditador benevolente ' de Eric S. Raymond e o 'comité ' exemplificado pelo projeto Apache . Dentre um conjunto de domínios de aplicação determinados , os projetos de software_livre  se concentram nas áreas de engenharia e desenvolvimento de software , redes e segurança , e aplicações multimídia . Com relação às atividades do processo de software , pode-se dizer que a maioria dos projetos tem requisitos fundamentalmente definidos pelos seus autores , e que a base de usuários de grande parte dos softwares é composta dos seus desenvolvedores e da comunidade de software_livre  . Uma parcela significativa dos projetos baseia-se em outros softwares pré-existentes , e em padrões publicados previamente . Pouca ênfase é dada à usabilidade , assim como às atividades de garantia de qualidade convencionais . Surpreendentemente , também recebem pouca atenção as atividades de revisão de código e teste sistemático . Entre as ferramentas que os projetos utilizam , se destacam as listas de discussão e os sistemas de controle de versão . Foi estabelecida uma correlação entre a dimensão da equipe do projeto e as atividades de engenharia de software que realiza , mas não se confirmou um vínculo entre estas atividades e a idade do projeto . Foram também estabelecidas relações entre o número de linhas de código do software do projeto e a sua idade , tamanho e domínio de aplicação . Estes resultados são exibidos neste trabalho , e estarão publicamente disponíveis no site Web do projeto . O trabalho conclui descrevendo partes essenciais do processo de software em projetos de software_livre  , e oferecendo sugestões para trabalhos posteriores 
 Mineração de dados refere-se ao processo responsável por identificar padrões em grandes_conjuntos  de dados com o objetivo de extrair um novo conhecimento . Depois de extraídos os padrões , a etapa de pós-processamento tem como objetivo avaliar alguns aspectos desses padrões , como precisão , compreensibilidade e interessabilidade . Um dos problemas da mineração de dados preditiva conhecido como regressão tenta predizer o valor de um atributo-meta contínuo baseado em um conjunto de atributos de entrada . No entanto , a regressão em mineração de dados preditivo é uma questão pouco explorada nas áreas de aprendizado de máquina e mineração de dados , uma vez que a maioria das pesquisas são voltadas para os problemas de classificação . Por outro_lado  , o DISCOVER é um projeto que está sendo desenvolvido no Laboratório de Inteligência Computacional . Ele tem como objetivo fornecer um ambiente integrado para apoiar as etapas do processo de descoberta de conhecimento , oferecendo funcionalidades voltadas para aprendizado de máquina e mineração de dados e textos . O DISCOVER tem motivado a realização de muitos trabalhos em diversas_áreas  , sendo este mais um projeto a ele integrado . Para auxiliar na preparação dos dados para a construção de um modelo de regressão simbólico e na posterior avaliação desse modelo foi proposto e implementado neste trabalho o ambiente computacional DISCOVER POSTPROCESSING ENVIRONMENT OF REGRESSION - DiPER . Esse ambiente é composto por uma biblioteca de classes , implementada de acordo com as especificações do DISCOVER , que oferece uma série de métodos para serem utilizados na etapa de pós-processamento do processo de mineração de dados 
 Recentemente , o desenvolvimento das tecnologias de coleta e armazenamento de dados , a popularização da Internet , e a evolução das necessidades de informação fazem com que qualquer esforço para facilitar a manipulação e o entendimento humano em relação a esse grande leque de informações ganhe grande_importância  . Nesse sentido , as organizações têm visto com crescente_interesse  o desenvolvimento de tecnologias automatizadas para extração de conhecimento de grandes volumes de dados . Estas tecnologias têm sido referenciada na literatura como Extração de Conhecimento de Bases de Dados ou Mineração de Dados ( MD ) . Um dos objetivos do processo de Extração de Conhecimento de Bases de Dados é que seus usuários finais possam analisar , compreender e utilizar o conhecimento_extraído  em um processo de tomada de decisão . Entretanto , do ponto de vista desses usuários , um problema encontrado no íinal do processo de Extração de Conhecimento é que muitos dos algoritmos utilizados geram uma enorme_quantidade  de padrões , dificultando consideravelmente sua análise . Outro problema frequentemente identificado é a dificuldade na compreensão dos modelos extraídos dos dados . Nesse sentido , diversos trabalhos têm sido desenvolvidos para apoiar os usuários na avaliação e interpretação do conhecimento_extraído  a partir dos dados . A existência de um Ambiente para Exploração de Regras em que as regras encontradas em um processo de Extração de Conhecimento estejam armazenadas , disponibilizadas aos usuários e que apresente diversos tipos de métodos para sua avaliação pode representar um auxílio efetivo para os usuários na compreensão , identificação das regras interessante e utilização das mesmas . Nesta dissertação , são apresentadas algumas características importantes de um Ambiente para Exploração de Regras , uma arquitetura geral e a implementação de um ambiente deste tipo , o Ambiente RulEE . É também apresentado um estudo de caso realizado com objetivo de mostrar essas características na análise e disponibilização de regras extraídas a partir de um conjunto de dados financeiros 
 Sistemas de Transportes Inteligentes representam um imenso impacto social e aos poucos tem modificado o paradigma de mobilidade atual . Desde a década de 80 , veículos_autônomos  vêm_sendo  desenvolvidos e estudados pela comunidade_científica  e hoje atrai o interesse de grandes empresas automobilísticas . Esses sistemas têm como objetivo a redução do número de acidentes de trânsito , aumento da eficiência dos transportes e inclusão social , sendo que neste contexto surge o projeto CaRINA . Através do desenvolvimento de uma plataforma robótica_móvel  pretende-se realizar a navegação completamente autônoma em ambiente urbano . Contudo , os experimentos_realizados  com a plataforma real são custosos , demorados e perigosos . A logística dos testes é complexa , uma vez que necessitam de local apropriado e disponibilidade de recursos . Portanto , o objetivo deste trabalho é desenvolver uma arquitetura de simulação para veículos_autônomos  que seja capaz de realizar experimentos em laboratório e facilite a portabilidade dos programas desenvolvidos em simulação para o veículo real . A flexibilidade da arquitetura do simulador também permite realizar experimentos utilizando múltiplos veículos 
 A atenção voltada à produção de conteúdos 3D atualmente tem sido alta , grande parte devido à aceitação e à manifestação de interesse do público para esta tecnologia . Novas técnicas de captação e codificação e modos de reprodução de vídeos 3D , particularmente vídeos estereoscópicos , vêm surgindo ou sendo melhorados , visando aperfeiçoar e integrar esta nova tecnologia com a infraestrutura disponível . No entanto , em relação a avanços na área de codificação , nota-se a ausência de uma técnica compatível com mais de um método de visualização de vídeos estereoscópicos - para cada método de visualização há uma técnica de codificação diferente , o que inviabiliza ao usuário escolher o método que deseja visualizar o conteúdo . Uma abordagem para resolver este problema é desenvolver uma técnica genérica , isto é , uma técnica que seja independentemente do método de visualização , que através de parâmetros adequados , produza um vídeo estereoscópico sem perda significativa de qualidade ou a percepção de profundidade , que é a característica marcante desse tipo de conteúdo . O método proposto neste trabalho , chamado HaaRGlyph , transforma um vídeo esterescópico em um único fluxo contendo um anáglifo , codificado de modo especial . Esse fluxo além de ser compatível com o método de visualização anaglífica é também reversível à uma aproximação do par estéreo original , possibilitando a independência de visualização . Além_disso  , a HaaRGlyph atinge maiores taxas de compressão do que o trabalho relacionado 
 Uma grande limitação dos métodos de diferenças_finitas  é que eles estão restritos a malhas e domínios retangulares . Para descrever escoamentos em domínios complexos , como , por exemplo , problemas com superfícies_livres  , faz-se necessário o uso de técnicas acessórias . O método de interfaces imersas é uma dessas técnicas . Nesse trabalho , primeiramente foi desenvolvido um método de projeção , totalmente livre de pressão , para as equações de Navier-Stokes com variáveis primitivas em malha_deslocada  . Esse método é baseado em diferenças_finitas  compactas , possuindo segunda_ordem  temporal e quarta ordem espacial . Esse método foi combinado com o método de interface imersa de Linnick e Fasel [ 2 ] para resolver numericamente as equações de Stokes com quarta ordem de precisão . A verificação do código foi feita por meio do método das soluções manufaturadas e da comparação com resultados de outros autores em problemas clássicos da literatura 
 A indústria de desenvolvimento de software está em constante evolução e em busca de novos desafios , como por exemplo : novas tecnologias , linguagens de programação e estratégias para aumentar a produtividade no desenvolvimento . Com o passar do tempo , essa evolução desejada tem se concretizado , em especial com o surgimento de conceitos como Service Oriented Architecture ( SOA ) e Model-Driven Development ( MDD ) . Nesse contexto , ganha importância a reutilização de software , por exemplo por meio do uso de serviços que facilitam a interoperabilidade entre diferentes sistemas . Ambientes de desenvolvimento de software , em especial ambientes de reúso de software , poderiam se adequar à arquitetura SOA para facilitar a disponibilização de serviços a outros ambientes , promovendo sua integração e aumentando a possibilidade de reúso . Após um levantamento bibliográfico sobre ambientes integrados de reúso que utilizassem serviços para permitir o compartilhamento de recursos , percebeu-se que os trabalhos nessa direção são poucos . Ao mesmo tempo , observou-se que para integração de sistemas por meio de serviços é necessário criar camadas de acesso às entidades dos sistemas que compartilharão informações . A implementação desses serviços de forma_manual  é repetitiva e sujeita a erros , principalmente por programadores que não estão familiarizados com SOA . Por outro_lado  , diagramas de classes em UML ( Unified Modeling Language ) são de amplo conhecimento por parte de desenvolvedores e estão em um nível de abstração que é suficientemente detalhado para permitir a derivação de serviços de acesso às suas classes . Desta forma , para atender uma demanda crescente de sistemas que precisam oferecer serviços para possibilitar sua integração com outros sistemas , este mestrado tem por objetivo promover a geração automatizada de código de serviços por meio de transformações de modelo para texto , em que o modelo de entrada é um diagrama de classes derivado da UML e os serviços gerados cobrem operações básicas do tipo CRUD . O gerador resultante foi validado por meio de duas provas de conceito e mostrou-se adequado para cumprir os objetivos estabelecidos , visto que o desenvolvedor trabalha com um modelo em alto nível de abstração , não precisando conhecer os detalhes inerentes à implementação dos serviços 
 A demanda por métodos de análise e descoberta de conhecimento em grandes bases de dados tem fortalecido a pesquisa em Mineração de Dados . Dentre as tarefas associadas a essa área , tem-se Regras de Associação . Vários algoritmos foram propostos para tratamento de Regras de Associação , que geralmente tem como resultado um elevado número de regras , tornando o Pós-processamento do conhecimento uma etapa bastante complexa e desafiadora . Existem medidas para auxiliar essa etapa de avaliação de regras , porém existem lacunas referentes a inexistência de um método intuitivo para priorizar e selecionar regras . Além_disso  , não é possível encontrar metodologias específicas para seleção de regras considerando mais de uma medida simultaneamente . Esta tese tem como objetivo a proposição , desenvolvimento e implementação de uma metodologia para o Pós-processamento de Regras de Associação . Na metodologia_proposta  , pequenos grupos de regras identificados como potencialmente interessantes são apresentados ao usuário especialista para avaliação . Para tanto , foram analisados métodos e técnicas utilizadas em Pós-processamento de conhecimento , medidas objetivas para avaliação de Regras de Associação e algoritmos que geram regras . Dessa perspectiva foram realizados_experimentos  para identificar o potencial das medidas a serem empregadas como filtros de Regras de Associação . Uma avaliação gráfica apoiou o estudo das medidas e a especificação da metodologia_proposta  . Aspecto inovador da metodologia_proposta  é a utilização do método de Pareto e a combinação de medidas para selecionar as Regras de Associação . Por fim foi implementado um ambiente para avaliação de Regras de Associação , denominado ARInE , viabilizando o uso da metodologia_proposta  


 Dois métodos distintos são descritos e implementados . O primeiro método , proposto por Ruppert , possui garantias teóricas de qualidade quando a fronteira do domínio obedece certas restrições . O segundo método , proposto por Persson , possibilita um maior controle na densidade dos elementos que discretizam o domínio . As vantagens , desvantagens e particularidades de cada um dos métodos são descritas e 
 A Engenharia de Software tem desenvolvido t [ écnicas e métodos para apoiar o desenvolvimento de software confiável , flexível , com baixo custo de desenvolvimento e fácil manutenção . Técnicas e critérios de teste contribuem nessa direção , fornecendo mecanismos para produzir software com alta qualidade . Este trabalho apresenta um estudo_experimental  para avaliar o custo , eficácia e a dificuldade de satisfação ( strength ) dos critérios estruturais propostos para programas concorrentes . Esta avaliação foi conduzida usando oito programas implementados em MPI e utilizando a ferramenta de teste ValiMPI . Com base em taxonomias , defeitos foram injetados nos programas de modo a avaliar a eficácia dos critérios de teste em revelar os defeitos inseridos . Os resultados obtidos demonstraram o aspecto complementar dos critérios e informações sobre o custo e eficácia , que contribuíram para o estabelecimento de uma estratégia de teste incremental para aplicar os critérios de teste em uma boa relação custo-eficácia . Para concluir , os resultados indicam que os critérios de teste_estrutural  propostos para programas concorrentes em MPI são promissores e podem auxiliar a detectar defeitos nessas aplicações , melhorando a qualidade das 
 Diversos problemas do mundo_real  estão relacionados ao projeto de redes , tais como projeto de circuitos de energia elétrica , roteamento de veículos , planejamento de redes de telecomunicações e reconstrução filogenética . Em geral , esses problemas podem ser modelados por meio de grafos , que manipulam milhares ou milhões de nós ( correspondendo às variáveis de entrada ) , dificultando a obtenção de soluções em tempo real . O Projeto de uma Rede é um problema combinatório , em que se busca encontrar a rede mais adequada segundo um critério como , por exemplo , menor_custo  , menor caminho e tempo de percurso . A solução desses problemas é , em geral , computacionalmente complexa . Nesse sentido , metaheurísticas como Algoritmos_Evolutivos  têm sido amplamente investigadas . Diversas pesquisas mostram que o desempenho de Algoritmos_Evolutivos  para Problemas de Projetos de Redes pode ser aumentado significativamente por meio de representações mais apropriadas . Este trabalho_investiga  a paralelização da Representação Nó-Profundidade ( RNP ) em hardware , com o objetivo de encontrar melhores soluções para Problemas de Projetos de Redes . Para implementar a arquitetura de hardware , denominada de HP-RNP ( Hardware Parallelized RNP ) , foi utilizada a tecnologia de FPGA para explorar o alto grau de paralelismo que essa plataforma pode proporcionar . Os resultados experimentais mostraram que o HP-RNP é capaz de gerar e avaliar novas redes em tempo_médio  limitado por uma constante ( O ( 1 ) 
 A tarefa de classificação em Aprendizado de Máquina consiste da criação de modelos computacionais capazes de identificar automaticamente a classe de objetos pertencentes a um domínio pré-definido a partir de um conjunto de exemplos cuja classe é conhecida . Existem alguns cenários de classificação nos quais cada objeto pode estar associado não somente a uma classe , mas a várias classes ao mesmo tempo . Adicionalmente , nesses cenários denominados multirrótulo , as classes podem ser organizadas em uma taxonomia que representa as relações de generalização e especialização entre as diferentes classes , definindo uma hierarquia de classes , o que torna a tarefa de classificação ainda mais específica , denominada classificação_hierárquica  . Os métodos utilizados para a construção desses modelos de classificação são complexos e dependem fortemente da disponibilidade de uma quantidade expressiva de exemplos previamente classificados . Entretanto , para muitas aplicações é difícil encontrar um número significativo desses exemplos . Além_disso  , com poucos exemplos , os algoritmos de aprendizado_supervisionado  não são capazes de construir modelos de classificação eficazes . Nesses casos , é possível utilizar métodos de aprendizado semissupervisionado , cujo_objetivo  é aprender as classes do domínio utilizando poucos exemplos conhecidos conjuntamente com um número considerável de exemplos sem a classe especificada . Neste trabalho são propostos , entre outros , métodos que fazem_uso  do aprendizado semissupervisionado baseado em desacordo coperspectiva , tanto para a tarefa de classificação multirrótulo plana quanto para a tarefa de classificação_hierárquica  . São propostos , também , outros métodos que utilizam o aprendizado ativo com intuito de melhorar a performance de algoritmos de classificação semissupervisionada . Além_disso  , são propostos dois métodos para avaliação de algoritmos multirrótulo e hierárquico , os quais definem estratégias para identificação dos multirrótulos majoritários , que são utilizados para calcular os valores baseline das medidas de avaliação . Foi desenvolvido um framework para realizar a avaliação_experimental  da classificação_hierárquica  , no qual foram implementados os métodos propostos e um módulo completo para realizar a avaliação_experimental  de algoritmos hierárquicos . Os métodos propostos foram avaliados e comparados empiricamente , considerando conjuntos de dados de diversos_domínios  . A partir da análise dos resultados observa-se que os métodos baseados em desacordo não são eficazes para tarefas de classificação complexas como multirrótulo e hierárquica . Também é observado que o problema central de degradação do modelo dos algoritmos semissupervisionados agrava-se nos casos de classificação multirrótulo e hierárquica , pois , nesses casos , há um incremento nos fatores responsáveis pela degradação nos modelos construídos utilizando aprendizado semissupervisionado baseado em desacordo 
 Este trabalho propõe um controlador PID ( Proporcional , Integrador , Derivativo ) , implementado em hardware_reconfigurável  , para controle de ganho de uma câmera com sensor CMOS . O conceito utilizado é o de sistemas SoC ( System-on-a-Chip ) . As principais funções realizadas pelo sistema são : Aquisição da imagem , montagem do histograma , análise do histograma , controle de ganho baseado na análise do histograma . O sistema proposto tem como objetivo conter algumas funções básicas de controle de ganho que possam servir de base para construção de sistemas de visão_computacional  que possibilitem a otimização do tempo gasto na construção de novos sistemas , deixando o projetista concentrado na parte mais específica do sistema . O algoritmo de controle de ganho através da análise de histograma demonstrou ser além de funcional , altamente flexível , pois pode ser aplicado a qualquer câmera , independente do tipo do sensor . Este algoritmo pode ser aplicado a tipos diferentes de sensores , com diferentes taxas de aquisição e transmissão de imagens . Este ambiente baseado em computação reconfigurável proporciona alta performance e flexibilidade no modo de implementação , possibilitando que o hardware seja configurado para satisfazer situações que exigem alto_desempenho  , que pode ser obtido através do paralelismo de operações . Esta arquitetura ainda possibilita a configuração de processadores que executam operações em software em conjunto com operações executadas em hardware . O sistema final controla a câmera CMOS de maneira adequada às aplicações robóticas de tempo 
 Recentemente , a área de mineração de fluxos contínuos de dados ganhou importância , a qual visa extrair informação útil a partir de conjuntos massivos e contínuos de dados que evoluem com o tempo . Uma das técnicas que mais se destaca nessa área e a de agrupamento de dados , a qual busca estruturar grandes volumes de dados em hierarquias ou partições , tais que objetos mais similares estejam em um mesmo grupo . Diversos algoritmos foram propostos nesse contexto , porém a maioria concentrou-se no agrupamento de fluxos compostos por pontos em um espaço multidimensional . Poucos trabalhos voltaram-se para o agrupamento de séries_temporais  , as quais se caracterizam por serem coleções de observações coletadas sequencialmente no tempo . Técnicas atuais para agrupamento de séries_temporais  em fluxos contínuos apresentam uma limitação na escolha da medida de similaridade , a qual na maioria dos casos e baseada em uma simples correlação , como a de Pearson . Este trabalho mostra que até para modelos clássicos de séries_temporais  , como os de Box e Jenkins , a correlação de Pearson não é capaz de detectar similaridade , apesar das séries serem provenientes de um mesmo modelo matemático e com mesma parametrização . Essa limitação nas técnicas atuais motivou este trabalho a considerar os modelos geradores de séries_temporais  , ou seja , as equações que regem sua geração , por meio de diversas medidas descritivas , tais como a Autoinformação Mútua , o Expoente de Hurst e várias outras . A hipótese considerada e que , por meio do uso de medidas descritivas , pode-se obter uma melhor caracterização do modelo gerador de séries_temporais  e , consequentemente , um agrupamento de maior qualidade . Nesse sentido , foi realizada uma avaliação de diversas medidas descritivas , as quais foram usadas como entrada para um novo algoritmo de agrupamento baseado em árvores , denominado TS-Stream . Experimentos com bases sintéticas compostas por diversos modelos de séries_temporais  foram realizados , mostrando a superioridade de TS-Stream sobre ODAC , a técnica mais popular para esta tarefa encontrada na literatura . Experimentos com séries reais provenientes de preços de ações da NYSE e NASDAQ mostraram que o uso de TS-Stream na escolha de ações , por meio da criação de uma carteira de investimentos diversificada , pode aumentar os retornos das aplicações em várias ordens de grandeza , se comparado a estratégias baseadas somente no indicador econômico Moving Average Convergence 
 Nesta tese , uma nova abordagem para o rastreamento de múltiplos objetos com o uso de informação estrutural é proposta . Os objetos são rastreados usando uma combinação de filtro de partículas com descrição das imagens por meio de Grafos Relacionais com Atributos ( ARGs ) . O processo é iniciado a partir do aprendizado de um modelo de grafo estrutural probabilístico utilizando imagens anotadas . Os grafos são usados para avaliar o estado atual do rastreamento e corrigi-lo , se necessário . Desta forma , o método proposto é capaz de lidar com situações desafiadoras como movimento abrupto e perda de rastreamento devido à oclusão . A principal_contribuição  desta tese é a exploração do modelo estrutural aprendido . Por meio dele , a própria informação estrutural da cena é usada para guiar o processo de detecção em caso de perda do objeto . Tal abordagem difere de trabalhos anteriores , que utilizam informação estrutural apenas para avaliar o estado da cena , mas não a consideram para gerar novas hipóteses de rastreamento . A abordagem proposta é bastante flexível e pode ser aplicada em qualquer situação em que seja possível encontrar padrões de relações estruturais entre os objetos . O rastreamento de objetos pode ser utilizado para diversas aplicações práticas , tais como vigilância , análise de atividades ou navegação_autônoma  . Nesta tese , ele é explorado para rastrear diversos objetos em vídeos de esporte , na qual as regras do jogo criam alguns padrões estruturais entre os objetos . Além de detectar os objetos , os resultados de rastreamento também são usados como entrada para reconhecer a ação que cada jogador está realizando . Esta etapa é executada classificando um segmento da sequência de rastreamento por meio de Modelos Ocultos de Markov ( HMMs ) . A abordagem de rastreamento proposta é testada em diversos vídeos de jogos de tênis de mesa e na base de dados ACASVA , demonstrando a capacidade do método de lidar com situações de oclusão ou cortes de câmera 
 Apesar do grau relativamente alto de maturidade existente na área de pesquisa de aprendizado_supervisionado  em lote , na qual são utilizados dados originários de problemas estacionários , muitas aplicações reais lidam com fluxos de dados cujas distribuições de probabilidade se alteram com o tempo , ocasionando mudanças de conceito . Diversas pesquisas vêm_sendo  realizadas nos últimos_anos  com o objetivo de criar modelos precisos mesmo na presença de mudanças de conceito . A maioria delas , no entanto , assume que tão logo um evento seja classificado pelo algoritmo de aprendizado , seu rótulo verdadeiro se torna conhecido . Este trabalho explora as situações complementares , com revisão dos trabalhos mais importantes publicados e análise do impacto de atraso na disponibilidade dos rótulos verdadeiros ou sua não disponibilização . Ainda , propõe um novo algoritmo que reduz drasticamente a complexidade de aplicação do teste de hipótese não-paramétrico Kolmogorov-Smirnov , tornado eficiente seu uso em algoritmos que analisem fluxos de dados . A exemplo , mostramos sua potencial aplicação em um método de detecção de mudança de conceito não-supervisionado que , em conjunto com técnicas de Aprendizado Ativo e Aprendizado por Transferência , reduz a necessidade de rótulos verdadeiros para manter boa performance de um classificador ao longo do tempo , mesmo com a ocorrência de mudanças de conceito 
 Com uma quantidade quase incontável de informação textual disponível na web , a automatização de diversas tarefas referentes ao processamento automático de textos é uma necessidade inegável . Em abordagens superficiais do PLN ( Processamento da Linguagem Natural ) , importantes propriedades do texto são perdidas , como posição , ordem , adjacência e contexto dos segmentos textuais . Uma análise textual mais profunda , como a realizada no nível do discurso , ocupa-se da busca e identificação da organização retórica do texto , gerando uma estrutura hierárquica em que as intenções do autor são explicitadas e relacionadas entre si . Para a automatização dessa tarefa , tem-se utilizado técnicas de aprendizado automático , predominantemente do paradigma supervisionado . Nesse paradigma , são necessários dados rotulados manualmente para a geração dos modelos de classificação . Como a anotação para essa tarefa é algo custoso , os resultados obtidos no aprendizado são insatisfatórios , pois estão bem aquém do desempenho humano na mesma tarefa . Nesta tese , o uso massivo de dados não rotulados no aprendizado semissupervisionado sem fim foi empregado na tarefa de identificação das relações retóricas . Foi proposto um framework que utiliza textos obtidos continuamente da web . No framework , realiza-se a monitoração da mudança de conceito , que pode ocorrer durante o aprendizado contínuo , e emprega-se uma variação dos algoritmos tradicionais de semissupervisão . Além_disso  , foram adaptados para o Português técnicas do estado da arte . Sem a necessidade de anotação humana , a medida-F melhorou , por enquanto , em 0,144 ( de 0,543 para 0,621 ) . Esse resultado consiste no estado da arte da análise discursiva automática para o Português 
 O número de acidentes veiculares têm aumentado mundialmente e a principal causa associada a estes acidentes é a falha humana . O desenvolvimento de veículos_autônomos  é uma área que ganhou destaque em vários grupos de pesquisa do mundo , e um dos principais objetivos é proporcionar um meio de evitar estes acidentes . Os sistemas de navegação utilizados nestes veículos precisam ser extremamente confiáveis e robustos o que exige o desenvolvimento de soluções específicas para solucionar o problema . Devido ao baixo custo e a riqueza de informações , um dos sensores mais utilizados para executar navegação_autônoma  ( e nos sistemas de auxílio ao motorista ) são as câmeras . Informações sobre o ambiente são extraídas por meio do processamento das imagens obtidas pela câmera , e em seguida são utilizadas pelo sistema de navegação . O objetivo principal desta tese consiste do projeto , implementação , teste e otimização de um comitê de Redes_Neurais  Artificiais utilizadas em Sistemas de Visão Computacional para Veículos Autônomos ( considerando em específico o modelo proposto e desenvolvido no Laboratório de Robótica Móvel ( LRM ) ) , em hardware , buscando acelerar seu tempo de execução , para utilização como classificadores de imagens nos veículos_autônomos  desenvolvidos pelo grupo de pesquisa do LRM . Dentre as contribuições deste trabalho , as principais são : um hardware configurado em um FPGA que executa a propagação do sinal em um comitê de redes neurais_artificiais  de forma rápida com baixo consumo de energia , comparado a um computador de propósito_geral  ; resultados práticos avaliando precisão , consumo de hardware e temporização da estrutura para a classe de aplicações em questão que utiliza a representação de ponto-fixo ; um gerador automático de look-up tables utilizadas para substituir o cálculo exato de funções de ativação em redes MLP ; um co-projeto de hardware/software que obteve resultados relevantes para implementação do algoritmo de treinamento Backpropagation e , considerando todos os resultados , uma estrutura que permite uma grande diversidade de trabalhos futuros de hardware para robótica por implementar um sistema de processamento de imagens em hardware 
 No Aprendizado de Máquina Supervisionado -- -AM -- -o algoritmo de indução trabalha com um conjunto de exemplos de treinamento , no qual cada exemplo é constituído de um vetor com os valores dos atributos e as classes , e tem como tarefa induzir um classificador capaz de predizer a qual classe pertence um novo exemplo . Em geral , os algoritmos de indução baseiam-se nos exemplos de treinamento para a construção do classificador , sendo que uma representação inadequada desses exemplos , bem como inconsistências nos mesmos podem tornar a tarefa de aprendizado difícil . Um dos problemas centrais de AM é a Seleção de um Subconjunto de Atributos -- -SSA -- -cujo objetivo é diminuir o número de atributos utilizados na representação dos exemplos . São três as principais razões para a realização de SSA . A primeira razão é que a maioria dos algoritmos de AM , computacionalmente viáveis , não trabalham bem na presença de vários atributos . A segunda razão é que , com um número menor de atributos , o conceito induzido através do classificador pode ser melhor compreendido . E , a terceira razão é o alto custo para coletar e processar grande quantidade de informações . Basicamente , são três as abordagens para a SSA : embedded , filtro e wrapper . A Teoria de Rough Sets -- -RS -- -é uma abordagem matemática criada no início da década de 80 , cuja principal funcionalidade são os redutos , e será tratada neste trabalho . Segundo essa abordagem , os redutos são subconjuntos mínimos de atributos que possuem a propriedade de preservar o poder de descrição do conceito relacionado ao conjunto de todos os atributos . Neste trabalho o enfoque esta na abordagem filtro para a realização da SSA utilizando como filtro os redutos calculados através de RS . São descritos vários experimentos sobre nove conjuntos de dados naturais utilizando redutos , bem como outros filtros para SSA . Feito isso , os atributos selecionados foram submetidos a dois algoritmos simbólicos de AM . Para cada conjunto de dados e indutor , foram realizadas várias medidas , tais como número de atributos selecionados , precisão e números de regras induzidas . Também , é descrito um estudo de caso sobre um conjunto de dados do mundo_real  proveniente da área médica . O objetivo desse estudo pode ser dividido em dois focos : comparar a precisão dos algoritmos de indução e avaliar o conhecimento_extraído  com a ajuda do especialista . Embora o conhecimento_extraído  não apresente surpresa , pôde-se confirmar algumas hipóteses feitas anteriormente pelo especialista utilizando outros métodos . Isso mostra que o Aprendizado de Máquina também pode ser visto como uma contribuição para outros campos científicos 
 Em diversas indústrias de manufatura ( por exemplo , papeleira , moveleira , metalúrgica , têxtil ) as decisões do dimensionamento de lotes interagem com outras decisões do planejamento e programação da produção , tais como , a distribuição , o processo de corte , entre outros . Porém , usualmente , essas decisões são tratadas de forma isolada , reduzindo o espaço de soluções e a interdependência entre as decisões , elevando assim os custos totais . Nesta tese , estudamos o processo produtivo de indústrias de móveis de pequeno_porte  , que consiste em cortar placas grandes disponíveis em estoque para obter diversos tipos de peças que são processadas posteriormente em outros estágios e equipamentos com capacidades limitadas para , finalmente , comporem os produtos demandados . Os problemas de dimensionamento de lotes e corte de estoque são acoplados em um modelo de otimização_linear  inteiro cujo_objetivo  é minimizar os custos de produção , estoque de produtos , preparação de máquinas e perda de matéria-prima . Esse modelo mostra o compromisso existente entre antecipar ou não a fabricação de certos produtos aumentando os custos de estoque , mas reduzindo a perda de matéria-prima ao obter melhores combinações entre as peças . O impacto da incerteza da demanda ( composta pela carteira de pedidos e mais uma quantidade extra estimada ) foi amortizado pela estratégia de horizonte de planejamento rolante e por variáveis de decisão que representam uma produção extra para a demanda esperada no melhor momento , visando a minimização dos custos totais . Dois métodos heurísticos são desenvolvidos para resolver uma simplificação do modelo matemático proposto , o qual possui um alto grau de complexidade . Os experimentos computacionais realizados com exemplares gerados a partir de dados reais coletados em uma indústria de móveis de pequeno_porte  , uma análise dos resultados , as conclusões e perspectivas para este trabalho são 
 Neste trabalho consideramos um problema da Antropologia . A modelagem de sociedades e casamentos de indivíduos é feita com grafos mistos e encontrar caminhos disjuntos é uma questão central no problema . O problema é NP-completo e , quando visto como um problema parametrizado , ele é W [ 1 ] -difícil . Alguns subproblemas que surgem durante o processo de obter uma solução para o problema , envolvem caminhos disjuntos e podem ser resolvidos em tempo polinomial . Implementamos algoritmos polinomiais que são usados em uma ferramenta desenvolvida para solucionar o problema na Antropologia considerado . Nossa solução funcionou bem para as sociedades fornecidas pelos nossos parceiros 
 Recentemente surgiram dispositivos sensores de profundidade capazes de capturar textura e geometria de uma cena em tempo real . Com isso , diversas técnicas de Visão Computacional , que antes eram aplicadas apenas a texturas , agora são passíveis de uma reformulação , visando o uso também da geometria . Ao mesmo tempo em que tais algoritmos , tirando vantagem dessa nova tecnologia , podem ser acelerados ou tornarem-se mais robustos , surgem igualmente diversos novos desafios e problemas interessantes a serem enfrentados . Como exemplo desses dispositivos podemos citar o do Projeto Vídeo 4D , do IMPA , e o Kinect ( TM ) , da Microsoft . Esses equipamentos fornecem imagens que vêm_sendo  chamadas de RGBD , fazendo referência aos três canais de cores e ao canal adicional de profundidade ( com a letra 'D ' vindo do termo depth , profundidade em inglês ) . A pesquisa descrita nesta tese apresenta uma nova abordagem não-supervisionada para a estimação de movimento a partir de vídeos compostos por imagens RGBD . Esse é um passo intermediário necessário para a identificação de componentes rígidos de um objeto articulado . Nosso método faz_uso  da técnica de casamento inexato ( homomorfismo ) entre grafos para encontrar grupos de pixels ( blocos ) que se movem para um mesmo sentido em quadros consecutivos de um vídeo . Com o intuito de escolher o melhor casamento para cada bloco , é minimizada uma função custo que leva em conta distâncias tanto no espaço de cores RGB quanto no XYZ ( espaço tridimensional do mundo ) . A contribuição metodológica consiste justamente na manipulação dos dados de profundidade fornecidos pelos novos dispositivos de captura , de modo que tais dados passem a integrar o vetor de características que representa cada bloco nos grafos a serem casados . Nosso método não usa quadros de referência para inicialização e é aplicável a qualquer vídeo que contenha movimento paramétrico por partes . Para blocos cujas dimensões causem uma relativa diminuição na resolução das imagens , nossa aplicação roda em tempo real . Para validar a metodologia_proposta  , são apresentados resultados envolvendo diversas classes de objetos com diferentes_tipos  de movimento , tais como vídeos de pessoas caminhando , os movimento de um braço e um casal de dançarinos de samba de gafieira . Também são apresentados os avanços obtidos na modelagem de um sistema de vídeo 4D orientado a objetos , o qual norteia o desenvolvimento de diversas aplicações a serem desenvolvidas na continuação deste trabalho 
 A programação concorrente é cada vez mais utilizada nos sistemas atuais com o objetivo de reduzir custos e obter maior eficiência no processamento . Com a importância da programação concorrente é imprescindível que programas que implementam esse paradigma apresentem boa_qualidade  e estejam livres de defeitos . Assim , diferentes técnicas e critérios de teste vêm_sendo  definidos para apoiar a validação de aplicações desenvolvidas nesse paradigma . Nesse contexto , a geração_automática  de dados de teste é importante , pois permite reduzir o custo na geração e seleção de dados relevantes . O uso de técnicas meta-heurísticas tem sido uma área de grande interesse entre os pesquisadores para geração de dados , pois essas técnicas apresentam abordagens aplicáveis a problemas complexos e de difícil solução . Considerando esse aspecto , este trabalho apresenta uma abordagem de geração_automática  de dados para o teste_estrutural  de programas concorrentes em MPI ( Message Passing Interface ) . A meta-heurística usada foi Algoritmo Genético em que a busca é guiada por critérios de teste que consideram características implícitas de programas concorrentes . O desempenho da abordagem foi avaliado por meio da cobertura dos dados detestes , da eficácia em revelar defeitos e do custo de execução . Para comparação , a geração aleatória foi considerada . Os resultados indicaram que é promissor usar geração de dados de teste no contexto de programas concorrentes , com resultados interessantes em relação à eficácia e cobertura dos requisitos de teste 
 Muitos problemas de otimização em grafos , em especial problemas métricos , são mais fáceis de resolver em árvores . Portanto , uma estratégia para obter um bom algoritmo para certos problemas é obter uma árvore que aproxime o grafo , e utilizar uma solução do problema nessa árvore como uma solução aproximada para o problema no grafo original . Neste trabalho é estudada a técnica de Fakcharoenphol , Rao e Talwar , que mostraram como aproximar uma métrica finita arbitrária com n pontos por uma métrica numa árvore com distorção esperada O ( lg n ) -- o ótimo assintótico . Essa estratégia resulta em algoritmos de aproximação com boas razões de aproximação , e em algoritmos com bom fator de competitividade para diversos problemas de otimização online e distribuídos . É apresentada especificamente a aplicação da técnica ao problema do emparelhamento mínimo bipartido online , que ilustra como a aproximação de métricas auxilia na resolução de um problema e os cuidados que devem ser tomados nessa aplicação 
 Testes educacionais possibilitam a obtenção de medidas e resultados , a realização de análises e o estabelecimento de objetivos para os processos de ensino e a aprendizagem , além de subsidiarem processos seletivos e políticas públicas . A avaliação de desempenho dos examinados pode considerar uma única ou múltiplas habilidades e/ou competências . Como alternativa para testes via lápis e papel , o Teste Baseado em Computador ( CBT ) pode compor , aplicar e corrigir testes e produzir estatísticas individuais ou do grupo de examinados automaticamente . Considerando que o examinado possua múltiplas habilidades , o Teste Adaptativo baseado na Teoria de Resposta ao Item Multidimensional ( MCAT ) mantém a mesma acurácia de um teste tradicional , baseando-se no conhecimento do examinado a partir do histórico de itens anteriormente respondidos . A seleção de itens por Kullback Leibler entre Posteriores Subsequentes ( 'K_POT  . p ' ) evita selecionar um item difícil para um examinado com baixa habilidade , sugerindo que 'K_POT  . p ' é um critério aplicável em testes educacionais . A revisão da literatura apontou para : ( i ) a carência de estudos para o critério 'K_POT  . P ' , ( ii ) a carência de estudos com MCATs operacionais em contextos educacionais para usuários reais , ( iii ) a carência de estudos e propostas de critérios iniciais e de parada para MCATs , quando o número de itens administrados pelo teste é variável , e ( iv ) a ausência de trabalhos brasileiros na área de MCATs . Diante das lacunas apresentadas , esta tese de doutoramento trata da seguinte questão de pesquisa : Qual a abordagem para viabilizar o uso do critério KP em MCATs operacionais para contextos educacionais , que permita que o sistema implementado seja aprovado nos critérios de funcionalidade , confiabilidade , eficiência , manutenibilidade e portabilidade da ISO-9126 , que é a base para avaliar testes computadorizados ? Os objetivos específicos desta pesquisa foram os seguintes : ( i ) implementar e validar o critério de seleção 'K_POT  . P ' , comparando-o com o critério bayesiano usual , ( ii ) propor melhorias e calcular o tempo computacional de processamento da seleção de itens por 'K_POT  . P ' , ( iii ) propor critérios iniciais consistentes com a realidade e a necessidade das avaliações educacionais , ( iv ) validar o critério de parada inédito KPIC , quando a intenção é se ter MCATs que administrem um número variável de itens para os examinados , ( v ) desenvolver uma arquitetura que viabilize a aplicação via Web de MCATs com usuários reais , ( vi ) discutir aspectos teóricos e metodológicos da nova abordagem CBMAT via prova de conceito , por meio da implementação do sistema MADEPT , que avalia examinados na perspectiva da avaliação diagnóstica , ( vii ) avaliar o MADEPT de acordo com as normas internacionais de produto de software ISO-9126 e apontar a factibilidade , a viabilidade , as dificuldades , as vantagens e as limitações do desenvolvimento CBMATs para o ambiente Web . A metodologia utilizada para responder a questão de pesquisa foi : ( i ) organizar e selecionar as teorias , os métodos , os modelos e os resultados inerentes a MCATs , ( ii ) expandir a equação de 'K_POT  . P ' , ( iii ) implementar o MCAT contemplando o critério de seleção 'K_POT  . P ' e a metodologia bayesiana para estimação e seleção de itens , ( iv ) validar estatisticamente 'K_POT  . P ' e KPIC , ( v ) implementar o CBMAT , contemplando o MCAT como um subsistema e ( vi ) avaliar o CBMAT via ISO-9126 . Os resultados deste trabalho são vários : ( i ) uma ampla revisão da literatura nas teorias/métodos/critérios necessários para a implementação computacional de MCATs , ( ii ) a reformulação da equação que expressa a seleção por 'K_POT  . P ' para implementação via linguagem de programação científica , ( iii ) os estudos de simulações do MCAT quando a seleção de itens é por 'K_POT  . P ' e o critério de parada por KPIC mostram que 'K_POT  . P ' é um critério adequado e indicado quando o objetivo é ter um teste com um número baixo e variável de itens administrados , mantendo um vício adequado e com alta acurácia na estimação da habilidade , ( iv ) o desenvolvimento de algoritmos inéditos para os critérios iniciais , ( v ) a validação de uma nova arquitetura que viabiliza a aplicação via Web de MCATs com usuários reais e ( vi ) a implementação e avaliação via ISO-9126 do sistema computacionalWeb MADEPT . Conclui-se que é possível desenvolver uma arquitetura que viabilize a aplicação viaWeb de MCATs com usuários reais , utilizando o critério de seleção 'K_POT  . P ' e critérios iniciais condizentes com as avaliações educacionais . Quando a intenção é aplicar MCATs em cenários reais , a seleção de itens por 'K_POT  . P ' combinado com o critério de parada KPIC proporcionam um teste mais curto e com mais acurácia do que aqueles que utilizam a metodologia bayesiana usual , e com um tempo computacional de processamento condizente com as características da abordagem 
 A área de Personalização de Conteúdo tem sido foco de pesquisas recentes em Ciências da Computação , sendo a segmentação automática de vídeos digitais em cenas uma linha importante no suporte à composição de serviços de personalização , tais como recomendação ou sumarização de conteúdo . Uma das principais abordagens de segmentação em cenas se baseia no agrupamento de tomadas relacionadas . Logo , para que esse processo seja bem sucedido , é necessário que as tomadas estejam bem representadas . Porém , percebe-se que esse tópico tem sido deixado em segundo plano pelas pesquisas relacionadas à segmentação . Assim , este trabalho tem o objetivo de desenvolver um método baseado nas características visuais dos quadros , que possibilite aprimorar a representação de tomadas de vídeos digitais e , consequentemente , contribuir para a melhoria do desempenho de técnicas de segmentação em cenas 
 geração numérica de malhas tomou-se uma importante ferramenta na obtenção de soluções numéricas de equações_diferenciais  parciais ( EDPs ) , em domínios físicos com geometrias complexas . Esta dissertação apresenta o Sistema Iterativo de Geração de Malha ( SIGEM ) . O SIGEM foi desenvolvido durante este trabalho e utiliza o método diferencial que emprega EDPs elípticas para a geração da malha . O SIGEM permite a geração de malha em domínios simplesmente conexos bidimensionais , com o controle do posicionamento dos pontos pelo usuário . Para mostrar uma aplicação típica do SIGEM , foram geradas malhas em domínios irregulares , onde foram resolvidas a equação de Laplace e a equação do calor 
 O presente_trabalho  tem como finalidade o ensino , e consistiu na implementação de urna classe de métodos de resolução numérica para equações_diferenciais  parciais que constam em `` Discretização de Equações Diferenciais Parciais : Técnica de Diferenças Finitas '' , Cuminato [ 7 ] . O software utilizado foi o MATLAB , e com ele foi desenvolvido um conjunto de programas que são acessados mediante uma interface padrão para entrada de dados e visualização de resultados . No que tange a visualização de resultados , o que foi desenvolvido permite ao usuário analisar os resultados , tanto através das aproximações obtidas como através dos gráficos das mesmas . Esses programas irão acompanhar o texto final de [ 7 ] para formar uma biblioteca que o acompanhará , para servir de apoio ao professor que vier a utilizá-lo 
 Esta dissertação descreve o processo de engenharia_reversa  do ASiA ( Ambiente de Simulação Automático ) , desenvolvida usando a Fusion-RE , que é uma abordagem para a engenharia_reversa  de sistemas implementados sem orientação a objetos , produzindo modelos de análise orientados a objetos . A dissertação também apresenta uma revisão bibliográfica cobrindo : modelagem e simulação de sistemas , o ASiA ( que é o ambiente simulação enfocado no trabalho ) e manutenção de software ( para justificar a escolha da engenharia_reversa  ) . A documentação produzida para a versão atual do ambiente e os modelos gerados para o futuro processo de reengenharia são incluídos . O processo utilizado para a aplicação da Fusion- RE ao ASiA , a proposta para sua reengenharia e sugestões de melhorias a serem implementadas também são discutidos 
 A criação de um ambiente para provimento de dados multimídia sobre uma rede de computadores , apresenta vários limitantes , sendo um deles a arquitetura do servidor responsável pela entrega de vídeo . Esse trabalho_investiga  , define e implementa os componentes que compõem um servidor de vídeo . Para contornar os problemas oriundos da precariedade dos protocolos TCP/IP e ethemet para a transmissão de mídia contínua , nota-se o aparecimento de várias tecnologias que possibilitam o uso de um sistema de armazenamento remoto de vídeo e sua apresentação local com razoáve| qualidade sem a utilização de sincronismo durante todo o processo ( p. ex . Vxtreme , Realvideo , CU-Seeme e o padrão JMF ) . Desta forma , a questão principal toma-se o provimento de largura de banda necessária para a aplicação . A maioria dos problemas inseridos pela rede e pelos sistemas operacionais são resolvidos através de esquemas avançados de bufferização . Como resultado desse trabalho , foi definido um sistema bem ajustado para o provimento de dados multimídia sobre uma rede de computadores , um sistema básico para o gerenciamento dos dados multimídia através do ambiente www , a avaliação de desempenho do sistema e detecção de possíveis gargalos do sistema . O trabalho mostra também algumas configurações possíveis de serem utilizadas para o serviço de armazenamento e distribuição de vídeo . os testes realizados mostram que é perfeitamente possível a implementação de sistemas de distribuição de vídeo via tecnologias de rede Intemet 
 Esta dissertação apresenta uma discussão geral sobre técnicas de avaliação de desempenho de sistemas computacionais , levando em consideração duas áreas principais : as técnicas que envolvem experimentação com os sistemas ( técnicas de aferição ) e a criação de abstrações para representar os sistemas ( técnicas de modelagem ) . O trabalho enfoca as técnicas de modelagem , especialmente as redes de fila , as redes de Petri e os statecharts . Algumas das abordagens preferidas para essas técnicas são discutidas na revisão bibliográfica mas , como essas técnicas são de propósito_geral  , elas podem ser aplicadas a qualquer sistema computacional . O objetivo principal é mostrar as vantagens e desvantagens na adoção de uma ou outra técnica , considerando as mesmas situações com diferentes modelos de sistemas . Uma extensão estocástica original para statecharts é proposta ( Stochastic Feature Charts - SFC ) , cujo_objetivo  é prover uma avaliação probabilistica , através da incorporação de cadeia de Markov nos statecharts . Assim , um sistema pode ser visualizado através de seus estados e transições entre esses estados . O objetivo principal é estender os statecharts e criar uma ferramenta com características similares àquelas encontradas em redes de fila e redes de Petri . A extensão proposta , SFC , é usada para avaliar o desempenho de um servidor de arquivos usado em rede local , considerando três modelos diferentes , com complexidade diferente . O comportamento das probabilidades é observado e os resultados obtidos são discutidos visando a validação da extensão dos statecharts proposta 
 Em meados dos anos 90 , alguns pesquisadores constataram a existência de certos interesses que , independente da técnica de programação utilizada ou da maneira como o sistema venha a ser decomposto , não se encaixam em módulos individuais , mas ficam espalhados por várias unidades do software ( também chamados de interesses_transversais  ) . A programação orientada a aspectos ( POA ) foi concebida como uma proposta de resolução desse problema , a partir do uso de mecanismos que permitem 
 isolamento dos interesses_transversais  . Entretanto , por ser uma técnica nova , nesses primeiros anos os pesquisadores preocuparam-se em estabelecer os conceitos e técnicas básicos das linguagens orientadas a aspectos , 
 para uma segunda fase a investigação de outras características do desenvolvimento de programas orientados a aspectos , como métodos de projeto 
 abordagens de teste . Nesta dissertação é apresentada uma abordagem de teste_estrutural  para programas orientados a aspectos baseados na linguagem AspectJ , que pode contribuir para o aumento da confiança no software 
 utilizando essa técnica e auxiliar o entendimento das novas construções e comportamentos envolvidos nesses programas . Modelos de fluxo de controle e de dados baseados no código-objeto resultante da 
 combinação de programas escritos na linguagem AspectJ são propostos , bem como nove critérios de teste baseados nesses modelos . Uma 
 desenvolvida para apoiar o teste_estrutural  de unidade de programas Java foi estendida para dar_apoio  aos modelos e critérios propostos nesta dissertação . Além_disso  , algumas propriedades do teste de integração de programas orientados a aspectos são discutidas teoricamente 
 O uso de técnicas e métodos formais contribui para o desenvolvimento de sistemas confiáveis . No entanto , apesar do rigor obtido , em geral , é necessário que essas técnicas sejam complementadas com atividades de teste e validação . Deve-se ressaltar que o custo para eliminar erros encontrados nas etapas iniciais de desenvolvimento é menor do que quando esses erros são encontrados nas fases posteriores . Dessa forma , é essencial a condução de atividades de VV & T - Verificação , Validação e Teste - desde as primeiras fases de desenvolvimento . Critérios de teste , como uma forma sistemática de avaliar e/ou gerar casos de teste de qualidade e , dessa forma , contribuir para aumentar a qualidade da atividade de teste , têm sido investigados para o teste de especificação de Sistemas Reativos 
 A técnica Redes de Petri Coloridas tem sido constantemente utilizada para a especificação do aspecto comportamental de Sistemas Reativos . Apesar de existirem diversas técnicas de análise , um aspecto não considerado é a cobertura alcançada , visto que , em geral , a aplicação exaustiva não é viável devido ao alto custo . Considerando a relevância do estabelecimento de métodos sistemáticos para o teste e validação dessas especificações , este trabalho propõe a aplicação do critério de teste Análise de Mutantes para o teste de Redes de Petri Coloridas 
 Neste trabalho foram almejados três objetivos principais , os quais podem ser divididos em estudos teóricos , estudos empíricos e automatização . No contexto de estudos teóricos , foi realizada a definição e embasamento teórico para possibilitar a aplicação da Análise de Mutantes no contexto de Redes de Petri Coloridas . Além_disso  , investigaram-se mecanismos genéricos para a descrição e geração de mutantes . Definiu-se um algoritmo para a geração de casos de teste baseado na Análise de Mutantes . No contexto de estudos empíricos , foram conduzidos estudos de caso para avaliar a aplicabilidade e eficácia dos resultados teóricos obtidos . Finalmente , no contexto de automatização , foram desenvolvidas ferramentas de apoio à aplicação da Análise de Mutantes 
 A atividade de teste de software tem recebido considerável atenção de pesquisadores e engenheiros de software que reconhecem a sua utilidade na criação de produtos de qualidade . No entanto , os testes são caros e propensos a erros , o que impõe a necessidade de sistematizar e , portanto , a definição de técnicas para aumentar a qualidade e produtividade na sua condução . Várias técnicas de teste têm sido desenvolvidas e têm sido utilizadas , cada um com características próprias em termos de eficácia , custo , fases de aplicação , etc . Sistemas de realidade virtual frequentemente utilizam uma estrutura hierárquica denominada grafo de cena para representar as características dos objetos em um ambiente virtual tridimensional . Os grafos de cena também armazenam informações sobre o relacionamento entre os objetos , permitindo respostas adequadas ao usuário quando ocorrem interações . Neste trabalho , critérios de teste baseados em grafo de cena são estudados e definidos afim de aumentar a qualidade de aplicações de realidade virtual . Além_disso  , estudos de caso são apresentados , utilizando os critérios definidos aplicados a um framework de realidade virtual construído para gerar aplicações na área médica , além de utilizar uma aplicação de demonstração . Como forma de apoio aos critérios definidos foi desenvolvida uma ferramenta de teste capaz de verificar se os nós , que representam os objetos virtuais na cena , satisfazem seus requisitos conforme foram 
 Com a evolução da tecnologia , novos dispositivos e técnicas de interação são desenvolvidas . Essas transformações criam desafios em termos de usabilidade e experiência do usuário . Essa pesquisa enfrenta alguns desafios para a entrada de dados e exploração de conteúdo em cenários com restrições . Não foi intenção da pesquisa investigar todos os possíveis cenários , mas sim a exploração em profundidade de uma ampla gama de dispositivos e restrições . Ao todo cinco cenários são investigados . Primeiramente é apresentada uma discussão_sobre  o uso de uma mesa de centro interativa para a exploração de fotos e vídeos pessoais , a qual também considera um aparelho de TV como tela adicional . Com base no segundo cenário , uma arquitetura que oferece a aplicações de TV digital interativa ( TVDI ) a possibilidade de receber dados multimodais de múltiplos dispositivos é apresentada . O terceiro cenário se concentra no suporte a entrada de texto para aplicações de TVDI usando o controle remoto , resultando na apresentação de um modelo de interface baseado em múltiplos modos de entrada como solução . Os dois últimos cenários permitem continuar a investigação por melhores formas de entrada de texto , porém , a restrição se torna a impossibilidade de usar as mãos , um dos desafios enfrentados por indivíduos com deficiência motora severa . No primeiro deles , são apresentados um método de entrada de texto baseado em dois símbolos de entrada e uma técnica de interação baseada na detecção de rotações do pé apoiado sobre o calcanhar usando acelerômetro , para aqueles que mantêm pelo menos um movimento parcial de uma perna e um pé . No senário seguinte , apenas os movimentos dos olhos são exigidos . Foi apresentada uma técnica de escrita com o olho que reconhece a palavra desejada ponderando o comprimento de a frequência de ocorrência de todas as palavras que podem ser formadas filtrando letras excedentes da lista de letras olhadas pelo usuário . A exploração de cada cenário em profundidade foi importante para a obtenção de resultados e contribuições relevantes . Por outro_lado  , o amplo escopo da dissertação permitiu ao estudante o aprendizado de diversas técnicas e tecnologias 
 A navegação_autônoma  é um dos problemas fundamentais na área de robótica_móvel  . Esse problema vem_sendo  pesquisado nessa área por décadas e ainda apresenta um grande potencial para pesquisas científicas . A maior_parte  dos algoritmos e soluções desenvolvidas nessa área foi concebida para que robôs operem em ambientes estruturados . No entanto , outra questão de grande interesse para pesquisadores da área é a navegação em ambientes externos . Em ambientes não estruturado os veículos_autônomos  ( robôs de grande porte ) devem ser capazes de desviar de obstáculos , que eventualmente apareçam no caminho . Esta dissertação aborda o desenvolvimento de um sistema inteligente capaz de gerar e executar um planejamento de caminho para o estacionamento de veículos_autônomos  em ambientes semi-estruturados . O sistema é capaz de reconhecer vagas de estacionamento por meio de sensores instalados no veículo , gerar uma trajetória válida que o conduza até a vaga e enviar os comandos de esterçamento e aceleração que guiam o veículo pelo caminho 
 Este trabalho aborda o problema de modelagem de séries de vazões médias mensais visando previsão e geração de séries sintéticas . Destaca-se que a importância da previsão de valores futuros das séries mensais de vazões assim como a geração de séries sintéticas são fundamentais para o planejamento da operação de sistemas hidroelétricos brasileiros . Estas séries possuem um comportamento periódico na média , na variância e na função de autocorrelação e , portanto , considera-se para a série padronizada os modelos autorregressivos periódicos PAR ( pm ) . Em relação a previsão clássica , a análise do erro de previsão e feita em função do horizonte de previsão . Neste estudo , os erros de previsão são calculados , na escala original da série de vazão , em função dos parâmetros dos modelos ajustados e avaliados para horizontes de previsão h variando de 1 a 12 meses . Estes erros são comparados com as estimativas das variâncias das vazões para o mês que está sendo previsto . Em relação à previsão bayesiana , adota-se os modelos Normal , Log-Normal e t-Student nos processos de estimação e , após , é realizado um estudo da perfomance destes modelos usando o erro quadrático médio , erro absoluto médio e erro percentual absoluto médio . Em relação à geração de séries sintéticas de vazões , um modelo multivariado Log-Normal com três parâmetros e um modelo Log-Normal generalizado foram desenvolvidos . As séries geradas são comparadas com as séries históricas reais utilizando o critério de Kullback-Leibler . Como resultado , tem-se uma avaliação da capacidade de previsão , em meses , dos modelos ajustados para cada mês e a escolha do modelo Log-Normal nos procedimentos de análise_bayesiana  . Além_disso  , o modelo utilizado para a geração de séries sintéticas de vazões mensais forneceu evidências que o apontam como uma alternativa ao modelo amplamente adotado no setor elétrico brasileiro para geração de séries de 
 Neste trabalho é apresentada uma revisão do método simplex com geração de colunas e sua aplicação ao problema de corte de estoque . É apresentado o problema combinado , que acopla 
 problemas de dimensionamento de lotes e de corte de estoque , incluindo uma formulação_matemática  deste problema . Em seguida consideramos algumas propriedades da matriz de restrições e 
 construir uma base esparsa para ela , utilizando um reordenamento estático das colunas básicas . Resultados_numéricos  de uma implementação em MATLAB que realiza trocas de colunas da base 
 verifica sua esparsidade , simulando o método simplex são apresentados . Após uma troca de colunas básicas , estas são atualizadas de forma eficiente , de modo que cause o menor preenchimento da matriz . Foram realizados também testes computacionais para verificar a robustez do método , através de operações inversas à decomposição e comparação com as colunas originais . Concluímos que a proposta de construção da base estática esparsa leva a bons_resultados  computacionais com relação à velocidade e robustez em comparação com abordagens que não consideram a estrutura esparsa da matriz de restrições 

 A mineração de dados é um processo computacionalmente caro , que se apoia no pré-processamento dos dados para aumentar a sua eficiência . As técnicas de redução de elementos do conjunto de dados , principalmente a amostragem de dados se destacam no pré-processamento . Os dados reais são caracterizados pela não uniformidade da distribuição , grande quantidade de atributos e presença de elementos considerados ruídos . Para esse tipo de dado , a amostragem uniforme , na qual cada elemento tem a mesma probabilidade de ser escolhido , é inefiiente . Os dados nos últimos_anos  , vem passando por transformações . Assim , não só o seu volume tem aumentado significantemente , mas também a maneira de como eles são representados . Os dados usualmente são divididos apenas em dados tradicionais ( número e pequenas cadeias de caracteres ) e dados complexos ( imagens , cadeias de DNA , vídeos , etc ) . Entretanto , uma representação mais rica , na qual não só os elementos do conjunto são representados mas também a suas ligações , vem_sendo  amplamente utilizada . Esse novo tipo de dado , chamado rede complexa , fez surgir uma nova área de pesquisa chamada mineração de redes complexas ou de grafos , já que estes são utilizados na representação das redes complexas . Para esta nova área é necessário o desenvolvimento de técnicas que permitam a mineração de grandes redes complexas , isto é , redes com centenas de milhares de elementos ( nós ) e ligações ( arestas ) . Esta tese teve como objetivo explorar a redução de elementos em conjuntos de dados chamados desbalanceados , isto é , que possuem agrupamentos ou classes de tamanhos bastantes distintos , e que também possuam alta quantidade de atributos e presença de ruídos . Além_disso  , esta tese também explora a mineração de redes complexas com a extração de padrões e propriedades e o desenvolvimento de algoritmos eficientes para a classificação das redes em reais e sintéticas . Também é proposto a mineração de redes complexas utilizando gerenciadores de base de dados para a mineração de cliques de tamanho 4 e 5 e a apresentação da extensão do coeficiente de 
 Esta tese tem como objetivo o projeto e implementação de uma arquitetura orientada a serviços , denominada WSARCH - Web_Services  Architecture , que possibilita acesso a Web_Services  com Qualidade de Serviço ( QoS ) . Os atributos de QoS que devem ser considerados , visando à avaliação de desempenho deWeb services e a obtenção de QoS em uma arquitetura orientada a serviços , são identificados e discutidos . Esses atributos de QoS são mapeados para os componentes participantes de uma arquitetura orientada a serviços que incorpora qualidade de serviço . A arquitetura proposta prevê a monitoração dos provedores de serviços e um módulo que utiliza os dados obtidos para a localização do serviço apropriado . Visando a validação da arquitetura proposta e dos atributos definidos desenvolveu-se um protótipo da WSARCH . O protótipo desenvolvido permite que estudos de avaliação de desempenho sejam realizados considerando os diferentes componentes da arquitetura , algoritmos , protocolos e padrões . A proposta da WSARCH se insere em um contexto em que é importante definir como deve ser projetada uma arquitetura SOA com foco em desempenho , uma vez que a correta caracterização do que avaliar , e como avaliar , se faz necessário . Nesta tese , a avaliação de desempenho está focada nas diferentes entidades que participam de uma arquitetura orientada a serviço : cliente , provedor e os demais 
 A robótica_móvel  autônoma é uma área relativamente recente que tem como objetivo a construção de mecanismos capazes de executar tarefas sem a necessidade de um controlador humano . De uma forma geral , a robótica_móvel  defronta com três problemas fundamentais : mapeamento de ambientes , localização e navegação do robô . Sem esses elementos , o robô dificilmente poderia se deslocar autonomamente de um lugar para outro . Um dos problemas existentes nessa área é a atuação de robôs_móveis  em ambientes externos como parques e regiões urbanas , onde a complexidade do cenário é muito maior em comparação aos ambientes internos como escritórios e casas . Para exemplificar , nos ambientes externos os sensores estão sujeitos às condições climáticas ( iluminação do sol , chuva e neve ) . Além_disso  , os algoritmos de navegação dos robôs nestes ambientes devem tratar uma quantidade bem maior de obstáculos ( pessoas , animais e vegetações ) . Esta dissertação apresenta o desenvolvimento de um sistema de classificação da navegabilidade de terrenos irregulares , como por exemplo , ruas e calçadas . O mapeamento do cenário é realizado através de uma plataforma robótica equipada com um sensor laser direcionado para o solo . Foram desenvolvidos dois algoritmos para o mapeamento de terrenos . Um para a visualização dos detalhes finos do ambiente , gerando um mapa de nuvem de pontos e outro para a visualização das regiões próprias e impróprias para o tráfego do robô , resultando em um mapa de navegabilidade . No mapa de navegabilidade , são utilizados métodos de aprendizado de máquina supervisionado para classificar o terreno em navegável ( regiões planas ) , parcialmente navegável ( grama , casacalho ) ou não navegável ( obstáculos ) . Os métodos empregados foram , redes neurais artificais e máquinas de suporte vetorial . Os resultados de classificação obtidos por ambos foram posteriormente comparados para determinar a técnica mais apropriada para desempenhar esta 
 A área de sistemas multiagentes é um promissor domínio tecnológico para uso em performances musicais interativas . Em trabalhos recentes , essa tecnologia vem_sendo  utilizada para resolver problemas musicais de escopo específico e alcance limitado , como a detecção de pulsação , a simulação de instrumentos e o acompanhamento musical automático . Neste trabalho , apresentamos uma taxonomia desses sistemas multiagentes musicais e uma arquitetura e implementação de um arcabouço computacional que generaliza os trabalhos anteriores e aborda problemas usuais como a sincronização em tempo real , a comunicação sonora e a mobilidade espacial dos agentes . Através do arcabouço , um usuário pode desenvolver um sistema multiagente musical focado em suas necessidades musicais , enquanto deixa grande parte dos problemas técnicos a cargo do arcabouço . Para validar o arcabouço , implementamos e discutimos dois estudos de caso que exploram diversos aspectos de um sistema multiagente musical , como a comunicação simbólica , a troca de áudio digital , o uso de trajetórias espaciais , a simulação acústica e conceitos de vida artificial , como códigos genéticos e reprodução , demonstrando a usabilidade do arcabouço em uma grande variedade de aplicações musicais 
 Apesar do crescente reconheciimento da importância de Sistemas de Sistemas ( SoS ) ainda não há um consenso sobre o que eles são um para que princípios devem ser construídos . De fato , existem várias definições de SoS na literatura . A dificuldade de especificar quais são os sistemas constituintes , as suas tarefas e como eles irão realizá-las frequentemente conduzem iniciativas de SoS ao completo fracasso . Guiados por um exemplo que inclui todas as características distintas de um SoS e um processo genérico de engenharia de SoS ( SoSE ) , esta tese explora o desenvolvimento de SoS a partir de diferentes perspectivas da engenharia de software ( SE ) , que incluem requisitos , análise , projeto e reengenharia . Para a engenharia de requisitos ( RE ) é proposta uma abordagem para descrever progressivamente um SoS como um arranjo de comportamentos mais simples , porém significativos , denominados 'cenas ' . O objetivo é facilitar a descrição e o entendimento do SoS e seu dinamismo . Para a análise , propõe-se as extensões de statecharts para melhorar a modelagem das interações entre sistemas . Elas são notações simbólicas que resultam de uma analogia com placas de circuito impresso multi camadas ( PCB ) . Os diagramas resultantes são denominados PCB-statecharts . Para o projeto , é proposta uma extensão para o processo convencional de engenharia de linha de produtos ( SPLE ) , de tal forma que linhas de produto ( SPL ) possam se tornar uma fonte natural de membros para SoS . A engenharia de domínio é estendida para prover componentes capazes de compartilhar habilidades em ambientes de SoS . Desta forma , engenheiros de aplicação podem projetar famílias de produtos complacentes com diferentes requisitos de SoS e ainda melhorar seus produtos usando habilidades de outros membros de um SoS . Para a reengenharia propõe-se extensão de uma abordagem existente para evoluir legados para SPL e depois para membros de um SoS . O objetivo é demonstrar que quando sistemas legados são tratados apropriadamente , eles podem compartilhar habilidades úteis , trabalhar de maneira cooperativa e compor 
 Em 2009 , N. Heninger e H. Shacham apresentaram um algoritmo de reconstrução que permite recuperar a chave secreta sk do criptossistema RSA básico em tempo polinomial tendo em forma aleatória 27 % dos seus bits . Sabemos que podemos obter uma versão com erros ( bits modicados ) da chave secreta RSA graças aos ataques cold boot . O algoritmo apresentado por Heninger-Shacham corrige esses erros fazendo_uso  das relações matemáticas que existe entre as chaves pública e secreta do criptossistema RSA básico . O objetivo deste trabalho é estudar esse algoritmo para implementar e analisar seu análogo para o criptossistema RSA multi-primo . Os resultados obtidos_mostram  que para reconstruir a chave secreta sk do criptossistema RSA u-primos é preciso ter uma fração de bits corretos maior a 2 - 2^ ( ( u+2 ) / ( 2u+1 ) ) , mostrando assim que a segurança oferecida pelo criptossistema RSA multi-primo ( u > / 3 ) é maior com relação ao criptossistema RSA básico ( u = 2 ) 
 O ensino e a aprendizagem do tema hierarquia de memória não são tarefas simples , pois muitos assuntos que são abordados em teoria podem desmotivar a aprendizagem em virtude de sua complexidade . Este projeto de mestrado apresenta a transformação do módulo de memória cache da ferramenta Amnesia em um objeto de aprendizagem , que visa facilitar a construção do conhecimento através da simulação da estrutura e da funcionalidade da hierarquia de memória na arquitetura von Neumann de uma maneira mais prática e didática . Este processo permitiu que funcionalidades existentes na ferramenta fossem readequadas e novas_funcionalidades  desenvolvidas . Aliado a isso , planos de aula e questionários de avaliação e usabilidade também foram concebidos , validados e implementados junto à elaboração de um tutorial para descrever o funcionamento do novo objeto . Os estudos_experimentais  realizados analisaram dois aspectos : o primeiro , se o objeto de aprendizagem melhorou , de fato , a aprendizagem dos alunos no assunto memória cache ; o segundo , a opinião dos alunos em relação à utilização do objeto . Após a análise e avaliação dos resultados obtidos nos experimentos , foi possível demonstrar uma evolução na aprendizagem quando se fez o uso do objeto , além de se perceber que a motivação dos alunos em utilizar outros objetos de aprendizagem aumentou 
 Este trabalho introduz os conceitos de controlabilidade fraca e estabilizabilidade 
 para sistemas_lineares  com parâmetros sujeitos a saltos Markovianos a tempo discreto . É 
 inicialmente , construída uma coleção de matrizes C que se assemelha às matrizes de 
 de sistemas_lineares  deterministicos . Essa coleção de matrizes C nos 
 definir um conceito de controlabilidade fraca , requerendo que elas sejam de posto completo 
 assim como introduzir um conceito de estabilizabilidade fraca , dual ao conceito 
 detetabilidade fraca encontrado na literatura de sistemas com saltos de Markov . Uma 
 importante do conceito de estabilizabilidade fraca é a de generalizar o 
 de estabilizabilidade na média quadrática , anteriormente encontrado na literatura 
 O papel do conceito da estabilizabilidade fraca no problema de filtragem é 
 através de casos de estudo . Estes casos de estudo são desenvolvidos no contexto do 
 de Kalman com observação do parâmetro de Markov e sugerem que a 
 fraca em conjunto com a detetabilidade na média quadrática garantem que o 
 seja estável na média quadrática 
 Este trabalho analisa o impacto causado no desempenho de uma simulação distribuída quando técnicas de particionamento convencionais são empregadas . Essas técnicas não levam em conta informações inerentes ao estado da simulação . Pelo fato da execução de uma simulação também estar sujeita a sofrer interferências da plataforma , informações sobre a potência computacional de cada recurso utilizado e sobre o tipo de simulação , podem ser aplicadas em seu particionamento . Foram utilizadas informações estáticas , geradas através da avaliação da plataforma com benchmarks , e dinâmicas , obtidas_através  de índices de carga . Os resultados obtidos da utilização destas técnicas se mostram atrativos , principalmente quando o objetivo é a execução das simulações em ambientes que não disponibilizam políticas de escalonamento específicas e sim políticas convencionais . Nos estudos de casos avaliados , observaram-se ganhos satisfatórios , como a redução de até 24 % do tempo de execução , um aumento de até 22 % de eficiência e 79 % menos rollbacks causados . Percebe-se que dependendo do tempo que se dispõe e dos objetivos pretendidos , as técnicas convencionais podem ser empregadas em simulações distribuídas . Este trabalho também contribui com o aperfeiçoamento das duas ferramentas utilizadas : WARPED e AMIGO . Uma interface de comunicação entre as duas ferramentas foi desenvolvida , ampliando assim seus campos de utilização 
 Esta dissertação apresenta uma proposta de um simulador de modelos de larga_escala  para o Ambiente de Simulação Distribuída Automático ( ASDA ) , uma ferramenta que facilita a utilização e o desenvolvimento de simulação distribuída e que vem_sendo  objeto de pesquisas e estudos no Laboratório de Sistemas_Distribuídos  e Programação Concorrente ( LaSDPC ) do ICMC-USP . Tal simulador permite ao ASDA a construção de modelos e programas que simulam modelos de redes de filas de larga_escala  , operações estas que tornam a ferramenta ainda mais completa . O simulador é baseado no padrão público para simulação distribuída de larga_escala  denominado Scalable Simulation Framework ( SSF ) . O protótipo do simulador desenvolvido é constituído de um programa cliente-servidor , mas podem ser observados três componentes principais : um compilador , que traduz os modelos escritos em linguagem de modelagem para linguagem C++ ; o módulo SSF que define a API utilizada pelos programas de simulação ; e um módulo de execução , que executa os programas de simulação , analisa os resultados e os repassa para um gerador de relatórios . O simulador contribui ainda com mais estudos acerca de simulação , simulação distribuída e modelagem de sistemas utilizando as ferramentas desenvolvidas pelo 
 Este trabalho de mestrado insere-se no contexto do projeto de uma Ferramenta Inteligente de Apoio à Pesquisa ( FIP ) , sendo desenvolvida no Laboratório de Inteligência Computacional do ICMC-USP . A ferramenta foi proposta para recuperar , organizar e minerar grandes_conjuntos  de documentos científicos ( na área de computação ) . Nesse contexto , faz-se necessário um repositório de artigos para a FIP . Ou seja , um Data Warehouse que armazene e integre todas as informações extraídas dos documentos recuperados de diferentes páginas pessoais , institucionais e de repositórios de artigos da Web . Para suportar o processamento analítico on-line ( OLAP ) das informações e facilitar a ? mineração ? desses dados é importante que os dados estejam armazenados apropriadamente . Dessa forma , o trabalho de mestrado teve como objetivo principal projetar um Data Warehouse ( DW ) para a ferramenta FIP e , adicionalmente , realizar experimentos com técnicas de mineração e Aprendizado de Máquina para automatizar o processo de indexação das informações e documentos armazenados no data warehouse ( descoberta de tópicos ) . Para as consultas multidimensionais foram construídos data marts de forma a permitir aos pesquisadores avaliar tendências e a evolução de tópicos de 
 Com o aumento crescente das capacidades dos circuitos integrado e conseqüente complexidade das aplicações , em especial as embarcadas , um requisito tem se tornado fundamental no desenvolvimento desses sistemas : ferramentas de desenvolvimento cada vez mais acessíveis aos engenheiros , permitindo , por exemplo , que um programa escrito em linguagem C possa ser convertido diretamente em hardware . Os FPGAs ( Field Programmable Gate Array ) , elemento fundamental na caracterização de computação reconfigurável , é um exemplo desse crescimento , tanto em capacidade do CI como disponibilidade de ferramentas . Esse projeto teve como objetivos : estudar algumas ferramentas de conversão C , C++ ou Java para hardware_reconfigurável  ; estudar benchmarks a serem executadas nessas ferramentas para obter desempenho das mesmas , e ter o domínio dos conceitos na conversão de linguagens de alto nível para hardware_reconfigurável  . A plataforma utilizada no projeto foi a da empresa Xilinx XUP 
 Atualmente , existem muitos estudos para agregar mais valor às informações disponíveis na Web visando melhorar os resultados da interação dos usuários com a Web ; uma das linhas de estudo é a Web Semântica , que propõe a adição de informação semântica à Web atual por meio de ontologias . A organização internacional que define os padrões para a Web ( W3C ) já propôs vários padrões para tornar a Web Semântica viável , porém , além de padrões , também é preciso criar ou adaptar ferramentas que explorem as suas potencialidades . Uma ferramenta que dá um suporte significativo para a Web atual e que pode ser adaptada para trabalhar com a Web Semântica é o Servidor de Aplicações . Com adição de informações_semânticas  , na forma de ontologias , tem-se um Servidor de Aplicações Baseado em Ontologias ( OBAS ) . Neste trabalho foi desenvolvido um sistema protótipo para oferecer as características mínimas de um OBAS , e desta forma , foram investigadas as tecnologias para a Web Semântica que viabilizassem uma solução de acordo com os padrões recomendados pela W3C . Os componentes de software de um OBAS têm suas propriedades e comportamentos relacionados de forma semântica usando-se ontologias . Como uma ontologia é um modelo_conceitual  explícito , suas descrições dos componentes podem ser consultadas e inferidas , melhorando o desempenho do servidor através da combinação dos componentes mais apropriados a uma tarefa , da simplificação da programação , pois não é mais necessário saber todos os detalhes de um componente para 
 Exploramos o seguinte problema : dadas duas sequências X e Y sobre um alfabeto finito , encontre uma subsequência comum máxima de X e Y sem símbolos repetidos . Estudamos a estrutura deste problema , particularmente do ponto de vista de grafos e de combinatória poliédrica . Desenvolvemos algoritmos de aproximação e heurísticas para este problema . O enfoque deste trabalho está na construção de um algoritmo baseado na técnica branch-and-cut , aproveitando-nos de um algoritmo de separação eficiente e de heurísticas e técnicas para encontrarmos uma solução ótima mais cedo . Também estudamos um problema mais fácil no qual este problema é baseado : dadas duas sequências X e Y sobre um alfabeto finito , encontre uma subsequência comum máxima de X e Y. Exploramos este problema do ponto de vista de combinatória poliédrica e descrevemos vários algoritmos conhecidos para resolvê-lo 
 O hipocampo é uma estrutura cerebral que possui importância primordial para o sistema de memória humana . Alterações no seus tecidos levam a doenças neurodegenerativas , tais como : epilepsia , esclerose múltipla e demência , entre outras . Para medir a atrofia do hipocampo é necessário isolá-lo do restante do cérebro . A separação do hipocampo das demais partes do cérebro ajuda aos especialistas na análise e o entendimento da redução de seu volume e detecção de qualquer anomalia presente . A extração do hipocampo é principalmente realizada de modo manual , a qual é demorada , pois depende da interação do usuário . A segmentação automática do hipocampo é investigada como uma alternativa para contornar tais limitações . Esta dissertação de mestrado apresenta um novo método de segmentação automático , denominado Modelo de Nuvem de Similaridade ( Similarity Cloud Model - SimCM ) . O processo de segmentação é dividido em duas etapas principais : i ) localização por similaridade e ii ) ajuste de nuvem . A primeira operação utiliza a nuvem para localizar a posição mais provável do hipocampo no volume destino . A segunda etapa utiliza a nuvem para corrigir o delineamento final baseada em um novo método de cálculo de readequação dos pesos das arestas . Nosso método foi testado em um conjunto de 235 MRI combinando imagens de controle e de pacientes com epilepsia . Os resultados alcançados indicam um rendimento superior tanto em efetividade ( qualidade da segmentação ) e eficiência ( tempo de processamento ) , comparado com modelos baseados em grafos e com modelos Bayesianos . Como trabalho futuro , pretendemos utilizar seleção de características para melhorar a construção da nuvem e o delineamento dos 
 Duas tarefas que desenvolvedores de software constantemente fazem são escrever código fácil de ser mantido e evoluído , e detectar pedaços de código problemáticos . Para a primeira tarefa , desenvolvedores comumente fazem_uso  de conhecidos padrões arquiteturais , como Model-View-Controller ( MVC ) . Para a segunda tarefa , desenvolvedores fazem_uso  de métricas de código e estratégias de detecção de maus_cheiros  de código ( code smells ) . No entanto , até o momento , métricas de código e estratégias de detecção de maus_cheiros  de código não levam em conta a arquitetura do software em análise . Isso significa que todas classes são avaliadas como se umas fossem iguais às outras . Na prática , sabemos que classes são diferentes em suas responsibilidades e implementação , e portanto , esperamos que elas variem em termos de acoplamento , coesão e complexidade . Por exemplo , em um sistema MVC , Controladores são responsáveis pelo fluxo entre a camada de Modelo e a camada de Visão , e Modelos representam a visão de negócios do sistema . Nesta tese , nós avaliamos o impacto dos papéis arquiteturais em técnicas de medição de métricas de código e de detecção de maus_cheiros  de código . Nós realizamos um estudo empírico em 120 sistemas de código_aberto  , e entrevistamos e realizamos questionários com mais de 50 desenvolvedores . Nossos resultados mostram que cada papel arquitetural possui distribuições diferentes de valores de métrica de código , consequência das diferentes responsabilidades de cada papel . Como consequência , propomos SATT , uma abordagem que provê thresholds específicos para papéis arquiteturais que são significantemente diferentes de outros em termos de métricas de código . Mostramos também que classes que cumprem um papel arquitetural específico também contêm maus_cheiros  de código específicos . Esses maus_cheiros  são percebidos por desenvolvedores como problemas reais e podem fazer com que essas classes sejam mais modificadas e apresentem mais defeitos do que classes limpas . Sugerimos então que desenvolvedores entendam a arquitetura dos seus sistemas , bem como as responsabilidades de cada papel arquitetural que as classes desempenham , para que tanto métricas de código quanto estratégias de detecção de maus_cheiros  de código possam prover um melhor retorno 
 Teste de Software é um dos processos mais importantes da Engenharia de Software , sendo a principal atividade para averiguar a conformidade de requisitos de software e suas saídas . A automatização das atividades de teste é essencial para conferir produtividade e efetividade em tais atividades . A automatização faz com que atividades de teste sejam conduzidas sob critérios sistemáticos e precisos , aumentando a chance dos testadores de revelarem falhas ou inconcistências . Oráculos de teste são membros elementares na automatização do teste de software , sendo o mecanismo responsável por indicar a corretude das saídas do softwre . Em ambientes de teste , oráculos de teste podem ser efetivamente implementados com base em diversos fontes de informação sobre o sistema em teste : especificações de software , assertivas , métodos formais ( máquinas de estados finitas , especificações formais , etc ) , métodos de aprendizagem de máquina e relações metamórficas . Independente da estratégia de implementação , oráculos de teste são vulneráveis a veridictos de falsos positivos/negativos , configurando o que é apresentado na literatura como O problema do Oráculo . Então , na área de engenharia de software , oráculos de teste são objetos de estudo não-triviais e desafiadores . O problema de oráculo é potencializado quando as saídas do sistema em teste são dadas em formatos não triviais como , por exemplo , audio , imagens , objetos tridimensionais , ambientes de realidade virtual , composições estatísticas complexas , etc . No contexto do teste de software , sistemas com saídas não triviais podem ser chamados de sistemas com saídas complexas . Esta tese de doutorado propões e avalia uma nova estratégia de oráculo de teste para sistemas com saídas complexas . O propósito de tal estratégia é a apropriação da técnica de processamento de imagem conhecida como CBIR ( Recuperação de Imagem Basead em Conteúdo CBIR ) para coletar informações de características extratídas do sistema em teste , compondo oráculos de teste . A partir de uma imagem de busca , o CBIR combina extração de características e funções de similaridade para aliviar problemas de busca em grandes based de imagens_digitais  . Em pesquisas anteriores , conceitos de CBIR foram integrados em um arcabouço de teste para apoiar a automatização de atividades de teste em systemas de processamento de imagens e sistemas com interfaces gráficas . Esta tese de doutorado estende o arcabouço e seus conceitos para sistemas com saídas complexas em geral . Sistemas Texto-Fala ( TTS ) foram utlizados para validações empíricas . Os resultados de seis análises empíricas , duas delas condizidas em consonância com problemas de um TTS industrial , revelam que a técnica proposta é um valioso instrumento para automatizar atividaes de teste e aliviar esforços de profissionais da indústria ao teste sistemas com saídas complexas . Conclui-se que a efetividade dos oráculos de teste propostos são devido às sistemáticas análises do conteúdo das saídas dos sistemas em teste , em vez da análises de especificações subjetivas . Os trabalhos futuros vislumbrados devem ser conduzidos no intuito de reduzir número de falsos positivos/negativos e a associação dos oráculos de teste com técnicas de aprendizado de máquina e relações metamórficas 
 As redes_sociais  se tornaram um novo e importante meio de intercâmbio de informações , ideias e comunicação que aproximam parentes e amigos sem importar as distâncias . Dada a natureza aberta da Internet , as informações podem fluir muito fácil e rápido na população . A rede pode ser representada como um grafo , onde os indivíduos ou organizações são o conjunto de vértices e os relacionamentos ou conexões entre os vértices são o conjunto de arestas . Além_disso  , as redes_sociais  representam intrinsecamente a estrutura de um sistema mais complexo que é a sociedade . Estas estruturas estão relacionadas com as características dos indivíduos . Por exemplo , os indivíduos mais populares são aqueles com maior número de conexões . Em particular , é aceito que a estrutura da rede pode afetar a forma como a informação se propaga nas redes_sociais  . No entanto , ainda não está claro como a estrutura influencia na propagação , como medir seu impacto e quais as possíveis estratégias para controlar o processo de difusão . Nesta tese buscamos contribuir nas análises da interação entre as dinâmicas de propagação de informações e rumores e a estrutura da rede . Propomos um modelo de propagação mais realista considerando a heterogeneidade dos indivíduos na transmissão de ideias ou informações . Nós confirmamos a presença de propagadores mais influentes na dinâmica de rumor e observamos que é possível melhorar ou reduzir expressivamente a difusão de uma informação ao selecionar uma fração muito pequena de propagadores influentes . No caso em que se objetiva selecionar um conjunto de propagadores iniciais que maximizem a difusão de informação , a melhor opção é selecionar os indivíduos mais centrais ou importantes nas comunidades . Porém , se o padrão de conexão dos vértices está negativamente correlacionado , a melhor alternativa é escolher entre os indivíduos mais centrais de toda a rede . Por outro_lado  , através de abordagens topológicas e de técnicas de aprendizagem máquina , identificamos aos propagadores menos influentes e mostramos que eles atuam como um firewall no processo de difusão . Nós propomos um método adaptativo de reconexão entre os vértices menos influentes para um indivíduo central da rede , sem afetar a distribuição de grau da rede . Aplicando o nosso método em uma pequena fração de propagadores menos influentes , observamos um aumento importante na capacidade de propagação desses vértices e da rede toda . Nossos resultados vêm de uma ampla gama de simulações em conjuntos de dados artificiais e do mundo_real  e a comparação com modelos clássicos de propagação da literatura . A propagação da informação em redes é de grande relevância para as áreas de publicidade e marketing , educação , campanhas políticas ou de saúde , entre outras . Os resultados desta tese podem ser aplicados e estendidos em diferentes campos de pesquisa como redes biológicas e modelos de comportamento social animal , modelos de propagação de epidemias e na saúde pública , entre outros 
 Os Sistemas_Distribuídos  ( SDs ) tem apresentado uma crescente complexidade no seu gerenciamento , além de possuir a necessidade de garantir Qualidade de Serviço ( QoS ) aos seus usuários . A Computação Autonômica ( CA ) surge como uma forma de transformar os SDs em Sistemas_Distribuídos  Autonômicos ( SDAs ) , com capacidade de auto-gerenciamento . Entretanto , não foi encontrado um processo de desenvolvimento de software , focado na criação de SDAs . Na grande maioria dos trabalhos_relacionados  , simplesmente é apresentado um SD , juntamente com qual aspecto da CA deseja-se implementar , a técnica usada e os resultados obtidos . Isso é apenas uma parte do desenvolvimento de um SDA , não abordando desde a definição dos requisitos até a manutenção do software . Mais importante , não mostra como tais requisitos podem ser formalizados e posteriormente solucionados por meio do auto-gerenciamento fornecido pela CA . Esta tese foca na proposta de um processo de desenvolvimento de software voltado para SDAs . Com esse objetivo , foram integradas diferentes áreas de conhecimento , compreendendo : Processo Unificado de Desenvolvimento de Software ( PU ) , SDs , CA , Pesquisa Operacional ( PO ) e Avaliação de Desempenho de Sistemas Computacionais ( ADSC ) . A prova de conceito foi feita por meio de três estudos de caso , todos focando-se em problemas NP-Difícil , são eles : ( i ) otimização off-line ( problema da mochila com múltiplas escolhas ) , ( ii ) otimização online ( problema da mochila com múltiplas escolhas ) e ( iii ) criação do módulo planejador de um gerenciador autonômico , visando realizar o escalonamento de requisições ( problema de atribuição generalizado ) . Os resultados do primeiro estudo de caso , mostram que é possível usar PO e ADSC para definir uma arquitetura de base para o SDA em questão , bem como reduzir o tamanho do espaço de busca quando o SDA estiver em execução . O segundo , prova que é possível garantir a QoS do SDA durante sua execução , usando a formalização fornecida pela PO e sua respectiva solução . O terceiro , prova que é possível usar a PO para formalizar o problema de auto-gerenciamento , bem como a ADSC para avaliar diferentes algoritmos ou modelos de arquitetura para o SDA 
 A fim de auxiliar usuários durante o consumo de produtos , sistemas Web passaram a incorporar módulos de recomendação de itens . As abordagens mais populares são a baseada em conteúdo , que recomenda itens a partir de características que são do seu interesse , e a filtragem colaborativa , que recomenda itens bem avaliados por usuários com perfis semelhantes ao do usuário alvo , ou que são semelhantes aos que foram bem avaliados pelo usuário alvo . Enquanto que a primeira abordagem apresenta limitações como a sobre-especialização e a análise limitada de conteúdo , a segunda enfrenta problemas como o novo usuário e/ou novo item , também conhecido como partida fria . Apesar da variedade de técnicas disponíveis , um problema comum existente na maioria das abordagens é a falta de informações_semânticas  para representar os itens do acervo . Trabalhos recentes na área de Sistemas de Recomendação têm estudado a possibilidade de usar bases de conhecimento da Web como fonte de informações_semânticas  . Contudo , ainda é necessário investigar como usufruir de tais informações e integrá-las de modo eficiente em sistemas de recomendação . Dessa maneira , este trabalho tem o objetivo de investigar como informações_semânticas  provenientes de bases de conhecimento podem beneficiar sistemas de recomendação por meio da descrição semântica de itens , e como o cálculo da similaridade semântica pode amenizar o desafio enfrentado no cenário de partida fria . Como resultado , obtém-se uma técnica que pode gerar recomendações adequadas ao perfil dos usuários , incluindo itens novos do acervo que sejam relevantes . Pode-se observar uma melhora de até 10 % no RMSE , no cenário de partida fria , quando se compara o sistema proposto com o sistema cuja predição de notas é baseada na correlação de notas 
 Recursos Educacionais Abertos ( REA ) podem ser definidos como materiais de ensino , aprendizagem e pesquisa , em qualquer meio de armazenamento , que estão amplamente disponíveis por meio de uma licença aberta que permite reuso , readequação e redistribuição sem restrições ou com restrições limitadas . Atualmente , diversas instituições de ensino e pesquisa têm investido em REA para ampliar o acesso ao conhecimento . Entretanto , os usuários ainda têm dificuldades de encontrar os REA com os mecanismos de busca atuais . Essa dificuldade deve-se principalmente ao fato dos mecanismos de busca na Web serem genéricos , pois buscam informação em qualquer lugar , desde páginas de vendas até materiais escritos por pessoas anônimas . De fato , esses mecanismos não levam em consideração as características intrínsecas de REA , como os diferentes padrões de metadados , repositórios e plataformas existentes , os tipos de licença , a granularidade e a qualidade dos recursos . Esta dissertação apresenta o desenvolvimento de um mecanismo de busca na Web especificamente para recuperação de REA denominado SeeOER . As principais_contribuições  desta pesquisa de mestrado consistem no desenvolvimento de um mecanismo de busca na Web por REA com diferenciais entre os quais se destacam a resolução de conflitos em nível de esquema oriundos da heterogeneidade dos REA , a busca em repositórios de REA , a consulta sobre a procedência de dados e o desenvolvimento de um crawler efetivo para obtenção de metadados específicos . Além_disso  , contribui na inclusão de busca de REA no cenário brasileiro , no mapeamento de padrões de metadados para mecanismos de busca na Web e a publicação de uma arquitetura de um mecanismo de busca na Web . Ademais , o SeeOER disponibiliza um serviço que traz um índice invertido de busca que auxilia encontrar REA nos repositórios dispersos na Web . Também foi disponibilizada uma API para buscas que possibilita consultas por palavras chaves e o uso de palavras booleanas . A forma de validação em mecanismos de busca na Web , como um todo , e de forma quantitativa e específica por componentes foi feita em grau de especialidade . Para validação de qualidade foram considerados 10 participantes com grupos distintos de escolaridade e área de estudo . Os resultados quantitativos demonstraram que o SeeOER é superior em 23.618 REA indexados em comparação a 15.955 do Jorum . Em relação à qualidade o SeeOER demonstrou ser superior ao Jorum considerando a função penalizada e o score utilizada nesta pesquisa 
 Reconstrução a partir de pontos não organizados é um problema relevante e comum a vários_tipos  de aplicações , como tratamento de imagens médicas e computação gráfica . Este tipo de abordagem é relativamente recente e busca gerar malhas não estruturadas a partir de conjuntos de pontos geralmente fornecidos por scanners tridimensionais , sondas sísmicas , radares e amostragens de superfícies implícitas . O principal desafio a ser superado neste tipo de reconstrução é a falta de informação , tanto geométrica quanto topológica , a respeito do objeto a ser reconstruído . Essa falta de informação tem impossibilitado a elaboração de algoritmos robustos e eficientes . Diversas técnicas para a resolução deste : problema são descritas na literatura , tais como técnicas baseadas em zeros de funções , em `` esculpimento '' , em modelos deformáveis e em métodos incrementais . Cada uma delas possui vantagens e desvantagens . No entanto , a maioria utiliza operações geométricas caras e pouco estáveis . Este projeto de doutorado propõe uma nova abordagem para o problema de reconstrução a partir de pontos não organizados . A abordagem é baseada em `` esculpimento '' , e sua principal característica é diminuir a quantidade de operações geométricas , substituindo-as por operações topológicas , mais robustas e mais eficientes . Para isso , a teoria de Morse em complexos simpliciais é utilizada como ferramenta de decisão da inclusão ou não de simplexos na malha do objeto reconstruído . Além_disso  , apresentam-se garantias teóricas de que , sob uma taxa de amostragem adequada , a reconstrução é homeomorfa e próxima do objeto original . Adicionalmente , esse projeto vem atender as necessidades do grupo de mecânica de fluídos computacional do ICMC-USP , fornecendo um gerador de malhas não estruturadas a partir de pontos não organizados , o qual deverá ser empregado onde técnicas mais convencionais , tais como técnicas de triangulação de politopos e de reconstrução por seções planares , não produzem resultados_satisfatórios  
 Em domínios de dados complexos ( tais como , dados multimídia , sequências genômicas , entre outros ) , a similaridade entre elementos surge naturalmente como a maneira mais adequada para consultar esses dados . Existem , basicamente , dois tipos de consulta por similaridade : por abrangência e aos k-vizinhos mais próximos . Com o aumento no volume de dados complexos armazenado em Sistemas de Gerenciamento de Bases de Dados ( SGBD ) , também chamados neste trabalho de gerenciadores , torna-se necessário prover suporte a esses tipos de dados . Um modo de dar suporte a tipos de dados complexos nos gerenciadores atuais é incluir consultas por similaridade em seu processador de consultas , e consequentemente , na álgebra relacional . Este fato leva à produção de maneiras para expressar tais consultas na linguagem do gerenciador como predicados em operações de seleção . Como uma consequência , os principais tipos de consultas por similaridade podem ser compostos em expressões mais complexas por meio de conjunções e disjunções booleanas entre eles , isto é , consultas por similaridade complexas . Entretanto , para que um gerenciador processe consultas por similaridade complexas eficientemente , é necessário dar suporte as etapas de otimização e execução na arquitetura do processamento de consultas . Embora diversos trabalhos envolvam o desenvolvimento de algoritmos para responder a uma simples e específica consulta por similaridade , não há um algoritmo genérico apto a manipular eficientemente consultas por similaridade complexas . Além_disso  , a otimização de consultas por similaridade é um aspecto ainda pouco explorado na literatura . Esta tese propõe um método estruturado de como analisar consultas por similaridade complexas . Esse método é utilizado para estender a álgebra relacional por meio de regras algébricas e determinar um pequeno conjunto de algoritmos que podem ser utilizados para responder a qualquer consulta por similaridade complexa . O método proposto também permite formalizar regras para estimar a seletividade dessas consultas auxiliando na previsão de custo . Para validar os conceitos apresentados , experimentos são realizados com conjuntos de dados reais e sintéticos destacando os resultados obtidos . As regras algébricas , os algoritmos e as métricas para se estimar a seletividade podem ser utilizados por um gerenciador relacional na etapa de otimização , para derivar planos de execução eficientes para consultas por similaridade complexas . Portanto , os aspectos abordados nesta tese contribuem para permitir o uso prático de consultas por similaridade em gerenciadores relacionais 
 Composição de serviços é um tópico que tem atraído cada vez mais o interesse por parte de pesquisadores na área de sistemas distribuídos . Além_disso  , o interesse por ambientes baseados em nuvem tem crescido significativamente conforme o seu uso aumenta e se firma como um importante modelo de negócios . Coreografias são formas de composições de serviços em que não há pontos centrais de falha ; a responsabilidade da sua execução é distribuída entre os vários serviços componentes . Devido à natureza distribuída do fluxo de informações e dados de controle , o cumprimento de \textit { Service Level Agreements } ( SLAs ) depende estritamente do monitoramento da Qualidade de Serviços ( QoS ) , recursos virtuais da nuvem e mecanismos de reconfiguração dinâmica , capazes de automaticamente adaptar composições a mudanças de estado no sistema . Nesta dissertação , abordamos o estudo do gerenciamento de QoS em coreografias de serviços . Para isso desenvolvemos um sistema de middleware capaz de implantar e gerenciar o QoS de composições . Este teve seu desempenho avaliado utilizando o serviço Amazon EC2 . Os resultados da avaliação mostram que com pouco esforço por parte dos desenvolvedores de composições , é possível cumprir o SLA de composições dentro do esperado utilizando escalabilidade horizontal ou vertical provida pelo middleware automaticamente . Adicionalmente , a nossa proposta traz economias em relação ao custo de implantação pois diminui a quantidade de recursos subutilizados 
 Diversos processos industriais , científicos e comerciais produzem sequências de observações continuamente , teoricamente infinitas , denominadas fluxos de dados . Pela análise das recorrências e das mudanças de comportamento desses fluxos , é possível_obter  informações sobre o fenômeno que os produziu . A inferência de modelos estáveis para tais fluxos é suportada pelo estudo das recorrências dos dados , enquanto é prejudicada pelas mudanças de comportamento . Essas mudanças são produzidas principalmente por influências externas ainda desconhecidas pelos modelos vigentes , tal como ocorre quando novas estratégias de investimento surgem na bolsa de valores , ou quando há intervenções humanas no clima , etc . No contexto de Aprendizado de Máquina ( AM ) , várias pesquisas têm sido realizadas para investigar essas variações nos fluxos de dados , referidas como mudanças de conceito . Sua detecção permite que os modelos possam ser atualizados a fim de apurar a predição , a compreensão e , eventualmente , controlar as influências que governam o fluxo de dados em estudo . Nesse cenário , algoritmos supervisionados sofrem com a limitação para rotular os dados quando esses são gerados em alta frequência e grandes volumes , e algoritmos não supervisionados carecem de fundamentação teórica para prover garantias na detecção de mudanças . Além_disso  , algoritmos de ambos paradigmas não representam adequadamente as dependências temporais entre observações dos fluxos . Nesse contexto , esta tese de doutorado introduz uma nova metodologia para detectar mudanças de conceito , na qual duas deficiências de ambos paradigmas de AM são confrontados : i ) a instabilidade envolvida na modelagem dos dados , e ii ) a representação das dependências temporais . Essa metodologia é motivada pelo arcabouço teórico de Carlsson e Memoli , que provê uma propriedade de estabilidade para algoritmos de agrupamento hierárquico com relação à permutação dos dados . Para usufruir desse arcabouço , as observações são embutidas pelo teorema de imersão de Takens , transformando-as em independentes . Esses dados são então agrupados pelo algoritmo Single-Linkage Invariante à Permutação ( PISL ) , o qual respeita a propriedade de estabilidade de Carlsson e Memoli . A partir dos dados de entrada , esse algoritmo gera dendrogramas ( ou modelos ) , que são equivalentes a espaços ultramétricos . Modelos sucessivos são comparados pela distância de Gromov-Hausdorff a fim de detectar mudanças de conceito no fluxo . Como resultado , as divergências dos modelos são de fato associadas a mudanças nos dados . Experimentos foram realizados , um considerando mudanças abruptas e o outro mudanças graduais . Os resultados confirmam que a metodologia_proposta  é capaz de detectar mudanças de conceito , tanto abruptas quanto graduais , no entanto ela é mais adequada para cenários mais complicados . As contribuições principais desta tese são : i ) o uso do teorema de imersão de Takens para transformar os dados de entrada em independentes ; ii ) a implementação do algoritmo PISL em combinação com a distância de Gromov-Hausdorff ( chamado PISLGH ) ; iii ) a comparação da metodologia_proposta  com outras da literatura em diferentes cenários ; e , finalmente , iv ) a disponibilização de um pacote em R ( chamado streamChaos ) que provê tanto ferramentas para processar fluxos de dados não lineares quanto diversos algoritmos para detectar mudanças de conceito 
 O aprendizado de máquina consiste em conceitos e técnicas que permitem aos computadores melhorar seu desempenho com a experiência , ou em outras_palavras  , aprender com dados . Duas de suas principais categorias são o aprendizado não-supervisionado e o semissupervisionado , que respectivamente consistem em inferir padrões em bases cujos dados não têm rótulo ( classe ) e classificar dados em bases parcialmente rotuladas . Embora muito estudado , trata-se de um campo repleto de desafios e com muitos tópicos abertos . Sistemas dinâmicos coletivos , por sua vez , são sistemas constituídos por muitos indivíduos , cada qual um sistema dinâmico por si só , de modo que todos eles agem coletivamente , ou seja , a ação de cada indivíduo é influenciada pela ação dos vizinhos . Uma característica notável desses sistemas é que padrões globais podem surgir espontaneamente das interações locais entre os indivíduos , fenômeno conhecido como emergência . Os desafios intrínsecos e a relevância do tema vêm motivando sua pesquisa em diversos ramos da ciência e da engenharia . Este trabalho de doutorado consiste no desenvolvimento e análise de modelos dinâmicos coletivos para o aprendizado de máquina , especificamente suas categorias não-supervisionada e semissupervisionada . As tarefas de segmentação de imagens e de detecção de comunidades em redes , que de certo modo podem ser entendidas como tarefas do aprendizado de máquina , são também abordadas . Em especial , desenvolvem-se modelos nos quais a movimentação dos objetos é determinada pela localização e velocidade de seus vizinhos . O sistema dinâmico assim modelado é então conduzido a um estado cujo padrão formado por seus indivíduos realça padrões subjacentes do conjunto de dados . Devido ao seu caráter auto-organizável , os modelos aqui desenvolvidos são robustos e as informações geradas durante o processo ( valores das variáveis do sistema ) são ricas e podem , por exemplo , revelar características para realizar soft labeling e determinar classes sobrepostas 
 A motivação que deu origem a esse trabalho foi um desejo de criarmos um simulador de esportes de invasão coletivos capaz de aplicar estratégias definidas pelo usuário para guiar o comportamento de agentes na simulação . Com esse objetivo em mente nós criamos um modelo formal de estratégia para descrever comportamentos complexos em equipe e desenvolvemos métodos para usar esse modelo no cálculo de planos coletivos . Definimos o modelo e os métodos de planejamento de uma forma abrangente que pode ser aplicada em muitos domínios diferentes . Definimos um domínio para a simulação de partidas de basquete e implementamos nossa metodologia para desenvolver um simulador . Também apresentamos uma arquitetura de controle que é compatível com o planejador proposto e mostramos como implementá-la na criação de um simulador de basquete . O modelo formal que desenvolvemos pode ser usado para representar comportamento coletivo , analisar eventos reais e criar simulações . Desenvolvemos um desenhador de estratégia que permite que o usuário final desenhe e visualize estratégias de equipes de basquete . Finalmente , desenvolvemos um sistema que interpreta o conteúdo gerado pelo usuário e cria uma simulação de basquete usando o comportamento descrito . Propusemos também uma metodologia para o desenvolvimento de sistemas de simulação envolvendo múltiplos agentes inteligentes . Nossa arquitetura de controle separa as várias camadas de controle , simplificando o processo de desenvolvimento e resultando em um sistema naturalmente expansível 
 Esta tese apresenta uma avaliação do desempenho de simulações distribuídas em tempo de execução . Baseando-se nos resultados obtidos nessa avaliação é proposto um mecanismo em que diferentes protocolos de sincronização coexistem em uma mesma simulação . Esse mecanismo tem por objetivo adequar a simulação em execução ao melhor protocolo de sincronização , para garantir melhor desempenho e , conseqüentemente , resultados mais rápidos . Todas as modificações que são necessárias nos protocolos e a definição da troca de mensagens entre os processos são detalhadas neste trabalho . Esta tese apresenta também os resultados dos testes realizados para identificar os casos onde é melhor manter o protocolo conservador ou onde uma troca de protocolo deve ser considerada . Os resultados obtidos são apresentados e mostram em que momento a troca deve ser considerada . Diferentes abordagens podem ser utilizadas para avaliar o desempenho da simulação , considerando cada processo individualmente ou todos os processos globalmente . De maneira análoga , a troca de protocolos pode ser realizada de forma local ou global . Essas considerações permitem a criação de uma taxonomia para a troca de protocolo que também é apresentada nesta tese 
 As organizações precisam trocar informações de forma simples e eficiente , com custos tão baixos quanto possível . Essas informações às vezes são apresentadas na forma de documentos com formato e conteúdos pré-definidos . Esses documentos podem ser equivalentes ou quase equivalentes , porém bastantes distintos em diferentes organizações . Numa mesma organização , os documentos podem ser diferentes em contextos históricos . O propósito deste trabalho é facilitar a distribuição dos documentos , superando o problema dos formatos com os quais foram criados . O objetivo é possibilitar a interoperabilidade de documentos e atingir a portabilidade simples e confiável de documentos através da reutilização de formatos e conteúdos , em diferentes combinações plausíveis . Propomos , usar ontologias como solução ao problema da falta de interoperabilidade nas implementações de formatos de documentos . Como prova de conceito consideramos a portabilidade entre os formatos padrão ODF ( Open Document Format ) e ( Office Open XML ) 
 O aumento no número de dispositivos_móveis  , como smartphones , tablets e laptops , e o avanço em seu potencial computacional permitiu considerá-los como recursos_computacionais  . O uso de recursos_computacionais  com maior proximidade vem crescendo ano após ano , sendo chamado de Fog computing , em que os elementos na borda da Internet são explorados , uma vez que os serviços computacionais convencionais podem_estar  indisponíveis ou sobrecarregados . Dessa forma , este projeto de Mestrado tem como foco possibilitar o uso de dispositivos_móveis  no provimento de serviços computacionais entre si de forma colaborativa através da heurística Maximum Regret adaptada , que busca alocar tarefas computacionais em dispositivos locais de forma a minimizar o consumo de energia e evitar dispositivos não confiáveis . Também há uma meta-heurística em um nível global , que interconecta os diferentes aglomerados de dispositivos_móveis  na borda da Internet , e possui informações globais de Quality of Service ( QoS ) . Foram realizados_experimentos  que mostraram que evitar dispositivos_móveis  como recursos com um baixo grau de confiabilidade possibilitou diminuir o impacto no consumo de energia , além de ser possível diminuir os tempos de resposta e de comunicação ao ajustar a política de seleção de aglomerados externos 
 Diversas aplicações são responsáveis por gerar dados ao longo do tempo de maneira contínua , ordenada e ininterrupta em um ambiente dinâmico , denominados fluxo de dados . Entre possíveis tarefas que podem ser realizadas com estes dados , classificação é uma das mais proeminentes . Devido à natureza não estacionária do ambiente responsável por gerar os dados , as características que descrevem os conceitos das classes do problema de classificação podem se alterar ao longo do tempo . Por isso , classificadores de fluxo de dados requerem constantes atualizações em seus modelos para que a taxa de acerto se mantenha estável ao longo do tempo . Na etapa de atualização a maior_parte  das abordagens considera que , após a predição de cada exemplo , o seu rótulo correto é imediatamente disponibilizado sem qualquer atraso de tempo ( latência nula ) . Devido aos altos custos do processo de rotulação , os rótulos corretos nem sempre podem ser obtidos para a maior_parte  dos dados ou são obtidos após um considerável atraso de tempo . No caso mais desafiador , encontram-se as aplicações em que após a etapa de classificação dos exemplos , os seus respectivos rótulos corretos nunca sã disponibilizados para o algoritmo , caso chamado de latência extrema . Neste cenário , não é possível o uso de abordagens tradicionais , sendo necessário o desenvolvimento de novos métodos que sejam capazes de manter um modelo de classificação atualizado mesmo na ausência de dados rotulados . Nesta tese , além de discutir o problema de latência na tarefa de classificação de fluxo de dados não estacionários , negligenciado por boa parte da literatura , também sã propostos dois algoritmos denominados SCARGC e MClassification para o cenário de latência extrema . Ambas as propostas se baseiam no uso de técnicas de agrupamento para a adaptação à mudanças de maneira não supervisionada . Os algoritmos propostos são intuitivos , simples e apresentam resultados superiores ou equivalentes a outros algoritmos da literatura em avaliações com dados sintéticos e reais , tanto em termos de acurácia de classificação como em tempo computacional . Aléem de buscar o avanço no estado-da-arte na área de aprendizado em fluxo de dados , este trabalho também apresenta contribuições para uma importante aplicação tecnológica com impacto social e na saúde pública . Especificamente , explorou-se um sensor óptico para a identificação automática de espécies de insetos a partir da análise de informações provenientes do batimento de asas dos insetos . Para a descrição dos dados , foi verificado que os coeficientes Mel-cepstrais apresentaram os melhores_resultados  entre as diferentes técnicas de processamento digital de sinais avaliadas . Este sensor é um exemplo concreto de aplicação responsável por gerar um fluxo de dados em que é necessário realizar classificações em tempo real . Durante a etapa de classificação , este sensor exige a adaptação a possíveis variações em condições ambientais , responsáveis por alterar o comportamento dos insetos ao longo do tempo . Para lidar com este problema , é proposto um Sistema com Múltiplos Classificadores que realiza a seleção dinâmica do classificador mais adequado de acordo com características de cada exemplo de teste . Em avaliações com mudanças pouco significativas nas condições ambientais , foi possível_obter  uma acurácia de classificação próxima de 90 % , no cenário com múltiplas classes e , cerca de 95 % para a identificação da espécie Aedes aegypti , considerando o treinamento com uma única classe . No cenário com mudanças significativas nos dados , foi possível_obter  91 % de acurácia em um problema com 5 classes e 96 % para a classificação de insetos vetores de importantes doenças como dengue e zika vírus 
 Nesse trabalho , estudamos um problema de otimização combinatorial conhecido por Problema da Mochila Compartimentada , que é uma extensão do clássico Problema da Mochila . O problema consiste em determinar as capacidades adequadas de vários compartimentos que podem vir a ser alocados em uma mochila e como esses compartimentos devem ser carregados , respeitando as restrições de capacidades dos compartimentos e da mochila . Busca-se maximizar o valor de utilidade total . O problema é muito pouco estudado na literatura , apesar de surgir naturalmente em aplicações práticas . Nesse estudo , propomos uma modelagem matemática não linear para o problema e verificamos algumas heurísticas para sua resolução 
 Neste trabalho apresentamos um modelo para ambientes inteligentes baseado em organizações de agentes , onde interações entre entidades são associadas a espaços físicos , pessoas carregam dispositivos e se movimentam entre diferentes espaços físicos e cada espaço físico contém definições de interações ( comportamentos definidos por normas ) próprias do seu contexto . São definidos três componentes deste modelo : ( 1 ) modelo_conceitual  , ( 2 ) linguagem de especificação e ( 3 ) ambiente de execução . A separação do modelo nestes três componentes traz como principais conseqüências : ( 1 ) a ativação de um ambiente inteligente é feita através de um mecanismo de alto nível , ( 2 ) a especificação de um ambiente inteligente é independente do domínio de aplicação e ( 3 ) as especificações podem ser executadas em mecanismos diferentes de execução 
 O problema de caminhos mínimos ( SP shortest path problem ) é frequentemente colo- cado em prática em uma grande variedade de aplicações em diversas_áreas  . Nessas aplicações geralmente se deseja realizar algum tipo de deslocamento ou transporte entre dois ou mais pontos específicos em uma rede . Tal ação deve ser executada de forma ótima em relação a algum critério , por exemplo o menor_custo  possível , ou o menor gasto de tempo ou o máximo de confiabilidade/segurança . Na prática , muitas_vezes  não desejamos apenas o menor_custo  ou o menor tempo , mas desejamos otimizar uma combinação de diferentes critérios , por exemplo , um caminho que seja rápido e barato . Como não é possível otimizar sobre todos os critérios de uma só vez , nós escolhemos um dos critérios para representar a função custo , que será minimizada , e para os demais critérios representamos como recursos e definimos os limites que julgamos aceitáveis para o consumo de cada um desses recursos . Esta variação é cha- mada de problema de caminhos mínimos com restrições por recursos , ou como preferimos chamar , problema de caminhos mínimos com recursos limitados ( RCSP resource constrained shortest path problem ) , o qual será o objeto de estudo neste trabalho . A adição de restrições por recursos no SP , infelizmente torna o problema NP-difícil , mesmo em grafos acíclicos , com restrições sobre um único recurso , e com todos os consu- mos de recursos positivos . Temos reduções dos famosos problemas N P-difíceis Mochila e Partição para o nosso problema . Em contextos diversos são encontrados problemas de cunho teórico e prático que po- dem ser formulados como problemas de caminhos mínimos com recursos limitados , o que nos motivou a estudá-lo a fim de desenvolver um trabalho que resumisse informações sufi- cientes para auxiliar pesquisadores ou desenvolvedores que tenham interesse no problema . Nós apresentamos aqui , uma detalhada revisão bibliográfica do RCSP , tendo como foco o desenvolvimento de algoritmos exatos para o caso onde possuímos um único recurso e a im- plementação e comparação dos principais algoritmos conhecidos , observando-os em situações práticas 
 Este trabalho propõe o desenvolvimento de Algoritmos_Evolutivos  ( AEs ) para estimação dos parâmetros que modelam sinais elétricos ( frequência , fase e amplitude ) em tempo-real . A abordagem proposta deve ser robusta a ruídos e harmônicos em sinais distorcidos , por exemplo devido à presença de faltas na rede elétrica . AEs mostram vantagens para lidar com tais tipos de sinais . Por outro_lado  , esses algoritmos quando implementados em software não possibilitam respostas em tempo-real para uso da estimação como relé de frequência ou Unidade de Medição Fasorial . O desenvolvimento em FPGA apresentado nesse trabalho torna possível paralelizar o cálculo da estimação em hardware , viabilizando AEs para análise de sinal elétrico em tempo real . Além_disso  , mostra-se que AEs multiobjetivos podem extrair_informações  não evidentes das três fases do sistema e estimar os parâmetros adequadamente mesmo em casos em que as estimativas por fase divirjam entre si . Em outras_palavras  , as duas principais_contribuições  computacionais são : a paralelização do AE em hardware por meio de seu desenvolvimento em um circuito de FPGA otimizado a nível de operações lógicas básicas e a modelagem multiobjetiva do problema possibilitando análises dos sinais de cada fase , tanto independentemente quanto de forma agregada . Resultados experimentais_mostram  superioridade do método proposto em relação ao estimador baseado em transformada de Fourier para determinação de frequência e 
 A atenção voltada à produção de conteúdos 3D atualmente tem sido alta , em grande parte devido à aceitação e à manifestação de interesse do público para esta tecnologia . Isso reflete num maior investimento das indústrias cinematográfica , de televisores e de jogos visando trazer o 3D para suas produções e aparelhos , oferecendo modos diferentes de interação ao usuário . Com isso , novas técnicas de captura , codificação e modos de reprodução de vídeos 3D , em especial , os vídeos estereoscópicos , vêm surgindo ou sendo melhorados , visando aperfeiçoar e integrar esta nova tecnologia com a infraestrutura disponível . Entretanto , notam-se divergências nos avanços feitos no campo da codificação , com cada método de visualização estereoscópica utilizando uma técnica de codificação diferente . Isso leva ao problema da incompatibilidade entre métodos de visualização . Uma proposta é criar uma técnica que seja genérica , isto é , independente do método de visualização . Tal técnica , por meio de parâmetros adequados , codifica o vídeo estéreo sem nenhuma perda significativa tanto na qualidade quanto na percepção de profundidade , característica marcante nesse tipo de conteúdo . A técnica proposta , denominada RevGlyph , transforma um par estéreo de vídeos em um único fluxo anaglífico , especialmente codificado . Tal fluxo , além de ser compatível com o método anaglífico de visualização , é também reversível a uma aproximação do par estéreo original , garantindo a independência do método de 
 Existem bases para as quais os dados são naturalmente representados por mais de uma visão . Por exemplo , imagens podem ser descritas por atributos de cores , textura e forma . Proteínas podem ser caracterizadas pela sequência de aminoácidos e pela representação tridimensional . A unificação das diferentes visões de uma base de dados pode ser problemática porque elas podem não ser comparáveis entre si ou podem apresentar diferentes graus de importância . Esses graus de importância podem , inclusive , se manifestar de maneira local , de acordo com a subestrutura dos dados em questão . Isso motivou o surgimento de algoritmos de agrupamento de dados capazes de lidar com bases multi-representadas ( i.e. , que possuem mais de uma visão dos dados ) , como o algoritmo SCAD . Esse algoritmo se mostrou promissor em experimentos relatados na literatura , mas possui problemas críticos identificados neste trabalho que o impedem de funcionar em determinados cenários . Tais problemas foram solucionados por meio da proposição de uma nova versão do algoritmo , denominada ASCAD , fundamentada em provas formais sobre a sua convergência . Foram desenvolvidas versões relacionais do algoritmo ASCAD , capazes de lidar com bases descritas apenas por relações de proximidade entre os objetos . Foi desenvolvido também um índice de validação interna e relativa de agrupamento voltado para dados multi-representados . A avaliação de agrupamento possibilístico e de bi-agrupamento por meio da comparação entre solução encontrada e solução de referência ( validação externa ) também foi explorada . Algoritmos de bi-agrupamento têm ganhado um interesse crescente da comunidade de análise de expressão_gênica  . No entanto , pouco se conhece do comportamento e das propriedades das medidas voltadas para validação externa de bi-agrupamento , o que motivou uma análise teórica e empírica dessas medidas . Essa análise mostrou que a maioria das medidas de biagrupamento possui problemas críticos e destacou duas delas como sendo as mais promissoras . Foram inclusas nessa análise três medidas de agrupamento particional não exclusivo , cujo uso na comparação de bi-agrupamentos é possível por meio de uma nova abordagem de avaliação de bi-agrupamento proposta nesta tese . Agrupamento particional não exclusivo faz parte de um domínio mais geral de soluções , i.e. , o domínio dos agrupamentos possibilísticos . Observou-se algumas falhas conceituais importantes das medidas de agrupamento possibilístico , o que motivou o desenvolvimento de novas medidas e de uma análise empírica e conceitual envolvendo 34 medidas . Uma das medidas propostas se destacou como sendo a única que apresentou avaliações imparciais com relação ao número de grupos , o valor máximo de similaridade ao comparar a solução ideal encontrada com a solução de referência e avaliações sensíveis às diferenças das soluções em todos os cenários 
 Com o objetivo de auxiliar a definição e evolução de estratégias de testes , estudos_experimentais  vêm_sendo  realizados comparando diferentes técnicas e critérios de teste em relação ao custo , eficácia e dificuldade de satisfação ( strength ) . Entretanto poucos_estudos  buscam avaliar os critérios em diferentes paradigmas . Esta avaliação é importante pois o paradigma de implementação influência significativamente no programa gerado e as características entre programas implementados em diferentes paradigmas pode influenciar em diversos aspectos da atividade de teste . Este estudo é complementar a um outro trabalho do grupo do laboratório de engenharia de software do ICMC em que foram comparados o custo da aplicação dos critérios da técnica Estrutural em relação aos paradigmas Procedimental e Orientado a Objetos . Este trabalho apresenta um estudo_experimental  comparando o custo e o strength do critério_Análise  de Mutantes nos dois paradigmas . Além da avaliação do critério_Análise  de Mutantes , o material gerado para este estudo será construído de forma que possa ser utilizado para o ensino e treinamento das principais técnicas e critérios de teste e espera-se que este possa contribuir de alguma_forma  para que o ensino de teste de software possa ser aplicado em paralelo com o ensino de algoritmos e estrutura de dados . Para a condução deste estudo , foi utilizado um conjunto de 32 programas do domínio de estrutura de dados com versões implementadas em C e em Java . O critério_Análise  de Mutantes foi aplicado com auxílio das ferramentas Proteum e MuClipse . Para a avaliação do strength , o conjunto de casos de teste adequado a um programa foi executado contra os mutantes gerados na mesma versão do programa implementado no outro paradigma de interesse e o escore de mutação avaliado ( cross scoring ) . Resultados indicam que tanto o custo quanto o strength do teste de mutação é maior em programas implementados no paradigma Procedimental do que no paradigma OO . Resultados estes certamente influenciados pelo conjunto de operadores implementado nas duas ferramentas . No paradigma procedimental , também foi avaliado o escore de mutação obtido por um subconjunto dos operadores da Proteum , construído com o objetivo de reduzir o custo da aplicação do critério . O escore obtido foi satisfatório e as reduções no custo significativas . Também foi avaliado strength das técnicas Funcional e Estrutural em relação ao critério AM nos dois paradigmas . Os resultados mostram que os conjuntos de casos de teste adequados aos critérios das técnicas Funcional e Estrutural no paradigma OO obtiveram , em geral , um escore de mutação maior do que no paradigma 
 Sistemas para análise de imagens partem da premissa de que o conjunto de dados sob investigação está corretamente representado por características . Entretanto , definir quais características representam apropriadamente um conjunto de dados é uma tarefa desafiadora e exaustiva . Grande parte das técnicas de descrição existentes na literatura , especialmente quando os dados têm alta dimensionalidade , são baseadas puramente em medidas estatísticas ou abordagens baseadas em inteligência_artificial  , e normalmente são caixas-pretas para os usuários . A abordagem proposta nesta dissertação busca abrir esta caixa-preta por meio de representações_visuais  criadas pela técnica Multidimensional Classical Scaling , permitindo que usuários capturem interativamente a essência sobre a representatividade das características computadas de diferentes descritores . A abordagem é avaliada sobre seis conjuntos de imagens que contém texturas , imagens médicas e cenas naturais . Os experimentos mostram que , conforme a combinação de um conjunto de características melhora a qualidade da representação visual , a acurácia de classificação também melhora . A qualidade das representações é medida pelo índice da silhueta , superando problemas relacionados com a subjetividade de conclusões baseadas puramente em análise visual . Além_disso  , a capacidade de exploração visual do conjunto sob análise permite que usuários investiguem um dos maiores desafios em classificação de dados : a presença de variação intra-classe . Os resultados sugerem fortemente que esta abordagem pode ser empregada com sucesso como um guia para auxiliar especialistas a explorar , refinar e definir as características que representam apropriadamente um conjunto de 
 JamSession foi proposto como uma plataforma para mediar e coordenar , por meio de protocolos de interação baseados em conhecimento , recursos_computacionais  existentes com o objetivo de compor novos serviços e desenvolver aplicações inovadoras . Entre as principais características da plataforma estão sua base formal e declarativa para permitir análise e verificação formal dos protocolos , alta performance e foco na usabilidade . A plataforma pode ser utilizada , por exemplo , na construção de ambientes inteligentes e no aprimoramento dos serviços de governo eletrônico , onde o JamSession pode atuar mediando a interação entre sistemas oferecidos por órgãos públicos visando a ampliação dos serviços oferecidos . O objetivo deste trabalho é o desenvolvimento da plataforma JamSession e sua aplicação em problemas concretos de integração e coordenação . Entre as aplicações consideradas para validar a plataforma desenvolvida estão a integração de workflows interorganizacionais e a demonstração do uso da plataforma na construção de ambientes virtuais interativos 
 Como uma das consequências do crescimento populacional que atualmente ocorre em escala mundial , e possível observar um aumento significante no número de pessoas mais velhas . Com a evolução da Internet , que oferece atualmente uma ampla variedade de serviços que ultrapassam os limites do entretenimento e da comunicação , os adultos de meia-idade e os idosos podem usufruir de vários benefícios , tais como : compras , bancos on-line , serviços governamentais e informações de forma geral , de modo a preservar sua autonomia e independência funcional na realização das suas tarefas diárias . Mas , apesar de todos os benefícios que podem ser obtidos por meio da Web , existe ainda grande resistência por parte dos adultos mais velhos em utilizá-la . Muitas pessoas , com o passar dos anos , passam a conviver com dificuldades sensoriais , motoras e principalmente com declínios da capacidade cognitiva , que podem comprometer o seu acesso a Web . A acessibilidade na Web se destina a garantir , principalmente , que haja meios para que pessoas com necessidades especiais possam utilizar a Web de forma natural e independente . Os adultos de meia-idade são usuários que , de modo geral , estão propensos a sofrer com os declínios de suas capacidades ao longo do tempo . Para enriquecer a experiência dos adultos de meia-idade e idosos com a Web e essencial considerar as necessidades de diferentes perfis de usuários , bem como suas necessidades visuais , de audição , motoras e cognitivas , as quais se tornam mais evidentes a partir dos 40 anos de idade . O objetivo do presente_trabalho  foi identificar recursos e mecanismos de usabilidade e acessibilidade que atendam as várias dificuldades encontradas por grande parte dos adultos mais velhos que buscam utilizar a Web , de modo que possam auxiliá-los a superarem os declínios provenientes do envelhecimento e os incentivem a continuar utilizando a Web como uma fonte de informação , comunicação e serviços . Muitos avanços ja foram obtidos com a utilização da tecnologia assistiva , que busca , por meio de equipamentos e softwares , dar_apoio  a vários_tipos  de deficiências apresentadas pelos usuários durante a sua interação com aplicativos e com a Web . Esforços também têm sido realizados por meio da criação de diretrizes e normas , como forma de incentivar programadores e autores de conteúdo a produzirem material para a Web que seja acessível . Nesta dissertação , defende-se a tese de que a melhoria da qualidade na experiência de uso da Web por parte dos adultos mais velhos também contribui para a usabilidade universal . Assim , é apresentada uma pesquisa que engloba revisão da literatura , observação em campo , aplicação de questionários , levantamento de diculdades com deficientes visuais , e testes com protótipos ; os resultados mostram que a utilização de mecanismos de apoio a interação de adultos com mais idade com a Web promove também a melhoria da interação das pessoas mais jovens . Finalmente , pôde-se diagnosticar barreiras que ainda permanecem e quais mecanismos de apoio foram mais bem aceitos pelos adultos de meia-idade e 
 Esta dissertação tem por objetivo apresentar as licenças de software_livre  mais importantes , sob a luz dos seus principais aspectos jurídicos e da inter-compatibilidade , de forma a auxiliar pessoas envolvidas no desenvolvimento de software a compreender as implicações destas licenças ao fazer uso delas em seus projetos . A dissertação contextualiza as licenças , tanto no tocante à legislação brasileira , quanto no que diz_respeito  às restrições de licenciamento , de forma a viabilizar a análise de compatibilidade que se segue . Casos de projetos proeminentes de software_livre  cujo desenvolvimento foi afetado pelas implicações mencionadas ilustram a investigação , que é complementada por uma análise de ferramentas e metodologias existentes que auxiliam na gestão dos aspectos de licenciamento 
 Sistemas criptográficos como o RSA e o Diffie-Hellman sobre Curvas Elípticas ( DHCE ) têm fundamento em problemas computacionais considerados difíceis , por exemplo , o problema do logaritmo ( PLD ) e o problema da fatoração de inteiros ( PFI ) . Diversos trabalhos têm relacionado a segurança desses sistemas com os problemas subjacentes . Também é investigada a segurança do LSB ( bit menos significativo ) da chave secreta no DHCE ( no RSA é o LSB da mensagem ) com relação à segurança de toda a chave . Nesses trabalhos são apresentados algoritmos que conseguem inverter os sistemas criptográficos citados fazendo_uso  de oráculos que predizem o LSB . Nesta dissertação , fazemos a implementação de dois desses algoritmos . Identificamos parâmetros críticos e mudamos a amostragem do formato original . Com essa mudança na amostragem conseguimos uma melhora significativa nos tempos de execução . Um dos algoritmos ( ACGS ) , para valores práticos do RSA , era mais lento que a solução para o PFI , com nosso resultado passou a ser mais veloz . Ainda , mostramos como provas teóricas podem não definir de maneira precisa o tempo de execução de um algoritmo 
 Diversas são as aplicações que podem ser expressas por meio de grafos [ 2 ] . Algoritmos [ 3 ] e modelos de visualização [ 15 ] podem ser encontrados amplamente na literatura . Todos os problemas de grafos possuem uma base em comum : um modelo genérico que nasce da própria natureza dos elementos e das relações que podem ser expressas entre eles , diferindo apenas pelo tipo de resposta que queremos obter desta complexa malha . Além_disso  , é natural que , para problemas que sejam de áreas distintas , mas que sejam semelhantes quanto ao processamento interno , apenas o que mude , seja a visualização dos elementos que o compõe ( nós , arestas , etc. ) . Da mesma forma , independente do tipo de processamento interno , os grafos devem manter a estrutura original de grafos , ou seja , ainda deve haver uma malha que descreve os nós e suas ligações . Neste aspecto , fundamentamos nosso estudo : propomos neste trabalho , desenvolver uma API que possa ser estendida para os mais diversos problemas na área de grafos , tanto na parte visual como na representação matemática do modelo e dos algoritmos , porém , robusta , no sentido de manter a complexidade dos algoritmos envolvidos na área de grafos , além de ser completamente dirigida as necessidades de cada aplicação , podendo-se alterar apenas algumas partes da aplicação para obter um produto específico ao trabalho do 
 A anotac~ao de papeis sem^anticos ( APS ) e uma tarefa do processamento de lngua natural ( PLN ) que permite analisar parte do signicado das sentencas atraves da detecc~ao dos participantes dos eventos ( e dos eventos em si ) que est~ao sendo descritos nelas , o que e essencial para que os computadores possam usar efetivamente a informac~ao codicada no texto . A maior_parte  das pesquisas desenvolvidas em APS tem sido feita para textos em ingl^es , considerando as particularidades gramaticais e sem^anticas dessa lngua , o que impede que essas ferramentas e resultados sejam diretamente transportaveis para outras lnguas como o portugu^es . A maioria dos sistemas de APS atuais emprega metodos de aprendizado de maquina supervisionado e , portanto , precisa de um corpus grande de senten cas anotadas com papeis sem^anticos para aprender corretamente a tarefa . No caso do portugu^es do Brasil , um recurso lexical que prov^e este tipo de informac~ao foi recentemente disponibilizado : o PropBank.Br . Contudo , em comparac~ao com os corpora para outras lnguas como o ingl^es , o corpus fornecido por este projeto e pequeno e , portanto , n~ao permitiria que um classicador treinado supervisionadamente realizasse a tarefa de anotac~ao com alto_desempenho  . Para tratar esta diculdade , neste trabalho emprega-se uma abordagem semissupervisionada capaz de extrair informac~ao relevante tanto dos dados anotados disponveis como de dados n~ao anotados , tornando-a menos dependente do corpus de treinamento . Implementa-se o algoritmo self-training com modelos de regress~ ao logstica ( ou maxima entropia ) como classicador base , para anotar o corpus Bosque ( a sec~ao correspondente ao CETENFolha ) da Floresta Sinta ( c ) tica com as etiquetas do PropBank.Br . Ao algoritmo original se incorpora balanceamento e medidas de similaridade entre os argumentos de um verbo especco para melhorar o desempenho na tarefa de classicac~ao de argumentos . Usando um benchmark de avaliac~ao implementado neste trabalho , a abordagem semissupervisonada proposta obteve um desempenho estatisticamente comparavel ao de um classicador treinado supervisionadamente com uma maior quantidade de dados anotados ( 80,5 vs. 82,3 de 'F IND . 1 ' , p > 0 , 01 
 Nesta dissertação de mestrado , apresentamos um estudo dos modelos de séries_temporais  com componentes sazonais , tais que a medida presente está correlacionada com a medida imediatamente passada e com médias passadas no mesmo ponto de períodos anteriores . Dentro da classe de modelos periódicos , vamos considerar os modelos auto-regressivos periódicos - PAR . Estes modelos são adequados quando a correlação entre os meses variam de forma periódica , estas séries são ditas periodicamente estacionárias . Na análise Clássica a identificação do modelo é feita através da função de autocorrelação periódica , PeFAC e função de autocorrelação parcial periódica , PeFACP , a escolha do melhor modelo é feita usando-se o Critério de Informação Bayesiano , BIC , apresentamos ainda um teste estatístico para verificar a periodicidade na função de autocorrelação . Na análise_Bayesiana  consideramos três alternativas de densidades a priori para os parâmetros . A densidade preditiva é usada na escolha do melhor modelo e para fazer previsões um passo a frente de valores Muros da série usando resultados da simulaçãO em Cadeia de Markov , MCMC . Exploramos ainda o uso dos algoritmos de MCMC para estimar as densidade a posteriori marginais dos parâmetros do modelo . A metodologia desenvolvida neste trabalho é exemplificada com conjuntos de dados reais e simulados 
 Este trabalho apresenta uma estratégia que define , relaciona e organiza as principais atividades que devem ser realizadas para avaliar um Processo de Software . É uma estratégia simples de executar e auxilia pequenas empresas a avaliar seu próprio processo de software em relação ao nível 2 do CMM . Foi desenvolvido também um instrumento de avaliação para coleta de dados , em forma de um questionário , que auxilia na execução da estratégia proposta . ' O trabalho também apresenta um estudo de caso realizado para a aplicação da estratégia e do questionário elaborados num centro de desenvoMmento de sistemas internos de uma universidade privada para validar o trabalho elaborado 
 Um Sistema de Gerenciamento de Worldiow para apoiar a gestão de documentos , em especial daqueles controlados por sistemas de qualidade baseados nas normas da série 150-9000 , é proposto . O sistema desenvolvido , que é parte do projeto GDOC ( Gestão de Documentos ) , incorpora características dos sistemas de workflow do tipo ad hoc e administrativo , estando também em conformidade com o modelo de referência proposto pela organização padronizadora `` Workflow Management Coalition '' . O sistema tem arquitetura cliente-servidor e executa em ambiente operacional Windows 95/NT . Sua interface pode ser visualizada por qualquer browser da web . Uma extensa revisão bibliográfica sobre aplicações , conceitos , tipos e produtos comerciais existentes , relacionados com worldiow , é apresentada . Apresenta-se também a especificação conceitual do sistema e os resultados de uma simulação de exemplos reais de especificação e execução de workflow , os quais são validados pelo protótipo . Com isso , este trabalho mostra a potencialidade da tecnologia de workflow e do protótipo construído para auxilio às soluções de problemas enfrentados pelas organizações em um mercado globalizado 
 O crescimento do número de usuários da Internet e a abundância de informação sobre o assunto segurança aumentaram ainda mais os problemas relacionados à invasões de sistemas e perda de informações . Desse modo , é imprescindivel que se avance rapidamente no conhecimento das técnicas de prevenção e detecção de intrusão dos sistemas . Atualmente , a grande maioria dos ambientes computacionais existentes e que implementam alguma_forma  de segurança faz_uso  de firewalls . Entretanto , o uso de firewalls , como estratégia única de defesa , pode deixar espaços para diversas formas de intrusão . Ou seja , é necessário , o uso integrado de diversas tecnologias para aumentar a capacidade de defesa de um site . Este trabalho apresenta um enfoque alternativo para o gerenciamento de segurança utilizando agentes_móveis  para distribuir a tarefa de monitoramento do sistema e agilizar a tomada de decisões no caso de ausência do administrador humano 
 A Injeção de Defeitos é uma técnica que tem sido empregada amplamente para a construção de sistemas que precisam ser altamente confiáveis . Dentre as atividades de Injeção de Defeitos , existem estudos que englobam a injeção de defeitos de hardware e de software . Observa-se que existem poucos trabalhos_relacionados  à injeção de defeitos de software na literatura , assim como modelos de defeitos e métodos de injeção relacionados . Desse modo , o objetivo deste trabalho é estudar modelos de defeitos de software e investigar métodos de injeção , baseado nos conceitos e princípios oriundos do critério_Análise  de Mutantes . Dada a crescente complexidade dos sistemas computacionais , o projeto e a implementação de uma ferramenta de suporte à atividade de injeção tornam-se necessários . Dessa forma , neste trabalho é apresentada uma ferramenta de injeção de defeitos de software , denominada ITool , baseada em um esquema de injeção de defeitos . Esse esquema caracteriza o mapeamento de uma taxonomia de defeitos de software ( Taxonomia de DeMillo ) para os operadores de mutação do critério de teste Análise de Mutantes para a linguagem C. Para ilustrar a relevância e a factibilidade das idéias exploradas neste trabalho , conduziu-se um experimento piloto utilizando-se o programa Space , um sistema real desenvolvido pela ESA ( European Space Agency ) 
 Este trabalho descreve o projeto , desenvolvimento e aplicação de um simulador de Redes_Neurais  Construtivas para o Reconhecimento de Padrões . Este trabalho faz parte do projeto SAPRI ( Sistema para Aquisição , Processamento e Reconhecimento de Imagens ) que está sendo desenvolvido para a Marinha do Brasil . A definição da topologia de uma Rede_Neural  é um dos principais aspectos a serem considerados quando da sua utilização . A alternativa mais comum envolve a utilização de Redes_Neurais  com topologia fixa . O problema apresentado por essa técnica é que um grande número de arquiteturas precisam ser testadas antes de encontrar a melhor arquitetura . Os algoritmos construtivos tentam encontrar a melhor topologia para resolver um determinado problema sem a intervenção do usuário . Para verificar o correto funcionamento dos algoritmos construtivos implementados junto ao simulador , foram realizados diversos experimentos utilizando três conjuntos de dados . O primeiro conjunto é formado por Imagens de Navios capturadas de um radar pela Marinha do Brasil . O segundo conjunto utilizado é formado por vetores de características de Imagens de Navios obtidos da Marinha Americana . Finalmente , o terceiro conjunto de dados utilizado é formado por vetores de características de Silhuetas de Veículos 
 Vários problemas do mundo_real  podem ser modelados como problemas de otimização global , os quais são comuns em diversos campos da Engenharia e Ciência . Em geral , problemas complexos e de larga-escala não podem ser resolvidos de forma eficiente por técnicas determinísticas . Desse modo , algoritmos probabilísticos , como as metaheurísticas , têm sido amplamente empregados para otimização global . Duas das principais dificuldades nesses problemas são escapar de regiões sub-ótimas e evitar convergência prematura do algoritmo . À medida que a complexidade do problema aumenta , devido a um grande número de variáveis ou de regiões sub-ótimas , o tempo computacional torna-se grande e a possibilidade de que o algoritmo encontre o ótimo global diminui consideravelmente . Para solucionar esses problemas , propõe-se o uso de técnicas de aumento ou melhoria de eficiência . Com essas técnicas , buscase desenvolver estratégias que sejam aplicáveis a diversos algoritmos de otimização global , ao invés de criar um novo algoritmo de otimização ou um algoritmo híbrido . No contexto de problemas contínuos , foram desenvolvidas técnicas para determinação de uma ou mais regiões promissoras do espaço de busca , que contenham uma grande quantidade de soluções de alta qualidade , com maior chance de conterem o ótimo global . Duas das principais técnicas propostas , o Algoritmo de Otimização de Domínio ( DOA ) e a arquitetura de Amostragem Inteligente ( SS ) , foram testadas com sucesso significativo em vários problemas de otimização global utilizados para benchmark na literatura . A aplicação do DOA para metaheurísticas produziu melhoria de desempenho em 50 % dos problemas testados . Por outro_lado  , a aplicação da SS produziu reduções de 80 % da quantidade de avaliações da função objetivo , bem como aumentou a taxa de sucesso em encontrar o ótimo global . Em relação a problemas discretos ( binários ) , foram abordados problemas nos quais existem correlações entre as variáveis , que devem ser identificadas por um modelo probabilístico . Das duas técnicas de aumento de eficiência propostas para esses problemas , a técnica denominada Gerenciamento do Tamanho da População ( PSM ) possibilita a construção de modelos_probabilísticos  mais representativos . Com o PSM foi possível atingir uma redução de cerca de 50 % na quantidade de avaliações , mantendo a taxa de sucesso em 100 % . Em resumo , as técnicas de aumento de eficiência propostas mostramse capazes de aumentar significativamente o desempenho de metaheurísticas , tanto para problemas contínuos quanto para 
 O mapeamento de ambientes é um dos maiores desafios para pesquisadores na área de navegação_autônoma  . As técnicas existentes estão divididas em dois importantes paradigmas , o mapeamento métrico e o topológico . Diversos métodos de mapeamento que combinam as vantagens de cada um desses paradigmas têm sido_propostos  . Este projeto consiste na adaptação e extensão de um sistema integrado para navegação_autônoma  de robôs_móveis  através do aperfeiçoamento da interface e também da incorporação de uma técnica de mapeamento topológico . Para isso , a técnica conhecida como Grade de Ocupação , utilizada em geral para mapeamento métrico é combinada com um método de esqueletização de imagens para a realização do mapeamento topológico . Além_disso  , transformações morfológicas de erosão e abertura , adequadas a ambientes reais , foram utilizadas , visando reduzir a influência de ruídos na abordagem proposta , uma vez que devido a ruídos inerentes as leituras sensoriais obtidas pelo robô , o mapa topológico gerado apresenta diversas linhas topológicas desnecessárias , dificultando consequentemente a tarefa de navegação_autônoma  . Vários experimentos foram executados para verificar a eficiência da combinação de técnicas proposta , tanto em nível de simulação quanto em um robô real . Os resultados obtidos demonstraram que a técnica de esqueletização de imagens combinada ao mapeamento métrico do ambiente é uma forma simples e viável de se obter as linhas topológicas do espaço livre do ambiente . A aplicação das transformações morfológicas demonstrou ser eficiente para a criação de mapas topológicos livres de ruído , uma vez que elimina grande parte das linhas topológicas geradas em conseqüência dos ruídos dos sensores do 
 Processos de Decisão Markovianos ( Markov Decision Process - MDP ) modelam problemas de tomada de decisão sequencial em que as possíveis ações de um agente possuem efeitos probabilísticos sobre os estados sucessores ( que podem ser definidas por matrizes de transição de estados ) . Programação dinâmica em tempo real ( Real-time dynamic programming - RTDP ) , é uma técnica usada para resolver MDPs quando existe informação sobre o estado inicial . Abordagens tradicionais apresentam melhor desempenho em problemas com matrizes esparsas de transição de estados porque podem alcançar eficientemente a convergência para a política ótima , sem ter que visitar todos os estados . Porém essa vantagem pode ser perdida em problemas com matrizes densas de transição , nos quais muitos estados podem ser alcançados em um passo ( por exemplo , problemas de controle com eventos exógenos ) . Uma abordagem para superar essa limitação é explorar regularidades existentes na dinâmica do domínio através de uma representação fatorada , isto é , uma representação baseada em variáveis de estado . Nesse trabalho de mestrado , propomos um novo algoritmo chamado de FactRTDP ( RTDP Fatorado ) , e sua versão aproximada aFactRTDP ( RTDP Fatorado e Aproximado ) , que é a primeira versão eficiente fatorada do algoritmo clássico RTDP . Também propomos outras 2 extensões desses algoritmos , o FactLRTDP e aFactLRTDP , que rotulam estados cuja função valor convergiu para o ótimo . Os resultados experimentais_mostram  que estes novos algoritmos convergem mais rapidamente quando executados em domínios com matrizes de transição densa e tem bom comportamento online em domínios com matrizes de transição densa com pouca dependência entre as variáveis de estado 
 Em simulações físicas baseadas em partículas , a informação sobre quais partículas pertencem à fronteira do sistema e quais são consideradas internas é , em geral , uma informação útil porém difícil de ser obtida eficientemente . Esta informação pode ser usada na geração da superfície_livre  de um fluido ou no cálculo da tensão superficial o mesmo , entre outras aplicações . Técnicas encontradas na literatura podem apresentar resultados_satisfatórios  , mas em geral são sensíveis à escala do problema , distribuição das partículas e envolvem operações computacionalmente caras como inversão de matrizes . O objetivo deste trabalho é estudar os métodos existentes e apresentar uma alternativa com custo_computacional  mais baixo e que seja capaz de lidar com problemas de diferentes escalas e naturezas de forma mais simples que os métodos existentes 
 No contexto de Redes Complexas , particularmente das redes_sociais  , grupos de objetos densamente conectados entre si , esparsamente conectados a outros grupos , são denominados de comunidades . Detecção dessas comunidades tornou-se um campo de crescente_interesse  científico e possui inúmeras aplicações práticas . Nesse contexto , surgiram várias pesquisas sobre estratégias multinível para particionar redes com elevada quantidade de vértices e arestas . O objetivo dessas estratégias é diminuir o custo do algoritmo de particionamento aplicando-o sobre uma versão reduzida da rede original . Uma possibilidade dessa estratégia , ainda pouco explorada , é utilizar heurísticas de refinamento local para melhorar a solução final . A maioria das abordagens de refinamento exploram propriedades gerais de redes complexas , tais como corte mínimo ou modularidade , porém , não exploram propriedades inerentes de domínios específicos . Por exemplo , redes_sociais  são caracterizadas por elevado coeficiente de agrupamento e assortatividade significativa , consequentemente , maximizar tais características pode conduzir a uma boa solução e uma estrutura de comunidades bem definida . Motivado por essa lacuna , neste trabalho é proposto um novo algoritmo de refinamento , denominado RSim , que explora características de alto grau de transitividade e assortatividade presente em algumas redes reais , em particular em redes_sociais  . Para isso , adotou-se medidas de similaridade híbridas entre pares de vértices , que utilizam os conceitos de vizinhança e informações de comunidades para interpretar a semelhança entre pares de vértices . Uma análise comparativa e sistemática demonstrou que o RSim supera os algoritmos de refinamento habituais em redes com alto coeficiente de agrupamento e assortatividade . Além_disso  , avaliou-se o RSim em uma aplicação real . Nesse cenário , o RSim supera todos os métodos avaliado quanto a eficiência e eficácia , considerando todos os conjuntos de dados selecionados 
 Este trabalho apresenta o projeto e a implementação da política de escalonamento com suporte à migração de processos JUMP . A migração de processos é uma ferramenta importante que complementa a alocação inicial realizada pela política de escalonamento em um ambiente paralelo distribuído , permitindo um balanceamento de carga dinâmico e mais refinado , resultando em um melhor desempenho do ambiente e menor tempo de resposta das aplicações paralelas distribuídas . A nova política unifica a alocação inicial e migração de processos em um único algoritmo , de forma a compartilhar decisões para o objetivo comum de prover um melhor desempenho para aplicações de uso intensivo de processamento em clusters heterogêneos . A política é implementada sobre o ambiente de escalonamento flexível e dinâmico AMIGO , adaptado para o suporte à migração de processos . A avaliação de desempenho mostrou que a nova política oferece ganhos expressivos nos tempos de resposta quando comparada às outras duas políticas de escalonamento implementadas no AMIGO , em quase todos os cenários , para diversas aplicações e diversas situações de carga do 
 Esta dissertação trata da solução_numérica  de problemas em dinâmica dos fluidos usando dois novos esquemas upwind de alta_resolução  , denominados FDPUS-C1 ( Five-Degree Polynomial Upwind_Scheme  of ' C POT . 1 ' Class ) e SDPUS-C1 ( Six-Degree Polynomial Upwind_Scheme  of 'C POT.1 ' Class ) , para a discretização de termos convectivos lineares e não-lineares . Os esquemas são baseados nos critérios de estabilidade TVD ( Total Variation Diminishing ) e CBC ( Convection Boundedness Criterion ) e são implementados , nos contextos das metodologias de diferenças_finitas  e volumes finitos , no ambiente de simulação Freeflow ( an integrated simulation system for Free surface Flow ) para escoamentos imcompressíveis 2D , 2D-1/2 e 3D , ou no código bem conhecido CLAWPACK ( Conservation LAW PACKage ) para problemaw compressíveis 1D e 2D . Vários testes computacionais são feitos com o objetivo de verificar e validar os métodos_numéricos  contra esquemas upwind populares . Os novos esqumas são então aplicados na resolução de uma gama ampla de problemas em CFD ( Computational Fluids Dynamics ) , tais como propagação de ondas de choque e escoamentos_incompressíveis  envolvendo superfícies_livres  móveis . Em particular , os resultados numéricos para leis de conservação hiperbólicas 2D e equações de Navier-Stokes incompressíveis 2D , 2D-1/2 e 3D demosntram que esses novos esquemas convectivos tipo upwind polinomiais funcionam muito 
 A predição de estruturas de proteínas ( Protein Structure Prediction PSP ) é um problema computacionalmente complexo . Para tratar esse problema , modelos simplificados de proteínas , como o Modelo HP , têm sido empregados para representar as conformações e Algoritmos_Evolutivos  ( AEs ) são utilizados na busca por soluções adequadas para PSP . Entretanto , abordagens utilizando AEs muitas_vezes  não tratam adequadamente as soluções geradas , prejudicando o desempenho da busca . Neste trabalho , é apresentada uma formulação multiobjetivo para PSP em Modelo HP , de modo a avaliar de forma mais robusta as conformações produzidas combinando uma avaliação baseada no número de contatos hidrofóbicos com a distância entre os monômeros . Foi adotado o Algoritmo Evolutivo Multiobjetivo em Tabelas ( AEMT ) a fim de otimizar essas métricas . O algoritmo pode adequadamente explorar o espaço de busca com pequeno número de indivíduos . Como consequência , o total de avaliações da função objetivo é significativamente reduzido , gerando um método para PSP utilizando Modelo HP mais rápido e 
 O tema deste trabalho é o estudo de métodos_numéricos  para a simulação de escoamentos_incompressíveis  com superfície_livre  a baixos Reynolds , por meio da metodologia GENSMAC e suas formulações explícita e implícita . Neste contexto , temos especial interesse na formulação implícita , objetivando o enfraquecimento da restrição de estabilidade parabólica no passo temporal em escoamentos com superfície_livre  . O trabalho pode ser dividido em duas partes : na primeira , algumas modificações são discutidas , propostas e testadas com o objetivo de tornar a formulação implícita mais eficiente e precisa ; em seguida , aproveitamos os resultados obtidos e generalizamos o método_numérico  existente para simular_escoamentos  viscoelásticos modelados pela equação_constitutiva  SXPP . Em ambos os casos , a formulação explícita também é usada para comparação e teste . Resultados que demonstram a eficiência e robustez das técnicas desenvolvidas são apresentados por meio da simulação_numérica  de complexos problemas envolvendo superfície livre.O tema deste trabalho é o estudo de métodos_numéricos  para a simulação de escoamentos incom- pressíveis com superfície_livre  a baixos Reynolds , por meio da metodologia GENSMAC e suas formulações explícita e implícita . Neste contexto , temos especial interesse na formulação implícita , objetivando o en- fraquecimento da restrição de estabilidade parabólica no passo temporal em escoamentos com superfície_livre  . O trabalho pode ser dividido em duas partes : na primeira , algumas modificações são discutidas , propostas e testadas com o objetivo de tornar a formulação implícita mais eficiente e precisa ; em seguida , aproveitamos os resultados obtidos e generalizamos o método_numérico  existente para simular_escoamentos  viscoelásticos modelados pela equação_constitutiva  SXPP . Em ambos os casos , a formulação explícita também é usada para comparação e teste . Resultados que demonstram a eficiência e robustez das técnicas desenvolvidas são apresentados por meio da simulação_numérica  de complexos problemas envolvendo superfície_livre 
 Sofisticadas técnicas para estimação de modelos baseadas em simulação , os filtros de partículas ou métodos de Monte_Carlo  Seqüenciais , foram empregadas recentemente para solucionar diversos problemas difícieis no campo da robótica_móvel  . No entanto , o sucesso dos fitros de partículas limitou-se à computação de parâmetros em espaços de baixa dimensionalidade . Os atuais esforços de pesquisa em robótica_móvel  têm comecado a explorar certas propriedades estruturais de seus domnios de aplicação que envolvem a utilização de filtros de partculas em espacos de maior dimensão , aumentando consideravelmente a complexidade da simulação envolvida . Simulações estatsticas dessa natureza requerem uma grande quantidade de numeros pseudo-aleatorios que possam ser gerados eficientemente e atendam a certos criterios de qualidade . O processo de geração de numeros pseudo-aleatorios torna-se o ponto crtico de tais aplicações em termos de desempenho . Neste contexto , a computação reconguravel insere-se como uma tecnologia capaz de satisfazer a demanda por alto_desempenho  das grandes simulações estatsticas pois sistemas baseados em arquiteturas reconguraveis possuem o potencial de mapear computação em hardware visando aumento de eficiência sem comprometer seriamente sua exibilidade . Tecnologias reconguraveis também possui o atrativo de um baixo consumo de energia , uma caracterstica essencial para os futuros robôs moveis embarcados . Esta dissertação apresenta a implementação um sistema embarcado baseado em FPGA e projetado para solucionar o problema de localização de robôs por meio de tecnicas probabilsticas . A parte fundamental de todo este sistema e um veloz gerador de numeros aleatorios mapeado ao hardware reconguravel que foi capaz de atender rígidos criterios estatsticos de 
 Os sistemas Gerenciadores de Bases de Dados ( SGBDs ) foram desenvolvidos para manipular domínios de dados numéricos e/ou pequenas seqüencias de caracteres ( palavras ) e não foram projetados prevendo a manipulação de dados complexos , como por exemplo dados multimídia . Os operadores em domínios de dados que requisitam a relação de ordem têm pouca utilidade para manipular operações que envolvem dados complexos . Uma classe de operadores que se adequa melhor para manipular esses dados são os operadores por similaridade : consulta por abrangência ( `` range queries ' ) e consulta de vizinhos mais próximos ( `` k-nearest neighbor queries ' ) . Embora muitos resultados já tenham sido obtidos na elaboração de algoritmos de busca por similaridade , todos eles consideram uma única função para a medida de similaridade , que deve ser universalmente aplicável a todos os pares de elementos do conjunto de dados . Este projeto propõe explorar a possibilidade de trabalhar com estruturas de dados concebidas dentro dos conceitos de dados em domínios métricos , mas que admitam o uso de uma função de distância adaptável , ou seja , que mude para determinados grupos de objetos , dependendo de algumas características universais , e assim permitindo acomodar características que sejam particulares a algumas classes de imagens e não de todo o conjunto delas , classificando as imagens em uma hierarquia de tipos , onde cada tipo está associado a uma função de distância diferente e vetores de características diferentes , todos indexados numa mesma árvore 
 Tratamos da generalização do problema da geração de caminho mínimo , no qual não apenas um , mas vários caminhos de menores custos devem ser produzidos . O problema dos k-menores caminhos consiste em listar os k caminhos de menores custos conectando um par de vértices . Esta dissertação trata de algoritmos para geração de k-menores caminhos em grafos simétricos com custos não-negativos , bem como algumas implementações destes 
 Nos últimos_anos  , há um crescente_interesse  por aplicações do Processamento de Língua Natural ( PLN ) que processam uma coleção de textos sobre um mesmo assunto e produzem um novo texto de saída , quer seja um sumário ou uma resposta para uma dada pergunta . Para se produzir textos com qualidade , essas aplicações precisam lidar adequadamente com vários fenômenos , tais como a redundância , a contradição e a complementaridade de informações . Nesse contexto , um processo que permita a identificação de informações comuns em um conjunto de sentenças relacionadas , e gere uma nova sentença a partir da fusão de informações das sentenças de entrada , sem redundâncias e sem contradições , é de grande relevância para as aplicações que processam múltiplos textos . A fusão automática de sentenças é um tema de pesquisa relativamente recente na literatura de PLN e para a língua_portuguesa  , em particular , não se tem conhecimento de trabalhos dessa natureza . Neste trabalho propõe-se um método inédito para a fusão de sentenças similares em português , baseado em uma abordagem simbólica e independente de domínio , e produz-se o Zíper , um sistema de fusão sentencial que implementa o método proposto . O Zíper é o primeiro sistema a contemplar a geração de sentenças que expressam todas as informações das sentenças de entrada , ou seja , que representam a união do conjunto . Além_disso  , ele permite a geração de sentenças que expressam apenas as informações redundantes do conjunto ( consideradas mais importantes ) , isto é , que representam a interseção das sentenças de entrada . O sistema foi avaliado intrinsecamente e os resultados obtidos_mostram  que , de modo geral , as sentenças produzidas são bem formadas e preservam a mensagem original do conjunto ( isto é , a mensagem toda , na fusão por união e apenas a mensagem principal , na fusão por interseção ) . Zíper também foi avaliado extrinsecamente no contexto de um sumarizador multidocumento do português . Os resultados alcançados sugerem que o método proposto contribui para melhorar a qualidade dos sumários , reduzindo a redundância de informações , que frequentemente provoca a perda de coesão e de 
 Mineração de fluxos contínuos de dados é uma área de pesquisa emergente que visa extrair conhecimento a partir de grandes quantidades de dados , gerados continuamente . Detecção de novidade é uma tarefa de classificação que consiste em reconhecer que um exemplo ou conjunto de exemplos em um fluxo de dados diferem significativamente dos exemplos vistos anteriormente . Essa é uma importante tarefa para fluxos contínuos de dados , principalmente porque novos conceitos podem aparecer , desaparecer ou evoluir ao longo do tempo . A maioria dos trabalhos da literatura apresentam a detecção de novidade como uma tarefa de classificação binária . Poucos trabalhos tratam essa tarefa como multiclasse , mas usam medidas de avaliação binária . Em vários problemas , o correto seria tratar a detecção de novidade em fluxos contínuos de dados como uma tarefa multiclasse , no qual o conceito conhecido do problema é formado por uma ou mais classes , e diferentes novas classes podem aparecer ao longo do tempo . Esta tese propõe um novo algoritmo MINAS para detecção de novidade em fluxos contínuos de dados . MINAS considera que a detecção de novidade é uma tarefa multiclasse . Na fase de treinamento , MINAS constrói um modelo de decisão com base em um conjunto de exemplos_rotulados  . Na fase de aplicação , novos exemplos são classificados usando o modelo de decisão atual , ou marcados como desconhecidos . Grupos de exemplos desconhecidos podem formar padrões-novidade válidos , que são então adicionados ao modelo de decisão . O modelo de decisão é atualizado ao longo do fluxo a fim de refletir mudanças nas classes conhecidas e permitir inserção de padrões-novidade . Esta tese também propõe uma nova metodologia para avaliação de algoritmos para detecção de novidade em fluxos contínuos de dados . Essa metodologia associa os padrões-novidade não rotulados às classes reais do problema , permitindo assim avaliar a matriz de confusão que é incremental e retangular . Além_disso  , a metodologia de avaliação propõe avaliar os exemplos desconhecidos separadamente e utilizar medidas de avaliação multiclasse . Por último , esta tese apresenta uma série de experimentos executados usando o MINAS e os principais algoritmos da literatura em bases de dados artificiais e reais . Além_disso  , o MINAS foi aplicado a um problema real , que consiste no reconhecimento de atividades humanas usando dados de acelerômetro . Os resultados experimentais_mostram  o potencial do algoritmo e da metodologia 
 A Arquitetura Orientada a Serviços ( SOA ) é um estilo arquitetural formado por um conjunto de restrições que visa promover a escalabilidade e a flexibilidade de um sistema , provendo suas funcionalidades como serviços . Nos últimos_anos  , um estilo alternativo foi proposto e amplamente adotado , que projeta as funcionalidades de um sistema como recursos . Este estilo arquitetural orientado a recursos é chamado de REST . O teste de serviços_web  em geral apresenta vários desafios devido a sua natureza distribuída , canal de comunicação pouco confiável , baixo acoplamento e a falta de uma interface de usuário . O teste de serviços RESTful ( serviços que utilizam o REST ) compartilham estes mesmos desafios e ainda necessitam que suas restrições sejam obedecidas . Estes desafios demandam testes mais sistemáticos e formais . Neste contexto , o teste baseado em modelos ( TBM ) se apresenta como um processo viável para abordar essas necessidades . O modelo que representa o sistema deve ser simples e ao mesmo tempo preciso para que sejam gerados casos de teste com qualidade . Com base nesse contexto , este projeto de mestrado_propõe  uma abordagem baseada em modelos para testar serviços RESTful . O modelo comportamental adotado foi a máquina de estados de protocolos UML , capaz de formalizar a interface do serviço enquanto esconde o seu funcionamento interno . Uma ferramenta foi desenvolvida para gerar automaticamente os casos de teste usando critérios de cobertura de estados e transições para percorrer o 
 Este trabalho trata do problema da construção de atributos para classificação quando atributos e rótulos são binários . A abordagem adotada visa reduzir efeitos de interação entre atributos , amenizando a necessidade dos classificadores lidarem com essas interações . Para tanto , é introduzida uma nova_técnica  que usa uma matriz de cálculo de paridade para transformar as coordenadas do vetor de atributos binários . Tal matriz permite a manipulação de diversas medidas derivadas da teoria da informação . A transformação resultante induz a formação de grupos de variáveis binárias . Baseando-se nessa técnica , um algoritmo inédito de análise de componentes independentes de variáveis binárias é apresentado , assim como um algoritmo que induz a independência condicional entre os atributos ( dado o valor do rótulo ) . Um terceiro algoritmo apresentado reduz a Informação de Interação entre os atributos , uma medida associada ao grau de redundância ou colaboração entre atributos . Tal algoritmo é empregado no problema do projeto de operadores em dois_níveis  para imagens , em que múltiplos operadores são combinados para a obtenção de uma imagem final . Nesse caso , o algoritmo apresentado guia a estratégia de divisão de uma imagem em sub-regiões . É apresentado um arcabouço para o projeto de operadores de imagens em dois_níveis  , incorporando métodos de seleção de atributos e comparação de modelos . Os resultados mostram que o método proposto propicia melhor desempenho , em comparação com operadores de nível único 
 Esta dissertação apresenta um estudo comparativo de diferentes métodos computacionais de segmentação estrutural musical , onde o principal objetivo é delimitar fronteiras de seções musicais em um sinal de áudio , e rotulá-las , i.e . agrupar as seções encontradas que correspondem a uma mesma parte musical . São apresentadas novas propostas para segmentação estrutural nãosupervisionada , incluindo métodos para processamento em tempo real , alcançando resultados com taxas de erro inferiores a 12 % . O método utilizado compreende um estudo dos descritores sonoros e meios de modelá-los temporalmente , uma exposição das técnicas computacionais de segmentação estrutural e novos métodos de avaliação dos resultados que penalizam tanto a incorreta detecção das fronteiras quanto o número incorreto de rótulos encontrados . O desempenho de cada técnica computacional é calculado utilizando diferentes conjuntos de descritores sonoros e os resultados são apresentados e analisados tanto quantitativa quanto qualitativamente 
 A mitigação de desastres naturais exige respostas rápidas e confiáveis . No Brasil , a estação de chuvas provoca muitos alagamentos em regiões urbanas e , para monitorar esse fenômeno , foi instalada em São Carlos-SP uma rede de sensores sem fio para acompanhar o nível de água dos rios da cidade . Entretanto , essa rede de sensores está suscetível a falhas que podem comprometer o funcionamento do sistema , e a adoção de mecanismos redundantes e de redes móveis 3G podem acarretar em custos proibitivos à monitoração desses rios , além de não garantirem a operação normal desse monitoramento . Assim , este trabalho apresenta uma solução baseada em veículo aéreo não tripulado ( VANT ) para reduzir os problemas oriundos das falhas em uma rede de sensores para detectar desastres naturais como enchentes e deslizamentos . Na solução proposta , o VANT pode ser transportado para o sítio do desastre para minimizar os problemas provenientes das falhas ( por exemplo , para servir como um roteador ou até mesmo para servir como uma mula de dados e transmitir imagens em tempo real para equipes de resgate ) . Estudos foram conduzidos em um protótipo real , equipado com o UAV Brain ( módulo computacional desenvolvido especificamente para este projeto ) , para uma análise exploratória do consumo energético do VANT e do rádio transmissor que equipa o VANT . Os resultados mostram que a melhor situação para o rádio comunicador se dá quando o VANT está no ar e com uma antena de maior ganho , e os fatores que mais influenciam no consumo energético do rádio são a altura do VANT e o tipo de antena utilizado . Além_disso  , tais resultados mostram também a viabilidade desta proposta em redes de sensores sem fio linear e esparsa 
 A área de recuperação de informação baseada em conteúdo visual vem ganhando importância graças ao volume de material visual existente ( imagens e vídeo digitais ) , compartilhado e distribuído principalmente via Internet , e à capacidade de processamento alcançada pelos computadores_pessoais  na última_década  
 Novas formas de consumo , manipulação e exploração de vídeo digital podem ser criadas através da organização e indexação apropriada desse material . A delimitação de tomadas fornece uma base para a abstração e estruturação de vídeo , agregando quadros contíguos em seqüências de mesmo contexto , isto é , trechos com unidade em termos de tempo e espaço 
 Nesta dissertação são apresentados os conceitos básicos de delimitação de tomadas e métodos tradicionais utilizados nesse tipo de segmentação , bem como vários resultados experimentais obtidos a partir de seqüências reais de TV . É analisada a distribuição das diferenças entre quadros sucessivos , calculada através de seus histogramas , na tentativa de caracterizar as transições entre tomadas e obter melhores parâmetros para a segmentação . Obtêm-se experimentalmente mais evidências que comprovam a superioridade da medida de intersecção de histogramas sobre outras medidas 
 A principal_contribuição  do trabalho consiste no desenvolvimento de um algoritmo baseado no método twin-comparison , que apresenta melhor desempenho que o método original na detecção dos limites de tomadas por utilizar análise local da variação visual entre os quadros do vídeo 
 O processo de Mineração de Dados ( MD ) consiste na extração automática de padrões que representam o conhecimento implícito em grandes bases de dados . Em geral , a MD pode ser classificada em duas categorias : preditiva e descritiva . Tarefas da primeira categoria , tal como a classificação , realizam inferências preditivas sobre os dados enquanto que tarefas da segunda categoria , tal como o clustering , exploram o conjunto de dados em busca de propriedades que o descrevem . Diferentemente da classificação , que analisa exemplos_rotulados  , o clustering utiliza exemplos para os quais o rótulo da classe não é previamente conhecido . Nessa tarefa , agrupamentos são formados de modo que exemplos de um mesmo cluster apresentam alta similaridade , ao passo que exemplos em clusters diferentes apresentam baixa similaridade . O clustering pode ainda facilitar a organização de clusters em uma hierarquia de agrupamentos , na qual são agrupados eventos similares , criando uma taxonomia que pode simplificar a interpretação de clusters . Neste trabalho , é proposto e desenvolvido um módulo de aprendizado não-supervisionado , que agrega algoritmos de clustering hierárquico e ferramentas de análise de clusters para auxiliar o especialista de domínio na interpretação dos resultados do clustering . Uma vez que o clustering hierárquico agrupa exemplos de acordo com medidas de similaridade e organiza os clusters em uma hierarquia , o usuário/especialista pode analisar e explorar essa hierarquia de agrupamentos em diferentes níveis para descobrir conceitos descritos por essa estrutura . O módulo proposto está integrado em um sistema maior , em desenvolvimento no Laboratório de Inteligência Computacional ? LABIC ? , que contempla todas as etapas do processo de MD , desde o pré-processamento de dados ao pós-processamento de conhecimento . Para avaliar o módulo proposto e seu uso para descoberta de conceitos a partir da estrutura hierárquica de clusters , foram realizados diversos experimentos sobre conjuntos de dados naturais , assim como um estudo de caso utilizando um conjunto de dados real . Os resultados mostram a viabilidade da metodologia_proposta  para interpretação dos clusters , apesar da complexidade do processo ser dependente das características do conjunto de dados 
 A Computação Ubíqua está revolucionando a interação do ser-humano com os dispositivos computacionais ao disponibilizar tecnologias pouco intrusivas ao cotidiano das pessoas . A Computação Ciente de Contexto , um dos temas de pesquisa em Computação Ubíqua , tem contribuído para a construção de aplicações capazes de customizar-se e adaptar-se às necessidades do usuário sem a intervenção explícita deste . Suportar o desenvolvimento de aplicações Cientes de Contexto é um dos desafios da Computação Ubíqua . Desenvolvido no mesmo grupo de pesquisa que este trabalho está inserido , o Serviço Web Context Kernel , gerencia informações de contexto e explora as especificações e tecnologias da Web como plataforma de intercâmbio para a informação contextual . O trabalho aqui reportado teve como objetivo investigar a utilização de Serviços_Web  por meio do emprego da infra-estrutura Context Kernel na integração de aplicações em cenários de trabalho colaborativo e de aprendizado eletrônico . Como resultado , foi elaborado um conjunto de lições aprendidas provenientes do estudo e do emprego das especificações para Serviços_Web  . Outros resultados foram : as especificações de informações de contexto de grupo e de metadados educacionais em dimensões de contexto e exemplos de utilização do Context Kernel para tornar aplicações cientes de contexto 
 O volume crescente de projetos de pesquisa que envolvem software motiva a realização de análises sobre os processos utilizados no desenvolvimento desses projetos , considerando-se as atividades realizadas , os resultados obtidos e a aplicação de recursos . Na área de Engenharia de Software , processos têm sido_propostos  nos últimos_anos  com o objetivo de contribuir para o desenvolvimento de projetos de pesquisa que envolvem software . Busca-se , de forma geral , auxiliar o gerenciamento dos diversos artefatos que podem ser gerados , por exemplo , modelos , código fonte , relatórios técnicos e artigos científicos . As soluções apresentadas na literatura começaram a ser propostas recentemente e , portanto , os estudos realizados nessa área apresentam-se em estágio bastante inicial . Dessa forma , o objetivo deste trabalho foi definir um processo para o desenvolvimento de projetos de pesquisa envolvendo software , que esteja de acordo com um conjunto de requisitos identificado e que possa contribuir para a evolução desses projetos . Para reforçar a continuidade dos projetos , foi estudada a abordagem de design rationale , com o objetivo de oferecer meios para que as decisões sejam capturadas e registradas em fases específicas do desenvolvimento . O processo de documentação foi enfatizado , ou seja , foi investigado como a abordagem de design rationale poderia ser utilizada para melhorar a documentação dos projetos de pesquisa . Como um resultado obtido , foi definido um modelo para representação de design rationale ( DR-SACI ) , implementado em uma ferramenta CASE e avaliado em um 
 Representações visuais têm sido adotadas na exploração de conjuntos de documentos , auxiliando a extração de conhecimento sem que seja necessária a análise individual de milhares de textos . Mapas de documentos , em particular , apresentam documentos individualmente representados espalhados em um espaço_visual  , refletindo suas relações de similaridade ou conexões . A construção destes mapas de documentos inclui , entre outras tarefas , o posicionamento dos textos e a identificação automática de áreas temáticas . Um desafio é a visualização de conjuntos dinâmicos de documentos . Na visualização de informação , é comum que alterações no conjunto de dados tenham um forte impacto na organização do espaço_visual  , dificultando a manutenção , por parte do usuário , de um mapa mental que o auxilie na interpretação dos dados apresentados e no acompanhamento das mudanças sofridas pelo conjunto de dados . Esta tese introduz um algoritmo para a construção dinâmica de mapas de documentos , capaz de manter uma disposição coerente à medida que elementos são adicionados ou removidos . O processo , inerentemente incremental e de baixa complexidade , utiliza um espaço bidimensional dividido em células , análogo a um tabuleiro de xadrez . Resultados consistentes foram alcançados em comparação com técnicas não incrementais de projeção de dados multidimensionais , tendo sido a técnica aplicada também em outros domínios , além de conjuntos de documentos . A visualização resultante não está sujeita a problemas de oclusão . A identificação de áreas temáticas é alcançada com técnicas de extração de regras de associação representativas para a identificação automática de tópicos . A combinação da extração de tópicos com a projeção incremental de dados em um processo integrado de mineração visual de textos compõe um espaço_visual  em que tópicos e áreas de interesse são destacados e atualizados à medida que o conjunto de dados é 
 O objetivo desse trabalho é desenvolver métodos_numéricos  para simular_escoamentos  de cristais líquidos nemáticos governados pelas equações dinâmicas de Ericksen-Leslie . São apresentados dois métodos_numéricos  para a simulação de escoamentos de cristais líquidos nemáticos . O primeiro método foi desenvolvido para simular_escoamentos  tridimensionais de cristais líquidos nemáticos sob efeito de forte campo magnético enquanto que o segundo método foi desenvolvido para a simulação de escoamentos bidimensionais . Utilizando a notação de Einstein , as equações dinâmicas de Ericksen-Leslie são apresentadas . Empregando variáveis primitivas e coordenadas cartesianas , as equações governantes para escoamentos de cristais líquidos nemáticos são derivadas e as formulações_matemáticas  para a obtenção dos métodos_numéricos  são apresentadas . As equações descrevendo os métodos_numéricos  são resolvidas por um método_numérico  baseado na metodologia GENSMAC3D para o caso tridimensional enquanto que o método bidimensional é baseado na metodologia GENSMAC ( GENeralized-Simplified-Marker-And-Cell ) . Em ambos os métodos , a técnica de diferenças_finitas  em uma malha_deslocada  é utilizada . As equações que descrevem as técnicas numéricas desenvolvidas foram incorporadas aos ambientes de simulação Freeflow2D e Freeflow3D . As condições de contorno para cada tipo de contorno são descritas em detalhes . A solução analítica apresentada por Stewart para o escoamento entre duas placas paralelas é utilizada para a validação do método_numérico  tridimensional . Empregando as hipóteses de escoamento desenvolvido e que o ângulo de orientação do diretor é pequeno , uma solução analítica para o escoamento em um canal bidimensional é encontrada . O método_numérico  bidimensional é então validado utilizando a solução analítica obtida . Utilizando refinamento de malha , resultados de convergência dos métodos_numéricos  são apresentados . Os métodos_numéricos  desenvolvidos nesse trabalho são aplicados para a simulação dos seguintes problemas : escoamento de um cristal líquido nemático em um canal tridimensional ; investigação numérica do escoamento em L-canais e escoamento através de uma contração 4:1 e de uma expansão planar 
 Este trabalho consiste em introduzir uma nova abordagem de representação de superfície no ambiente de simulação Freeflow2D . Consiste em usar Partição da Unidade Implícita para estimar da superfície a geometria , normais e curvatura . Procurando se valer das vantagens de métodos do tipo meshless ( sem malha ) conservando no entanto a malha Lagrangiana , no interesse de manter o fácil acesso de vizinhança , inserção e eliminação de 
 Os modelos normais de regressão têm sido utilizados durante muitos anos para a análise de dados . Mesmo nos casos em que a normalidade não podia ser suposta , tentava-se algum tipo de transformação com o intuito de alcançar a normalidade procurada . No entanto , na prática , essas suposições sobre normalidade e linearidade nem sempre são satisfeitas . Como alternativas à técnica clássica , foram desenvolvidas novas classes de modelos de regressão . Nesse contexto , focamos a classe de modelos em que a distribuição assumida para a variável resposta pertence à classe de distribuições simétricas . O objetivo geral desse trabalho é a modelagem desta classe no contexto bayesiano , em particular a modelagem da classe de modelos não-lineares heterocedásticos simétricos . Vale ressaltar que esse trabalho tem ligação com duas linhas de pesquisa , a saber : a inferência estatística abordando aspectos da teoria assintótica e a inferência bayesiana considerando aspectos de modelagem e critérios de seleção de modelos baseados em métodos de simulação de Monte_Carlo  em Cadeia de Markov ( MCMC ) . Uma primeira etapa consiste em apresentar a classe dos modelos não-lineares heterocedásticos simétricos bem como a inferência clássica dos parâmetros desses modelos . Posteriormente , propomos uma abordagem_bayesiana  para esses modelos , cujo_objetivo  é mostrar sua viabilidade e comparar a inferência bayesiana dos parâmetros estimados via métodos MCMC com a inferência clássica das estimativas obtidas por meio da ferramenta GAMLSS . Além_disso  , utilizamos o método bayesiano de análise de influência caso a caso baseado na divergência de Kullback-Leibler para detectar observações influentes nos dados . A implementação computacional foi desenvolvida no software R e para detalhes dos programas pode ser consultado aos autores do 
 Em aplicações de sensoriamento remoto , há diversos problemas nos quais há conhecimento predominante sobre uma categoria ou classe alvo , e pouco conhecimento sobre as demais categorias . Nesses casos , o treinamento de um classificador é prejudicado pelo desbalanceamento de classes . Assim , o estudo de características visuais para se definir o melhor subespaço de características pode ser uma alternativa viável para melhorar o desempenho dos classificadores . O uso de abordagens baseadas em detecção de anomalias também pode auxiliar por meio da modelagem da classe normal ( comumente majoritária ) enquanto todas as outras classes são consideradas como anomalias . Este estudo apresentou uma base de imagens de sensoriamento remoto , cuja aplicação é identificar entre regiões de cobertura vegetal e regiões de não cobertura vegetal . Para solucionar o problema de desbalanceamento entre as classes , foram realizados estudos das características visuais a fim de definir qual o conjunto de atributos que melhor representa os dados . Também foi proposta a criação de um pipeline para se tratar bases desbalanceadas de cobertura vegetal . Este pipeline fez uso de técnicas de seleção de características e aprendizado ativo . A análise de características apresentou que o subespaço usando o extrator BIC com o índice de vegetação ExG foi o que melhor distinguiu os dados . Além_disso  , a técnica de ordenação proposta mostrou bom_desempenho  com poucas dimensões . O aprendizado ativo também ajudou na criação de um modelo melhor , com resultados comparáveis com as melhores características visuais 
 A suposta ubiquidade de sistemas decomponíveis foi interpretada por Holland ( 1975 ) como o principal motivo para o desempenho dos algoritmos genéticos ( Genetic Algorithms ( GAs ) ) . A hipótese de Building Blocks ( BBs ) sugere que algoritmos genéticos mais eficientes poderiam ser implementados , contudo , apenas anos depois essas ideias puderam ser avaliadas experimentalmente no contexto de algoritmos de estimação de distribuição ( Estimation of Distribution Algorithms ( EDAs ) ) . EDAs utilizam modelos_probabilísticos  , estimados a partir da população , para inferir características do espaço de busca que poderiam ser utilizadas para implementar operadores de reprodução mais eficazes . Tanto em problemas mono- quanto multi-objetivo , EDAs emergiram sob a premissa de que a eficácia dos operadores de reprodução seria proporcional à representatividade dos modelos_probabilísticos  utilizados . No entanto , estudos recentes tem demonstrado que a dificuldade em se construir modelos confiáveis pode tornar essa premissa inviável . Ou seja , para certos problemas de otimização os modelos_probabilísticos  utilizados seriam , em geral , de baixa qualidade e , portanto , não produziriam operadores eficazes . Esta tese trata das limitações encontradas na construção de modelos_probabilísticos  ( linkage learning ) sob a perspectiva da multimodalidade dos problemas em questão . A análise teórica considerou problemas aditivamente separáveis , enquanto a generalização das conclusões foi investigada em instâncias do modelo NK-landscapes e do problema da mochila multidimensional ( Multidimensional Knapsack Problem ( MKP ) ) . Os resultados indicaram que a acurácia dos modelos_probabilísticos  é se relaciona inversamente ao grau de multimodalidade da função objetivo e que , em casos de extrema multimodalidade a construção de modelos_probabilísticos  confiáveis pode ser tornar infactível . Este resultado poderia inviabilizar o uso de EDAs no contexto multiobjetivo , devido a intrínseca multimodalidade de tais problemas . No entanto , observou-se que apesar da ausência de estatísticas confiáveis sobre cada uma das funções objetivo , a correlação entre elas se torna estatisticamente observável e útil aos operadores de reprodução na manutenção da diversidade e controle convergência da população 
 Com a popularização do uso de recursos tecnológicos na educação , uma enorme_quantidade  de dados , relacionados às interações entre alunos e esses recursos , é armazenada . Analisar esses dados , visando caracterizar os alunos , é tarefa muito importante , uma vez que os resultados dessa análise podem auxiliar professores no processo de ensino e aprendizagem . Entretanto , devido ao fato de as ferramentas utilizadas para essa caracterização serem complexas e pouco intuitivas , os profissionais da área de ensino acabam por não utilizá-las , inviabilizando a implementação de tais ferramentas em ambientes educacionais . Dentro desse contexto , a dissertação de mestrado aqui apresentada teve como objetivo analisar os dados provenientes de um sistema tutor inteligente , o MathTutor , que disponibiliza exercícios específicos de matemática , para identificar padrões de comportamento dos alunos que interagiram com esse sistema durante um determinado período . Essa análise foi realizada por meio de um processo de Mineração de Dados Educacionais ( EDM ) , utilizando a ferramenta DAMICORE , com o intuito de possibilitar que fossem geradas , de forma rápida e eficaz , informações úteis à caracterização dos alunos . Durante a realização dessa análise , seguiram-se algumas fases do processo de descobrimento de conhecimento em bases de dados , seleção , pré-processamento , mineração dos dados e avaliação e interpretação . Na fase de mineração de dados , foi utilizada a ferramenta DAMICORE , que encontrou padrões que foram estudados na fase de avaliação e interpretação . A partir dessa análise foram encontrados padrões comportamentais dos alunos , por exemplo , alunos do sexo masculino apresentam rendimento superior ou inferior ao de alunas do sexo feminino e quais alunos terão um bom ou mau rendimento nas etapas finais do processo de ensino . Como principal resultado temos que uma das hipóteses criadas , Alunos que obtiveram bom_desempenho  no pós-teste imediato apresentaram dois dos três seguintes comportamentos : poucas interações na intervenção , baixo tempo interagindo com o sistema na intervenção e poucos misconceptions no pré-teste , teve sua acurácia comprovada dentre os dados utilizados nessa pesquisa . Assim , por meio desta pesquisa concluiu-se que a utilização da DAMICORE em contexto educacional pode auxiliar o professor a inferir o desempenho dos seus alunos oferecendo a ele a oportunidade de realizar as intervenções pedagógicas que auxiliem alunos com possíveis dificuldades e apresente novos desafios para aqueles com facilidade no tema 
 A crescente importância de conteúdo multimídia gerado por usuários amadores exige pesquisas por modelos , métodos , tecnologias e sistemas que apoiem a produção multimídia . Apesar dos recentes resultados que permitem captura colaborativa em vídeo utilizando dispositivos_móveis  , existe uma lacuna no apoio à captura colaborativa de múltiplas mídias . O trabalho apresentado nesta dissertação_propõe  que a produção multimídia colaborativa ubíqua possa ser alcançada por usuários que realizem a captura de múltiplas mídias e de anotações utilizando o aplicativo móvel CMoViA . CMoViA também permite que o conteúdo gerado por esses usuários seja exportado para a plataforma CI+WaC , a qual permite editar e anotar documentos_multimídia  interativos . Essa proposta requer a extensão de trabalho recentes reportados na literatura : o modelo I+WaC-IE ( Interactors+WaC-Interaction Events ) , a ferramenta I+WaC-Editor e a ferramenta MoViA . Assim , a aplicação CMoViA segue o modelo CI+WaC-IE proposto neste trabalho como extensão do modelo I+WaC-IE . A proposta foi avaliada por meio de estudo de caso realizado no domínio educacional , no qual estudantes capturam colaborativamente uma palestra 
 Este trabalho aplica a teoria de redes complexas para o estudo de uma técnica aplicada ao problema de aprendizado semissupervisionado e não-supervisionado em redes , especificamente , aquelas que representam conjuntos de dados multivariados . Redes complexas permitem o emprego de sistemas dinâmicos não-lineares que podem apresentar comportamentos de acordo com os padrões de conectividade de redes . Inspirado pelos comportamentos observados na natureza , tais como a competição por recursos limitados , sistema dinâmicos podem ser utilizados para revelar a estrutura da organização de uma rede . Nesta dissertação , desenvolve-se uma técnica aplicada ao problema de classificação de dados representados por redes de interação . Como parte da técnica , um sistema dinâmico inspirado na competição por recursos foi modelado . Métodos similares concentraram-se em vértices como o recurso da concorrência . Neste trabalho , introduziu-se arestas como o recurso-alvo da competição . Ao fazê-lo , utilizar-se-á o padrão de conectividade de uma rede tanto na simulação do sistema dinâmico , quanto na tarefa de aprendizado 
 Ferramentas de virtualização de redes podem ser utilizadas com a finalidade de criar experimentos envolvendo redes de computadores e sistemas distribuídos em diversas aplicações , seja este para avaliação e validação de um sistema distribuído , teste de novas configurações em um ambiente de produção , ou mesmo para uso educacional . Várias são as opções de ferramentas disponíveis para a realização de tais experimentos . Entretanto , as ferramentas disponíveis não oferecem um suporte adequado a virtualização de redes móveis . O presente_trabalho  de mestrado apresenta uma proposta para a modificação de uma ferramenta de virtualização de código livre chamada Netkit , de modo a viabilizar a criação de experimentos que envolvam redes móveis . Neste caso , a ferramenta prevê que os dispositivos possam ser localizados em um espaço virtual e deslocados , de modo que variações como intensidade de sinal , trocas de pontos de acesso e experimentos que envolvam a localização espacial dos dispositivos possam ser efetuados 
 O teste de programas concorrentes é essencial para assegurar a qualidade das atuais aplicações distribuídas/paralelas em desenvolvimento . Apesar de ser essencial , essa atividade de teste é dificilmente empregada adequadamente , devido a fatores como : alto custo de execução , grande lacuna entre desenvolvedores e resultados de pesquisas em testes para programas concorrentes e acesso às ferramentas de teste de programas concorrentes que automatizem/viabilizem o emprego do teste . Este projeto visa definir os parâmetros da atividade de teste_estrutural  de programas concorrentes que nortearão a composição de diferentes serviços na Web . Tais serviços dão suporte à atividade de teste_estrutural  de programas concorrentes , estabelecendo fronteiras claras em ferramentas de teste para os módulos relativos ao modelo de teste , aos critérios de teste , à linguagem de programação e aos paradigmas de sincronização . Desse modo , novas ferramentas de teste poderão ser construídas de maneira mais flexível , com menos custo de desenvolvimento e com mais eficácia . Tal abordagem traz como benefícios diretos : ( 1 ) facilitar a interação entre os setores da indústria , ensino e pesquisa que estejam interessados no desenvolvimento de programas concorrentes com qualidade ; ( 2 ) diminuir os custos de instalação e manutenção de ferramentas de teste_estrutural  pelos desenvolvedores ; ( 3 ) facilitar a incorporação da atividade de teste de programas concorrentes no ciclo de desenvolvimento das aplicações distribuídas e paralelas ; ( 4 ) aumentar a abrangência do projeto TestPar , permitindo que novos usuários ( desenvolvedores , professores e outros grupos de pesquisa ) possam utilizar facilmente os conhecimentos gerados no projeto ; e ( 5 ) realimentar o projeto TestPar com novas demandas qualificadas , estas advindas de novos programas concorrentes submetidos para teste 
 Dentre as técnicas de mineração existentes encontra-se a associação , responsável por identificar relações que ocorrem no conjunto de dados . Embora a associação seja uma das técnicas mais utilizadas , a quantidade de padrões extraídos pode vir a sobrecarregar o usuário de tal maneira que encontrar algo interessante dentre a imensidão de padrões obtidos passa a ser um novo desafio . Para solucionar esse problema , uma grande parte dos trabalhos_relacionados  à associação está voltada a etapa de pós-processamento . Esses trabalhos geralmente propõem abordagens de pós-processamento que visam , segundo determinada estratégia , facilitar a busca pelos padrões interessantes ao domínio . Nos últimos_anos  , essas abordagens têm incluído no processo o conhecimento e/ou interesse do usuário sobre o domínio . Contudo , nas abordagens atualmente existentes , o usuário deve , por meio de algum formalismo descrever explicitamente seu conhecimento e/ou interesse , requerendo do usuário um tempo considerável , podendo levar , inclusive , a especificações incompletas e/ou incorretas . Além_disso  , na maioria das vezes , o usuário não tem ideia do que é provavelmente interessante , nem a partir de quais relações iniciar a busca . Nota-se , portanto , que um dos desafios dessas abordagens é considerar o conhecimento e/ou interesse do usuário . Além_disso  , é necessário considerar também o número de regras que o usuário analisará . A análise de regras feita por um especialista é custosa e , na maioria dos casos , o usuário quer explorar as regras geradas sem limitar a exploração ao conhecimento que ele já possui . Portanto , é importante que o usuário avalie o menor número de regras possível e , com base nessa avaliação , abordagens de pós-processamento consigam o auxiliar na busca pelas regras que ele poderá considerar interessante . Para tanto , é proposto neste trabalho que o pós-processamento seja tratado como um problema de classificação semissupervisionada transdutiva , uma vez que permite que o usuário rotule , considerando classes pré-definidas ( por exemplo , `` Interessante '' ou `` Não Interessante '' ) , apenas algumas regras do conjunto a ser explorado para que todas as outras regras sejam automaticamente rotuladas . Além_disso  , por meio da definição dos rótulos de algumas regras , é possível capturar implicitamente o conhecimento e/ou interesse do usuário sobre o domínio . Para tanto , é necessário que as regras sejam modeladas de maneira a permitir : ( a ) selecionar as regras a serem rotuladas pelo usuário a fim de capturar implicitamente seu conhecimento e/ou interesse ; ( b ) propagar os rótulos das regras já classificadas pelo usuário a todas as outras regras não rotuladas . Desse modo , neste trabalho , as regras foram modeladas via redes , uma vez que : ( i ) uma vasta quantidade de medidas de exploração de redes pode ser utilizada , em conjunto com as informações fornecidas pelo usuário , a fim de viabilizar o item ( a ) ; ( ii ) algoritmos de propagação de rótulos podem ser utilizados a fim de viabilizar o item ( b ) . Diante do apresentado , ressalta-se que as contribuições deste trabalho estão na capacidade de se extrair o conhecimento e/ou interesse do usuário de acordo com as características da base de dados e direcionar sua exploração sem a necessidade de se definir previamente o que será explorado . Além_disso  , os resultados obtidos demonstram a capacidade da PARLP em direcionar o usuário para o conhecimento considerado interessante , reduzindo , para tanto , a quantidade de regras a serem exploradas . Por fim , este trabalho contribui também para demonstrar que é possível tratar o pós-processamento de regras de associação como um problema de propagação de rótulos 
 Uma parte significativa dos escoamentos encontrados em aplicações tecnológicas 
 caracterizada por envolver altos números de Reynolds , principalmente aqueles em regime turbulento e com superfície_livre  
 Obter soluções numéricas representativas para essa classe de problemas é extremamente difícil , devido à natureza não-linear das equações_diferenciais  parciais envolvidas nos modelos . Conseqüentemente , o tema tem sido uma das principais preocupações da comunidade_científica  moderna em dinâmica de fluidos computacional 
 Aproximações de primeira_ordem  para os termos convectivos são as mais adequadas para amortecer oscilações que estão associadas às aproximações de alta ordem não-limitadas 
 Todavia , elas introduzem dissipação artificial nas representações discretas comprometendo os resultados numéricos . Para minimizar esse efeito não-físico e , ao mesmo tempo , conseguir aproximações incondicionalmente estáveis , é indispensável adotar uma estratégia que combine aproximações de primeira_ordem  com as de ordem mais alta e que leve em conta a propagação de informações físicas . Os resultados dessa composição são os esquemas `` upwind '' limitados de alta ordem . Em geral , espera-se que esses esquemas sejam apropriados para a representação das derivadas convectivas nos modelos de turbulência kappa-varepsilon 
 No contexto de diferenças_finitas  , a presente tese dedica-se à solução_numérica  das equações de Navier-Stokes no regime de números de Reynolds elevados 
 Em particular , ela contém uma análise de algoritmos monotônicos e antidifusivos e modelos de turbulência kappa-varepsilon para a simulação de escoamentos_incompressíveis  envolvendo superfícies_livres  
 Esquemas de convecção são implementados nos códigos GENSMAC para proporcionar um tratamento robusto dos termos convectivos nas equações de transporte 
 Duas versões do modelo kappa-varepsilon de turbulência são implementadas nos códigos GENSMAC , para problems bidimensionais e com simetria radial , para descrever os efeitos da turbulência sobre o escoamento médio 
 Resultados_numéricos  de escoamentos com simetria radial são comparados com resultados experimentais e analíticos . Simulações numéricas de problemas tridimensionais complexos são apresentadas para avaliar o desempenho de esquemas `` upwind '' . Finalmente , os modelos de turbulência kappa-varepsilon são utilizados para a simulação de escoamentos confinados e com superfícies_livres  
 Sistemas de Recuperação de Imagens por Conteúdo ( SiRICs ) têm como objetivo realizar buscas em grandes coleções de imagens , recuperando aquelas cujos conteúdos são mais similares à uma imagem fornecida como parte do predicado de busca . A similaridade é expressada por uma função de distância ( dissimilaridade ) que calcula a relação entre um par de imagens para permitir que ambas sejam comparadas . Funções de distância usam características_extraídas  de cada imagem por um conjunto de algoritmos chamados extratores de características . Para melhorar o desempenho do processo de buscas , as características selecionadas são extraídas de cada imagem no momento em cada uma é armazenada na base de dados , criando um vetor de características para cada imagem . As operações subsequentes da busca são realizadas usando os vetores de características no0 lugar das imagens . Antes de extrair as características , outros algoritmos de processamento de imagem são utilizados para pré-processar cada imagem , de forma a prepará-la para os extratores . Adicionalmente , é comum a existência de vários critérios que podem ser considerados no processo de cálculo da similaridade de duas imagens . Nos SiRICs atuais , para comparar imagens , é preciso definir ( 1 ) o critério de comparação , ( 2 ) os pré-processamentos necessários para execução dos extratores , ( 3 ) quais são os extratores utilizados , ( 4 ) quais características devem ser consideradas , ( 5 ) e qual função de distância deve ser aplicada . Todas essas definições têm de ser configuradas antes da comparação ser realizada . A complexidade desse processo levou ao desenvolvimento de SiRICs com poucas opções para configuração das operações de comparação . Além_disso  , não existe nenhuma representação formal do processo SiRIC como uni todo . Este trabalho apresenta uma representação formal do conjunto completo de operações que compõem o processo de buscas por conteúdo em imagens , objetivando consultas por similaridade em grandes bases de dados relacionais . A aplicação desse formalismo é apresentada com os resultados experimentais gerados sobre imagens médicas de urna base de dados hospitalar 
 Este trabalho apresenta parte de um sistema de simulação integrado para escoamento de fluido incompressível bidimensional , usando malhas não estruturadas , chamado Umflow- 2D . O sistema consiste de três_módulos  : um módulo modelador , um módulo simulador e um módulo visualizador . A parte do sistema apresentado neste trabalho é o módulo modelador , o módulo visualizador e o gerador de malhas . O módulo modelador tem uma interface_gráfica  que auxilia o usuário a rotular o domínio computacional , a malha , as condições de contorno , e a inicialização de outros dados . A malha não estruturada pode ser gerada usando dois algoritmos : Algoritmo de Refinamento de Chew e Ruppert . O módulo de visualização é um sistema que permite a visualização dos resultados gerados pelo módulo de simulação . Este trabalho usa uma estrutura de dados chamada Singular Handle Edge ( SHE ) para manipular e representar as malhas 
 Os veículos usarão pseudônimos em vez de certificados de longo prazo para fornecer segurança e privacidade . Os pseudônimos são certificados de chaves públicas de curto prazo que não contêm informação da identidade do veículo . No entanto , existe risco que veículos autorizados possam enviar mensagens falsas ou se comportar de maneira egoísta , e isso pode afetar o desempenho das redes veiculares ( VANETs ) . Nesse contexto , o gerenciamento de confiança é um importante serviço de segurança nas VANETs , o qual fornece um sistema unificado para estabelecer relações entre os nós e ajuda a manter um registro do comportamento dos veículos . No entanto , é uma tarefa desafiante monitorar o padrão evolutivo do comportamento veicular , já que a comunicação entre os veículos é anônima . Não é uma tarefa fácil encontrar uma solução equilibrada que atenda aos requisitos de segurança , privacidade e gerenciamento de confiança em VANET . Em vista disso , apresentamos um Esquema de Reputação Preservando a Privacidade ( ERPP ) aplicado a VANETs , no qual um servidor de reputação através de uma unidade de acostamento recebe avaliações sobre o comportamento dos veículos . O servidor atualiza e certifica a reputação dos veículos relacionando seus identidades anônimas com as reais . ERPP introduz áreas geográficas de segurança , na qual a segurança de uma área pode ser adaptada a níveis mais elevados ou mais baixos dependendo da reputação dos veículos . Além , uma reputação complexa é examinada , na qual a reputação de um veículo está vinculada a vários fatores do comportamento . Uma outra área que é explorada é a avaliação de desempenho do ERPP o qual é conduzida através de simulações em um cenário urbano , com base na aplicação de encaminhamento oportunista de mensagens . Os resultados mostraram a eficácia do ERPP em termos de avaliar o comportamento dos veículos e tomar medidas contra os veículos mal comportados . Utilizamos SUMO para simular o modelo de mobilidade ; OMNET++ e Veins suportaram o modelo de red ; and Crypto++ foi usado para implementar as funções criptográficas de curvas elípticas de assinatura e verificação de mensagens como recomendam os padrões de segurança . Finalmente , empregamos uma estratégia de mudança de pseudônimo na qual a reputação é discretizada em dois_níveis  de reputação . A estratégia foi implementada em um cenário de simulação de tráfego realista e foi comparada com as estratégias nomeadas de estado e síncrona mediante simulações . Os resultados mostraram que o número de pseudônimos utilizados em nossa estratégia é menor que os esquemas mencionados , e mantém a taxa de sucesso de mudança de pseudônimo alcançada pela estratégia síncrona 
 Redes complexas vem_sendo  aplicadas com sucesso em diferentes domínios , sendo o tema de estudo de distintas áreas que incluem , por exemplo , a física e a computação . A descoberta de que métodos de redes complexas podem ser utilizados para analisar textos em seus distintos níveis de complexidade proporcionou avanços em tarefas de processamento de línguas naturais ( PLN ) . Exemplos de aplicações analisadas com os métodos de redes complexas são a detecção de palavras-chave , a criação de sumarizadores automáticos e o reconhecimento de autoria . Esta última tarefa tem sido estudada com certo sucesso através da representação de redes de co-ocorrência ( ou adjacência ) de palavras que conectam apenas as palavras mais próximas no texto . Apesar deste sucesso , poucos trabalhos tentaram estender essas redes ou utilizar diferentes representações . Além_disso  , muitas das abordagens utilizam um conjunto semelhante de medidas de redes complexas e não combinam suas técnicas com as utilizadas tradicionalmente na tarefa de reconhecimento de autoria . Esta pesquisa de mestrado_propõe  extensões à modelagem tradicional de co-ocorrência e investiga a adequabilidade de novos atributos e de outras modelagens ( como as redes mesoscópicas e de entidades nomeadas ) para a tarefa . A informação de conectividade de palavras funcionais é utilizada para complementar a caracterização da escrita dos autores , uma vez que essas palavras são relevantes para a tarefa . Finalmente , a maior contribuição deste trabalho consiste no desenvolvimento de classificadores híbridos , denominados labelled motifs , que combinam fatores tradicionais com as propriedades fornecidas pela análise topológica de redes complexas . A relevância desses classificadores é verificada no contexto de reconhecimento de autoria e identificação de translationese . Com esta abordagem híbrida , mostra-se que é possível melhorar o desempenho de técnicas baseadas em rede ao combiná-las com técnicas tradicionais em PLN . Através da adaptação , combinação e aperfeiçoamento da modelagem , não apenas o desempenho dos sistemas de reconhecimento de autoria foi melhorado , mas também foi possível entender melhor quais são os fatores quantitativos textuais ( medidos via redes ) que podem ser utilizados na área de estilometria . Os avanços obtidos durante este projeto podem ser utilizados para estudar aplicações relacionadas , como é o caso da análise de inconsistências estilísticas e plagiarismos , e análise da complexidade textual . Além_disso  , muitos dos métodos propostos neste trabalho podem ser facilmente aplicados em diversas línguas naturais 
 Nesta tese de doutorado , propomos novos métodos iterativos para a solução de uma classe de problemas de otimização convexa . Em geral , consideramos problemas nos quais a função objetivo é composta por uma soma finita de funções convexas e o conjunto de restrições é , pelo menos , convexo e fechado . Os métodos iterativos que propomos são criados , basicamente , através da junção de métodos de subgradientes incrementais e do algoritmo de média das sequências . Além_disso  , visando obter métodos flexíveis para soluções de problemas de otimização com muitas restrições ( e possivelmente em altas dimensões ) , dadas em geral por funções convexas , a nossa análise inclui um operador que calcula projeções aproximadas sobre o conjunto viável , no lugar da projeção Euclideana . Essa característica é empregada nos dois métodos que propomos ; um determinístico e o outro estocástico . Uma análise de convergência é proposta para ambos os métodos e experimentos numéricos são realizados a fim de verificar a sua aplicabilidade , principalmente em problemas de grande escala 
 Este trabalho de mestrado apresenta o desenvolvimento de uma extensão no benchmark Bench4Q . A extensão proposta é uma nova funcionalidade para o benchmark . O referido framework é utilizado para gerar carga sintética para um sistema e-commerce acoplado ao benchmark . Seu principal emprego na literatura tem sido em avaliação de desempenho sob carga estacionária . Contudo , recentes pesquisas tem apresentado interesse no estudo de arquiteturas adaptativas de autogerenciamento de recursos , o que implica em responder às perturbações e atender os requisitos de desempenho em regime transiente propostos para o sistema . No entanto , este benchmark não abrange os estados transiente do sistema . O presente_trabalho  tem por objetivo estender o benchmark Bench4Q acrescentando-lhe capacidade de excitar a resposta transiente do sistema mediante as perturbações da carga de trabalho . Para isso , o software foi acrescido de funcionalidade capaz de gerenciar a modulação da carga de trabalho . Os experimentos foram executados em um ambiente multicamadas que apresentou resultados compatíveis ao objetivo , representando contribuições para a área de avaliação de desempenho . A motivação da pesquisa , inserção em outros trabalhos em andamento e direções futuras são introduzidas 
 Esta trabalho apresenta a utilização de Queuing SLatecharts para a especificação de modelos de sistemas computacionais em um sistema de simulação automático . Durante o trabalho de mestrado , foi implementado o Módulo de Interface Gráfica com o Usuário e o Módulo Avaliador Nívl 1 do ASDA ( Ambiente de Simulação Automática Distribuída ) , e o desenvolvido urri gerador de programas de simulação para a extensão funcional SMPL ( Simulation Prograrnming Language ) . Forma definidas as estruturas de dados que permitem o armazenamento , a alteração e recuperação do modelo . O módulo de Interface com o Usuário implementado permite que o usuário defina o modelo , a parametrização e o gerenciamento das informações dos usuários do ambiente que atua cm conjunto com o Módulo Avaliador Nível I consiste na definição das regras para a verificação da consistência do modelo e a validação dos parâmetros . O gerador de programas de simulação desenvolvido possibilita que o usuário gere automaticamente seus programas de simulação a partir da especificação gráfica e dos parâmetros do modelo definidos pelo usuário de simulação . As regras para a verificação da consistência do modelo e da validação dos parâmetros foram definidas durante o desenvolvimento do Módulo Avaliador Nível 1 . O gerador para programas de simulação desenvolvido neste trabalho permite ao usuário utilizar o modelo armazenado na estrutura de dados e gerar automaticamente o seu programa de simulação . São apresentados exemplos e resultados de programas de simulação gerador 
 Fotografias aéreas possuem aplicações nos mais variados tipos de atividades . Como exemplos , pode-se citar agricultura , vigilância , mapeamento e planejamento urbano , levantamento de malha rodoviária , levantamento topográfico e atividades militares , entre outras . E de grande_importância  a associação de imagens aéreas às coordenadas geográficas do local onde são obtidas . Esse tipo de associação é chamado de georeferenciamento , e é necessário para que seja possível utilizar , de forma satisfatória , os dados contidos nas imagens . Recentemente , o avanço da tecnologia na área de posicionamento global tem fornecido sensores capazes de determinar coordenadas geográficas em tempo real com grande precisão . Isso permite a criação de sistemas que automatizam o processo de georeferenciamento das imagens a um baixo custo . Além_disso  , o uso de equipamentos de fotografia aérea de pequeno formato associado ao uso de aeronaves do tipo ultraleve como plataforma também tem permitido uma significativa redução no custo de obtenção de imagens aéreas . Neste trabalho de mestrado foi desenvolvido o SciaGeo , um sistema para aquisição automática de imagens aéreas georefcrenciadas utilizando aeronaves do tipo ultraleve . Foram obtidos bons_resultados  com o SciaGeo , por meio de testes em campo , realizados em uma região agrícola , sobre culturas de soja e milho . Esses testes geraram imagens georeferenciadas de boa_qualidade  , permitindo a extração de informações importantes da cultura por meio de processamento digital de imagens 
 Uma hiper-heurística é uma heurística que pode ser utilizada para lidar com qualquer problema de otimização , desde que a ela sejam fornecidos alguns parâmetros , como estruturas e abstrações , relacionados ao problema considerado . As hiper-heurísticas têm sido aplicadas a alguns problemas práticos e apresentadas como métodos de grande potencial , no que diz_respeito  à capacidade de possibilitar o desenvolvimento , em tempo bastante reduzido , de algoritmos capazes de lidar satisfatoriamente , do ponto de vista prático , com problemas de otimização complexos e pouco conhecidos . No entanto , é difícil situar as hiper-heurísticas em algum nível de qualidade e avaliar a robustez dessas abordagens caso não as apliquemos a problemas para os quais existam diversas instâncias disponíveis publicamente e já experimentadas por algoritmos relevantes . Este trabalho procura dar alguns passos importantes rumo a essas avaliações , além de ampliar o conjunto das hiper-heurísticas , compreender o impacto de algumas alternativas naturais de desenvolvimento e estabelecer comparações entre os resultados obtidos por diferentes métodos , o que ainda nos permite confrontar as duas diferentes classes de hiper-heurísticas que identificamos . Com essas finalidades em mente , desenvolvemos 3 novas hiper-heurísticas e implementamos 2 das hiper-heurísticas mais importantes criadas por outros autores . Para estas últimas , experimentamos ainda algumas extensões e modificações . Os dois métodos hiper-heurísticos selecionados podem ser vistos como respectivos representantes de duas classes distintas , que aparentemente englobam todas as hiper-heurísticas já desenvolvidas e nos permitem denominar cada um desses métodos como `` hiper-heurística de busca direta por entornos '' ou como `` hiper-heurística evolutiva indireta '' . Implementamos cada hiper-heurística como uma biblioteca ( em linguagem C ) , de forma a evidenciar e estimular a independência entre o nível em que se encontra a hiper-heurística e aquele onde se apresentam as estruturas e abstrações diretamente relacionadas ao problema considerado . Naturalmente , essa separação é de ingente importância para possibilitar a reutilização imediata das hiper-heurísticas e garantir que nelas haja total ausência de informações relativas a um problema de otimização específico 
 Simulações por computador permitem reduzir custo e , muitas_vezes  , realizar experimentos que na vida real seriam impraticáveis , ou por questões ambientais ( explosões nucleares ) , ou por fatores que estão fora do controle do ser humano ( colisões entre estrelas ) . Entretanto , e muito difícil manipular e analisar as centenas de gigabytes , ou mesmo terabytes , que tais simulações produzem como resultado . Os trabalhos que lidam com tais conjuntos de dados , tipicamente , empregam tanto técnicas de visualização científica como técnicas de visualização da informação , em geral refletindo o comportamento dos dados em um único instante de tempo . Entretanto , a análise da evolução_temporal  e a disponibilização de representações_visuais  integradas ainda é um grande desafio . Esse trabalho introduz diversas estratégias buscando tratar estes problemas , as quais tem em comum a utilização de projeções_multidimensionais  para apoiar a análise exploratória dos de dados , tanto em um instante de tempo específico , como ao longo da evolução_temporal  . O objetivo é favorecer a localização de grupos de elementos com comportamento similar e acompanhar sua evolução ao longo da simulação . Uma das estratégias introduzidas resume o comportamento temporal dos dados multidimensionais em uma única visualização , o que permite rastrear as entidades com comportamento similar e analisá-las ao longo da 
 Neste trabalho propomos algumas soluções para problemas de seleção e controle introduzidos por Marsh e Zellner ( 1994 ) . Estudamos soluções_ótimas  obtidas_através  de diferentes funções de perda e comparamos tais soluções com a chamada `` solução do diretor '' . Além_disso  , enfocamos os problemas de controle de uma forma distinta da proposta em Marsh e Zellner , utilizando o conceito de densidade preditiva . Introduzimos o modelo de regressão logística em problemas de seleção e controle com heterogeneidade , obtendo soluções via Inferência Clássica e Inferência Bayesiana Assintótica . Analisamos um conjunto de dados simulados a fim de exemplificar o emprego do modelo de regressão logística em tais problemas . Baseando-nos em Geisser ( 1982 ) , utilizamos funções de perda e densidade preditiva para obter soluções_ótimas  para problemas de seleção e controle . Procedemos a uma análise para verificar qual o impacto da função de perda na escolha dos parâmetros da distribuição a priori de O : probabilidade dos indivíduos aceitarem uma oferta para um curso a ser realizado . Na tentativa de solucionar problemas de captura-recaptura através de problemas de controle investigamos um estimador bastante conhecido , o estimador de Petersen ( 1896 ) e , através da Inferência Bayesiana , propomos um ajuste para o mesmo , similar às correções de Bartlett ( 1937 ) . Através de um exemplo com dados simulados é possível verificar que o ajuste melhora sensivelmente as estimativas do tamanho de uma população animal 
 Neste trabalho é apresentado um ambiente integrado para o auxílio à paralelização de aplicações escritas na linguagem Fortran . O sistema foi desenvolvido com o objetivo de sanar uma necessidade de melhores ferramentas para programação paralela , conforme identificado na revisão bibliográfica apresentada . Diversas técnicas de Engenharia de Software são utilizadas neste trabalho . A análise do sistema é apresentada , e são discutidos os seus objetivos e a descrição das suas características principais . O projeto do sistema e o modelo de geração do código paralelo são descritos . E feita uma análise dos algoritmos envolvidos na sua implementação . Também são apresentados exemplos de utilização do sistema e os resultados obtidos são avaliados . Finalmente , podem ser encontradas diversas sugestões de novos projetos , importantes para a continuidade do trabalho até aqui desenvolvido 
 Esta dissertação apresenta uma discussão de diversos algoritmos paralelos para ordenação encontrados na literatura . Os algoritmos são analisados visando selecionar os mais adequados para a implementação em sistemas distribuídos . O algoritmo base utilizado foi o Quicksort Paralelo , que foi implementado em uma rede de SUNs utilizando a plataforma de programação PVM ( Parallel Virtual Machine ) . Os resultados obtidos foram analisados e alterações visando adequar os algoritmos ao sistema utilizado ( máquinas heterogêneas , granularidade grossa ) foram propostas . Dentre as modificações propostas cabe ressaltar : a divisão não uniforme dos vetores a serem ordenados com o intuito de obter melhor balanceamento de carga ; a divisão de vetores utilizando-se pivôs para que as listas geradas em paralelo não necessitem de intercalação ; e a liberação do processador mestre , evitando que este seja sobrecarregado com a ordenação de uma lista . Os resultados obtidos com as modificações são analisados 
 Este trabalho apresenta o GASC ( Gerador de Aplicação para Sistemas Concorrentes ) constituído de uma linguagem de especificação , um analisador estruturural e semântico , e um gerador de código . A linguagem do GASC é baseada em características de outras linguagens de programação e de especificação formal . O objetivo do GASC é oferecer uma ferramenta de fácil utilização para especificação de sistemas concorrentes , juntamente com a validação da especificação e a geração_automática  de código na linguagem de programação OCCAM . Os conceitos envolvidos com processamento paralelo , especificações formais e geradores de aplicações são apresentados e discutidos com o objetivo de se formar uma base para o desenvolvimento do trabalho . Exemplos de todas as fases , desde a representação do problema até a geração do código , são apresentados 
 Dois modelos principais de planejamento em inteligência_artificial  são os usados , respectivamente , em planejamento probabilístico ( MDPs e suas generalizações ) e em planejamento não-determinístico ( baseado em model checking ) . Nessa dissertação será : ( 1 ) exibido que planejamento probabilístico e não-determinístico são extremos de um rico contínuo de problemas capaz de lidar simultaneamente com risco e incerteza ( Knightiana ) ; ( 2 ) obtido um modelo para unificar esses dois tipos de problemas usando MDPs imprecisos ; ( 3 ) derivado uma versão simplificada do princípio ótimo de Bellman para esse novo modelo ; ( 4 ) exibido como adaptar e analisar algoritmos do estado-da-arte , como ( L ) RTDP e LDFS , nesse modelo unificado . Também será discutido exemplos e relações entre modelos já propostos para planejamento sob incerteza e o modelo proposto 
 Este trabalho de pesquisa apresenta um protótipo de servidor Web distribuído com diferenciação de serviços baseado em QoS relativa e absoluta . São implementados e comparados diferentes algoritmos de escalonamento . Um dos algoritmos avaliados é o EBS . O objetivo é transpor a teoria da política para o mundo_real  , e comparar o seu comportamento com os resultados das simulações , utilizando o tempo de resposta como medida de desempenho . Além do EBS , outros algoritmos são avaliados , tais como , Round-Roubin e Weighted Round 
 Esta Tese apresenta um conjunto de técnicas propostas com o objetivo de aprimorar processos de Agrupamento de Dados ( AD ) . O principal objetivo é fornecer à comunidade_científica  um ferramental para uma análise completa de estruturas implícitas em conjuntos de dados , desde a descoberta dessas estruturas , permitindo o emprego de conhecimento prévio sobre os dados , até a análise de seu significado no contexto em que eles estão inseridos . São dois os pontos principais desse ferramental . O primeiro se trata do algoritmo para AD fuzzy semi-supervisionado SSL+P e sua evolução SSL+P* , capazes de levar em consideração o conhecimento prévio disponível sobre os dados em duas formas : rótulos e níveis de proximidade de pares de exemplos , aqui denominados Dicas de Conhecimento Prévio ( DCPs ) . Esses algoritmos também permitem que a métrica de distância seja ajustada aos dados e às DCPs . O algoritmo SSL+P* também busca estimar o número ideal de clusters para uma determinada base de dados , levando em conta as DCPs disponíveis . Os algoritmos SSL+P e SSL+P* envolvem a minimização de uma função objetivo por meio de um algoritmo de Otimização Baseado em População ( OBP ) . Esta Tese também fornece ferramentas que podem ser utilizadas diretamente neste ponto : as duas versões modificadas do algoritmo Particle Swarm Optimization ( PSO ) , DPSO-1 e DPSO-2 e 4 formas de inicialização de uma população inicial de soluções . O segundo ponto principal do ferramental proposto nesta Tese diz_respeito  à análise de clusters resultantes de um processo de AD aplicado a uma base de dados de um domínio específico . É proposta uma abordagem baseada em Mineração de Textos ( MT ) para a busca em informações textuais , disponibilizadas digitalmente e relacionadas com as entidades representadas nos dados . Em seguida , é fornecido ao pesquisador um conjunto de palavras associadas a cada cluster , que podem sugerir informações que ajudem a identificar as relações compartilhadas por exemplos atribuídos ao mesmo 
 Mapas de documentos são representações_visuais  que permitem analisar de forma eficiente diversas relações entre documentos de uma coleção . Técnicas de projeção_multidimensional  podem ser empregadas para criar mapas que refletem a similaridade de conteúdo , favorecendo a identificação de agrupamentos com conteúdo similar . Este trabalho aborda uma evolução do arcabouço genérico oferecido pelas projeções_multidimensionais  para apoiar a análise interativa de documentos textuais , implementado na plataforma PEx . Foram propostas e implementadas técnicas que permitem ao usuário interagir com o mapa de documentos utilizando tópicos extraídos do próprio corpus . Assim a representação visual pode gradualmente evoluir para refletir melhor os interesses do usuário , e apoiá-lo de maneira mais efetiva em tarefas exploratórias . A interação foi avaliada utilizando uma técnica de inspeção de usabilidade , que visa identificar os principais problemas enfrentados pelos usuários ao interagir com as funcionalidades desenvolvidas . Adicionalmente , a utilidade das funcionalidades foi avaliada pela condução de dois estudos de caso , em que foram definidas tarefas a serem conduzidas pelo usuário sobre os mapas de documentos . Os resultados mostram que com o auxílio das visualizações foi possível conduzir as tarefas satisfatoriamente , permitindo manipular de forma eficiente milhares de documentos sem a necessidade de ler individualmente cada 
 Nesta tese , estudamos um arcabouço , introduzido por Frank , que denominamos de sistemas generalizados de núcleos . Provamos teoremas sobre empacotamentos de certos objetos combinatórios neste arcabouço , tanto para o caso inteiro quanto para o fracionário . Estes teoremas , em particular , implicam em uma melhora nos limitantes superiores de Schrijver , para o empacotamento de ramificações , e de Gabow e Manu , para o empacotamento de arborescências . Além_disso  , também provamos que o problema de minimização num poliedro relacionado pode ser resolvido em tempo polinomial , dado um oráculo de separação 
 Conjuntos de dados multidimensionais são cada vez mais proeminentes e importantes em data science e muitos domínios de aplicação . Esses conjuntos de dados são tipicamente constituídos de um grande número de observações , ou objetos , cada qual descrito por várias medidas , ou dimensões . Durante o projeto de técnicas e ferramentas para processar tais dados , um dos focos principais é prover meios para análise e levantamento de hipóteses a partir das principais estruturas e padrões . Esse objetivo é perseguido por métodos de visualização multidimensional . Estruturas e padrões em dados multidimensionais podem ser descritos , em linhas gerais , pela noção de similaridade das observações . Portanto , para visualizar esses padrões , precisamos de meios efetivos e eficientes para retratar relações de similaridade dentre um grande número de observações , que potencialmente possuem um grande número de dimensões cada . No contexto dos métodos de visualização multidimensional , existem duas categorias de técnicas projeções e árvores de similaridade que efetivamente capturam padrões de similaridade e oferecem boa escalabilidade , tanto para o número de observações e quanto de dimensões . No entanto , embora essas técnicas exibam padrões de similaridade , o entendimento e interpretação desses padrões , em termos das dimensões originais dos dados , ainda é difícil . O trabalho desenvolvido nessa tese visa o desenvolvimento de técnicas explicativas para a fácil interpretação de padrões de similaridade presentes em projeções_multidimensionais  e árvores de similaridade . Primeiro , propomos métodos que possibilitam a computação eficiente de árvores de similaridade para grandes_conjuntos  de dados , e também a sua explicação visual em multiescala , ou seja , em vários níveis de detalhe . Também propomos modos de construir representações simplificadas de árvores de similaridade , e desse modo estender ainda mais a sua escalabilidade visual . Segundo , propomos métodos para explicar visualmente projeções_multidimensionais  em termos de grupos de observações relacionadas , detectadas e anotadas automaticamente para explicitar aspectos de sua similaridade no espaço de alta dimensionalidade . Mostramos em seguida como esses mecanismos explicativos podem ser adaptados para lidar com dados de natureza estática e dependentes no tempo . Nossas técnicas sã construídas visando fácil utilização , funcionamento semi automático , aplicação em quaisquer tipos de dados multidimensionais quantitativos e quaisquer técnicas de projeção_multidimensional  . Demonstramos a sua utilização em uma variedade de conjuntos de dados reais , obtidos a partir de coleções de imagens , arquivos textuais , medições científicas e de engenharia de software 
 As imagens de Angiografia por Ressonância Magnética ( angio-RM ) e Tomografia Computadorizada ( angio-TC ) são ferramentas amplamente usadas em processos de quantificação vascular e no diagnóstico de doenças cardiovasculares , as quais são consideradas entre as principais causas de morte . Contudo , a análise dos vasos em larga_escala  a partir das imagens é dificultada , tanto pela variabilidade natural dos vasos no corpo humano , quanto pela grande quantidade de dados disponíveis . Além_disso  , os métodos de quantificação existentes , usualmente extraem as características a partir dos esqueletos , ou até mesmo das próprias imagens de angiografia , razão pela qual tais métodos podem fazer necessária a reanálise das imagens repetidas vezes . Com o intuito de facilitar a análise e de fornecer uma ferramenta de apoio ao diagnóstico , neste trabalho são apresentados um modelo de representação textual de redes vasculares e uma metodologia de quantificação vascular automática , que é feita a partir dessa representação . A representação é obtida a partir da segmentação de imagens volumétricas de angio-RM e angio-TC , seguida da extração de trajetórias e diâmetros de redes vasculares . Tal representação é híbrida , combinando grafos e uma sequência textual de instruções , e permite não apenas a extração de caraterísticas morfológicas da rede vascular , como também a compressão das imagens e , ainda , a reconstrução de imagens similares às imagens originais . A partir das características_extraídas  , foram realizados estudos comparativos entre arquiteturas vasculares , o que é feito tanto por meio do uso de imagens sintéticas , como por meio de imagens reais , imagens nas quais foi possível encontrar diferenças entre arquiteturas , além de viabilizar a caracterização de aneurismas em um indivíduo . Paralelamente , desenvolvemos um método que permite identificar similaridade entre segmentos vasculares , o que por sua vez possibilita o reconhecimento e rotulação de segmentos em um conjunto de redes vasculares . A metodologia por nós desenvolvida deve também auxiliar no desenvolvimento de processos de classificação de vasos sanguíneos , de ferramentas para o diagnóstico automático de doenças vasculares , e para a melhora de técnicas utilizadas na prática clínica 
 No Aprendizado de Máquina Supervisionado - AM - é apresentado ao algoritmo de indução um conjunto de instâncias de treinamento , no qual cada instância é um vetor de features rotulado com a classe . O algoritmo de indução tem como tarefa induzir um classificador que será utilizado para classificar novas instâncias 
 Algoritmos de indução convencionais baseam-se nos dados fornecidos pelo usuário para construir as descrições dos conceitos . Uma representação inadequada do espaço de busca ou da linguagem de descrição do conjunto de instâncias , bem como erros nos exemplos de treinamento , podem tornar os problemas de aprendizado difícies 
 Um dos problemas centrais em AM é a Seleção de um Subconjunto de Features - SSF - na qual o objetivo é tentar diminuir o número de features que serão fornecidas ao algoritmo de indução 
 São várias as razões para a realização de SSF . A primeira é que a maioria dos algoritmos de AM , computacionalmente viáveis , não trabalham bem na presença de muitas features , isto é a precisão dos classificadores gerados pode ser melhorada com a aplicação de SSF . Ainda , com um número menor de features , a compreensibilidade do conceito induzido pode ser melhorada . Uma terceira razão é o alto custo para coletar e processar grande quantidade de dados . Existem , basicamente , três abordagens para a SSF : embedded , filtro e wrapper 
 Por outro_lado  , se as features utilizadas para descrever os exemplos de treinamento são inadequadas , os algoritmos de aprendizado estão propensos a criar descrições excessivamente complexas e imprecisas . Porém , essas features , individualmente inadequadas , podem algumas vezes serem , convenientemente , combinadas gerando novas features que podem mostrar-se altamente representativas para a descrição de um conceito . O processo de construção de novas features é conhecido como Construção de Features ou Indução Construtiva - IC 
 Neste trabalho são enfocadas as abordagens filtro e wrapper para a realização de SSF , bem como a IC guiada pelo conhecimento . É descrita uma série de experimentos usando SSF e IC utilizando quatro conjuntos de dados naturais e diversos algoritmos simbólicos de indução . Para cada conjunto de dados e cada indutor , são realizadas várias medidas , tais como , precisão , tempo de execução do indutor e número de features selecionadas pelo indutor . São descritos também diversos experimentos_realizados  utilizando três conjuntos de dados do mundo_real  . O foco desses experimentos não está somente na avaliação da performance dos algoritmos de indução , mas também na avaliação do conhecimento_extraído  . Durante a extração de conhecimento , os resultados foram apresentados aos especialistas para que fossem feitas sugestões para experimentos futuros . Uma parte do conhecimento_extraído  desses três estudos de casos foram considerados muito interessantes pelos especialistas . Isso mostra que a interação de diferentes áreas de conhecimento , neste caso específico , áreas médica e computacional , pode produzir resultados interessantes 
 Assim , para que a aplicação do Aprendizado de Máquina possa gerar frutos é necessário que dois grupos de pesquisadores sejam unidos : aqueles que conhecem os métodos de AM existentes e aqueles com o conhecimento no domínio da aplicação para o fornecimento de dados e a avaliação do conhecimento adquirido 
 Métodos baseados em grafos consistem em uma poderosa forma de representação e abstração de dados que proporcionam , dentre outras vantagens , representar relações topológicas , visualizar estruturas , representar grupos de dados com formatos distintos , bem como , fornecer medidas alternativas para caracterizar os dados . Esse tipo de abordagem tem sido cada vez mais considerada para solucionar problemas de aprendizado de máquina , principalmente no aprendizado não supervisionado , como agrupamento de dados , e mais recentemente , no aprendizado semissupervisionado . No aprendizado_supervisionado  , por outro_lado  , o uso de algoritmos baseados em grafos ainda tem sido pouco explorado na literatura . Este trabalho apresenta um algoritmo não paramétrico baseado em grafos para problemas de classificação com distribuição estacionária , bem como sua extensão para problemas que apresentam distribuição não estacionária . O algoritmo desenvolvido baseia-se em dois conceitos , a saber , 1 ) em uma estrutura chamada grafo K-associado ótimo , que representa o conjunto de treinamento como um grafo esparso e dividido em componentes ; e 2 ) na medida de pureza de cada componente , que utiliza a estrutura do grafo para determinar o nível de mistura local dos dados em relação às suas classes . O trabalho também considera problemas de classificação que apresentam alteração na distribuição de novos dados . Este problema caracteriza a mudança de conceito e degrada o desempenho do classificador . De modo que , para manter bom_desempenho  , é necessário que o classificador continue aprendendo durante a fase de aplicação , por exemplo , por meio de aprendizado incremental . Resultados experimentais sugerem que ambas as abordagens apresentam vantagens na classificação de dados em relação aos algoritmos 
 O desenvolvimento da padronização do modelo de gerenciamento de rede é uma necessidade que todos os grupos envolvidos ( pesquisadores , fabricantes de equipamentos de rede , fornecedores de soluções de rede , etc . ) , buscam para possibilitar a interoperabilidade . Neste trabalho foram realizados estudos dos principais modelos de gerenciamento , Internet e o OS/ , apresentando suas características e as deficiências . Os estudos concentraram-se sobre os protocolos de transporte das mensagens de gerenciamento , CMIP e SNMP , por serem a chave para realizar o gerenciamento , e especialmente o SNMP versão 2 . Uma ferramenta de gerenciamento de rede vem_sendo  desenvolvida no Instituto de Ciências_Matemáticas  de São Carlos USP , denominada NetTracker , que passou por uma reestruturação para adaptar às novas tendências tecnológicas , Java , H I P , CORBA , SSL , entre outras . Recentemente foram publicadas documentos ( RFCs ) que propõem alterações no protocolo SNMP , batizado de SNMPv3 . Verifica-se que a nova versão do modelo estrutural apresenta similaridades ao do NetTracker . Com as formalizações das especificações tanto da ferramenta de gerenciamento como do protocolo , o projeto estruturou um modelo de um sistema gerente que absorve as novas tecnologias , adaptado-o as novas características do protocolo e da ferramenta 
 Nesta dissertação , desenvolvemos uma análise_Bayesiana  de modelos de mistura finita de distribuições , para dados de sobrevivência sem censura , com censura tipo II e dados censurados por intervalos , na presença de uma covariável . Consideramos os algoritmos amostrador de Gibbs com Metropolis-Hastings , e utilizamos os estimadores de Monte_Carlo  para conseguir as quantitades à posteriori de interesse , assumindo diferentes escolhas para as ( J = 2 ) densidades no modelo de mistura , como por exemplo a mistura de , duas distribuições potência exponencial a qual considera uma grande classe de distribuições simétricas , duas distribuições normais , normal-exponencial e gamma-normal . Apresentamos também ah gumas considerações na seleção do modelo utilizando as densidades preditivas ( CP0 ) preditivas condicionais ordenadas e introduzimos três exemplos numéricos para ilustrar a metodologia_proposta  
 A utilização de recursos Hipermídia e técnicas de Inteligência_Artificial  em ambientes de ensino e aprendizado oferecem uma melhor apresentação das informações aos usuários e proporcionam melhores_resultados  por habilitar o sistema a `` raciocinar '' sobre o que e como apresentar ensinamentos efetivos , estimulando o estudante a aprender . Assim sendo , propõe-se , em um projeto de maior abrangência , a arquitetura de um ambiente denominado SIATE Sistema Inteligente de Apoio a Treinamento e Ensino que integra características de Sistemas Baseados em Conhecimento , Sistemas Tutores , Raciocínio_Baseado  em Casos , Hiperraídia e Simulação , com o objetivo de enriquecer um ambiente exploratório de ensino com conhecimento especialista do domínio e de melhorar o aprendizado do estudante . O presente_trabalho  , parte integrante do SIATE no domínio de Aquisição de Conhecimento , corresponde ao projeto e desenvolvimento de um Sistema Híbrido de Conhecimento , o qual contém o conhecimento especializado do domínio de aplicação e fornece apoio à geração de roteiros de páginas de uni hiperdocumento e à ferramenta de treinamento do SIATE 
 A aplicação da informática na educação tem sido alvo de investigação devido à importância da exploração do uso da tecnologia nos processos de ensino e aprendizagem . Em adição , o uso da Internet tem se expandido em aplicações para o domínio ensino . Neste contexto , esta dissertação apresenta aspectos de projeto , especificação e implementação de um conjunto de ferramentas que auxiliam a autoria e disponibilização de material didático no ambiente World_Wide  Web ( WWW ) . Como motivação , sistemas e aplicações recentes são apresentados e o modelo de hiperdocumentos atualmente utilizado na WWW é discutido . O Relationship Management Model ( RMM ) é então utilizado para a modelagem do conjunto de ferramentas de autoria . Como passo seguinte da modelagem , são discutidas as vantagens do uso fie SGML como uma técnica de especificação formal , e apresentadas as especificações para os documentos suportados pelas ferramentas de autoria . Após uma discussão_sobre  os aspectos da interação usuário-hipertexto , são apresentadas as ferramentas de autoria implementadas . Após uma discussão dos pontos fortes e fracos das ferramentas de autoria implementadas quando comparadas a um ambiente completo para autoria de material didático , são relatados os resultados de um experimento preliminar de usabilidade . Finalmente , são apresentados alguns projetos de pesquisa como continuidade do trabalho aqui reportado 
 Esse trabalho apresenta algumas técnicas e métodos que apoiam a fase de engenharia de requisitos , bem como urna comparação entre as abordagens revisadas . Uma proposta de um processo para a engenharia de requisitos baseada na construção de cenários , compatível com a UML , ( S.apresentada . A notação introduzida , o processo de construção dos modelos de requisitos e um conjunto- de heurísticas para a construção de um modelo de enálisasão apresentados . Um estudo cie caso referente a um sistema de apoio à escrita de docurnentos técnicos ilustra a construção dos modelos propostos pelo processo . Finalmente , , uma ferramenta que apeia a construção dos modelos introduzidos pelo processo é apresentada 
 Atualmente , segurança de redes é um assunto de vital importância nas mais diversas_áreas  . O crescimento e popularização da Internet fizeram com que diversos serviços essenciais fossem realizados através da rede e cada vez mais redes privadas se conectassem para desfrutar de todos os benefícios oferecidos . Por outro_lado  , atos de pirataria e ataques aos computadores tem causado enormes prejuízos e danos . Este cenário exige mecanismos altamente eficazes para se aumentar a segurança cias redes conectadas à Internet . Atualmente , a solução mais utilizada é o firewall , porém , sob diversas circunstâncias ele não é capaz de impedir um ataque . Sistemas de detecção de intrusão podem se tornar uma ferramenta altamente útil trabalhando em conjunto com firewalls . Este trabalho propõe que o uso conjunto de um firewall e um sistema de detecção de intrusão adaptativo baseado em redes neurais pode fornecer um alto grau de segurança à uma rede privada na medida em que une características complementares dos dois sistemas , além de permitir respostas rápidas e automáticas frente a uma invasão em andamento . Esta integração se dará com base em um sistema de segurança distribuído com as seguintes características : modularidade , escalabilidade , gerenciamento remoto , comunicação segura entre os diversos módulos e portabilidade 
 Neste trabalho comparamos modelos de séries_temporais  auto-regresivos de ordem p AR ( p ) , ajustados pela abordagem clássica e Bayesiana . Na análise clássica a identificação do modelo é feita através da função de autocorrelação ( FAC ) e função de autocorrelação parcial ( FACP ) , a escolha do melhor modelo para um conjunto de dados é feita usando-se o Critério de Informação de Alcaike ( MC ) e o Critério de Informação Bayesiano ( MC ) . Na análise_Bayesiana  consideramos três alternativas de densidades a priori para os parâmetros , aqui a escolha do melhor modelo é feita pela densidade preditiva . Primeiramente consideramos a priori não informativa de Jeffireys , onde a densidade a posteriori marginal , para os parâmetros do modelo , pode ser calculada analiticamente e mostra-se que o valor esperado dessa posteriori coincide com o estimador de máxima_verossimilhança  . No segundo caso , adotamos uma função densidade a priori conjugada normal-gama . Aqui , a densidade a posteriori também pode ser calculada analiticameMe , resultando em uma densidade t-Student p-dimensional , no entanto em muitas situações reais adotar priori conjugada é pouco realista . Para contornar esse problema , no terceiro caso adotamos uma densidade a priori informativa t-Student , pdimensional , para os parâmetros e uma densidade a priori gama para o inverso da variância dos resíduos . Isto resulta em uma densidade a posteriori não padronizada . Neste caso a análise a posteriori só pode ser feita usando-se algoritmos de simulação em cadeia de Markov , MCMC 
 Atualmente as organizações devem trocar dados entre si , e a tendência é que estas trocas sejam efetuadas cada vez mais através de meios digitais . Os dados são consultados ( alimentados ) livremente nas bases de dados de organizações independentes entre si , porém quando é necessária a troca de dados , como não existe uma previsão de integração , os dados somente podem ser trocados após uma preparação que impõe alguma_forma  de intervenção manual , construção de filtros especiais , etc. , uma vez que a não existência de .um esquema comum impede que os dados de uma base possa ser intercambiados com os de outra base . No entanto , embora as bases de dados de diferentes organizações possam ser construídas de maneira totalmente independentes , a necessidade de uma troca significa que a semântica dos elementos manipulados , em particular daqueles que devem ser compartilhados é , no mínimo , semelhante . Por exemplo , se duas organizações devem trocar dados sobre pessoas , não importa se para as diferentes organizações as pessoas são clientes , empregados , alunos ou pacientes , o significado de `` pessoa '' é sempre entendido pelos membros das organizações . O mesmo ocorre com qualquer entidade que se deseje trocar informações . Este trabalho parte da suposição que possa existir alguma_forma  de definição primitiva para os diversos elementos de dados que devem ser compartilhados , a partir da qual sua instanciação em elementos de um esquema de dados em particular possa ser reconhecido . Assim , busca-se identificar estruturas primitivas , com a finalidade de integrar os sistemas . Contudo , para se chegar a tal estrutura , é preciso definir regras que garantam a preservação de propriedades da mesma com a finalidade de permitir que sempre que o esquema de uma organização A , construído a partir de uma mesma estrutura primitiva que foi utilizada por outro esquema de uma organização B , a integração entre ambas possa ser feita de forma automática , e que isso não provoque inconsistência nas bases de dados que estarão recebendo as informações . Para atingir esses objetivos , este trabalho propõe que tais estruturas de esquemas primitivos sejam parte das Bibliotecas de Templates de Objetos ( BTO ) que são distribuídas como parte das Ferramentas de Desenvolvimento Rápido de Aplicativos ( RAD ) comerciais . Como um exemplo de como isso poderia ser feito , apresenta-se um componente que define genericamente um objeto pessoa , desenvolvido a partir de como `` pessoas '' são tratadas em dois sistemas reais , centrando essa estrutura em uma única Abstração : a Abstração de Generalização . Apresenta também o conjunto de regras que deve ser utilizado para a integração de componentes centrados nessa abstração , que é a mais universalmente compatível entre os diversos modelos de dados orientados a objetos disponíveis atualmente 
 O projeto SMmD Sistemas Multimídia Distribuídos investiga a construção de uma estrutura de middleware para dar suporte à implementação de aplicações multimídia interativas em ambientes distribuídos e heterogêneos . Para tanto , devenvolveu-se o Ambiente SMmD , o qual inclui módulos para armazenamento e recuperação de objetos de mídia tais como áudio e vídeo , e também módulos para autoria e intercâmbio de objetos multimídia em conformidade com o padrão ISO MHEG-5 . Esta dissertação descreve o trabalho de implementação de um dos módulos do Projeto SMmD , chamado Módulo de Apresentação e Sincronização ( MAS ) . Este módulo foi implementado a partir de outro , chamado Java MBEG-5 Engine ( JHEG ) , cuja função é interpretar e manipular objetos multimídia em conformidade com aquele padrão . Incialmente são apresentados o contexto , a motivação e os objetivos que levaram à execução deste trabalho . A seguir , é feita uma revisão dos conceitos relacionados à apresentação multimídia , enfatizando os aspectos_relacionados  à sincronização de seus componentes . Em seguida , apresenta-se o padrão MBEG-5 juntamente com outros padrões e recomendações relevantes no contexto deste trabalho . Com o objetivo de contextualizar o trabalho , é apresentada , então , uma visão geral dos módulos que compõem o Ambiente SMmD , seguida de uma descrição do trabalho de pesquisa que resultou na implementação do applet SyncEvent , embrião do Módulo de Apresentação e Sincronização . O Módulo de Apresentação e Sincronização , resultado principal deste trabalho , é então descrito , detalhando-se aspectos de sua arquitetura e de sua implementação . Finalmente , na conclusão , as contribuições deste trabalho são discutidas juntamente com suas limitações e possíveis trabalhos futuros , dentre os quais está a integração do MAS com os outros módulos do Projeto SMmD 
 Este trabalho apresenta a definição de um módulo de análise de saídas para o ASiA ( Ambiente de Simulação Automático desenvolvido pelo grupo de Sistemas_Distribuídos  e Programação Concorrente do ICMSC-USP ) . O objetivo deste módulo de análise de saídas é proporcionar ao usuário do ASIA uma ferramenta para validar os resultados de sua simulação , através do cálculo do intervalo de confiança para a média de uma dada medida de interesse . Os métodos estatísticos adotados são replicações e batch means para variáveis simples e múltiplas . Além destes métodos , considera-se ainda um método para comparação entre sistemas utilizando-se diferenças entre médias . Diversos métodos estatísticos são revisados nesta dissertação , sendo que a escolha dos que compõem o módulo de análise foi efetuada adotando-se diversos critérios . Os métodos escolhidos foram testados ( os diversos programas que compõem o módulo são apresentados ) , exemplificados e discutida sua integração com o ASiA 
 O objetivo deste trabalho é apresentar um algoritmo paralelo que implementa o método do gradiente conjugado com pré condicionamento para resolver equações elípticas de segunda_ordem  em um domínio retangular . A principal aproximação consiste em substituir as derivadas parciais por diferenças_finitas  para obter um sistema linear esparso . O domínio é então decomposto de acordo com o número de processadores , e cada um executa o trabalho em um subdomínio específico . A decomposição escolhida minimiza a comunicação entre os processadores , reduzindo substancialmente o tempo de solução 
 Apresentamos nesta dissertação , uma análise do modelo algébrico para o código genético proposto por Homos e Homos , o qual procura explicar as degenerescências do código genético como o resultado de uma seqüência de quebras de simetrias que tenha ocorrido durante a evolução . Fizemos uma procura sistematizada por possíveis simetrias no código genético , através de uma análise minuciosa de todas as álgebras de Lie simples que possuem uma representação irredutível de dimensão 64 e de todas as suas cadeias de subálgebras maximais . Os resultados confirmam e sistematizam as conclusões de Homos e Homos e Forger et all 
 Nesta última_década  , houve um grande crescimento na capacidade de gerar e coletar dados , devido principalmente a três fatores : primeiro , ao constante aumento do poder de processamento dos computadores ; segundo , ao armazenamento continuo de grande quantidade de dados a um baixo custo ; e por último , à introdução de novas e melhores tecnologias relacionadas ao processamento e transmissão de dados . Embora os gerenciadores de bases de dados forneçam ferramentas básicas para otimizar o armazenamento e busca em grande quantidade de dados , o fato de como ajudar os humanos a entender e analisar estas grandes estruturas de dados é um problema de dificil solução . Nesse contexto , o Processo de Extração de Conhecimento de Bases de Dados ( Knowledge Discovery in Databases KDD ) emerge como uma nova tecnologia orientada à compreensão e busca de conhecimento embutido dentro destas grandes massas de dados , fazendo_uso  , principalmente , de várias técnicas apoiadas na estatística , bases de dados , ferramentas de visualização e Aprendizado de Máquina . De um modo geral , o processo KDD é composto de várias etapas , partindo da definição do domínio , um pré-processamento dos dados , uma etapa de Data_Mining  ( Mineração de Dados ) e , finalmente , uma análise e interpretação do conhecimento obtido . Este trabalho visa compreender e delimitar as diferentes etapas dentro do processo KDD , analisando o papel da etapa de Data_Mining  dentro deste processo . Dois estudos de casos ( uma base de dados petroleira e uma base de dados do Programa de Melhoramento Genético da Raça Nelore ) foram realizados para este fim . Este trabalho aborda importantes aspectos , principalmente quanto à relevância da discretizaç'âo de dados contínuos na obtenção de melhores regras de classificação , além de mostrar o processo KDD em uma base de dados real , destacando a problemática encontrada e a importância da presença do especialista do domínio para o êxito deste processo 
 O desenvolvimento da tecnologia de computadores tem facilitado a construção de máquinas_paralelas  MIME ) com memória distribuída e de alto_desempenho  , a exemplo do SPP2 . Exceto pelo maior desempenho apresentado , essas máquinas têm operação similar a de uma rede local de computadores , permitindo a utilização direta de urna ampla gama de ferramentas para o desenvolvimento de aplicações paralelas disponíveis para essa classe de sistemas . Por outro_lado  , a disponibilidade de ferramentas para o monitoramento e gerenciamento da máquina_paralela  é muito escassa e as poucas ferramentas disponíveis não satisfazem as necessidades dos usuários . A ferramenta Mirador , apresentada neste trabalho , cobre essa lacuna . Essa ferramenta permite o monitoramento e o gerenciamento do SPP2 em particular e , com pequenas alterações , de uma rede de computadores que esteja sendo utilizada como uma máquina_paralela  . Para tomar seu uso mais flexível , a ferramenta Mirador usa como interface com o usuário um navegador ( browser ) Internet com suporte à linguagem Java , permitindo o monitoramento e o gerenciamento da arquitetura alvo independentemente da plataforma de hardware ou de sistema_operacional  que o usuário utilize . A funcionalidade da ferramenta Mirador foi testada e os primeiros resultados de utilização mostram que ela pode simplificar bastante as tarefas diárias de programadores e administradores de sistemas paralelos 
 Neste projeto , abordamos os modelos de séries_temporais  estacionárias do tipo AR ( p ) e MA ( q ) . O interesse é obter para estes modelos as- estimativas de máxima_verossimilhança  exata . A diferenciação explicita da função de verossimilhança exata para se obter estas estimativas , não é recomendável por envolver operações complicadas . Assim , [ Box , Jenkins e Reinsel - 1994 ] sugerem métodos_numéricos  baseados em aproximações . Em [ Miller - 1995 ] são apresentadas expressões mais simples para as derivadas da função de verossimilhança junto com um algoritmo iterativo , no caso de modelos AR ( p ) . O objetivo do presente projeto é propor o uso de algoritmos de simulação de Monte_Carlo  com Cadeia de Markov ( MCMC ) para o cálculo das estimativas de máxima_verossimilhança  . Aqui , os algoritmos utilizados foram o amostrador de Gibbs em conjunto com o algoritmo de Metropolis-Hastings . Os resultados obtidos usando MCMC são comparados com as estimativas feitas pelos métodos_numéricos  propostos em [ Box , Jenlcins e Reinsel - 1994 ] e [ Miller - 1995 ] 
 Um novo método orientado a objetos para apoiar o projeto e o desenvolvimento estruturado de aplicações_hipermídia  , denominado HMBS/M , é proposto . O HMBS/M possui como principal característica o uso do HMBS ( Hypertext Model Based on Statecharts ) que utiliza como modelo formal subjacente a técnica de Statecharts para especificar a estrutura organizacional e a semântica de navegação de uma aplicação hipermídia . As quatro etapas que compõe o método - modelagem conceituai , modelagem navegacional , modelagem da interface e implementação - são apresentadas . Em cada fase do método são construídos modelos que podem ser melhorados ou incrementados na fase posterior , permitindo assim um desenvolvimento interativo . São discutidas três formas de implementação para uma aplicação hipermídia especificada pelo HMBS/M interpretada , traduzida e de tradução livre com ênfase para as duas primeiras , que são implementadas usando uma ferramenta que suporta o HMBS , o sistema HyScharts , e usando o ambiente WWW ( padrão HMTL ) . Para ilustrar e validar o HMBS/M apresenta-se a especificação de uma aplicação hipermídia baseada nos catálogos de Graduação e Pós-graduação do Instituto de Ciências_Matemáticas  e de Computação da USP 
 A descoberta da história taxonômica e evolutiva das espécies é a principal fronteira das pesquisas científicas em diversas_áreas  do conhecimento . A biodiversidade dos indivíduos associada a grande variabilidade morfológica , torna essa tarefa um verdadeiro desafio . Os métodos tradicionais baseados na inspeção visual estão ultrapassados . Hoje em dia , os avanços tecnológicos têm colocado à disposição dos pesquisadores um arcabouço de ferramentas para o estudo das espécies . Este trabalho tem como objetivo investigar e desenvolver modelos computacionais capazes de analisar e classificar espécies biológicas por meio de características morfométricas . Para isso , técnicas de análise de imagens foram utilizadas para determinar a variabilidade das espécies em função de três informações de interesse : forma , estrutura tubular e textura . A potencialidade das metodologias foi avaliada por meio das seguintes espécies biológicas : maracujás silvestres do gênero Passiflora , eucaliptos do gênero Eucalyptus e arroz do gênero Oryza . Os experimentos produziram um conjunto de dados que representam uma detalhada descrição sobre a morfometria das espécies . Os resultados demonstraram que as técnicas de visão artificial são importantes para o estudo das espécies . As técnicas de análise de formas indicaram a viabilidade dessas metodologias na classificação das espécies , em particular , as redes complexas , a transformada de wavelets e a dimensão_fractal  multiescala que alcançaram altas taxas de classificações corretas . Os métodos desenvolvidos para análise de estruturas tubulares também demonstraram grande potencialidade na discriminação das espécies , principalmente a assinatura fractal multiescala a partir de pontos biométricos . As técnicas de análise de textura desenvolvidas também contribuíram para o estudo das espécies . Os resultados obtidos com as abordagens sinalizam que a relação entre biologia e computação é essencial para o desenvolvimento de metodologias eficientes . A inferência evolutiva das espécies foi um importante resultado obtido com os dados morfométricos , tanto a partir da forma , quanto da estrutura tubular e da textura . Testes estatísticos comprovaram a correlação entre os dados morfométricos obtidos por visão artificial e os dados moleculares de reconstrução filogenética . A multidisciplinaridade é o ponto central do trabalho , que está inserido na fronteira das áreas de visão artificial , morfometria e biologia . Com isso , essa simbiose resultou em promissoras contribuições para as áreas 
 A procedência dos dados consiste no conjunto de metadados que possibilita identificar as fontes e os processos de transformação aplicados aos dados , desde a criação até o estado atual desses dados . Existem diversas motivações para se incorporar a procedência ao processo de integração , tais como avaliar a qualidade dos dados das fontes heterogêneas , realizar processos de auditoria dos dados e de atribuição de autoria aos proprietários dos dados e reproduzir decisões de integração . Nesta dissertação é proposto o MPPI , um modelo de procedência para subsidiar processos de integração . O modelo enfoca sistemas nos quais as fontes de dados podem ser atualizadas somente pelos seus proprietários , impossibilitando que a integração retifique eventuais conflitos de dados diretamente nessas fontes . O principal requisito do MPPI é que ele ofereça suporte ao tratamento de todas as decisões de integração realizadas em processos anteriores , de forma que essas decisões possam ser reaplicadas automaticamente em processos de integração subsequentes . O modelo MPPI possui quatro características . A primeira delas consiste no mapeamento da procedência dos dados em operações de cópia , edição , inserção e remoção , e no armazenamento dessas operações em um repositório de operações . A segunda característica é o tratamento de operações de sobreposição , por meio da proposta das políticas blind , restrict , undo e redo . A terceira característica consiste na identificação de anomalias decorrentes do fato de que fontes de dados autônomas podem alterar os seus dados entre processos de integração , e na proposta de quatro tipos de validação das operações frente a essas anomalias : validação completa , da origem , do destino , ou nenhuma . A quarta característica consiste na reaplicação de operações , por meio da proposta dos métodos VRS ( do inglês Validate and Reapply in Separate ) e VRT ( do inglês Validate and Reapply in Tandem ) e da reordenação segura do repositório , os quais garantem que todas as decisões de integração tomadas pelo usuário em processos de integração anteriores sejam resolvidas automaticamente e da mesma forma em processos de integração subsequentes . A validação do modelo MPPI foi realizada por meio de testes de desempenho que investigaram o tratamento de operações de sobreposição , o método VRT e a reordenação segura , considerando como base as demais características do modelo . Os resultados obtidos mostraram a viabilidade de implementação das políticas propostas para tratamento de operações de sobreposição em sistemas de integração reais . Os resultados também mostraram que o método VRT proporcionou ganhos de desempenho significativos frente à coleta quando o objetivo é restabelecer resultados de processos de integração que já foram executados pelo menos uma vez . O ganho médio de desempenho do método VRT foi de pelo menos 93 % . Ademais , os testes também mostraram que reordenar as operações antes da reaplicação pode melhorar ainda mais o desempenho do método 
 O contínuo avanço da capacidade dos circuitos integrados e a necessidade de sistemas embarcados cada vez mais complexos para lidar com os problemas atuais , com prazos cada vez mais curtos , estão direcionando o desenvolvimento de sistemas de circuitos integrados para ambientes de alto nível de abstração cada vez mais distantes dos detalhes de hardware . O uso de linguagens de alto nível para auxiliar o desenvolvimento de sistemas embarcados é uma tendência atual pois tal abordagem tende a reduzir a complexidade e o tempo de desenvolvimento . Este trabalho propõe o desenvolvimento de uma nova ferramenta para geração de arquiteturas de hardware em Bluespec em um ambiente gráfico utilizando diagramas da UML . Esta ferramenta permite que o projetista descreva o comportamento utilizando máquina de estados finita no padrão UML 2.0 , onde cada estado pode conter a codificação do comportamento com as linguagens Bluespec e C. Dada uma máquina de estados , a mesma é traduzida para a linguagem Bluespec por meio de um compilador e templates . Como resultado , é apresentado a geração de duas arquiteturas de hardware a fim de demonstrar as vantagens e limitações da ferramenta 
 A queda do interesse por parte de novos universitários , para cursos de ciência da computação em várias universidades do mundo [ 55 , 68 ] , é um sinal para começarmos a pensar se um dos motivos dessa queda tem relação com a forma pela qual o ensino de computação está sendo conduzido . Nessa linha , perguntamos se existem maneiras de tornar o ensino de computação mais interativo e motivante para os alunos da nova geração , os quais cresceram no meio de uma das categorias mais complexas de software existentes hoje : os jogos de computador [ 10 ] . Esses softwares ficam cada vez mais interativos , complexos e ricos em detalhe com o passar do tempo . Conforme será exposto , por meio do estudo de algumas iniciativas de pesquisadores nesse sentido , o ensino de ciência da computação pode se tornar mais interessante e rico com a utilização de jogos de computador como recurso didático , para capturar a atenção dessa nova geração de estudantes . Com base nesse resultado , vamos focar nossa contribuição no ensino de lógica em cadeiras de Inteligência_Artificial  ( IA ) , uma área de concentração da Ciência da Computação . Apresentamos uma ferramenta que chamamos de Odin , para construir e visualizar especificações executáveis de IA , por meio da linguagem PROLOG , em ambientes tridimensionais de jogos de computador . Entendemos que essa ferramenta pode ser utilizada como um recurso didático em cursos de lógica para alunos em nível de graduação . Como principal benefício , o aluno tem a possibilidade de explorar , observar e interagir com os resultados de seu trabalho . Essa possibilidade de visualização é o que parece reter a atenção do aluno , de acordo com pesquisadores na área . Disponibilizamos dois cenários de uso : O labirinto e o Mundo de Wumpus , dois cenários que juntos podem ser utilizados para cobrir uma boa parte da carga didática de um curso de lógica para graduação . Outros cenários podem ser desenvolvidos posteriormente por meio de extensão do framework composto por classes C++ . A ferramenta foi utilizada em duas cadeiras de inteligência_artificial  no Instituto de Matemática e Estatística , da Universidade de são Paulo . Consideramos que a recepção da ferramenta por parte dos alunos foi positiva 
 O aumento da geração e do intercâmbio de imagens médicas digitais tem incentivado profissionais da computação a criarem ferramentas para manipulação , armazenamento e busca por similaridade dessas imagens . As ferramentas de recuperação de imagens por conteúdo , foco desse trabalho , têm a função de auxiliar na tomada de decisão e na prática da medicina baseada em estudo de casos semelhantes . Porém , seus principais obstáculos são conseguir uma rápida recuperação de imagens armazenadas em grandes bases e reduzir o gap semântico , caracterizado pela divergência entre o resultado obtido pelo computador e aquele esperado pelo médico . No presente_trabalho  , uma análise das funções de distância e dos descritores computacionais de características está sendo realizada com o objetivo de encontrar uma aproximação eficiente entre os métodos de extração de características de baixo_nível  e os parâmetros de percepção do médico ( de alto nível ) envolvidos na análise de imagens . O trabalho de integração desses três elementos ( Extratores de Características , Função de Distância e Parâmetro Perceptual ) resultou na criação de operadores de similaridade , que podem ser utilizados para aproximar o sistema computacional ao usuário final , visto que serão recuperadas imagens de acordo com a percepção de similaridade do médico , usuário final do 
 Esta tese de doutorado apresenta a investigação de alternativas para melhorar o desempenho de ambientes Web , avaliando o impacto da utilização de mecanismos de diferenciação de serviços em todos os pontos do sistema . Foram criados e modelados no OPNET Modeler cenários com diferentes configurações voltadas tanto para a diferenciação de serviços , quanto para o congestionamento da rede . Foi implementado um servidor cache com suporte à diferenciação de serviços ( cache CDF ) , que constitui uma contribuição dentro deste trabalho , complementando o cenário de diferenciação de serviços de forma positiva , assegurando que os ganhos obtidos em outras etapas do sistema não sejam perdidos no momento da utilização do cache . Os principais resultados obtidos_mostram  que a diferenciação de serviços introduzida de forma isolada em partes do sistema , pode não gerar os ganhos de desempenho desejados . Todos os equipamentos considerados nos cenários propostos possuem características reais e os modelos utilizados no OPNET foram avaliados e validados pelos seus fabricantes . Assim , os modelos que implementam os cenários considerados constituem também uma contribuição importante deste trabalho , uma vez que o estudo apresentado não se restringe a uma modelagem teórica , ao contrário , aborda aspectos bem próximos da realidade , constituindo um possível suporte de gerenciamento de sistemas 
 Segundo uma pesquisa recente realizada junto ao setor de fundições , uma importante preocupação do setor é melhorar seu planejamento de produção . Um plano de produção em uma fundição envolve duas etapas interdependentes : a determinação das ligas a serem fundidas e dos lotes que serão produzidos . Neste trabalho , estudamos o problema de dimensionamento de lotes para fundições de pequeno_porte  , cujo_objetivo  é determinar um plano de produção de mínimo custo . Como sugerido na literatura , a heurística proposta trata as etapas do problema de forma hierárquica : inicialmente são definidas as ligas e , posteriormente , os lotes que são produzidos a partir delas . Para a solução do problema , propomos um algoritmo genético que explora um conjunto de possibilidades para a determinação das ligas e utiliza uma heurística baseada em relaxação lagrangiana para determinação dos itens a serem produzidos . Além_disso  , uma abordagem para o mesmo problema é proposta utilizando o problema da mochila para determinar os itens a serem produzidos . Bons resultados foram obtidos pelos métodos 
 A área de pesquisa de Arquitetura de Software tem recebido cada vez mais atenção dentro da Engenharia de Software , visto que arquiteturas de software têm tido papel determinante para a qualidade de sistemas de software . Nesse contexto , arquiteturas de referência surgiram como um tipo especial de arquitetura que provê diretrizes para construção de arquiteturas de software de uma dada classe de sistemas , promovendo também o reúso do conhecimento sobre um dado domínio de aplicação . Considerando sua relevância , arquiteturas de referência têm sido propostas e utilizadas com sucesso em diferentes domínios . Em outra perspectiva , a indústria de software tem cada vez mais adotado métodos ágeis de desenvolvimento de software , visando , entre outros aspectos , diminuir o tempo de entrega de seus produtos . De um modo geral , esses métodos têm como característica comum ciclos curtos de produção e entrega no desenvolvimento de software . Consequentemente , minimizam a quantidade de documentação gerada no processo . Dessa forma , parece ser conflitante a adoção de arquiteturas de referência no contexto de métodos ágeis . No entanto , iniciativas já podem ser encontradas , apesar de haver uma carência de trabalhos que propõem a utilização de ambas de forma integrada . Dessa forma , o principal objetivo desse projeto de mestrado foi o estabelecimento um processo ágil de desenvolvimento de software , denominado AGIRA , que explora as vantagens do uso de arquiteturas de referência . A avaliação de tal processo foi realizada em duas frentes . A primeira foi por meio da condução de um action case com alunos de uma disciplina de Arquitetura de Software da Universidade de Bolzano . A segunda frente foi conduzida através de um questionário com especialistas da indústria . Resultados alcançados mostram que o AGIRA tem potencial para ser utilizado pela indústria e que , dessa forma , arquiteturas de referência podem ser aplicadas juntamente com métodos 
 Apesar dos processos de recuperação e filtragem de informação sempre terem usado técnicas básicas de Processamento de Linguagem Natural ( PLN ) no suporte à estruturação de documentos , ainda são poucas as indicações sobre os avanços relacionados à utilização de técnicas mais sofisticadas de PLN que justifiquem o custo de sua utilização nestes processos , em comparação com as abordagens tradicionais . Este trabalho_investiga  algumas evidências que fundamentam a hipótese de que a aplicação de métodos que utilizam conhecimento linguístico é viável , demarcando importantes contribuições para o aumento de sua eficiência em adição aos métodos estatásticos tradicionais . É proposto um modelo de representação de texto fundamentado em sintagmas nominais , cuja representatividade de seus descritores é calculada utilizando-se o conceito de evidência , apoiado em métodos estatísticos . Filtros induzidos a partir desse modelo são utilizados para classificar os documentos recuperados analisando-se a relevância implícita no perfil do usuário . O aumento da precisão ( e , portanto , da eficácia ) em sistemas de Recuperação de Informação , conseqüência da pós-filtragem seletiva de informações , demonstra uma clara evidência de como o uso de técnicas de PLN pode auxiliar a categorização de textos , abrindo reais possibilidades para o aprimoramento do modelo 
 A possibilidade de armazenamento de imagens no formato digital favoreceu a evolução de diversos ramos de atividades , especialmente as áreas de pesquisa e clínica médica . Ao mesmo tempo , o volume crescente de imagens armazenadas deu origem a um problema de relevância e complexidade consideráveis : a Recuperação de Imagens Baseada em Conteúdo , que , em outras_palavras  , diz_respeito  à capacidade de um sistema de armazenamento processar operações de consulta de imagens a partir de características visuais , extraídas automaticamente por meio de métodos computacionais . Das principais questões que constituem este problema , amplamente conhecido pelo termo CBIR - Content-Based Image Retrieval , fazem parte as seguintes : Como interpretar ou representar matematicamente o conteúdo de uma imagem ? Quais medidas que podem caracterizar adequadamente este conteúdo ? Como recuperar imagens de um grande repositório utilizando o conteúdo extraído ? Como estabelecer um critério matemático de similaridade entre estas imagens ? O trabalho desenvolvido e apresentado nesta tese busca , exatamente , responder perguntas deste tipo , especialmente para os domínios de imagens médicas e da biologia genética , onde a demanda por sistemas computacionais que incorporam técnicas CBIR é consideravelmente alta por diversos motivos . Motivos que vão desde a necessidade de se buscar informação visual que estava até então inacessível pela falta de anotações textuais , até o interesse em poder contar com auxílio computacional confiável para a importante tarefa de diagnóstico clínico . Neste trabalho são propostos métodos e soluções inovadoras para o problema de segmentação e extração de características de imagens médicas e imagens de padrões espaciais de expressão genética . A segmentação é o processo de delimitação automático de regiões de interesse da imagem que possibilita uma caracterização bem mais coerente do conteúdo visual , comparado com as tradicionais técnicas de caracterização global e direta da imagem . Partindo desta idéia , as técnicas de extração de características desenvolvidas neste trabalho empregam métodos adaptativos de segmentação de imagens e alcançam resultados excelentes na tarefa de recuperação baseada em 
 Este projeto consiste em um sistema de navegação autônomo baseado em redes neurais nebulosas modulares capacitando o robô a alcançar alvos , ou pontos metas , em ambientes desconhecidos . Inicialmente , o sistema não tem habilidade para a navegação , após uma fase de experimentos com algumas colisões , o mecanismo de navegação aprimora-se guiando o robô ao alvo de forma eficiente . Uma arquitetura híbrida inteligente é apresentada para este sistema de navegação , baseada em redes neurais_artificiais  e lógica nebulosa . A arquitetura é hierárquica e costitiui-se de dois módulos responsáveis por gerar comportamentos inatos de desvio de obstáculos e de busca ao alvo . Um mecanismo de aprendizagem por reforço , baseada em uma extensão da lei de Hebb , pondera os comportamentos inatos conflitantes ajustando os pesos sinápticos das redes neurais nos instantes de captura do alvo e de colisão contra obstáculos . A abordagem consolidada em simulação é validada em ambientes reais neste trabalho . Para tanto , este sistema foi implementado e testado no simulador Saphira , ambiente de simulação que acompanha o robô Pioneer I e que denota um estágio anterior aos testes em ambientes reais por apresentar comportamentos do robô similares aos comportamentos do robô_móvel  . Modificações na arquitetura híbrida foram necessárias para adaptar o sistema de navegação simulado ao sistema incorporado no Pioneer I. Experimentos em ambientes reais demonstraram a eficiência e a capacidade de aprendizagem do sistema de navegação , validando a arquitetura híbrida inteligente para aplicação em robôs_móveis 
 Esta tese analisa aplicações de transmissão de voz e vídeo sobre redes Wi-Fi ( IEEE 802.11 ) . Os principais problemas observados foram a maior incidência de tráfegos em rajada e os problemas associados à execução de handoffs . Foram propostos algoritmos adaptativos para monitorar e contornar esses problemas 
 Casamento de padrões de pontos é um problema fundamental em reconhecimento de padrões . O objetivo é encontrar uma correspondência entre dois conjuntos de pontos , associados a características relevantes de objetos ou entidades , mapeando os pontos de um conjunto no outro . Este problema está associado a muitas aplicações , como por exemplo , reconhecimento de objetos baseado em modelos , imagens estéreo , registro de imagens , biometria , entre outros . Para encontrar um mapeamento , os objetos são codificados por representações abstratas , codificando as características relevantes consideradas na comparação entre pares de objetos . Neste trabalho , objetos são representados por grafos , codificando tanto as características ` locais ' quanto as relações espaciais entre estas características . A comparação entre objetos é guiada por uma formulação de atribuição quadrática , que é um problema NP-difícil . Para estimar uma solução , duas técnicas de casamento entre grafos são propostas : uma baseada em grafos auxiliares , chamados de grafos deformados ; e outra baseada em representações ` esparsas ' , campos aleatórios de Markov e propagação de crenças . Devido as suas respectivas limitações , as abordagens são adequadas para situações específicas , conforme mostrado neste documento . Resultados envolvendo as duas_abordagens  são ilustrados em quatro importantes aplicações : casamento de imagens de gel eletroforese 2D , segmentação interativa de imagens naturais , casamento de formas , e colorização assistida por computador 
 O objetivo dessa dissertação é apresentar um problema de otimização do uso de recursos críticos no desenvolvimento de poços de petróleo marítimos e a técnica empregada para a abordagem proposta ao problema . A revisão da técnica de Programação por Restrições é feita analisando aspectos relevantes de modelagem , propagação , busca e paradigmas de programação . A especialização da técnica para problemas de escalonamento , o Escalonamento Baseado em Restrições , é descrita com ênfase nos paradigmas descritivos e nos mecanismos de propagação de restrições . Como subsídio ao uso da técnica em outros problemas , a linguagem comercial de modelagem OPL é apresentada no Apêndice . O objetivo da abordagem ao problema é obter um escalonador para maximizar a produção de óleo obtida no curto prazo . O escalonador proposto baseia-se na declaração de um modelo empregando variáveis de intervalo . Um algoritmo e um modelo de Programação Linear Inteira abordando relaxações do problema são apresentados para que se obtenha um limitante superior ao valor de produção ótimo . Para o cenário real no qual a análise experimental foi feita , foram obtidas soluções a menos de 16 % do ótimo após uma hora de execução ; e os testes em instâncias de tamanhos variados evidenciaram a robustez do escalonador . Direções para trabalhos futuros são apresentadas ponderando os resultados obtidos 
 Modelos com erros de medição têm recebido a atenção de vários pesquisadores das mais diversas_áreas  de conhecimento . O principal objetivo desta dissertação consiste no estudo de um modelo funcional com erros de medição heteroscedásticos na presença de réplicas das observações . O modelo proposto estende resultados encontrados na literatura na medida em que as réplicas são parte do modelo , ao contrário de serem utilizadas para estimação das variâncias , doravante tratadas como conhecidas . Alguns procedimentos de estimação tais como o método de máxima_verossimilhança  , o método dos momentos e o método de extrapolação da simulação ( SIMEX ) na versão empírica são apresentados . Além_disso  , propõe-se o teste da razão de verossimilhanças e o teste de Wald com o objetivo de testar algumas hipóteses de interesse relacionadas aos parâmetros do modelo adotado . O comportamento dos estimadores de alguns parâmetros e das estatísticas propostas ( resultados assintóticos ) são analisados por meio de um estudo de simulação de Monte_Carlo  , utilizando-se diferentes números de réplicas . Por fim , a proposta é exemplificada com um conjunto de dados reais . Toda parte computacional foi desenvolvida em linguagem R ( R Development Core Team , 2011 
 Veículo autônomo inteligente ( ou apenas veículo autônomo VA ) é um tipo de sistema embarcado que integra componentes físicos ( hardware ) e computacionais ( software ) . Sua principal característica é a capacidade de locomoção e de operação de modo semi ou completamente autônomo . A autonomia cresce com a capacidade de percepção e de deslocamento no ambiente , robustez e capacidade de resolver e executar tarefas lidando com as mais diversas situações ( inteligência ) . Veículos autônomos representam um tópico de pesquisa importante e que tem impacto direto na sociedade . No entanto , à medida que esse campo avança alguns problemas secundários aparecem como , por exemplo , como saber se esses sistemas foram suficientemente testados . Uma das fases do teste de um VA é o teste de campo , em que o veículo é levado para um ambiente pouco controlado e deve executar livremente a missão para a qual foi programado . Ele é geralmente utilizado para garantir que os veículos_autônomos  mostrem o comportamento desejado , mas nenhuma informação sobre a estrutura do código é utilizada . Pode ocorrer que o veículo ( hardware e software ) passou no teste de campo , mas trechos importantes do código nunca tenham sido executados . Durante o teste de campo , os dados de entrada são coletados em logs que podem ser posteriormente analisados para avaliar os resultados do teste e para realizar outros_tipos  de teste offline . Esta tese apresenta um conjunto de propostas para apoiar a análise do teste de campo do ponto de vista do teste_estrutural  . A abordagem é composta por um modelo de classes no contexto do teste de campo , uma ferramenta que implementa esse modelo e um algoritmo genético para geração de dados de teste . Apresenta também heurísticas para reduzir o conjunto de dados contidos em um log sem diminuir substancialmente a cobertura obtida e estratégias de combinação e mutação que são usadas no algoritmo . Estudos de caso foram conduzidos para avaliar as heurísticas e estratégias e são também apresentados e discutidos 
 Coleções de músicas estão amplamente disponíveis na internet e , graças ao crescimento na capacidade de armazenamento e velocidade de transmissão de dados , usuários podem ter acesso a uma quantidade quase ilimitada de composições . Isso levou a uma maior necessidade de organizar , recuperar e processar dados musicais de modo automático . Visualização de informação é uma área de pesquisa que possibilita a análise visual de grandes_conjuntos  de dados e , por isso , é uma ferramenta muito valiosa para a exploração de bibliotecas musicais . Nesta dissertação , metodologias para a construção de duas técnicas de visualização de bases de dados de música são propostas . A primeira , Grafo de Similaridades , permite a exploração da base de dados em termos de similaridades hierárquicas . A segunda , RadViz Concêntrico , representa os dados em termos de tarefas de classificação e permite que o usuário altere a visualização de acordo com seus interesses . Ambas as técnicas são capazes de revelar estruturas de interesse no conjunto de dados , facilitando o seu entendimento e exploração 
 No desenho de mecanismos lubrificados , tais como Mancais hidrodinâmicos ou anéis de pistões de Motores a Combustão , atrito e desgaste são efeitos não desejados . Por exemplo , é sabido que aproximadamente 5 % da energia perdida em um motor a combustão esta associada ao atrito presente no sistema de anéis/cilindro do pistão . Após vários trabalhos experimentais e teóricos , as superfícies texturizadas hão mostrado serem capazes de reduzir o atrito em algumas condições de funcionamento . O estudo da relação entre o atrito e os parâmetros de texturização é um problema difícil e de interesse tanto industrial como acadêmico . O contexto matemático e computacional destes trabalhos apresentam desafios por si mesmos , como o estudo da boa colocação dos modelos matemáticos , a consideração adequada das descontinuidades das superfícies . Este trabalho enfoca-se no contexto matemático , apresentando e estudando a equação de Reynolds junto com diferentes modelos de cavitação que podem encontrar-se na literatura . Começamos estudando a matemática da equação de Reynolds . Depois disso , modelos de cavitação são inclusos , aumentando a complexidade da matemática envolvida . Seguidamente , como aplicação da teoria apresentada , um rolamento deslizante será estudado junto com uma texturização da superfície móvel . Os resultados deste estudo revelam mecanismos básicos de redução de atrito e propriedades gerais que não haviam sido reportadas anteriormente . Possíveis trabalhos futuros são apresentados , tal como o uso de Métodos Descontínuos de Galerkin em vez dos Métodos de Volumes Finitos . O último em procura de uma melhor acomodação da formulação_matemática  , tentando melhorar a flexibilidade da malha e a precisão 
 Este doutorado investigou as contribuições dos jogos digitais no desenvolvimento de conhecimentos matemáticos previstos nas competências curriculares em uma escola estadual de tempo integral , situada em Cotia - São Paulo . As análises realizadas foram fundamentadas a partir das categorias estudadas pelo psicólogo e educador Reuven Feuerstein quanto às mudanças na estrutura cognitiva ( EAM ) de alunos do Ensino Fundamental II . Esta investigação foi desenvolvida com 60 alunos e três professoras de Matemática em Oficinas Curriculares denominadas Experiências Matemáticas . Em sua trajetória metodológica contou com a participação da equipe gestora , professores de Matemática , alunos do Ensino Fundamental II e Grupo Alpha de Pesquisa - FEUSP durante dois anos . A investigação , de natureza qualitativa , caracterizou-se como pesquisa-ação e contou com a imersão total do pesquisador no campo amostral . Foram adotados técnicas e procedimento de pesquisa triangulados como a observação participante , entrevistas semiestruturadas , entrevistas informais , grupos focais , gravação de áudio e vídeos , fotos , diários de campo , atividades com os jogos digitais , um ambiente virtual ( Moodle ) e a combinação de duas redes_sociais  , FaceBook e WhatsApp . Os resultados apontaram que : i ) o contexto escolar representa espaço privilegiado de sistematização e compreensão do complexo registro notacional da Matemática com a mediação dos jogos digitais ; ii ) o ensino da Matemática por meio de jogos digitais conferem sentido e significado às aprendizagens dos alunos ; iii ) os jogos digitais conferem ao desenvolvimento de competências e habilidades cognitivas com flexibilidade , autonomia , transcendência e construção de significados que são alguns critérios de mediação apontados por Feuerstein ; iv ) os jogos digitais favorecem a compreensão de conteúdos matemáticos de forma colaborativa e lúdica ; v ) os professores de Matemática necessitam de formação permanente que possa ampliar as transformações pedagógicas inovadoras de novos modos de aprender e de ensinar ; vi ) as abordagens pedagógicas podem se beneficiar de perspectivas contemporâneas como mobile-learning , Flipped-classroom e Bring Your Own Device como formas de reduzir os desafios e dificuldades das escolas públicas ( políticas educacionais , infraestrutura , formação docente ) 
 Em Morfologia Matemática diversos operadores são definidos pela diferença entre outros dois operadores , como por exemplo , o gradiente morfológico , definido como a diferença entre a dilatação e a erosão . Estes operadores são denominados operadores residuais , sendo alguns deles definidos por valores residuais extraídos de famílias indexadas de operadores , como por exemplo , o esqueleto por discos maximais e a última abertura . Neste sentido , visa-se neste trabalho investigar a extração de informações residuais em famílias indexadas de operadores . Mais precisamente , em famílias de operadores conexos conhecidos como levelings . Os levelings são operadores que não criam novas estruturas ( contornos e extremos regionais ) e seus valores são limitados pelos valores da imagem de referência . Assim , é apresentada nesta tese uma classe de operadores residuais denominada últimos levelings , a qual consiste de poderosos operadores residuais definidos a partir de resíduos gerados por operadores consecutivos de um espaço de escala baseado em levelings . Dessa forma , objetos contrastantes podem ser detectados se relevantes resíduos são gerados quando eles são filtrados por um desses levelings . Os valores residuais revelam importantes informações sobre contrastes presentes em uma imagem . Além dos valores residuais , outras informações associadas com eles podem ser obtidas no momento da extração residual , tais como os índices dos operadores que produziram os valores residuais . Com base nessas considerações , as principais_contribuições  originais desta pesquisa , incluem : ( i ) demonstrar que árvores construídas a partir de conjuntos de níveis representam espaços de escalas baseados em levelings ; ( ii ) introduzir a classe dos últimos levelings , passando por definições , conceitos , algoritmos , propriedades e relações com outros operadores conhecidos na literatura ; ( iii ) apresentar estratégias para construção de operadores últimos levelings . Por fim , são apresentadas aplicações dos últimos levelings em problemas de análise e processamento de imagens 
 Este projeto de mestrado apresenta uma visão de como áreas da Ciência da Computação relacionadas a desempenho ( Avaliação de Desempenho , Engenharia de Desempenho de Software e Teste de Desempenho de Software ) podem ser relacionas e também propõe uma metodologia de análise de desempenho que contém conceitos das áreas anteriormente apontadas , para que esta seja mais completa e possa ser compreendida por profissionais das três áreas . Para formalizar esse relacionamento , foi construída uma ontologia que mostra como ocorre a correlação entre as áreas , e a partir dessa metodologia_proposta  , foi possível analisar o desempenho da ferramenta ValiPar , em sua versão paralela e concluir que o principal gargalo com relação a escalabilidade dela é sua porção executada sequencialmente . Por fim , foi observado que a metodologia apresenta vantagens se comparada as demais , como uma formalização nas suas etapas de análise 
 O aumento constante da demanda por sistemas computacionais cada vez mais eficientes tem motivado a busca por sistemas híbridos customizados compostos por GPP ( General Purpose Processor ) , FPGAs ( Field-Programmable Gate Array ) e GPUs ( Graphics Processing Units ) . Quando utilizados em conjunto possibilitam otimizar a relação entre desempenho e consumo de energia . Tais sistemas dependem de técnicas que façam o mapeamento mais adequado considerando o perfil do código fonte . Nesse sentido , este projeto propõe uma técnica para realizar o mapeamento entre GPP e FPGA . Para isso , utilizou-se como base uma abordagem de mineração de dados que avalia a similaridade entre código fonte . A técnica aqui desenvolvida obteve taxas de acertos de 65,67 % para códigos sintetizados para FPGA com a ferramenta LegUP e 59,19 % para Impulse C , considerando que para GPP o código foi compilado com o GCC ( GNU Compiler Collection ) utilizando o suporte a OpenMP . Os resultados demonstraram que esta abordagem pode ser empregada como um ponto de decisão inicial no processo de mapeamento em sistemas híbridos , somente analisando o perfil do código fonte sem que haja a necessidade de execução do mesmo para a tomada de decisão 
 Um indicador importante na área acadêmica está relacionado ao grau de impacto de uma publicação , o que pode auxiliar na avaliação da qualidade e do grau de internacionalização de uma instituição . Para melhor delimitar esse indicador torna-se necessária a realização de uma análise das redes de colaboração dos autores envolvidos . Considerando que o modelo de dados relacional é o modelo predominante dos bancos de dados atuais , observa-se que a análise das redes de colaboração é prejudicada pelo fato desse modelo não atender , com o mesmo desempenho , a todos os tipos de consultas realizadas . Uma alternativa para executar as consultas que perdem desempenho no modelo de banco de dados relacional é a utilização do modelo de banco de dados orientado a grafos . Porém , não é claro quais parâmetros podem ser utilizados para definir quando utilizar cada um dos modelos de bancos de dados . Assim , este trabalho tem como objetivo fazer uma análise de consultas que , a partir da sintaxe da consulta e do ambiente de execução , possa apontar o modelo de dados mais adequado para execução da referida consulta . Com essa análise , é possível delimitar em que cenários uma integração entre o modelo relacional e o orientado a grafos é mais adequada 
 Com base em três modelos de mobilidade MapBasedMovement , RandomWayPoint e RandomWalk presentes no simulador The One , sugerimos e discutimos vários modelos es- tocásticos para mobilidade . Primeiramente , a dinâmica das unidades móveis é reduzida a um processo chamado grafo dinâmico , de forma que a configuração espacial das unidades móveis em cada instante de tempo está resumida em um grafo . Os vértices desse grafo são unidades móveis e não mudam conforme o tempo : consideramos um sistema fechado , as unidades não desaparecem e não aparecem novas . O elo entre duas unidades ( vértices ) em um instante de tempo significa um contato neste instante ( a distância entre as unidades é menor que um raio de contato ) , assim o conjunto de elos muda durante a evolução do sistema . Em seguida , modelamos a evolução do grafo dinâmico como um conjunto de pro- cessos aleatórios binários de forma que cada componente do processo está associada com um par de unidades móveis indicando presença ou ausência de contato entre elas . Três componentes principais constroem o processo : ( i ) distribuição de tempo de intercontato , ( ii ) distribuição de tempo de contato , e ( iii ) independência/interação entre as unidades . Nesta Tese mostramos teoricamente e através de simulações como escolher todos os três componentes para três modelos de mobilidade mencionados acima na situação de baixa densidade de unidades móveis , chamado DTNs ( Delay Tolerant Networks ) . Considerar a modelagem da mobilidade desse ponto de vista é novo e não existe na literatura , até onde sabemos . Existe uma discussão na literatura sobre o tempo de intercontato , mas não conhecemos os resultados e discussão_sobre  a distribuição do tempo de contato e a interdependência de processos de contatos 
 As pragas e doenças apresentam-se como um desafio para a citricultura brasileira em razão do impacto económico que elas causam à produção . Neste trabalho é dado destaque à doença da mancha preta ( MPC ) , causada pelo fungo Guignardia citricarpa . Essa doença provoca lesões no fruto , depreciando-o no mercado de frutas in natura , além de causar amadurecimento e queda precoce . Um dos principais agravantes da doença é a demora no aparecimento dos sintomas , sendo muito importante detectar a presença dos esporos do fungo no pomar , antes que os sintomas apareçam . Dessa maneira , há a possibilidade de se controlar a doença de forma eficaz , aplicando-se quantidades menores de fungicidas e , consequentemente , reduzindo os custos da produção e os efeitos deletérios ao meio-ambiente . Atualmente , a detecção desses esporos é realizada por meio da análise de amostras coletadas nos pomares . Essa análise é efetuada por especialistas que realizam a identificação e a contagem dos ascósporos manualmente . Com o objetivo de automatizar esse processo , um conjunto de técnicas para a análise das imagens e a caracterização dos ascósporos do fungo a partir da forma foi estudado e comparado . Dentre as técnicas , a curvatura e os descritores de Fourier apresentaram resultados bastante satisfatórios e foram utilizados na implementação do protótipo de um sistema de visão_computacional  - o CITRUSVIS , que analisa e identifica os ascósporos existentes nas imagens dos discos de coleta 
 Este trabalho apresenta o Animbs ( Animation for MBS ) , um sistema capaz de visualizar dados gerados por um sistema de simulação de engenharia ( SD/FAST ) na forma de animações por computador . SD/FAST é um sistema utilizado para o modelamento e a simulação de sistemas mecânicos multicorpos ( MBS ) . O sistema Animbs permite a associação de uma geometria ao MBS sendo simulado e utiliza os dados produzidos pela simulação do SD/FAST para criar uma animação do comportamento do sistema mecânico e , dessa forma , melhorar a análise de dados feita pelos usuários do SD/FAST 
 O grande volume de dados disponíveis em uma diversa gama de atividades humanas cria várias oportunidades para entendermos , melhorarmos e revelarmos padrões previamente desconhecidos em tais atividades . Métodos automáticos para extrair esses conhecimentos a partir de dados já existem em áreas como aprendizado de máquina e mineração de dados . Entretanto , eles dependem da perícia do analista para obter melhores_resultados  quando estes não são satisfatórios . Neste contexto , técnicas de projeção_multidimensional  interativas são uma ferramenta útil para a análise de dados multidimensionais , revelando sua estrutura subjacente ao mesmo tempo que permite ao analista manipular os resultados interativamente , estendendo o processo de exploração . Essa interação , entretanto , não foi estudada com profundidade com respeito à sua real influência nos mapeamentos , já que podem causar mudanças não esperadas no mapeamento final . Essa é a principal motivação desta pesquisa : entender os efeitos causados pelas mudanças em tais mapeamentos . Abordamos o problema de duas perspectivas . Primeiro , da perspectiva do usuário , desenvolvemos visualizações que ajudam a diminuir tentativas e erros neste processo provendo a informação necessária a cada passo da interação . Além_disso  , essas visualizações ajudam a explicar as mudanças causadas no mapeamento pela manipulação . A segunda perspectiva é a efetividade da manipulação . Definimos de forma quantitativa a efetividade da manipulação , e então desenvolvemos um arcabouço para avaliar manipulações sob a visão da efetividade . Este arcabouço é baseado em melhorias nos mapeamentos usando medidas de avaliação conhecidas para tais técnicas . Usando tais melhorias como diferentes formas de manipulação , realizamos uma série de experimentos em cinco bases de dados , cinco medidas e quatro técnicas . Nossos resultados experimentais nos dão evidências que existem certos tipos de manipulação que podem acontecer efetivamente , com algumas técnicas sendo mais suscetíveis a manipulações do que outras 
 Escoamentos viscoelásticos não estacionários com superfícies_livres  são comuns em muitos processos industriais e diversas técnicas numéricas têm sido empregadas para reproduzir computacionalmente estes processos . A maioria dos modelos empregados utiliza equações_diferenciais  na definição do tensor de tensões . Porém , para alguns grupos de fluidos complexos , por exemplo , fluidos de Boger , os modelos integrais mostram-se mais capacitados em fornecer uma boa aproximação para os comportamentos não lineares desses fluidos . Este trabalho trata da solução_numérica  do modelo constitutivo integral KBKZ-PSM para escoamentos transientes bidimensionais com superfícies_livres  . O método_numérico  proposto é uma técnica numérica que utiliza diferenças_finitas  para simular_escoamentos  com superfícies_livres  na presença de paredes sólidas . As principais características do método_numérico  proposto são : solução das equações de conservação de quantidade de movimento e massa utilizando um método semi-implícito ; a condição de contorno na superfície_livre  é acoplada à equação de Poisson , o que garante conservação de massa ; a discretização do tempo t é realizada por uma nova_técnica  numérica ; o tensor de Finger é calculado pelo método dos campos de deformação e avançado no tempo pelo método de Euler modificado . Essa nova_técnica  é verificada em escoamentos cisalhantes e elongacionais . Adicionalmente , uma solução analítica desenvolvida para escoamentos em canais bidimensionais é empregada para verificar e analisar a convergência do método proposto . Com relação a escoamentos com superfícies_livres  , a convergência é verificada por meio de refinamento de malha nas simulações de um jato incidindo sobre placa rígida e no problema do inchamento do extrudado . Finalmente , o método é aplicado para investigar os problemas jet buckling e inchamento do extrudado de fluidos KBKZ-PSM 
 Configuração de Software compreende itens de informação relevantes , produzidos durante o processo de engenharia de software , e constitui um importante recurso para os profissionais que trabalham com o software , desde que a configuração do software esteja completa e atualizada . A existência de uma Configuração de software completa e atualizada depende de um efetivo processo de Gerenciamento de Configuração de Software . As atividades que envolvem um processo de gerenciamento de configuração de software são complexas e necessitam de orientação e de ferramentas de apoio . Usualmente , essas ferramentas possuem um alto custo e não estão disponíveis para todos que delas necessitam . O Revision Control System ( RCS ) não é uma completa ferramenta de apoio ao gerenciamento de configuração de software , pois suporta apenas algumas funções das atividades do processo de gerenciamento . No entanto , o RCS pode ser muito útil na realização desse processo , além de ser um sistema de domínio público . Uma dificuldade no uso do RCS é sua interface , usualmente , baseada em comandos de linha . Para possibilitar uma maior facilidade de uso do sistema , neste trabalho foi desenvolvida uma interface_gráfica  para o RCS e um roteiro para implantar as atividades de gerenciamento de configuração de software em uma organização . Para a elaboração da interface_gráfica  , primeiramente , o RCS foi submetido a um processo de engenharia_reversa  , usando o método FUSION-RE . Este método produz um modelo de análise orientado a objetos ( método FUSION ) de sistemas desenvolvidos sem a tecnologia de orientação a objetos . O resultado desse processo de engenharia_reversa  foi uma visão mais abstrata do sistema que foi utilizada no desenvolvimento da interface_gráfica  usando XView 
 Atividades de teste no desenvolvimento de sistemas Reativos são de grande relevância , bem como a disponibilidade de ferramentas que apóiem essas atividades , pois falhas nesses sistemas podem causar graves conseqüências econômicas e/ou sociais . A Análise de Mutantes tem sido explorada no teste de especificações do aspecto comportamental de Sistemas Reativos baseadas em Máquinas de Estados_Finitos  , Statecharts e Redes de Petri . Este trabalho tem como objetivo a implementação da Proteum-RS/ST , que apóia o teste de especificações Statecharts baseada na Análise de Mutantes . Dessa forma , fornecem-se subsídios para se investigar a adequação de critérios tradicionalmente usados no teste de programas , como a Análise de Mutantes , no teste de especificações de Sistemas Reativos , em particular no contexto de especificações baseadas em Statecharts 
 Neste trabalho , a mineração de regras de associação é utilizada para dar suporte a dois tipos de sistemas médicos : os sistemas de busca por conteúdo em imagens ( Content-based Image Retrieval - CBIR ) e os sistemas de auxílio ao diagnóstico ( Computer Aided Diagnosis - CAD ) . Na busca por conteúdo , regras de associação são empregadas para reduzir a dimensionalidade dos vetores de características que representam as imagens e para diminuir o `` gap semântico '' , que existe entre as características de baixo_nível  das imagens e seu significado semântico . O algoritmo StARMiner ( Statistical Association Rule Miner ) foi desenvolvido para associar características de baixo_nível  das imagens com o seu significado semântico , sendo também utilizado para realizar seleção de características em bases de imagens médicas , melhorando a precisão dos sistemas CBIR . Para dar suporte aos sistemas CAD , o método IDEA ( Image Diagnosis Enhancement through Association rules ) foi desenvolvido . Nesse método regras de associação são empregadas para sugerir uma segunda opinião ou diagnóstico preliminar de uma nova imagem para o radiologista . A segunda opinião automaticamente gerada pelo método pode acelerar o processo de diagnóstico de uma imagem ou reforçar uma hipótese , trazendo ao especialista médico um apoio estatístico da situação sendo analisada . Dois novos algoritmos foram propostos : um para pré-processar as características de baixo_nível  das imagens médicas e , o outro , para propor diagnósticos baseados em regras de associação . Vários experimentos foram realizados para validar os métodos desenvolvidos . Os experimentos_realizados  indicam que o uso de regras de associação pode contribuir para melhorar a busca por conteúdo e o diagnóstico de imagens médicas , consistindo numa poderosa ferramenta para descoberta de padrões em sistemas 
 Arquitetura de software e teste de software são importantes áreas com o objetivo comum de prover meios para que sistemas de software sejam desenvolvidos com alta qualidade . Foi constatado , por meio de um mapeamento sistemático , que existem poucos_estudos  relacionando essas duas áreas de modo que uma possa contribuir para a outra , principalmente em arquiteturas de referência e teste de software . Esse projeto tem por objetivo estender o modelo para arquiteturas de referência ( RAModel ) , para que arquiteturas de referência sejam estabelecidas com informações de teste de software . Assim , além de promover padronização , uniformização e reúso a informações relacionadas a arquiteturas de software , promove-se também o reúso , a padronização e a uniformização das informações de teste de software no desenvolvimento das soluções para um dado domínio de aplicação . Para isso , elementos de teste relacionados ao planejamento de teste foram extraídos do TMMi e da norma ISO/IEC/IEEE 29119 e inseridos no RAModel , definindo , assim , o RAModelT . Dessa forma , a atividade de teste é incorporada aos estágios iniciais do desenvolvimento de software . Um estudo de viabilidade realizado para avaliar a extensão do RAModelT e um exemplo de aplicação são apresentados 
 A grande quantidade de dados gerada em diversas_áreas  do conhecimento cria a necessidade do desenvolvimento de técnicas de mineração de dados cada vez mais eficientes e eficazes . Técnicas de agrupamento têm sido utilizadas com sucesso em várias áreas , especialmente naquelas em que não há conhecimento prévio sobre a organização dos dados . Contudo , a utilização de diferentes algoritmos de agrupamento , ou variações de um mesmo algoritmo , pode gerar uma ampla variedade de resultados . Tamanha variedade cria a necessidade de métodos para avaliar e selecionar bons_resultados  . Uma forma de avaliar esses resultados consiste em utilizar índices de validação de agrupamentos . Entretanto , uma grande diversidade de índices de validação foi proposta na literatura , o que torna a escolha de um único índice de validação uma tarefa penosa caso os desempenhos dos índices comparados sejam desconhecidos para a classe de problemas de interesse . Com a finalidade de obter um consenso entre resultados , é possível combinar um conjunto de agrupamentos ou índices de validação em uma única solução final . Combinações de agrupamentos ( clustering ensembles ) foram bem sucedidas em obter soluções robustas a variações no cenário de aplicação , o que faz do uso de comitês de agrupamentos uma alternativa interessante para encontrar soluções de qualidade razoável , segundo diferentes índices de validação . Adicionalmente , utilizar uma combinação de índices de validação pode tornar a avaliação de agrupamentos mais completa , uma vez que uma maioria dos índices combinados pode compensar o fraco desempenho do restante . Em alguns_casos  , não é possível lidar com um único conjunto de dados centralizado , por razões físicas ou questões de privacidade , o que gera a necessidade de distribuir o processo de mineração . Combinações de agrupamentos também podem ser estendidas para problemas de agrupamento de dados distribuídos , uma vez que informações sobre os dados , oriundas de diferentes fontes , podem ser combinadas em uma única solução global . O principal objetivo desse trabalho consiste em investigar técnicas de combinação de agrupamentos e de índices de validação aplicadas na seleção de agrupamentos para combinação e na mineração distribuída de dados . Adicionalmente , algoritmos_evolutivos  de agrupamento são estudados com a finalidade de selecionar soluções de qualidade dentre os resultados obtidos . As técnicas desenvolvidas possuem complexidade computacional reduzida e escalabilidade , o que permite sua aplicação em grandes_conjuntos  de dados ou cenários em que os dados encontram-se 
 Uma das etapas mais importantes da análise de imagens e , que conta com uma enorme_quantidade  de aplicações , é a segmentação . No entanto , uma boa parte das técnicas tradicionais apresenta alto custo_computacional  , dificultando sua aplicação em imagens de alta_resolução  como , por exemplo , as imagens de ninhais de aves do Pantanal que também serão analisadas neste trabalho . Diante disso , é proposta uma nova abordagem de segmentação que combina algoritmos de detecção de comunidades , pertencentes à teoria das redes complexas , com técnicas de extração de superpixels . Tal abordagem é capaz de segmentar imagens de alta_resolução  mantendo o compromisso entre acurácia e tempo de processamento . Além_disso  , como as imagens de ninhais analisadas apresentam características peculiares que podem ser mais bem tratadas por técnicas de segmentação por textura , a técnica baseada em Markov Random Fields ( MRF ) é proposta , como um complemento à abordagem de segmentação inicial , para realizar a identificação final das aves . Por fim , devido à importância de avaliar quantitativamente a qualidade das segmentações obtidas , um nova métrica de avaliação baseada em ground-truth foi desenvolvida , sendo de grande_importância  para a área . Este trabalho contribuiu para o avanço do estado da arte das técnicas de segmentação de imagens de alta_resolução  , aprimorando e desenvolvendo métodos baseados na combinação de redes complexas com superpixels , os quais alcançaram resultados_satisfatórios  com baixo tempo de processamento . Além_disso  , uma importante contribuição referente ao censo demográfico de aves por meio da análise de imagens aéreas de ninhais foi viabilizada por meio da aplicação da técnica de segmentação MRF 
 A sumarização_automática  multidocumento visa à produção de um sumário a partir de um conjunto de textos relacionados , para ser utilizado por um usuário particular e/ou para determinada tarefa . Com o crescimento exponencial das informações disponíveis e a necessidade das pessoas obterem a informação em um curto espaço de tempo , a tarefa de sumarização_automática  tem recebido muita atenção nos últimos tempos . Sabe-se que em um conjunto de textos relacionados existem informações redundantes , contraditórias e complementares , que representam os fenômenos multidocumento . Em cada texto-fonte , o assunto principal é descrito em uma sequência de subtópicos . Além_disso  , as sentenças de um texto-fonte possuem graus de relevância diferentes . Nesse contexto , espera-se que um sumário multidocumento consista das informações relevantes que representem o total de textos do conjunto . No entanto , as estratégias de sumarização_automática  multidocumento adotadas até o presente utilizam somente os relacionamentos entre textos e descartam a análise da estrutura textual de cada texto-fonte , resultando em sumários que são pouco representativos dos subtópicos textuais e menos informativos do que poderiam ser . A fim de tratar adequadamente a relevância das informações , os fenômenos multidocumento e a distribuição de subtópicos , neste trabalho de doutorado , investigou-se como modelar o processo de sumarização_automática  usando o conhecimento semântico-discursivo em métodos de seleção de conteúdo e o impacto disso para a produção de sumários mais informativos e representativos dos textos-fonte . Na formalização do conhecimento semântico-discursivo , foram utilizadas as teorias semântico-discursivas RST ( Rhetorical Structure Theory ) e CST ( Cross-document Structure Theory ) . Para apoiar o trabalho , um córpus multidocumento foi anotado com RST e subtópicos , consistindo em um recurso disponível para outras pesquisas . A partir da análise de córpus , foram propostos 10 métodos de segmentação em subtópicos e 13 métodos inovadores de sumarização_automática  . A avaliação dos métodos de segmentação em subtópicos mostrou que existe uma forte relação entre a estrutura de subtópicos e a análise retórica de um texto . Quanto à avaliação dos métodos de sumarização_automática  , os resultados indicam que o uso do conhecimento semântico-discursivo em boas estratégias de seleção de conteúdo afeta positivamente a produção de sumários informativos 
 Este trabalho apresenta um conjunto de diretrizes que apóiam o processo de reengenharia de um sistema procedimental ( escrito na linguagem Clipper ) para um sistema orientado a objeto ( escrito na linguagem Delphi ) . As diretrizes foram baseadas na experiência adquirida com a realização da reengenharia no sistema SAPES ( um sistema que apóia o uso de um banco de dados bibliográfico pessoal na escrita de documentos ) . As diretrizes envolvem o método FUSION-RE/I , no passo de engenharia_reversa  , e a ferramenta FusionCASE , no passo de reconstrução 
 Neste projeto de mestrado foi estudado um processo de geração de modelos tridimensionais de dentes a partir de dados reais . Foram elaborados e implementados um algoritmo para reconstrução 3D de dentes , e vários programas complementares para apoiar a reconstrução . Um modelo tridimensional é criado a partir de contornos planares que descrevem as estruturas de interesse , representados por polígonos extraídos de fatias ou cortes paralelos de um dente . O algoritmo foi implementado na forma de uma classe , em extensão a uma biblioteca de visualização de propósito_geral  , o VTK ( Visualization Toolkit ) , com resultados bastante promissores 
 Esta dissertação visa analisar os construtores de Modelos de Dados Orientado a Objetos , para verificar sua usuabilidade nas situações de utilização real em projetos de aplicações centradas em Bases de Dados . O trabalho aqui apresentado se finidamenta no modelo SlRIUS , um Modelo de Dados Orientado a Objetos , baseado em Abstrações de Dados . O desenvolvimento de SlRIUS utiliza como ponto de partida um metamodelo que permite representar , de maneira uniforme , os elementos essenciais de qualquer modelo de dados orientado a objetos . Seu desenvolvimento teve como objetivo atender às necessidades de suporte à construção de software para ambientes de projetos de engenharia . A representação gráfica dos elementos construtivos de um modelo de dados é um dos principais recursos que permitem registrar o conhecimento representado por uma modelagem . Para que um usuário possa elaborar e/ou utilizar uma modelagem , é necessário que essa representação gráfica seja clara , limpa , concisa e efetiva . Por outro_lado  , deve permitir que os conceitos de modelagem do modelo em si também sejam claramente expressos . Logo , para que este modelo possa ser uma ferramenta adequada para ser utilizada por projetistas , analistas e programadores de gerenciadores de Bases de Dados , é necessário que a notação diagramática do modelo priorize a intuitividade e a clareza , e não só o compromisso com a teoria que embasa o modelo . Para atender estas necessidades , este documento mostra uma nova representação , mais voltada para a modelagem de situações do mundo_real  , de fundamental_importância  para que um modelo possa ser aceito e realmente utilizado , e com isso incrementar sua utilidade prática . Conseqüentemente , alguns pontos do modelo foram validados e outros reformulados , acrescentando-se aspectos que não foram cobertos no modelo original . O texto apresenta também o editor E2SIRIUS , criado com o objetivo de servir tanto para edição de esquemas quanto para auxiliar no treinamento de projetistas nos conceitos do modelo 
 O cérebro humano é formado por um conjunto de neurônios de diferentes_tipos  , cada um com sua especialidade . A combinação destes diferentes_tipos  de neurônios é um dos aspectos responsáveis pelo desempenho apresentado pelo cérebro na realização de várias tarefas . Redes_Neurais  Artificiais são técnicas computacionais que apresentam um modelo matemático inspirado no sistema nervoso e que adquirem conhecimento através da experiência . Uma alternativa para melhorar o desempenho das Redes_Neurais  Artificiais é a utilização de técnicas de Combinação de Classificadores . Estas técnicas de combinação exploram as diferenças e as semelhanças das redes para a obtenção de resultados melhores . Dentre as principais aplicações de Redes_Neurais  Artificiais está o Reconhecimento de Padrões . Neste trabalho , foram utilizadas técnicas de Combinação de Classificadores para a combinação de Redes_Neurais  Artificiais em problemas de Reconhecimento de Padrões 
 Este trabalho aborda o problema de sincronização entre processos em uma simulação distribuída orientada a eventos . Dentre os mecanismos voltados à sincronização entre processos , propostos na literatura , optou-se pela abordagem otimista Time Warp . O trabalho enfoca a dificuldade de processamento que um modelo de um sistema real resolvido por simulação seqüencial pode vir a apresentar . Com o intuito de minimizar o tempo de execução de uma simulação seqüencial , vem-se adotando a simulação distribuída . No entanto , a utilização desta técnica introduz outros problemas , entre eles , o problema de sincronismo entre processos . Um sistema que descreve o funcionamento do mecanismo Time Warp é desenvolvido . Esse sistema , chamado STW , incorpora alguns algoritmos que otimizam a utilização do mecanismo Time Warp . As otimizações introduzidas no sistema são a estratégia de cancelamento agressivo e o cancelamento preguiçoso . Uma biblioteca destinada à construção de modelos de sistemas de filas , denominada SimTW , é implementada e utilizada no estudo de caso . O modelo utilizado pelo estudo de caso é o STW . Os resultados obtidos com a simulação desse modelo indicam uma análise comportamental das otimizações introduzidas no sistema STW 
 A área de computação oferece uma variedade de técnicas e ferramentas com o objetivo de auxiliar na resolução de problemas de diversas_áreas  de aplicação . O sucesso de um projeto depende muitas_vezes  da escolha correta dessas ferramentas e das técnicas que serão utilizadas durante as fases do desenvolvimento . Além_disso  , é importante conhecer previamente as expectativas do usuário e qual seu grau de familiarização com o uso de sistemas informatizados . Esta dissertação de mestrado aborda a utilização de diferentes técnicas e ferramentas de programação na solução de um problema especifico o acasalamento de canários de cor . O trabalho apresenta um levantamento detalhado da evolução dos canários , da sua descoberta até a última mutação registrada envolvendo os genes responsáveis pela cor da plumagem . Para uma melhor compreensão do assunto , é apresentada uma pequena revisão sobre genética e sobre o conhecimento de especialistas da área de canaricultura de cor . Para solucionar o problema , são considerados neste trabalho a utilização de diferentes métodos de desenvolvimento propostos por duas grandes áreas : a Engenharia de Software e a Inteligência_Artificial  . As técnicas estudadas são implementadas , integrando um Sistema Baseado em Conhecimento e a programação convencional num único sistema . O resultado obtido demonstra que pode-se obter uma solução mais adequada ao problema quando diferentes métodos são combinados 
 O processo de desenvolvimento de software consiste em um complexo interrelacionamento de recursos com o objetivo de produzir e manter um software em operação . O emprego de tais recursos , se gerenciados adequadamente , pode permitir que a produção de software seja feita dentro de prazos e orçamentos estabelecidos , além de influenciar na qualidade final do produto . Para tanto , mais que planejar o desenvolvimento , é necessário acompanhar e avaliar a evolução do mesmo a cada momento do desenvolvimento . Mudanças de estado entre o início e o fim do processo de desenvolvimento exigem uma avaliação que considere os ciclos de retroalimentação . A modelagem de processos dinâmicos permite que a avaliação seja feita visualizando-se os elementos do sistema e seus relacionamentos . A simulação do comportamento do sistema é feita com base em relações matemáticas , permitindo a visualização do consumo de recursos no decorrer do tempo . Este trabalho apresenta uma ferramenta de apoio à modelagem de sistemas dinâmicos que permite simular o comportamento do modelo , apresentando os resultados obtidos em tabelas e gerando gráficos que representam as tendências do modelo criado . Uma atividade gerencial do projeto de software é modelada e simulada para exemplificar a utilização da ferramenta e outro modelo é apresentado , mostrando a possibilidade de aplicação da ferramenta em outras áreas 
 Um ambiente para provimento de dados multimídia sobre uma rede de computadores , apresenta vários limitantes , sendo o principal deles , a taxa de transmissão de dados na entrega de vídeo . Para contornar os problemas oriundos da precariedade dos protocolos TCP/IP e Ethernet para a transmissão de mídia contínua , nota-se o aparecimento de várias tecnologias que possibilitam o uso de um sistema de armazenamento remoto de vídeo e sua apresentação local com razoável qualidade sem a utilização de sincronismo durante o processo , através de técnicas de streaming de vídeo . Este trabalho procura avaliar como podem ser balanceados o desempenho do servidor com o desempenho das tecnologias de rede disponíveis no mercado ( Ethernet , Switched-Ethernet , Fast Ethernet e FDDI ) e a distribuição e apresentação de vídeos utilizando técnicas de downloading e streaming , de modo que o usuário possa ter uma idéia de como especificar seu ambiente à partir de um determinado perfil de aplicação . Os testes realizados através das aplicações desenvolvidas mostram que é perfeitamente possível a implementação de sistema de distribuição de vídeo via tecnologias padrão de rede utilizadas correntemente . A utilização das técnicas de downloading de vídeo , torna-se inviável para a distribuição de grandes arquivos ou em ambientes onde a quantidade de usuários é grande , devido ao tempo de transmissão do arquivo as estações . O trabalho quantifica estas observações e abre caminho para que avaliações mais finas de performance possam ser feitas 
 A proposta deste trabalho é o desenvolvimento de um Gerador de Códigos baseado em Statechart integrado ao ambiente gráfico StatSim , desenvolvido pelo grupo de Engenharia de Software do ICMC-USP . Para isso , estudos sobre ferramentas e ambientes de sistemas baseados em Statecharts foram realizados . Durante o desenvolvimento do projeto , a partir da análise dos métodos existentes , foram desenvolvidos dois protótipos de código a serem gerado pela ferramenta pretendida , com o intuito de avaliar o algoritmo a ser empregado . Um protótipo foi implementado em Occarn e o outro em C++ . Paralelamente foi desenvolvida uma ferramenta gráfica denominada HySChart ( Hyperdocument System Based on stateCharts ) para apoio ao modelo HMBS ( Hypertext Model Based on Statechart ) , e validação de hiperdocumentos . Com base nesse ambiente , foi desenvolvido um gerador de código que transforma o hiperdocumento criado pelo HySChart em um hiperdocumento em HTML , que pode ser navegado com auxilio de um navegador HTML qualquer . Duas implementações foram feitas para esse gerador : uma que preserva a semântica de navegação do HMBS e outra que não preserva 
 Este trabalho aborda a utilização da técnica de mapeamento por cores no contexto da Visualização Científica . É apresentada uma revisão bibliográfica sobre percepção visual de cores e um estudo de abordagens que buscam apoiar o usuário no processo de geração de visualizações . A abordagem de visualização baseada em regras , que prevê a utilização de regras heurísticas que embutem conhecimentos sobre percepção visual de cores e informações relativas aos dados e aos objetivos do usuário ao gerar a visualização , é discutida . Nessa abordagem , as regras restringem o conjunto de mapeamentos gerados para uma visualização , de forma a garantir resultados mais efetivos , sendo que essa abordagem foi implementada na forma de um módulo de apoio à geração de tabelas de cores . Esse módulo restringe o número de cores que podem compor uma tabela e o comportamento das mesmas ao longo da tabela , a partir das informações obtidas sobre a freqüência espacial , a natureza dos dados , e a tarefa de visualização , segundo uma taxonomia baseada em conhecimentos sobre percepção visual de cores proposta na literatura . Alguns resultados obtidos com a aplicação das diferentes tabelas de cores geradas pelo módulo são apresentados 
 Neste trabalho apresentamos uma análise_Bayesiana  para modelos de sobrevivência e de contagem com pontos de mudança , assumindo diferentes densidades a priori e métodos computacionais para obter as inferências a posteriori de interesse . Em particular , destacamos o método de Laplace e métodos de simulação de Monte Cano em cadeias de Markov . Além_disso  introduzimos um critério de seleção de modelos Bayesiano . Ilustramos a metodologia_proposta  em alguns exemplos numéricos 
 Este trabalho propõe um modelo estocástico em conjunto com conceitos de análise de confiabilidade . Nestes modelos , a afluência que atinge o reservatório durante uma cheia é considerada como uma carga , e a capacidade do reservatório para amortecer esta cheia é considerada como uma resistência que o reservatório oferece à sua propagação . Aqui , a carga e a resistência são modeladas como processo do tipo difusão , e utili7a-se a fórmula de Itô para calcular o volume de espera associado a um determinado risco de falha A inferência dos parâmetros do modelo é feita por meio de inferência bayesiana usando-se o algoritmo de simulação de Monte_Carlo  em cadeias de Markov . Os estimadores de máxima_verossimilhança  são usados para comparação . O método foi aplicado para um conjunto de dados utilizando-se nove anos de vazões afluentes diárias em período de cheia que chegam à usina de Xavantes . O volume de espera foi calculado pelo método proposto , usando-se descarga de referência constante , e comparado com os volumes obtidos por outros métodos existentes . O método da curva volume x duração deixa um volume de espera constante durante todo o período chuvoso , usando-se somente 83,6 % da capacidade do reservatório . O método proposto faz uma alocação dinâmica do volume de espera , utilizando-se 96.5 % e 95.3 % da capacidade do reservatório quando a abordagem_bayesiana  e o método de máxima_verossimilhança  são adotados , respectivamente . Dentre os métodos propostos , o método bayesiano mostrou-se mais vantajoso em relação ao desperdício de energia devido à utilinção da capacidade do reservatório . O método das trajetórias críticas necessita de séries com um comprimento significativo de vazões diárias , e na prática só pode ser aplicado com séries sintéticas de vazões 
 Ferramentas de otimização_linear  são usadas para resolver diversos problemas do nosso dia-a- dia . Os modelos e as metodologias de otimização_linear  ajudam a obter , por exemplo , a melhor quantidade de ingredientes na nossa alimentação , os horários e as rotas de ônibus e trens que tomamos , e a maneira certa para investir nossas economias . Muitas outras situações que envolvem otimização_linear  poderiam ser aqui citadas , já que um grande número de empresas em todo o mundo baseia suas decisões em soluções obtidas pelos métodos de otimização_linear  . Nesta tese , são propostos desenvolvimentos teóricos e computacionais para melhorar o desempenho de métodos de otimização_linear  . Em particular , serão abordados métodos tipo simplex , métodos de pontos_interiores  , a técnica de geração de colunas e o método branch-and-price . Em métodos tipo simplex , é investigada uma variante que explora as características especiais de problemas formulados na forma geral . Uma nova descrição teórica do método é apresentada e , também , são propostas técnicas computacionais para a implementação eciente do método . Além_disso  , propõe-se como utilizar o método primal-dual de pontos_interiores  para melhorar a técnica de geração de colunas . Isto resulta no método primal-dual de geração de colunas , que é mais estável na prática e tem melhor desempenho geral em relação a outras estratégias de geração de colunas . O método primal-dual de pontos_interiores  também oferece características vantajosas que podem ser exploradas em conjunto com o método branch-and-price . De acordo com a investigação realizada , estas características melhoram a operação de ramificação e a geração de colunas e de desigualdades válidas . Para todas as estratégias propostas neste trabalho , são apresentados os resultados de experimentos computacionais envolvendo problemas de teste bem conhecidos e disponíveis publicamente . Os resultados indicam que as estratégias propostas ajudam a melhorar o desempenho das metodologias de otimização_linear  . Em particular para uma classe de problemas , o problema de roteamento de veículos com janelas de tempo , o método branch-and-price de pontos_interiores  proposto neste estudo foi até 33 vezes mais rápido que uma implementação estado-da-arte disponível na 
 Neste trabalho propomos um modelo de mercado através de uma discretização aleatória do movimento browniano proposta por Leão & Ohashi ( 2010 ) . Com este modelo , dada uma função payoff , vamos desenvolver uma estratégia de hedging e uma metodologia para precificação de 
 Estruturas de dados volumétricas são de extrema utilidade em várias aplicações , e em particular na área de Visualização Científica . Essas estruturas são úteis em duas etapas do processo de visualização . A primeira é na representação de dados , isto é , de informações associadas aos valores , medidos ou simulados , os quais se deseja visualizar . A outra fase que necessita de uma estrutura de dados é a fase de exploração , ou seja , o modelo criado é usado tanto para exploração interativa quanto para a realização de simulações sobre o mesmo , por exemplo , numa cirurgia virtual 
 Está em desenvolvimento no ICMC , uma estrutura de dados volumétrica , chamada Singular Half-Face , que tem como característica a modelagem explícita das singularidades presentes no modelo , além de outros elementos topológicos . Este trabalho de mestrado visa testar a viabilidade da estrutura num contexto de visualização em fluxo de dados , incorporando-a à biblioteca gráfica de visualização Visualization ToolKit ( VTK ) , que possui definição extremamente pobre da topologia dos objetos que representa . Adicionando essa nova classe e realizando sobre ela técnicas convencionais de visualização e exploração de dados , é estudada sua capacidade de apoiar todas as fases do processo de visualização 
 O desenvolvimento de trabalhos em grupo tornou-se uma tarefa cada vez mais comum nos dias atuais . Com isso , houve também um crescimento na utilização e no desenvolvimento de aplicações computacionais que implementam os processos de apoio à cooperação entre as pessoas , as quais possibilitam um considerável ganho de produtividade nas tarefas realizadas pelo grupo . A integração de várias dessas aplicações pode resultar em um acréscimo de funcionalidades , aumentando as possibilidades de uso e a flexibilidade para o usuário . Com o objetivo de agregar funcionalidades e melhorar os potenciais usos dos sistemas de software , aplicações integradas possuem , em geral , mais funcionalidades e maiores possibilidades de uso do que aplicações isoladas . Assim , foi desenvolvido o WS4CSCW ( Web_Services  for Computer Supported Cooperative Work ) , um Web Service que tem como objetivo facilitar a integração de ferramentas e sistemas CSCW . O WS4CSCW permite o gerenciamento de usuários , grupos , recursos , permissões e papéis de usuários e grupos , disponibilizando funcionalidades para aplicações voltadas ao trabalho em grupo , bem como para facilitar a integração entre essas aplicações , permitindo um co-funcionamento mais direto , consistente e coeso 
 O problema de dimensionamento de lotes determina um plano de produção que apoia às tomadas de decisões , a médio prazo , em meios industriais . Este plano de produção indica as quantidades de cada item que devem ser produzidas em cada_período  do horizonte de planejamento , de acordo com um objetivo dado e satisfazendo a demanda dos clientes . Diversos métodos de solução foram propostas na literatura , considerando a dificuldade de solução de algumas classes de problemas e a necessidade de métodos que gerem soluções de alta qualidade em um tempo computacional adequado . Neste trabalho , abordamos heurísticas baseadas na formulação_matemática  ( LP-and-fix , relax-and-fix e fix-and-optimize ) , uma metaheurística ( algoritmo de abelhas ) e dois métodos híbridos , utilizados na solução de dois problemas distintos de dimensionamento de lotes multiestá- gio com limitação de capacidade . Consideramos também , a utilização de três formulações da literatura , para verificar a influência de cada uma sobre as abordagens de solução verificadas . Os resultados computacionais demonstraram que os métodos baseados na formulação_matemática  do problema se mostraram eficientes , mas limitados normalmente a ótimos locais , enquanto os métodos híbridos puderam superar estes ótimos locais , utilizando conceitos da metaheurística algoritmo de abelhas para isto . Além_disso  , pudemos verificar a influência de uma formulação `` forte '' sobre as soluções geradas pelas abordagens de solução , demonstrando que métodos baseados em relaxação linear conseguem obter maiores vantagens deste tipo de formulação , mas outras abordagens podem ou não obter estas vantagens , dependendo do problema 
 Neste trabalho , consideramos uma nova parametrização para o problema de empacotar a maior quantidade possível de círculos idênticos uma região elíptica dada . Apresentamos algoritmos com propriedades de convergência global e algumas estratégias heurísticas . Ilustramos com experimentos numéricos extensivos cada uma das estratégias 
 Testes Adaptativos Computadorizados ( TAC ) são aqueles que selecionam gradativamente as questões ( itens ) a serem apresentadas ao indivíduo de acordo com o seu nível de conhecimento ( traco latente ) . Um TAC pode se basear em um modelo da Teoria da Resposta ao Item ( TRI ) para a estimação do traco latente e escolha do item a ser apresentado em cada passo do teste . Este trabalho apresenta modelos da TRI utilizados em TAC encontrados na literatura e descreve alguns métodos de calibração de itens para a formação e manutenção do banco de questões do teste sob o modelo de Samejima ( 1969 ) , estimação do traço latente , seleção de itens com restrições utilizando a abordagem Shadow test e critérios de parada normalmente utilizados . Foram realizadas simulações com um banco grande ( 500 itens ) e com um banco pequeno ( 21 itens ) e avaliada a qualidade das estimativas dos traços latente ( através do cálculos dos vícios e erros quadráticos médios ) de TACs com diferentes números de itens . Foi aplicado o modelo de Samejima às respostas de estudantes do Exame ao proficiência em inglês ( EPI ) do ICMC - USP , que é aplicado semestralmente no formato lápis e papel , para a formação de um banco de itens e posterior construção de um TAC . Também foi aplicado o modelo às respostas de pacientes clínicos do Hospital das Clínicas da Medicina da USP , cedido pelo doutor Yuang-Pang Wang , ao Inventário de Depressão de Beck ( BDI ) para os mesmos propósitos . Comparações com a atual metodologia para avaliação da proficiência em língua inglesa do EPI ( Medida de Probabilidade Admissível , MPA ) e para o diagnóstico de depressão do BDI ( critério sugerido por Kendall et al. , 1987 ) foram realizadas demonstrando as vantagens e maior riqueza dos resultados obtidos com a TRI e com os TACs implementados . Adcionalmente foi desenvolvido um programa Same-CAT que armazena bancos de itens e possibilita a criação e aplicação de TACs com restrições , através da abordagem Shadow 
 As estratégias de mapeamento utilizando múltiplos robôs_móveis  possuem uma série de vantagens quando comparadas àquelas estratégias baseadas em um único robô . As principais vantagens que podem ser elucidadas são : flexibilidade , ganho de informação e redução do tempo de construção do mapa do ambiente . No presente_trabalho  , um método de integração de mapas locais é proposto baseado em observações inter-robôs , considerando uma nova abordagem para a exploração do ambiente . Tal abordagem é conhecida como Sistema de Vigilância baseado na Modificação do Sistema Colônias de Formigas , ou IAS-SS . A estratégia IAS-SS é inspirada em mecanismos biológicos que definem a organização social de sistemas de enxames . Especificamente , esta estratégia é baseada em uma modificação do tradicional algoritmo de otimização por colônias de formiga . A principal_contribuição  do presente_trabalho  é a adaptação de um modelo de compartilhamento de informações utilizado em redes de sensores móveis , adaptando o mesmo para tarefas de mapeamento . Outra importante contribuição é a colaboração entre o método proposto de integração de mapas e a estratégia de coordenação de múltiplos robôs baseada na teoria de colônias de formigas . Tal colaboração permite o desenvolvimento de uma abordagem de exploração que emprega um mecanismo não físico para depósito e detecção de feromônios em ambientes reais por meio da elaboração do conceito de feromônios virtuais integrados . Resultados obtidos em simulação demonstram que o método de integração de mapas é eficiente , de modo que os ensaios experimentais foram realizados considerando-se um número variável de robôs_móveis  durante o processo de exploração de ambientes internos com diferentes formas e estruturas . Os resultados obtidos com os diversos experimentos_realizados  confirmam que o processo de integração é efetivo e adequado para executar o mapeamento do ambiente durante tarefas de exploração e vigilância do 
 Nos últimos_anos  o mundo tem vivenciado uma avalanche de novas tecnologias para auxílio ao diagnóstico médico . Esses esforços buscam um diagnóstico rápido e preciso através de exames e informações sobre a condição física do paciente . Através do uso de imagens médicas , a radiologia busca a visualização de órgãos ou estruturas internas do corpo humano para encontrar respostas às suspeitas de problemas físicos expressos por sinais e sintomas relatados pelo paciente . Nessa área , os Sistemas de Comunicação e Armazenamento de Imagens ( PACS ) têm ajudado no armazenamento e organização do crescente número de imagens geradas pelos exames realizados nos hospitais . Trabalhos de pesquisa médica têm evidenciado o potencial de uso dessas imagens como auxílio à prática da Medicina Baseada em Casos Similares ( MBCS ) . Por esse motivo , há na literatura um esforço contínuo em desenvolver técnicas computacionais para recuperação de imagens baseada em conteúdos similares ( CBIR ) em grandes_conjuntos  de dados . As consultas por similaridade são essenciais para apoiar a prática da MBCS e a descoberta de comportamentos de lesões causadas por diversas doenças . A evolução e intensificação das pesquisas em CBIR têm encontrado vários desafios . Um desses é a divergência entre os resultados obtidos automaticamente e aqueles esperados pelos radiologistas ( descontinuidade semântica ) . Outro desafio é a falta de estudos sobre a viabilidade clínica dessas ferramentas como forma de auxílio ao diagnóstico . Esses obstáculos são dois dos principais responsáveis pela não efetivação dessa tecnologia no ambiente médico-hospitalar . Mediante o exposto acima , este trabalho de pesquisa propõe um mecanismo para contornar essa descontinuidade semântica e ao mesmo tempo aproximar o CBIR do ambiente real de aplicação . A contribuição principal deste trabalho foi o desenvolvimento de uma metodologia baseada em parâmetros perceptuais que aproximam o sistema ao nível de percepção do usuário médico . Em seguida , foi realizado um estudo sobre a viabilidade clínica do sistema CBIR no Hospital das Clínicas de Ribeirão Preto . A metodologia_proposta  foi aplicada e os resultados comprovaram a aplicabilidade de Sistemas CBIR como ferramenta de auxílio ao diagnóstico em um ambiente clínico 
 A visão de muitas pessoas sobre uma colônia de formigas , em geral , é de que estes pequenos e inofensivos insetos somente se movem aleatoriamente para coletar alimento e conservá-los em seus ninhos . Um olhar destreinado não conseguiria notar o nível de complexidade e organização que é requerido por uma colônia de formigas para sua sobrevivência . Uma formiga simples é parte de um grande grupo que coopera entre si para criar um superorganismo . Sem uma autoridade central ou indivíduos com habilidade de um pensamento cognitivo complexo , a colônia se auto-organiza , e , de fato , ajusta seus recursos de uma maneira muito eficiente . Essa dissertação investiga o papel da comunicação indireta nas tarefas de exploração e forrageamento , e como isso afeta as decisões de um agente simples e traz um comportamento emergente útil à toda colônia . Por fim , este trabalho implementa uma plataforma de simulação multi-agente inspirado em 
 Este trabalho trata de um problema de programação da produção em fundições de pequeno_porte  , que consiste em programar as ligas que devem ser produzidas em cada_período  do planejamento e como tais ligas devem ser usadas para a produção de itens sob encomenda , de modo que atrasos e custos operacionais sejam minimizados . Devido à certa incerteza nos dados do problema , a estratégia de horizonte rolante foi empregada . Este problema é representado por um modelo matemático de programação linear_inteira  mista . Neste trabalho foi desenvolvida uma heurística do tipo residual para obter uma boa solução inteira factível do problema , partindo da solução contínua encontrada pelos métodos relaxe-e-fixe e busca 
 Atualmente , otimizar uma arquitetura ou um sistema de software não significa , necessariamente , aumentar o seu desempenho computacional . Devido a popularização de sistemas embutidos energizados por bateria , um item de grande_importância  a ser otimizado é o consumo de energia . De forma a obedecer às restrições de consumo , pesquisadores têm concentrado seus esforços na criação de ferramentas que possibilitam a modelagem , a otimização e a estimação do consumo de energia . Nos últimos_anos  , FPGAs têm apresentado um grande desenvolvimento nos quesitos densidade , velocidade e capacidade de armazenamento . Essas características tornaram possível a construção de sistemas complexos formados por um ou mais processadores soft-core . Esse tipo de processador permite uma personalização detalhada de suas características arquiteturais , possibilitando uma melhor adequação às restrições de tempo e espaço em um projeto . O objetivo deste trabalho é construir um estimador de potência para softwares que têm como alvo o processador soft-core Nios_II  da Altera , permitindo saber com antecedência quanta energia será consumida devido à execução de programas e aplicações de robótica_móvel  . O modelo implementado neste trabalho foi testado com vários benchmarks padronizados e os resultados obtidos provaram ser bastante adequados para estimar a energia consumida por um programa , obtendo erros de estimação máximos de 4,78 
 O método do Gradiente Espectral , introduzido por Barzilai e Borwein e analisado por Raydan , para minimização irrestrita , é um método simples cujo desempenho é comparável ao de métodos tradicionais como , por exemplo , gradientes conjugados . Desde a introdução do método , assim como da sua extensão para minimização em conjuntos convexos , foram introduzidas várias combinações de passos espectrais diferentes , assim como de buscas lineares não monótonas diferentes . Dos resultados numéricos apresentados em vários trabalhos não é possível inferir se existem diferenças significativas no desempenho dos diversos métodos . Além_disso  , também não fica clara a relevância das buscas não monótonas como uma ferramenta em si próprias ou se , na verdade , elas são úteis apenas para permitir que o método seja o mais parecido possível com o método original de Barzilai e Borwein . O objetivo deste trabalho é comparar os diversos métodos recentemente introduzidos como combinações de diferentes buscas lineares não monótonas e diferentes passos espectrais para encontrar a melhor combinação e , a partir daí , aferir o desempenho numérico do método 
 Os problemas de corte de estoque consistem em cortar um conjunto de objetos dispon´veis em estoque para produzir um conjunto de itens em quantidades e tamanhos especificados , de modo a otimizar uma fun¸cao objetivo . Tais problemas tem in´umeras aplica¸coes industriais e tem sido bastante estudados na literatura . Tipicamente , problemas de corte tem como principal objetivo a minimiza¸cao das sobras . Entretanto , como a qualidade dos padroes de corte depende diretamente dos tamanhos e quantidades dos itens a serem produzidos , nesta tese , consideramos que se a demanda presente gerar sobras indesej´aveis ( nem tao grandes para serem aproveit´aveis , nem tao pequenas para serem perdas aceit´aveis ) , entao conv´em gerar retalhos ( nao comput´aveis como perda ) que serao utilizados para produzir itens de demandas futuras . Desta forma , algumas caracter´sticas desej´aveis para uma boa solu¸cao sao definidas e altera¸coes em m´etodos heur´sticos cl´assicos sao apresentadas , de modo que os padroes de corte com sobras indesej´aveis sao alterados . Para os problemas de corte unidimensionais , desenvolvemos procedimentos heur´sticos que consideram o aproveitamento de sobras , mantendo como o principal objetivo a minimiza ¸cao das perdas . Outra abordagem para este problema , considera o caso em que al´em da minimiza¸cao das perdas , os retalhos dispon´veis em estoque devem ter prioridade de uso em rela¸cao aos demais objetos durante o processo de corte . A an´alise do desempenho dos procedimentos heur´sticos propostos quando somente a minimiza¸cao das perdas ´e considerada , ´e realizada com base em exemplos da literatura , exemplos pr´aticos e exemplares gerados aleatoriamente . Para os procedimentos heur´sticos que priorizam o corte dos retalhos do estoque , al´em de exemplares da literatura , simulamos uma situa¸cao em m´ultiplos per´odos na qual problemas de corte de estoque em sucessivos per´odos sao resolvidos . A cada per´odo , um problema para o per´odo seguinte ´e gerado considerando atualiza¸coes do estoque , os retalhos gerados nos per´odos anteriores e uma nova demanda de itens que ´e v gerada aleatoriamente . No caso bidimensional , tamb´em consideramos problemas em que , al´em da perda m´nima , os retalhos dispon´veis em estoque devem ter prioridade de corte em rela¸cao aos demais objetos . Para resolver este problema , altera¸coes foram realizadas na abordagem grafo E/OU e em procedimentos heur´sticos da literatura . A an´alise do desempenho dos procedimentos heur´sticos propostos considera problemas pr´aticos retirados da carteira de pedidos de uma pequena empresa de esquadrias met´alicas . Devido ` a dificuldade na an´alise dos procedimentos heur´sticos desenvolvidos que consideram o aproveitamento de sobras ( as solu¸coes apresentam caracter´sticas importantes e conflitantes ) , tamb´em apresentamos neste trabalho uma estrat´egia fuzzy para facilitar a analise das solu¸coes obtidas . Os testes computacionais sao realizados considerando os procedimentos heur´sticos desenvolvidos para os problemas de corte unidimensionais com sobras aproveit´aveis e problemas gerados 
 O presente_trabalho  tem por objetivo o estudo da otimização multiobjetivo aplicada ao projeto de perfis aerodinâmicos em regime transônico , analisando comparativamente diferentes formas de definir as funções objetivo . A otimização é efetuada pelo algoritmo genético NSGA-II . Os resultados são avaliados utilizando métricas de diversidade da população e otimalidade das soluções , das quais duas são propostas . As funções objetivo são constituidas de diferentes parametrizações da geometria e diferentes técnicas de simulação_numérica  . A parametrização da geometria é feita utilizando a paramentrização Parsec ou a parametrização baseada em pontos de controle . A discretização do domínio espacial é feita utilizando malha estruturada conformada ao perfil e suavização por EDP elíptica . As duas técnicas de volumes finitos com diferentes modelos para o cálculo do fluxo na face do volume implementadas foram o método de Jameson ( esquema centrado ) e o método de Roe ( esquema upwind ) . As comparações feitas são as seguintes : utilização de modelo viscoso e invíscido , com o uso do código Mses com a parametrização por ponto de controle ; a utilização da parametrização por ponto de controle e parametrização Parsec usando o método de Jameson ; e a comparação entre o método centrado e o upwind , utilizando a parametrização Parsec . Conclui-se dos resultados obtidos que a utilização da parametrização por pontos de controle é melhor . Entretanto , ainda é necessária a utilização de uma parametrização que garanta maior suavidade ou a imposição de restrições sobre a suavidade da solução . A utilização do modelo viscoso torna os resultados da otimização melhores do ponto de vista da otimalidade . Na utilização de modelos de correção viscosa , como no caso do Mses , é necessária a utilização de métodos invíscidos que forneçam resultados com maior representatividade 
 PARA sistematizar os testes e contornar as restrições de tempo e custo a ele associadas , diversas técnicas , critérios e ferramentas têm sido desenvolvidas . Além_disso  , visando ao estabelecimento de uma estratégia de teste que apresente baixo custo de aplicação e alta eficácia em revelar a presença de erros , estudos teóricos e empíricos vêm_sendo  conduzidos pela comunidade de teste . Os critérios de teste , Análise de Mutantes e Mutação de Interface , apresentam problemas de custo relacionados ao grande número de mutantes gerados e equivalentes , sendo de fundamental_importância  o desenvolvimento de abordagens que viabilizem a sua aplicação prática . O presente_trabalho  está inserido nesse contexto e tem como objetivo analisar os operadores de mutação implementados nas ferramentas Proteum e PROTEUM/IM , que apoiam a aplicação dos critérios Análise de Mutantes e Mutação de Interface , respectivamente , levando em consideração não só o número de mutantes gerados mas também o esforço requerido na identificação e eliminação de mutantes equivalentes e mutantes que morrem facilmente . Tal análise permitiu a determinação de heurísticas para eliminação de mutantes equivalentes e o estabelecimento de estratégias incrementais para aplicação dos operadores de mutação , reduzindo com isso o custo do teste em nível de unidade e de integração sem comprometer sua qualidade e contribuindo para a evolução dessas ferramentas para a sua aplicação em ambientes industriais de produção de software 
 A otimização_linear  tem sido objeto de estudo desde a publicação do método simplex em 1947 , o qual vem_sendo  utilizado na prática com relativa eficiência . Com isso , inúmeras variantes deste método surgiram na tentativa de se obter métodos mais eficientes , além de várias implementações objetivando a resolução de problemas de grande porte . Os problemas de otimização_linear  canalizados e esparsos , objeto principal deste trabalho , são problemas de grande interesse prático , pois representam vários problemas reais , como por exemplo , problemas da programação da produção , problemas de mistura e muitos outros . O método dual simplex canalizado com busca linear por partes é um método do tipo simplex especializado para os problemas de otimização_linear  canalizados e será detalhado neste trabalho . Experiências computacionais foram realizadas para algumas classes de problemas de otimização_linear  com o objetivo de analisar o desempenho deste método , o qual foi implementado com algumas heurísticas de pivoteamento e formas de atualização da matriz básica para tentar manter a esparsidade presente e reduzir o tempo de resolução dos problemas 
 Sistemas baseados na captura de experiências ao vivo têm sido investigados em diversos contextos . O presente_trabalho  teve como objetivo investigar mecanismos para a captura de informação multimídia em ambientes de reuniões de modo a permitir a geração de documentos hipermídia associados na World_Wide  Web . O foco do trabalho é na captura e no acesso à informação multimídia associada a reuniões distribuídas síncronas . A investigação resultou no levantamento de requisitos necessários para a construção de uma ferramenta instrumentada para capturar informação multimídia associada ; no levantamento das limitações existentes para a implementação da ferramenta ; na modelagem e na implementação de uma ferramenta , eMeet/InCA , que atende aos requisitos levantados , consideradas as limitações identificadas 
 Muitas de nossas experiências cotidianas são apoiadas pela troca de informações a. qual pode ser capturada de modo a suportar a geração_automática  de hiperdocumentos que correspondem às experiências capturadas . O Projeto eClass do Geórgia Institute of Technology utiliza computação ubíqua para a captura de informações em ambientes de sala de aula a fim de permitir a produção automática de documentos hipermídia que refletem o conteúdo capturado . Ambientes como os do Projeto eClass permitem a autoria automática de ligações liipertexto a partir da interação do usuário com o ambiente ubíquo . Serviços de criação automática de ligações entre repositórios Web podem explorar técnica , de Recuperação de Informação , como demonstram trabalhos que veem sendo desenvolvidos pelo grupo de pesquisa no qual este trabalho está inserido . No entanto , tais técnicas normalmente permitem a identificação de ligações que não deveriam ser geradas ( ligações imprecisas ) , ao mesmo tempo que não permitem identificar todas as ligações que deveriam ter sido geradas ( ligações relevantes ) . Esses fatos motivaram o trabalho apresentado , que tem como objetivo aprimorar a precisão dos resultados obtidos pelos serviços de criação automática de ligações e. consequentemente , aproximar as ligações criadas àquelas esperadas pelos usuários . Para alcançar esse objetivo . foram utilizadas abordagens de Recuperação de Informação e de Sistemas Hipermídia Abertos para armazenamento das ligações criadas . Através das investigações , foi possível a verificação e a seleção de informações que representam melhor o conteúdo das coleções de informações manipuladas no sentido de prover ligações mais precisas . Como resultado , foram definidos procedimentos , que deram origem a um serviço configurável de criação automática de ligações 
 Este trabalho foca na geração genética de sistemas fuzzy . Uma das principais_contribuições  deste trabalho é a proposta do método FCA-BASED , que gera o espaço de busca genético usando a teoria de análise de conceitos formais por meio da extração de regras dos dados . Os resultados da avaliação_experimental  do método FCA-BASED demonstram sua robustez . O método FCABASED também produz um bom trade-off entre acurácia e interpretabilidade dos modelos gerados . Além_disso  , o método FCA-BASED apresenta melhorias em relação ao método DOC-BASED , uma abordagem proposta anteriormente . Essas melhorias estão relacionadas à redução do custo_computacional  para a geração do espaço de busca genético . Para ser capaz de trabalhar com conjuntos de dados de alta dimensão , foi também proposto o método FUZZYDT , uma versão fuzzy da clássica árvore de decisão C4.5 . FUZZYDT é um método altamente escalável que apresenta baixo custo_computacional  e acurácia competitiva . Devido a essas características , o FUZZYDT é usado nesse trabalho como um método baseline para a avaliação_experimental  e comparações de outros métodos de classificação , fuzzy e clássicos . Também está incluido nesse trabalho a aplicação do método FUZZYDT em um problema do mundo_real  , o alerta da doença da ferrugem cafeeira em plantações brasileiras . Além_disso  , esse trabalho_investiga  a tarefa de seleção de atributos como forma de atacar o problema da dimensionalidade de sistemas fuzzy . Para esse fim , foi proposto o método FUZZYWRAPPER , uma abordagem baseada em wrapper que seleciona atributos levando em consideração as informações relevantes sobre a fuzificação dos atributos durante o processo de seleção . Esse trabalho também investiga a construção automática de bases de dados fuzzy , incluindo a proposta do método FUZZYDBD , que estima o número de conjuntos_fuzzy  que define todos os atributos de um conjunto de dados e distribui os conjuntos_fuzzy  proporcionalmente nos domínios dos atributos . Uma versão modificada do método FUZZYDBD , o método FUZZYDBD-II , também é proposta nesse trabalho . O método FUZZYDBD-II define números independentes de conjuntos_fuzzy  para cada atributo de um conjunto de dados por meio de funções de 
 Uma tendência crescente entre os pesquisadores da Robótica Móvel é a elaboração de sistemas robóticos descentralizados denominados enxames de robôs , nos quais a ação conjunta de cada agente leva à execução de tarefas de maneira mais robusta que quando realizada por um único robô . Um acréscimo adicional à robustez é conveniente em tais sistemas para que eles sejam de maior confiabilidade no mundo_real  . Neste trabalho , uma rede_neural  hierárquica desenvolvida para o aprendizado em tempo real inicialmente elaborada para o aprendizado de navegação de um único robô será estendida para controlar um enxame de robôs . O sistema realiza um balanceamento da influência de comportamentos implementados previamente em um robô de acordo com conhecimentos obtidos através da interação do mesmo com o ambiente . Cada robô possui sua própria rede_neural  , adquirindo seu conhecimento tanto independentemente quanto com o compartilhamento de informações com outros robôs . Espera-se que o uso de tal arquitetura permita uma adaptação mais rápida dos robôs ao ambiente , permitindo uma mudança em tempo real de seus parâmetros de acordo com as peculiaridades do ambiente no qual os robôs estão inseridos . A tarefa de escolta de um robô pelos demais é adotada para a avaliação de desempenho do modelo de rede_neural  proposto . Dois comportamentos são ponderados pela rede_neural  hierárquica : o de manutenção de uma distância preestabelecida a um agente e um outro de cobertura de área baseado em Diagramas Centroidais de Voronoi . Os testes foram feitos nos ambientes Player/Stage e indicam que a rede_neural  hierárquica torna os robôs capazes não apenas de aprender à medida que interagem com ambiente como de utilizar este conhecimento em tempo real para realizar a escolta de forma bem 
 O teste baseado em modelos visa à derivação de casos de teste a partir de modelos produzidos ao longo do desenvolvimento de software . Nesse contexto , as Máquinas de Estados_Finitos  têm sido amplamente pesquisadas e utilizadas para derivação de seqüências de teste . Para isso , vários métodos de geração de seqüências de teste têm sido desenvolvidos há várias décadas . O objetivo desses métodos é a obtenção de um conjunto de teste que seja capaz de revelar os defeitos de uma implementação . Entretanto , muitas_vezes  os conjuntos gerados são muito grandes , o que torna sua aplicação inviável . Trabalhos recentes definiram condições que podem ser utilizadas para investigar mecanismos de redução de casos de teste . Este trabalho apresenta uma estratégia para a redução de conjuntos de seqüências de teste a partir de Máquinas de Estados_Finitos  com base em condições de suficiência . A estratégia baseia-se na combinação de seqüências de um conjunto de teste , de forma a reduzir o número de seqüências e o tamanho delas , mantendo a completude do conjunto . São apresentadas seis abordagens de redução baseadas na estratégia proposta , as quais foram implementadas em uma ferramenta . Para avaliar as abordagens foram conduzidos estudos_experimentais  , os quais também serviram para inferir sobre as características e propriedades de cada abordagem . Além_disso  , um estudo de caso com MEFs reais também foi 
 Diversos pesquisadores vêm investigando métodos e técnicas para tornar possível às pessoas anotarem vídeos de modo transparente . A anotação pode ser realizada com a fala , com o uso de tinta digital ou algum outro meio que possa ser capturado enquanto a pessoa assiste ao vídeo . Tais anotações podem ser compartilhadas com outras pessoas , que podem_estar  assistindo ao mesmo vídeo em um mesmo instante ou em momentos diferentes , sendo interessante ainda que as anotações possam ser realizadas por várias pessoas de modo colaborativo . O paradigma Watch-and-Comment ( WaC ) propõe a captura transparente de anotações multimodais de usuários enquanto os mesmos assistem e comentam um vídeo . Como resultado desse processo , é gerado um vídeo digital interativo integrando o conteúdo original às anotações realizadas . Esta dissertação tem por objetivo explorar conceitos de computação ubíqua , redes_sociais  , redes peer-to-peer e TV interativa na proposta de um modelo arquitetural de ciência de informações de contexto para aplicações definidas segundo o paradigma WaC . O modelo explora a integração de um serviço ao paradigma , que auxilie ou forneça alternativas para que aplicações , do momento da captura ao acesso das anotações , utilizem informações de contexto do usuário , do vídeo e das anotações . O modelo também auxilia no estudo de colaboração entre usuários que realizam anotações em vídeos . Outra contribuição da dissertação é a prototipação de aplicações para avaliar e refinar o modelo proposto . São apresentadas extensões para a aplicação WaCTool , considerando o uso de redes_sociais  e de alternativas para a anotação em 
 Sejam GI = ( VI , AI ) e GM = ( VM , AM ) dois grafos simples . Um mapeamento de GI para GM é um conjunto de associações , tal que cada vértice de VI está associado a um vértice de VM , e cada aresta de AI está associada a um par de vértices de VM . A cada possível associação é atribuído um custo . O problema de correspondência inexata entre grafos ( PCIG ) consiste em encontrar um mapeamento de GI para GM , tal que a soma dos custos de suas associações seja mínima . Nesta dissertação , resumimos os resultados encontrados na literatura sobre o PCIG e algumas de suas variações . Os resultados que incluímos aqui tratam sobre a questão de como formular o PCIG e algumas de suas variações , através de programação linear_inteira  . Provamos alguns resultados de complexidade computacional que relacionam variações do PCIG a problemas clássicos , como isomorfismo e partição de grafos . Fornecemos uma formulação através de programação linear_inteira  para o PCCA ( uma variante do PCIG com conexidade e cobertura de arestas ) . Mostramos que o PCCA é NP-difícil quando os grafos de entrada são completos ou árvores ( chamamos o segundo caso de PCCA para árvores ) . Apresentamos uma formulação linear_inteira  e um algoritmo - que é polinomial se o grau máximo dos vértices de VM for limitado por uma constante - para o PCCA para árvores . Mostramos um caso especia em que o PCCA para árvores pode ser resolvido em tempo polinomial . Por último , exibimos alguns resultados experimentais , inclusive com instâncias reais de uma aplicação do problema 
 O Aprendizado de Máquina trata da questão de como desenvolver programas de computador capazes de aprender um conceito ou hipótese a partir de um conjunto de exemplos ou casos observados . Baseado no conjunto de treinamento o algoritmo de aprendizado induz a classificação de uma hipótese capaz de determinar corretamente a classe de novos exemplos ainda não rotulados . Linguagens de descrição são necessárias para escrever exemplos , conhecimento do domínio bem como as hipóteses aprendidas a partir dos exemplos . Em geral , essas linguagens podem ser divididas em dois tipos : linguagem baseada em atributo-valor ou proposicional e linguagem relacional . Algoritmos de aprendizado são classificados como proposicional ou relacional dependendo da liguagem de descrição que eles utilizam . Além_disso  , no aprendizado simbólico o objetivo é gerar a classificação de hipóteses que possam ser facilmente interpretadas pelos humanos . Algoritmos de aprendizado proposicional utilizam a representação atributo-valor , a qual é inadequada para representar objetos estruturados e relações entre esses objetos . Por outro_lado  , a Programação lógica Indutiva ( PLI ) é realizada com o desenvolvimento de técnicas e ferramentas para o aprendizado relacional . Sistemas de PLI são capazes de aprender levando em consideração conhecimento do domínio na forma de um programa lógico e também usar a linguagem de programas lógicos para descrever o conhecimento induzido . Neste trabalho foi implementado um módulo chamado Kaeru para converter dados no formato_atributo-valor  para o formato relacional utilizado pelo sistema de PLI Aleph . Uma série de experimentos foram realizados com quatro conjuntos de dados naturais e um conjunto de dados real no formato atributo valor . Utilizando o módulo conversor Kaeru esses dados foram convertidos para o formato relacional utilizado pelo Aleph e hipóteses de classificação foram induzidas utilizando aprendizado proposicional bem como aprendizado relacional . É mostrado também , que o aprendizado proposicional pode ser utilizado para incrementar o conhecimento do domínio utilizado pelos sistemas de aprendizado relacional para melhorar a qualidade das hipóteses induzidas 
 A area de revisão de crenças estuda como agentes racionais mudam suas crencas ao receberem novas informações . O marco da area de revisão de crenças foi a publicacão do trabalho de Alchourron , Gardenfors e Makinson . Nesse trabalho conhecido como paradigma AGM foram denidos criterios de racionalidade para tipos de mudanca de crencas . Desde então , a área de revisão de crenças foi influenciada por diversas disciplinas como filosoa , computacão e direito . Paralelamente ao desenvolvimento da area de revisão de crenças , os últimos 20 anos foram marcados por um grande avanço no estudo das logicas de descrição . Tal avanço , impulsionado pelo desenvolvimento da web-semântica , levou a adoção de linguagens inspiradas em logicas de descrição ( OWL ) como padrão para se representar ontologias na web . Nessa tese tratamos do problema de aplicar a teoria da revisão de crenças a lógicas não clássicas e especialmente a logicas de descric~ao . Trabalhos recentes mostraram que o paradigma AGM e incompatvel com diversas logicas de descricão . Estendemos esses resultados mostrando outras lógicas que não são compatíveis com o paradigma AGM . Propomos formas de aplicar a teoria de revisão tanto em bases quanto em conjuntos de crencas a essas logicas . Alem disso , usamos algoritmos conhecidos da área de depuração de ontologias para implementar operações em bases de crenças 
 Esta Dissertação aborda o Problema de Alocação de Aulas a Salas ( PAAS ) , também conhecido como Problema de Alocação de Salas ( PAS ) . As instituições de ensino superior , no começo de seus calendários letivos , resolvem um PAAS ao determinar os espaços a serem utilizados para as atividades didáticas . Porém , em muitas destas instituições o PAAS é ainda resolvido manualmente , gerando altas cargas de trabalho para os responsáveis . Neste trabalho , o Instituto de Ciências_Matemáticas  e de Computação ( ICMC ) da Universidade de São Paulo ( USP ) foi tomado como caso de estudo para o PAAS . Um modelo de programação matemática inteiro é proposto e abordado por técnicas de resolução exata , metaheurísticas mono-objetivo e uma abordagem multi-objetivo . Uma estrutura de vizinhança proposta obteve resultados comparáveis à da metodologia exata , para um tempo fixo de execução . Demonstra-se que , a abordagem multi-objetivo é uma possibilidade de contornar algumas dificuldades clássicas do problema , como incertezas sobre a escolha dos pesos das métricas . Os métodos de solução propostos para o problema fornecem , aos responsáveis , bons instrumentos de auxílio à tomada de decisão para o PAAS 
 
 A área de aprendizado de máquina adquiriu grande_importância  na última_década  graças à sua capacidade de analisar conjuntos de dados complexos em larga_escala  . Em diversas_áreas  do conhecimento existe a demanda pela análise de dados por especialistas , seja para obter agrupamentos latentes ou classificar instâncias em classes conhecidas . As ferramentas acessíveis a especialistas leigos em programação são limitadas a problemas específicos e demandam um custo de desenvolvimento às vezes proibitivo , sendo interessante buscar por ferramentas genéricas e aplicáveis a qualquer área do conhecimento . Este trabalho busca estender e implementar uma metodologia genérica de aprendizado de máquina capaz de analisar quaisquer conjuntos de arquivos de forma praticamente livre de configuração . Foram obtidos resultados_satisfatórios  de sua aplicação em um conjunto amplo de problemas para agrupamento e classificação de executáveis , spam e detecção de línguas 
 Redes sociais online ( OSNs ) são plataformas Web que oferecem serviços para promoção da interação social entre usuários . OSNs que adicionam serviços relacionados à geolocalização são chamadas redes_sociais  baseadas em localização ( LBSNs ) . Um dos maiores desafios na análise de LBSNs é a predição de links . A predição de links refere-se ao problema de estimar a probabilidade de conexão futura entre pares de usuários que não se conhecem . Grande parte das pesquisas que focam nesse problema exploram o uso , de maneira isolada , de informações sociais ( e.g . amigos em comum ) ou de localização ( e.g . locais comuns visitados ) . Porém , algumas pesquisas mostraram que a combinação de diferentes fontes de informação pode influenciar o incremento da acurácia da predição . Motivado por essa lacuna , neste trabalho foram desenvolvidos diferentes métodos para predição de links combinando diferentes fontes de informação . Assim , propomos sete métodos que usam a informação relacionada à participação simultânea de usuários en múltiples grupos sociais : common neighbors within and outside of common groups ( WOCG ) , common neighbors of groups ( CNG ) , common neighbors with total and partial overlapping of groups ( TPOG ) , group naïve Bayes ( GNB ) , group naïve Bayes of common neighbors ( GNB-CN ) , group naïve Bayes of Adamic-Adar ( GNB-AA ) , e group naïve Bayes of Resource Allocation ( GNB-RA ) . Devido ao fato que a presença de grupos sociais não está restrita a alguns tipo de redes , essas propostas podem ser usadas nas diversas OSNs existentes , incluindo LBSNs . Também , propomos oito métodos que combinam o uso de informações sociais e de localização : Check-in Observation ( ChO ) , Check-in Allocation ( ChA ) , Within and Outside of Common Places ( WOCP ) , Common Neighbors of Places ( CNP ) , Total and Partial Overlapping of Places ( TPOP ) , Friend Allocation Within Common Places ( FAW ) , Common Neighbors of Nearby Places ( CNNP ) , e Nearby Distance Allocation ( NDA ) . Tais propostas são para uso exclusivo em LBSNs . Os resultados obtidos indicam que nossas propostas são tão competitivas quanto métodos do estado da arte , podendo até superá-los em determinados cenários . Ainda mais , devido a que na maioria dos casos nossas propostas são computacionalmente mais eficientes , seu uso resulta mais adequado em aplicações do mundo_real  
 A navegação_autônoma  é um dos problemas fundamentais da robótica_móvel  . Para um robô executar esta tarefa , é necessário determinar a região segura para a navegação . Este trabalho propõe um sistema de identificação de superfícies navegáveis baseado em visão_computacional  utilizando redes neurais_artificiais  . Mais especificamente , é realizado um estudo sobre a utilização de diferentes atributos de imagem , como descritores estatísticos e elementos de espaços de cores , para serem utilizados como entrada das redes neurais_artificiais  que tem como tarefa a identificação de superfícies navegáveis . O sistema desenvolvido utiliza resultados de classificação de múltiplas configurações de redes neurais_artificiais  , onde a principal diferença entre elas é o conjunto de atributos de imagem utilizados como entrada . Essa combinação de diversas classificações foi realizada visando maior robustez e melhor desempenho na identificação de vias em diferentes 
 No presente_trabalho  estudamos duas variações do problema de empacotamento de itens retangulares idênticos , permitindo rotações de 90 graus , em um poliedro . Uma variação consiste em encontrar a maior quantidade de itens retangulares idênticos que podem ser empacotados em um poliedro . A outra consiste em encontrar o poliedro de um determinado tipo com menor área para empacotar uma quantidade fixa de itens retangulares idênticos . Desenvolvemos restrições de eliminação de simetrias para estes problemas , o que tornou a resolução dos mesmos mais eficiente , por métodos do tipo branch- & -bound . Estudamos também o problema de corte no qual há uma determinada demanda ( de itens ) a ser cortada e um conjunto de objetos disponíveis . Desejamos satisfazer a demanda minimizando o custo dos objetos utilizados e , dentre as diferentes possibilidades de se fazer isso , desejamos aquela que maximize as sobras aproveitáveis . De forma geral , sobras aproveitáveis podem ser entendidas como regiões retangulares de um objeto que possuem altura e largura iguais ou superiores a de um item de referência e representam sobras do processo de corte que podem se tornar objetos e serem reaproveitadas em um novo procedimento de corte . Apresentamos modelos de otimização em dois_níveis  para duas variações do problema de corte com sobras aproveitáveis a saber : o problema de corte de itens retangulares em dois estágios e o problema de corte de itens retangulares não guilhotinado . Como formas de resolver os modelos propostos , apresentamos reformulações destes modelos de programação em dois_níveis  em modelos de programação inteira_mista  . Lidamos também com uma variação do problema de corte com sobras aproveitáveis considerando a minimização da quantidade de sobras . Aplicamos restrições de eliminação de simetrias aos modelos desenvolvidos para o problema de corte de itens retangulares com sobras aproveitáveis , a fim de resolver instâncias maiores , e desenvolvemos uma estratégia de solução alternativa para os modelos . Os modelos desenvolvidos foram implementados computacionalmente e fomos capazes de resolver instâncias pequenas dos problemas em questão 
 Aprendizado semissupervisionado ( ASS ) é o nome dado ao paradigma de aprendizado de máquina que considera tanto dados rotulados como dados não rotulados . Embora seja considerado frequentemente como um meio termo entre os paradigmas supervisionado e não supervisionado , esse paradigma é geralmente aplicado a tarefas preditivas ou descritivas . Na tarefa preditiva de classificação , p . ex. , o objetivo é rotular dados não rotulados de acordo com os rótulos dos dados rotulados . Nesse caso , enquanto que os dados não rotulados descrevem as distribuições dos dados e mediam a propagação dos rótulos , os itens de dados rotulados semeiam a propagação de rótulos e guiam-na à estabilidade . No entanto , dados são gerados tipicamente não rotulados e sua rotulação requer o envolvimento de especialistas no domínio , rotulando-os manualmente . Dificuldades na visualização de grandes volumes de dados , bem como o custo associado ao envolvimento do especialista , são desafios que podem restringir o desempenho dessa tarefa . Por- tanto , o destacamento automático de bons candidatos a dados rotulados , doravante denominados indivíduos representativos , é uma tarefa de grande_importância  , e pode proporcionar uma boa relação entre o custo com especialista e o desempenho do aprendizado . Dentre as abordagens de ASS discriminadas na literatura , nosso interesse de estudo se concentra na abordagem baseada em redes , onde conjuntos de dados são representados relacionalmente , através da abstração gráfica . Logo , o presente_trabalho  tem como objetivo explorar a influência dos nós rotulados no desempenho do ASS baseado em redes , i.e. , estudar a caracterização de nós representativos , como a estrutura da rede pode realçá-los , o ganho de desempenho de ASS proporcionado pela rotulação manual dos mesmos , e aspectos filosóficos relacionados . Em relação à caracterização , critérios de caracterização de nós centrais em redes são estudados considerando-se redes com estruturas modulares bem definidas . Contraintuitivamente , nós bastantes conectados ( hubs ) não são muito representativos . Nós razoavelmente conectados em vizinhanças pouco conectadas , por outro_lado  , são ; estritamente local , esse critério de caracterização é escalável a grandes volumes de dados . Em redes com distribuição de grau homogênea - modelo Girvan-Newman ( GN ) , nós com alto coeficiente de agrupamento também mostram-se representativos . Por outro_lado  , em redes com distribuição de grau heterogênea - modelo Lancichinetti-Fortunato-Radicchi ( LFR ) , nós com alta intermedialidade se destacam . Nós com alto coeficiente de agrupamento em redes GN estão tipicamente situados em motifs do tipo quase-clique ; nós com alta intermedialidade em redes LFR são hubs situados na borda das comunidades . Em ambos os casos , os nós destacados são excelentes regularizadores . Além_disso  , como critérios diversos se destacam em redes com características diversas , abordagens unificadas para a caracterização de nós representativos também foram estudadas . Crítica para o realce de indivíduos representativos e o bom_desempenho  da classificação semissupervisionada , a construção de redes a partir de bases de dados vetoriais também foi estudada . O método denominado AdaRadius foi proposto , e apresenta vantagens tais como adaptabilidade em bases de dados com densidade variada , baixa dependência da configuração de seus parâmetros , e custo_computacional  razoável , tanto sobre dados pool-based como incrementais . As redes resultantes , por sua vez , são esparsas , porém conectadas , e permitem que a classificação semissupervisionada se favoreça da rotulação prévia de indivíduos representativos . Por fim , também foi estudada a validação de métodos de construção de redes para o ASS , sendo proposta a medida denominada coerência grafo-rótulos de Katz . Em suma , os resultados discutidos apontam para a validade da seleção de indivíduos representativos para semear a classificação semissupervisionada , corroborando a hipótese central da presente tese . Analogias são encontrados em diversos problemas modelados em redes , tais como epidemiologia , propagação de rumores e informações , resiliência , letalidade , grandmother cells , e crescimento e auto-organização 
 Cada imagem pode ser representada como uma combinação de diversas características , como por exemplo o histograma de intensidades de cor ou propriedades de textura da imagem . Essas características compõem um vetor multidimensional que representa a imagem . É comum esse vetor ser dado como entrada para um método de classificação de padrões que , após aprender por meio de diversos exemplos , pode gerar um modelo de decisão . Estudos sugerem evidências de que a preparação das imagens -- por meio da especificação cuidadosa da aquisição , pré-processamento e segmentação -- pode impactar significativamente a classificação . Além da falta de tratamento das imagens antes da extração de características , o desbalanceamento de classes também se apresenta como um obstáculo para que a classificação seja satisfatória . Imagens possuem características que podem ser exploradas para melhorar a descrição dos objetos de interesse e , portanto , sua classificação . Entre as possibilidades de melhorias estão : a redução do número de intensidades das imagens antes da extração de características ao invés de métodos de quantização no vetor já extraído ; e a geração de imagens a partir das originais , de forma a promover o balanceamento de bases de dados cujo número de exemplos de cada classe é desbalanceado . Portanto , a proposta desta dissertação é melhorar a classificação de imagens utilizando métodos de processamento de imagens antes da extração de características . Especificamente , busca analisar a influência do balanceamento de bases de dados e da quantização na classificação . Este estudo analisa ainda a visualização do espaço de características após os métodos de geração artificial de imagens e de interpolação das características_extraídas  das imagens originais ( SMOTE ) , comparando como espaço original . A ênfase dessa visualização se dá na observação da importância do rebalanceamento das classes . Os resultados obtidos indicam que a quantização simplifica as imagens antes da extração de características e posterior redução de dimensionalidade , produzindo vetores mais compactos ; e que o rebalanceamento de classes de imagens através da geração de imagens artificiais pode melhorar a classificação da base de imagens , em relação à classificação original e ao uso de métodos no espaço de características já extraídas 
 O objetivo deste projeto é o estudo de técnicas numéricas robustas para aproximação da solução de leis de conservação hiperbólicas escalares unidimensionais e bidimensionais e de sistemas de leis de conservação hiperbólicas . Para alcançar tal objetivo , estudamos esquemas conservativos com propriedades especiais , tais como , esquemas upwind , TVD , Godunov , limitante de fluxo e limitante de inclinação 
 A solução de um sistema de leis de conservação pode exibir descontinuidades do tipo choque , rarefação ou de contato . Assim , o desenvolvimento de técnicas numéricas capazes de reproduzir e tratar esses comportamentos é desejável . Além de representar corretamente a descontinuidade os esquemas numéricos têm ainda uma tarefa mais árdua ; aquela de escolher a solução singular correta , a chamada solução entrópica . Os métodos de Godunov , limitantes de fluxo e limitantes de inclinação são técnicas numéricas que possuem as características apropriadas para aproximar a solução entrópica de uma lei de conservação 
 A análise de séries_temporais  gera muitos desafios para profisionais em um grande número de domínios . Várias soluções de visualização integrada com algoritmos de mineração já foram propostas para tarefas exploratórias em coleções de séries_temporais  . À medida que o conjunto de dados cresce , estas soluções falham em promover uma boa associação entre séries_temporais  similares . Neste trabalho , é apresentada uma ferramenta para a análise exploratória e mineração de conjuntos de séries_temporais  que adota uma representação visual baseada em medidas de dissimilaridade entre séries . Esta representação é criada usando técnicas rápidas de projeção , de forma que as séries_temporais  possam ser visualizadas em espaços bidimensionais . Vários tipos de atributos visuais e conexões no grafo resultante podem ser utilizados para suportar a exploração dessa representação . Também é possível aplicar algumas tarefas de mineração de dados , como a classificação , para apoiar a busca por padrões . As visualizações resultantes têm se mostrado muito úteis na identificação de grupos de séries com comportamentos similares , que são mapeadas para a mesma vizinhança no espaço bidimensional . Grupos visuais de elementos , assim como outliers , são facilmente identficáveis . A ferramenta é avaliada por meio de sua aplicação a vários conjuntos de séries . Um dos estudos de caso explora dados de vazões de usinas hidrelétricas no Brasil , uma aplicação estratégica para o planejamento energético 
 A constante evolução da Web tem se mostrado como um fenômeno mundial que rapidamente precisa responder aos diversos segmentos da sociedade atual , com websites e aplicativos de compras , governo , bancos , entretenimento e outros . Nesse contexto , é necessário que o conteúdo Web possibilite acesso aos mais diferentes perfis de usuários , independentemente de suas deficiências ou necessidades especiais . Um outro fenômeno mundial é o envelhecimento da população . Os usuários senescentes ( pessoas que estão em processo de envelhecimento , o qual acarreta declínio físico e mental gradual , ocorrendo geralmente nos indivíduos a partir de 60 anos de idade ) , apresentam algumas de suas capacidades reduzidas e , encontram , naturalmente , barreiras ao interagir com os serviços e conteúdos disponibilizados na Web . Essa população tem apresentado um índice de crescimento demográfico grande neste século em relação ao que se via nas gerações passadas . Apesar das exigências de legislação específica , recomendações e diretrizes que auxiliam o desenvolvimento de conteúdo acessível e usável , ainda há muitos problemas de acessibilidade e usabilidade que precisam ser resolvidos , diante do rápido avanço tecnológico observado nos recursos da Web atual . Em especial , observa-se pouca atenção às dificuldades que usuários senescentes possuem , pois a maioria de websites são projetados considerando como público-alvo , pessoas mais jovens e especializadas , muitas_vezes  , treinadas para interagir nos websites . Portanto , é relevante um apoio para avaliação das páginas_Web  , visando atender as necessidades dos usuários senescentes . A proposta deste projeto foi desenvolver um apoio para a avaliação de acessibilidade e usabilidade na Web , considerando-se o perfil dos senescentes , com vistas a proporcionar um feedback objetivo , aos desenvolvedores e especialistas . Esse apoio foi elaborado por meio de um Checklist que foi desenvolvido , com base em procedimentos científicos , e análises realizadas , sobre sua aplicação 
 Segmentar uma imagem consiste em particioná-la em regiões relevantes para uma dada aplicação , como para isolar um objeto de interesse no domínio de uma imagem . A segmentação é um dos problemas mais fundamentais e desafiadores em processamento de imagem e visão_computacional  . Ela tem desempenhado um papel_importante  , por exemplo , na pesquisa em neurologia , envolvendo imagens de Ressonância Magnética ( RM ) , para fins de diagnóstico e tratamento de doenças relacionadas com alterações na anatomia do cérebro humano . Métodos de segmentação baseados na transformada imagem- floresta ( IFT , Image Foresting Transform ) , com funções de conexidade suaves , possuem resultados ótimos , segundo o critério da otimalidade dos caminhos descrito no artigo original da IFT , e têm sido usados com sucesso em várias aplicações , como por exemplo na segmentação de imagens RM de 1.5 Tesla . No entanto , esses métodos carecem de restrições de regularização de borda , podendo gerar segmentações com fronteiras muito irregulares e indesejadas . Eles também não distinguem bem entre bordas similares com orientações opostas , e possuem alta sensibilidade à estimativa dos pesos das arestas do grafo , gerando problemas em imagens com efeitos de inomogeneidade . Nesse trabalho são propostas extensões da IFT , do ponto de vista teórico e experimental , através do uso de funções de conexidade não suaves , para a segmentação interativa de imagens por região . A otimalidade dos novos métodos é suportada pela maximização de energias de corte em grafo , ou como o fruto de uma sequência de iterações de otimização de caminhos em grafos residuais . Como resultados principais temos : O projeto de funções de conexidade mais adaptativas e flexíveis , com o uso de pesos dinâmicos , que permitem um melhor tratamento de imagens com forte inomogeneidade . O uso de grafos direcionados , de modo a explorar a polaridade de borda dos objetos na segmentação por região , e o uso de restrições de forma que ajudam a regularizar a fronteira delineada , favorecendo a segmentação de objetos com formas mais regulares . Esses avanços só foram possíveis devido ao uso de funções não suaves . Portanto , a principal_contribuição  desse trabalho consiste no suporte teórico para o uso de funções não suaves , até então evitadas na literatura , abrindo novas perpectivas na pesquisa de processamento de imagens usando grafos 
 Sistemas embarcados , inclusive para eletrônica de consumo , vêm se tornando cada vez mais complexos , requerendo a utilização de novas tecnologias , bem como novas abordagens para o seu desenvolvimento . Em particular , o desenvolvimento de aplicações para TV Digital interativa têm requerido crescente aplicação de novas técnicas de programação e engenharia de software , visando facilitar o desenvolvimento e manutenção desses sistemas . Em paralelo , arquiteturas de referência , um tipo especial de arquitetura de software , têm sido proposta para diversos_domínios  de aplicação e têm contribudo efetivamente para o desenvolvimento , padronização e evolução de sistemas de software de tais domínios . Contudo , o uso de arquiteturas de referência ainda não foi explorado em profundidade no desenvolvimento de aplicações para o domínio de TV Digital . Nesse contexto , o principal objetivo desse trabalho e propor uma arquitetura de referência para o domínio de TV Digital interativa , que facilite o desenvolvimento de aplicações para o ambiente procedural do middleware para o receptor digital . Como principais resultados alcançados neste trabalho , têm-se a contribuição para a área de TV Digital , buscando promover essa área que têm se destacado consideravelmente nos últimos_anos 
 Três esquemas de captura de descontinuidade são apresentados para simular hiperbólicos de leis de conservação e equações de Navier-Stokes incompressíveis , a saber : FDHERPUS ( Five Degree Hermite Upwind_Scheme  ) ; RUS ( Rational Upwind_Scheme  ) ; e CSPUS ( Cubic Spline Polynomial Upwind_Scheme  ) . Esses esquemas são baseados nos critérios de estabilidade CBC e TVD e implementados nos contextos das metodologias diferenças_finitas  e volumes finitos . A precisão local dos esquemas é verificada acessando o erro e a taxa de convergência em problemas testes de referência . Um estudo comparativo entre os esquemas estudados ( incluido o WENO5 ) e o esquema bem estabelecido de van Albada , para resolver leis de conservação lineares e não lineares , é também realizado . O esquema de convecção que fornece melhores_resultados  em leis de conservação hiperbólicas é então examinado na simulação de escoamentos de fluidos newtonianos com superfícies_livres  móveis de complexidade crescente ; resultados_satisfatórios  têm sido observados em termos do comportamento 
 Neste trabalho estudamos a estabilidade assintótica de Lagrange para duas classes de sistemas , sob as hipóteses de detetabilidade fraca e de limitação do custo medio a longo prazo . Para sistemas_lineares  com saltos markovianos com rudo aditivo , a equivalência entre estabilidade e as condições mencionadas sera provada . Para sistemas dinâmicos generalizados , provaremos a estabilidade sob uma condição 
 Contexto : Uma tomada de decisão precisa exige informações mais precisas e atualizadas para estabelecer a realidade da situação geral . Novas fontes de dados ( e.g , tecnologias vestíveis ) tem aumentado a quantidade de dados úteis disponíveis , que agora é chamado de big data . Isso tem grande potencial para transformar todo o processo de negócio e melhorar a precisão na tomada de decisão . Neste contexto , a gestão de desastres representa um interessante cenário que depende de big data para aprimorar a tomada de decisão . Isso porque , ela tem que lidar com dados fornecidos não apenas por fontes tradicionais ( e.g. , sensores estáticos ) , mas também por fontes emergentes por exemplo , informações compartilhadas por voluntários locais , i.e. , as informações geográficas de voluntários ( VGI ) . Quando combinadas , estas fontes de dados podem ser consideradas grandes em volume , com diferentes velocidades e uma variedade de formatos . Além_disso  , uma análise com relação à sua veracidade é necessaria uma vez que estas fontes de dados são desconectadas e propensas à erros . Estes são os 4Vs que caracterizam big data . Problema : No entanto , embora todos estes dados abrem novas oportunidades , seu grande volume em conjunto com uma integração inapropriada e uma visualização inadequada , podem tornar as informações ignoradas por tomadores de decisão . Isso ocorre , pois , a integração dos dados disponíveis torna-se complicada devido a heterogeneidade intrínseca nas suas características ( e.g. , dados em formatos diferentes ) . Quando integradas , estas informações frequentemente também não chegam aos tomadores de decisão em uma condição apropriada ( por exemplo , no formato de visualização adequado ) . Além_disso  , não existe uma clara compreensão sobre as necessidades dos tomadores de decisão ou sobre como os dados disponíveis podem ser usados para atender essas necessidades . Objetivo : Dessa forma , esta tese de doutorado apresenta uma abordagem para melhorar a tomada de decisões com grande volume de dados espaciais heterogêneos baseada em sistemas de suporte à decisão espacial e informações geográficas de voluntários na gestão de desastres . Métodos : Mapeamentos sistemáticos foram conduzidos para identificar lacunas de pesquisa no uso de dados voluntários e sistemas de suporte à decisão na gestão de desastres . Com base nestes estudos , dois projetos de design science foram conduzidos . O primeiro deles buscou definir elementos essências para entender a integração de dados heterogêneos , enquanto o segundo projeto buscou fornecer um melhor entendimento das necessidades dos tomadores de decisão . Também foi conduzido um projeto de pesquisa-ação interinstitucional para definir princípios de projeto que deveriam ser observados para um sistema de suporte à decisão espacial ser efetivo no apoio a tomada de decisão com grande volume de dados espaciais heterogêneos . Uma série de estudos de caso empíricos foram conduzidos para avaliar os resultados destes projetos . Resultados : A abordagem geral então é composta pelos três resultados significantes que foram derivados destes projetos . Em primeiro lugar , uma arquitetura conceitual que especifica a integração de fontes de dados heterogêneas . O segundo elemento é uma estrutura baseada em modelo que descreve a conexão entre a tomada de decisão com as fontes de dados mais adequadas . Com base nesta estrutura , o terceiro elemento consiste em um conjunto de princípios de design que guiam o desenvolvimento de um sistema de suporte à decisão espacial para tomada de decisão com grande volume de dados espaciais heterogêneos . Conclusão : Esta tese de doutorado realizou importantes contribuições para a prática e pesquisa . Em resumo , ela define formas para integrar fontes de dados heterogêneos , fornece uma melhor compreensão sobre as necessidades dos tomadores de decisão e ajuda no desenvolvimento de sistemas de suporte à decisão espacial para tomada de decisão com grande volume de dados espaciais heterogêneos 
 Este projeto de mestrado visa a pesquisa e o desenvolvimento de métodos e algoritmos , relacionados ao uso de radares , visão_computacional  , calibração e fusão de sensores em veículos autônomos/inteligentes para fazer a detecção de obstáculos . O processo de detecção de obstáculos se divide em três etapas , a primeira é a leitura de sinais de Radar , do LiDAR e a captura de dados da câmera estéreo devidamente calibrados , a segunda etapa é a fusão de dados obtidos na etapa anterior ( Radar+câmera , Radar+LIDAR 3D ) , a terceira etapa é a extração de características das informações obtidas , identificando e diferenciando o plano de suporte ( chão ) dos obstáculos , e finalmente realizando a detecção dos obstáculos resultantes da fusão dos dados . Assim é possível diferenciar os diversos tipos de elementos identificados pelo Radar e que são confirmados e unidos aos dados obtidos por visão_computacional  ou LIDAR ( nuvens de pontos ) , obtendo uma descrição mais precisa do contorno , formato , tamanho e posicionamento destes . Na tarefa de detecção é importante localizar e segmentar os obstáculos para posteriormente tomar decisões referentes ao controle do veículo autônomo/inteligente . É importante destacar que o Radar opera em condições adversas ( pouca ou nenhuma iluminação , com poeira ou neblina ) , porém permite obter apenas pontos isolados representando os obstáculos ( esparsos ) . Por outro_lado  , a câmera estéreo e o LIDAR 3D permitem definir os contornos dos objetos representando mais adequadamente seu volume , porém no caso da câmera esta é mais suscetível a variações na iluminação e a condições restritas ambientais e de visibilidade ( p.ex . poeira , neblina , chuva ) . Também devemos destacar que antes do processo de fusão é importante alinhar espacialmente os dados dos sensores , isto e calibrar adequadamente os sensores para poder transladar dados fornecidos por um sensor referenciado no próprio sistema de coordenadas para um outro sistema de coordenadas de outro sensor ou para um sistema de coordenadas global . Este projeto foi desenvolvido usando a plataforma CaRINA II desenvolvida junto ao Laboratório LRM do ICMC/USP São Carlos . Por fim , o projeto foi implementado usando o ambiente ROS , OpenCV e PCL , permitindo a realização de experimentos com dados reais de Radar , LIDAR e câmera estéreo , bem como realizando uma avaliação da qualidade da fusão dos dados e detecção de obstáculos comestes sensores 
 O problema do sequenciamento da produção consiste em determinar a sequência de operações a processar em cada uma das máquinas disponíveis na fabrica de modo que a duração total de sequenciamento seja mínima . As peças são processadas de acordo com roteiros de fabricação fixados e as durações operatórias são conhecidas . Nesta dissertação estudamos o problema do sequenciamento da produção com restrições disjuntivas através de duas_abordagens  : programação inteira e teoria dos grafos . Três programas computacionais baseados em teoria dos grafos foram desenvolvidos e testados . Estes programas permitiram a resolução eficiente de vários exemplos apesar do caráter não-polinomial do problema estudado 
 Neste trabalho são apresentadas diversas ferramentas disponíveis para análise de desempenho de sistemas computacionais . Aspectos como adequabilidade das ferramentas para uma dada proposta de análise , facilidade de utilização , dificuldades envolvidas nas alterações do modelo do sistema , tempo necessário para efetuar a análise , custo da implementação e a precisão dos resultados obtidos , são considerados e discutidos nesta dissertação . As ferramentas e metodologias abordadas nesta dissertação , aplicam-se a sistemas computacionais em geral , embora o interesse deste trabalho é direcionado para a análise de desempenho em sistema computacionais_distribuídos  . Os conceitos envolvidos com sistemas computacionais_distribuídos  são apresentados e discutidos com o objetivo de se formar uma base para o desenvolvimento desse trabalho . Quatro modelos distintos são propostos para o estudo do desempenho de um servidor de arquivos , com o objetivo de verificar o comportamento das ferramentas de simulação e modelos analíticos . Análises estatísticas são realizadas , visando qualificar e quantificar a precisão dos dados fornecidos pela solução de cada um dos modelos , para cada uma das técnicas utilizadas . O trabalho discute a possibilidade de se obter dados significativos a respeito do comportamento de um sistema , dependendo do objetivo da análise , utilizando-se técnicas simples . Cálculo de assíntotas e modelos simplificados como M/M/1 , são utilizados para estudar a relação entre o grau de complexidade de um modelo e o grau de precisão fornecido pela solução obtida . O trabalho aborda também a implementação de um sistema de simulação através de uma extensão funcional da linguagem C , em ambiente UNIX ; discute-se também uma implementação em ambiente DOS 
 Os problemas de corte de estoque unidimensional consistem em cortar um conjunto de peças disponíveis em estoque para produzir um conjunto de itens em quantidades especificadas , em que apenas uma dimensão é relevante . Tais problemas têm inúmeras aplicações industriais e são bastante estudados na literatura . Tipicamente , esses problemas de corte apresentam uma característica comum - a minimização das perdas -entretanto , neste trabalho , consideramos que se uma perda é suficientemente grande para ser reaproveitada no futuro , não deve ser contabilizada como perda . Isto introduz uma postura diferente frente ao problema de corte : até que ponto a solução de perda mínima é a mais interessante , já que sobras podem ser reaproveitadas ? Algumas características para considerar se uma solução é desejável são definidas e alterações em métodos heurísticos clássicos são propostas , de modo que os padrões de corte com perdas indesejáveis ( nem tão grande , nem tão pequena ) sejam alterados . As análises das soluções heurísticas são realizadas com base na resolução de um conjunto de classes de exemplos geradas aleatoriamente 
 O problema de agrupamento de dados em grafos consiste em encontrar clusters de nós em um dado grafo , ou seja , encontrar subgrafos com alta conectividade . Esse problema pode receber outras nomenclaturas , algumas delas são : problema de particionamento de grafos e problema de detecção de comunidades . Para modelar esse problema , existem diversas formulações_matemáticas  , cada qual com suas vantagens e desvantagens . A maioria dessas formulações tem como desvantagem a necessidade da definição prévia do número de grupos que se deseja obter . Entretanto , esse tipo de informação não está contida em dados para agrupamento , ou seja , em dados não rotulados . Esse foi um dos motivos da popularização nas últimas_décadas  da medida conhecida como modularidade , que tem sido maximizada para encontrar partições em grafos . Essa formulação , além de não exigir a definição prévia do número de clusters , se destaca pela qualidade das partições que ela fornece . Nesta Tese , metaheurísticas Greedy Randomized Search Procedures para dois modelos existentes para agrupamento em grafos foram propostas : uma para o problema de maximização da modularidade e a outra para o problema de maximização da similaridade intra-cluster . Os resultados obtidos por essas metaheurísticas foram melhores quando comparadas àqueles de outras heurísticas encontradas na literatura . Entretanto , o custo_computacional  foi alto , principalmente o da metaheurística para o modelo de maximização da modularidade . Com o passar dos anos , estudos revelaram que a formulação que maximiza a modularidade das partições possui algumas limitações . A fim de promover uma alternativa à altura do modelo de maximização da modularidade , esta Tese propõe novas formulações_matemáticas  de agrupamento em grafos com e sem pesos que visam encontrar partições cujos clusters apresentem alta conectividade . Além_disso  , as formulações propostas são capazes de prover partições sem a necessidade de definição prévia do número de clusters . Testes com centenas de grafos com pesos comprovaram a eficiência dos modelos propostos . Comparando as partições provenientes de todos os modelos estudados nesta Tese , foram observados melhores_resultados  em uma das novas formulações propostas , que encontrou partições bastante satisfatórias , superiores às outras existentes , até mesmo para a de maximização de modularidade . Os resultados apresentaram alta correlação com a classificação real dos dados simulados e reais , sendo esses últimos , em sua maioria , de origem 
 A complexidade dos dados armazenados em grandes bases de dados aumenta cada vez mais , criando a necessidade de novas operações de consulta . Uma classe de operações que tem apresentado interesse crescente são as chamadas Consultas por Similaridade , sendo as mais conhecidas as consultas por Abrangência ( 'R IND . q ' ) e por k-Vizinhos mais Proximos ( kNN ) , sendo que esta ultima obtem quais são os k elementos armazenados mais similares a um dado elemento de referência . Outra consulta que é interessante tanto para consultas diretas quanto como parte de operações de análises mais complexas e a operação de consulta aos k-Vizinhos mais Próximos Reversos ( RkNN ) . Seu objetivo e obter todos os elementos armazenados que têm um dado elemento de referência como um dos seus k elementos mais similares . Devido a complexidade de execução da operação de RkNN , a grande maioria das soluções existentes restringem-se a dados representados em espaços multidimensionais euclidianos ( nos quais estão denidas tambem operações cardinais e topológicas , além de se considerar a similaridade como sendo a distância Euclidiana entre dois elementos ) , ou então obtém apenas respostas aproximadas , sujeitas a existência de falsos negativos . Várias aplicações de análise de dados científicos , médicos , de engenharia , financeiros , etc . requerem soluções eficientes para o problema da operação de RkNN sobre dados representados em espaços_métricos  , onde os elementos não podem ser considerados estar em um espaço nem Euclidiano nem multidimensional . Num espaço métrico , além dos próprios elementos armazenados existe apenas uma função de comparação métrica entre pares de objetos . Neste trabalho , são propostas novas podas de espaço de busca e o algoritmo RkNN-MG que utiliza essas novas podas para solucionar o problema de consultas RkNN exatas em espaços_métricos  sem limitações . Toda a proposta supõe que o conjunto de dados esta em um espaço métrico imerso isometricamente em espaço euclidiano e utiliza propriedades da geometria métrica válida neste espaço para realizar podas eficientes por lei dos cossenos combinada com as podas tradicionais por desigualdade triangular . Os experimentos demonstram comparativamente que as novas podas são mais eficientes que as tradicionais podas por desigualdade triangular , tendo desempenhos equivalente quando comparadas em conjuntos de alta dimensionalidade ou com dimensão_fractal  alta . Assim , os resultados confirmam as novas podas propostas como soluções alternativas eficientes para o problema de consultas 
 Nos últimos_anos  , percebeu-se uma crescente busca por softwares e arquiteturas alternativas . Essa busca acontece porque houve avanços na tecnologia do hardware e estes avanços devem ser complementados por inovações nas metodologias de projetos , testes e verificação para que haja um uso eficaz da tecnologia . Muitos dos softwares e arquiteturas alternativas , geralmente partem para modelos que exploram o paralelismo das aplicações , ao contrário do modelo de von Neumann . Dentre as arquiteturas alternativas de alto_desempenho  , tem-se a arquitetura a fluxo de dados . Nesse tipo de arquitetura , o processo de execução de programas é determinado pela disponibilidade dos dados . Logo , o paralelismo está embutido na própria natureza do sistema . O modelo a fluxo de dados possui a vantagem de expressar o paralelismo de maneira intrínseca , eliminando a necessidade de o programador explicitar em seu código os trechos onde deve haver paralelismo . As arquiteturas a fluxo de dados voltaram a ser um tema de pesquisa devido aos avanços do hardware , em particular , os avanços da Computação Reconfigurável e os FPGAs ( Field-Programmable Gate Arrays ) . O projeto ChipCflow é uma ferramenta para execução de algoritmos usando o modelo a fluxo de dados dinâmico em FPGA . Este trabalho apresenta o formato para os tagged-tokens do ChipCflow , os operadores de manipulação das tags dos tokens e suas implementações a fim de que se tenha a PROVA-DE-CONCEITOS para tais operadores na arquitetura 
 Apresentamos dois novos métodos para a solução de problemas de otimização convexa em dois_níveis  não necessariamente diferenciáveis , i.e. , mostramos que as sequências geradas por ambos os métodos convergem para o conjunto ótimo de uma função não suave sujeito a um conjunto que também envolve a minimização de uma função não diferenciável . Ambos os algoritmos dispensam qualquer tipo de resolução de subproblemas ou busca linear durante suas iterações . Ao final , para demonstrar que os métodos são viáveis , resolvemos um problema de reconstrução de imagens 
 As imagens de ressonância_magnética  ponderadas por difusão retratam a difusividade de moléculas de água presentes em tecidos biológicos . Em estruturas biológicas altamente organizadas e compactas como fibras nervosas e musculares , a difusividade é maior na direção paralela às fibras do que perpendicularmente às mesmas . Essa propriedade permite a reconstrução digital das trajetórias das fibras , técnica denominada tractografia , representando uma das poucas formas não invasivas de investigação da conectividade anatômica e organização estrutural do cérebro e do coração . A metodologia de tractografia mais difundida faz_uso  da integração numérica da direção principal de difusividade para reconstruir essas trajetórias . Porém , esta técnica apresenta problemas como o erro intrínseco a métodos de integração numérica e o erro associado a regiões de incerteza nos dados de difusividade . Uma metodologia considerada mais robusta consiste da modelagem da tractografia como a simulação de um sistema de partículas . No entanto , tal metodologia possui diversos parâmetros que precisam ser otimizados para cada caso e apresenta alta complexidade computacional . Esta dissertação apresenta uma metodologia de tractografia global baseada em sistema de partículas , mas com custo_computacional  reduzido pois evita passos desnecessários da otimização para reconstrução das trajetórias . Avaliamos sua acurácia em conjuntos de dados com graus de complexidade crescentes utilizando imagens sintéticas de difusão construídas digitalmente e em imagens reais de difusão do miocárdio humano . Nesses testes foram observadas reduções no consumo de tempo e maior acurácia para metodologia global apresentada com relação às descritas na literatura . Essa metodologia possui o potencial de evidenciar a organização e arquitetura de diversos tecidos do corpo humano com maior fidelidade e menor tempo de reconstrução 
 O número de contato do Rn ( em inglês , kissing number ) é o maior número de esferas de raio unitário e interiores dois-a-dois disjuntos que podem tocar simultaneamente uma esfera de raio unitário central . Nesta dissertação estudamos métodos que limitam o tamanho de tais configurações através de técnicas de otimização , como dualidade e programação semidefinida . O principal resultado obtido foi o cálculo de melhores limitantes para o número de contato nas dimensões 9 a 23 ; o que foi possível graças à exploração de simetrias dos polinômios presentes no limitante proposto por Bachoc e Vallentin ( 2008 ) , levando à consideração de programas semidefinidos menores . Por fim , o limitante estudado é estendido para uma classe mais geral de problemas 
 Pesquisas em sistemas paralelos e distribuídos de alto_desempenho  apresentam limitações no que se refere a análise , projeto , implementação e execução automática e transparente de aplicações . Essas limitações motivaram o projeto do MidHPC ( do inglês Middleware for High Performance Computing , ou seja , Middleware para Computação de Alto Desempenho ) , que balanceia transparente e automaticamente cargas de trabalho considerando a capacidade dos recursos_computacionais  e o comportamento das aplicações envolvendo : processamento , acesso a disco , memória e rede . Para utilizar todo o potencial do MidHPC , aplicações devem ser escritas utilizando o modelo de programação concorrente , tal como o padrão POSIX de threads ( pthreads ) . Aplicações desenvolvidas seguindo esse modelo de programação podem ser executadas em ambientes de Grid sem alteração de código fonte ou recompilação . Durante a execução , tarefas de uma mesma aplicação paralela comunicam-se , transparentemente , por meio de um sistema de memória_compartilhada  distribuída . O objetivo deste trabalho foi desenvolver alguns dos módulos do projeto MidHPC e integrar demais ferramentas que haviam sido previamente desenvolvidas pelo grupo . Este trabalho permite aplicar , em ambientes reais , todos os conceitos de escalonamento de processos estudados e desenvolvidos durante o projeto 
 Estruturas de dados ( ED ) topológicas oferecem diversas vantagens quando se deseja executarumadeformação sobreumamalha . Essas EDs permitem movimentar os nós da malha sem modificar sua topologia , são relativamente simples de seremimplementadas e também são passíveis de serem incorporadas a um ciclo simulação/deformação de forma completamente automática e eficiente . O primeiro objetivo deste trabalho é a concepção de uma ED topológica para representação de malhas elásticas . Tais malhas podem ser do tipo superficial ou volumétrica , e ainda simples ou mista . Para melhor desempenho , confiabilidade e menor consumo de memória , deseja-se que a ED seja implícita quanto à representação de componentes incidentes e adjacentes dos elementos presentes na malha . Outro objetivo deste trabalho é abordar o problema de geração de malhas em domínios arbitrários definidos por uma função implícita . O método proposto é uma extensão do algoritmo de Partição da Unidade Implícita ( PUI ) . Para isso , o método proposto é baseado numa abordagem de preenchimento de superfícies . Este método proposto gera adaptativamente tetraedros em diferentes níveis de refinamento de acordo com o nível de detalhe presente na região do domínio . Diferentemente de trabalhos anteriores , esta característica é feita naturalmente sem necessitar de uma estrutura auxiliar . Para este fim , usa-se uma estrutura algébrica chamada de triangulação Ja1 que é capaz de lidar com tais refinamentos . Além do mais , a triangulação Ja1 permite que se percorra a estrutura simplesmente através de regras algébricas que é uma outra vantagem do método 
 Recentemente , as tecnologias de telecomunicações esão convergindo para a concepção da Next Generation Network , onde propõe-se que todas as informações trocadas sejam classificadas por prioridade e segurança . Porém , como as redes atuais ainda não promovem tais práticas , protocolos VoIP , em conjunto a outras soluçõoes , buscam a melhoria da qualidade das ligações . Como o protocolo VoIP IAX vem ganhando credibilidade na comunidade open source nos úlltimos anos , torna-se relevante compará-lo ao protocolo SIP , o qual é bastante investigado pela literatura . Desta forma , o objetivo deste trabalho é o estudo e avaliação dos protocolos SIP e IAX , através de verificações de qualidade do áudio em ligações VoIP . Para a realização dos experimentos foi desenvolvida uma estrutura que representasse chamadas VoIP no simulador Network Simulator e , para tais ligações , empregou-se método de avaliação de qualidade PESQ . Assim , foi possível a verficação das semelhanças compreendidas entre os protocolos SIP e IAX diante dos problemas de perda de pacotes , atraso , limitação da taxa de dados e 
 A literatura apresenta regras de usabilidade para orientar o projeto de interfaces de sistemas de software . No entanto , essas regras geralmente estão relacionadas a aspectos de apresentação de mídias de informação isoladas , mostrando-se limitadas quando analisadas sob a perspectiva de diferentes mídias . Propõe-se , a partir de regras de usabilidade propostas na literatura e da avaliação de usabilidade de hiperdocumentos multimídia de um sistema Web , um conjunto de regras que se aplica ao projeto de hiperdocumentos multimídia voltados para o contexto da World_Wide  Web . As novas regras foram utilizadas para orientar a implementação de uma nova versão de hiperdocumentos para o sistema originariamente avaliado . No contexto do mesmo sistema , as regras propostas foram comparadas com um conjunto clássico de regras de usabilidade . Finalmente , as novas regras foram investigadas em termos de sua adequação à avaliação de hiperdocumentos multimídia de outros dois sistemas Web 
 O desenvolvimento de recursos_computacionais  para PLN é , na maioria das vezes , uma tarefa árdua e demorada , principalmente na fase inicial , de aquisição de conhecimento . Esta tarefa pode ser simplificada com a centralização dos dados em um repositório que armazene todas as informações lexicais disponíveis de uma determinada língua . Este trabalho documenta o desenvolvimento da DlADORIM , uma base relacional de dados lexicais para a língua_portuguesa  , com cerca de 1.5 milhão de entradas . Os desafios linguísticos e computacionais encontrados durante todo o processo são discutidos . Várias interfaces de acesso e edição foram implementadas e avaliadas 
 Neste trabalho , fazemos uma releitura crítica do problema do aconselhamento ortográfico para o português do Brasil , entendido como a correção interativa de erros ortográficos ortográficos em palavras isoladas , e reagimos . Em primeiro lugar , identificamos um parâmetro de qualidade importante - utilidade - e argumentamos que as soluções correntes o negligenciam . Procuramos meios de maximizar esse parâmetro , um dos quais justificamos ser o embasamento linguístico e daí propomos uma arquitetura genérica de sistema corretor interativo centrado em utilidade . Continuamos o trabalho levantando conhecimentos linguísticos relevantes e fazendo considerações úteis ao desenvolvimento de um conselheiro ortográfico segundo o modelo proposto 
 Os processos de negócio desempenham um papel muito importante na indústria , principalmente pela evolução das tecnologias da informação . As plataformas de computação em nuvem , por exemplo , com a alocação de recursos_computacionais  sob_demanda  , possibilitam a execução de processos altamente requisitados . Para tanto , é necessário definir o ambiente de execução dos processos de tal modo que os recursos sejam utilizados de forma ótima e seja garantida a correta funcionalidade do processo . Nesse contexto , diferentes métodos já foram propostos para modelar os processos de negócio e analisar suas propriedades quantitativas e qualitativas . Há , contudo , vários desafios que podem restringir a aplicação desses métodos , especialmente para processos com alta demanda ( como os workflows de numerosas instâncias ) e que dependem de recursos limitados . A análise de desempenho de workflows de numerosas instâncias via modelagem analítica é o objeto de estudo deste trabalho . Geralmente , para a realização desse tipo de análise usa-se modelos matemáticos baseados em técnicas Markovianas ( sistemas estocásticos ) , que sofrem do problema da explosão do espaço de estados . Entretanto , a Teoria do Campo Médio indica que o comportamento de um sistema estocástico , sob certas condições , pode ser aproximado por o de um sistema determinístico , evitando a explosão do espaço de estados . Neste trabalho usamos tal estratégia e , com base na definição formal de aproximação determinística e suas condições de existência , elaboramos um método para representar os workflows , e seus recursos , como equações_diferenciais  ordinárias , que descrevem um sistema determinístico . Uma vez definida a aproximação determinística , realizamos a análise de desempenho no modelo determinístico , verificando que os resultados obtidos são uma boa aproximação para a solução estocástica 
 A área de gerenciamento de processos de negócio apresenta-se ativa e tem recebido atenção tanto da comunidade de pesquisa como da indústria . Uma das principais preocupações nessa área é a escolha da melhor abordagem para modelagem e implementação de processos de negócio . Atualmente , organizações possuem processos de negócio com complexas estruturas que são reavaliados e ajustados com frequência , exigindo flexibilidade das linguagens para modelagem de processos . Além_disso  , processos de negócio atuais também exigem ambientes para implementação capazes de garantir confiabilidade na execução de instâncias do processo modelado , especialmente em caso de falhas . Embora diversos modelos transacionais tenham sido_propostos  com o objetivo de garantir propriedades transacionais adaptadas ao contexto de processos de negócio , a implementação de processos transacionais ainda oferece um cenário com desafios teóricos e práticos . Neste trabalho apresentamos uma implementação da abordagem WED-flow para controle de processos transacionais . A WED-flow é uma abordagem que combina conceitos de modelos transacionais avançados , eventos e estados de dados com o objetivo principal de reduzir a complexidade no tratamento de exceções . A ferramenta de software desenvolvida é capaz de controlar a execução de instâncias de processos de negócio , permite a evolução incremental do modelo projetado e ainda disponibiliza a estrutura necessária para apoiar a implementação de mecanismos de recuperação para tratar interrupções em instâncias causadas por exceções 
 Com o aumento do uso das atividades de modelagem em processos de desenvolvimento de software , a participação de pessoas com deficiência visual em tais processos requer esforços dedicados para que os modelos sejam passíveis de entendimento , caso contrário essa participação fica comprometida . Os modelos são em sua maioria visuais e , portanto , seu processo de construção requer o posicionamento de elementos no espaço do documento por meio de um dispositivo de apontar , como o mouse , e sua leitura requer o uso da visão , uma vez que os diagramas são compostos não apenas por textos , mas também por elementos visuais como retângulos e arcos conectando-os . Neste contexto , o objetivo deste projeto foi desenvolver uma técnica textual para representação e interação com diagramas que possibilite que pessoas com deficiência visual sejam capazes de colaborar em projetos de software , tanto utilizando uma abordagem de desenvolvimento orientado a modelos , quanto em uma abordagem de desenvolvimento tradicional . Para atingir o objetivo proposto foi desenvolvido um protótipo de uma ferramenta Web , a AWMo ( lê-se letra a letra : A-W-M-O ) , a partir da qual a edição de modelos pode ser realizada por meio de duas visões equivalentes : uma visão gráfica , na qual o engenheiro de software poderá inserir novos elementos no diagrama , posicioná-los e definir suas propriedades de modo visual ; e uma visão textual , na qual o engenheiro de software pode inserir novos elementos , propriedades e relacioná-los utilizando uma gramática textual . Um estudo de caso foi conduzido para avaliar sua eficácia e os resultados mostraram que a linguagem textual desenvolvida não representa uma barreira para a utilização da abordagem proposta pela AWMo . Os resultados sugerem que a AWMo é uma opção viável para facilitar o acesso de deficientes visuais a modelos de software , ajudando a promover a colaboração e comunicação efetiva e de maneira independente entre usuários com e sem visão para atividades de modelagem de 
 O desenvolvimento de software baseado no paradigma Orientado a Objetos ( OO ) e baseado em componentes é uma realidade . Este trabalho trata de teste e validação dentro desse contexto . Observa-se que diversos trabalhos_relacionados  ao teste de programas_OO  vêm_sendo  desenvolvidos . Apesar de ser um ponto controverso , alguns pesquisadores consideram que critérios de teste desenvolvidos para o teste de programas procedimentais podem ser facilmente estendidos para o teste de programas_OO  , pelo menos para o teste de métodos . Ainda são poucas as iniciativas de estender critérios de fluxo de dados e critérios baseados em mutação , tradicionalmente utilizados no teste de programas procedimentais , para o teste de programas_OO  . O presente_trabalho  visa a contribuir na identificação e definição de recursos de teste e validação que possam ser utilizados no teste de programas_OO  , com ênfase nos critérios de teste baseados em fluxo de dados e em mutação , cobrindo as fases do teste de unidade e de integração . Além_disso  , para apoiar a aplicação desses critérios , é de fundamental_importância  o desenvolvimento de ferramentas automatizadas que permitam a realização de estudos comparativos e a transferência tecnológica para a indústria . Em suma , o presente_trabalho  traz contribuições teóricas , com a definição de critérios de teste ; empírica , com a realização de estudos empíricos ; e de automatização , com a definição e implementação de um ambiente integrado de teste e validação para programas_OO  . Exemplos são utilizados para ilustrar as idéias e ferramentas apresentadas neste trabalho 

 Classificação é , provavelmente , a tarefa mais estudada na área de Aprendizado de Máquina , possuindo aplicação em uma grande quantidade de problemas reais , como categorização de textos , diagnóstico médico , problemas de bioinformática , além de aplicações comerciais e industriais . De um modo geral , os problemas de classificação podem ser categorizados quanto ao número de rótulos de classe que podem ser associados à cada exemplo de entrada . A abordagem mais investigada pela comunidade de Aprendizado de Máquina é a de classes mutuamente exclusivas . Entretanto , existe uma grande variedade de problemas importantes em que cada exemplo de entrada pode ser associado a mais de um rótulo ou classe . Esses problemas são denominados problemas de classificação multirrótulo . Os Learning Classifier Systems ( LCS ) constituem uma técnica de Indução de Regras de Classificação que tem como principal mecanismo de busca um Algoritmo Genético . Essa técnica busca encontrar um conjunto de regras que tenha alta precisão de classificação , que seja compreensível e que possua regras consideradas interessantes sob o ponto de vista de classificação . Apesar de existirem na literatura diversos trabalhos sobre os LCS para problemas de classificação com classes mutuamente exclusivas , pouco se tem conhecimento sobre um LCS que seja capaz de lidar com problemas multirrótulo . Dessa maneira , o objetivo desta monografia é apresentar uma proposta de LCS para problemas multirrótulo , que pretende induzir um conjunto de regras de classificação que produza um resultado eficaz e comparável com outras técnicas de classificação . De acordo com esse objetivo , apresenta-se também uma revisão bibliográfica dos temas envolvidos na proposta , que são : Sistemas Classificadores Evolutivos e Classificação 
 Na área de inteligência_artificial  existem algoritmos de aprendizado , notavelmente aqueles pertencentes à área de aprendizado de máquina AM , capazes de automatizar a extração do conhecimento implícito de um conjunto de dados . Dentre estes , os algoritmos de AM simbólico são aqueles que extraem um modelo de conhecimento inteligível , isto é , que pode ser facilmente interpretado pelo usuário . A utilização de AM simbólico é comum no contexto de classificação , no qual o modelo de conhecimento_extraído  é tal que descreve uma correlação entre um conjunto de atributos denominados premissas e um atributo particular denominado classe . Uma característica dos algoritmos de classificação é que , em geral , estes são utilizados visando principalmente a maximização das medidas de cobertura e precisão , focando a construção de um classificador genérico e preciso . Embora essa seja uma boa abordagem para automatizar processos de tomada de decisão , pode deixar a desejar quando o usuário tem o desejo de extrair um modelo de conhecimento que possa ser estudado e que possa ser útil para uma melhor compreensão do domínio . Tendo-se em vista esse cenário , o principal objetivo deste trabalho é pesquisar métodos de computação evolutiva multiobjetivo para a construção de regras de conhecimento individuais com base em critérios definidos pelo usuário . Para isso utiliza-se a biblioteca de classes e ambiente de construção de regras de conhecimento ECLE , cujo desenvolvimento remete a projetos anteriores . Outro objetivo deste trabalho consiste comparar os métodos de computação evolutiva pesquisados com métodos baseado em composição de rankings previamente existentes na ECLE . É mostrado que os métodos de computação evolutiva multiobjetivo apresentam melhores_resultados  que os métodos baseados em composição de rankings , tanto em termos de dominância e proximidade das soluções construídas com aquelas da fronteira Pareto-ótima quanto em termos de diversidade na fronteira de Pareto . Em otimização multiobjetivo , ambos os critérios são importantes , uma vez que o propósito da otimização multiobjetivo é fornecer não apenas uma , mas uma gama de soluções eficientes para o problema , das quais o usuário pode escolher uma ou mais soluções que apresentem os melhores compromissos entre os 
 A computação reconfigurável tem se tornado cada vez mais importante em sistemas computacionais embarcados e de alto_desempenho  . Ela permite níveis de desempenho próximos aos obtidos com circuitos integrados de aplicação específica ( ASIC ) , enquanto ainda mantém flexibilidade de projeto e implementação . No entanto , para programar eficientemente os dispositivos , é necessária experiência em desenvolvimento e domínio de linguagem de descrição de hardware ( HDL ) , tais como VHDL ou Verilog . As técnicas empregadas na compilação em alto nível ( por exemplo , a partir de programas em C ) ainda possuem muitos pontos em aberto a serem resolvidos antes que se possa obter resultados eficientes . Muitos esforços em se obter um mapeamento direto de algoritmos em hardware se concentram em loops , uma vez que eles representam as regiões computacionalmente mais intensivas de muitos programas . Uma técnica particularmente útil para isto é a de loop pipelining , a qual geralmente é adaptada de técnicas de software pipelining . A aplicação dessas técnicas está fortemente relacionada ao escalonamento das instruções , o que frequentemente impede o uso otimizado dos recursos presentes nos FPGAs modernos . Esta tese descreve uma abordagem alternativa para o mapeamento direto de loops descritos em uma linguagem de alto nível para FPGAs . Diferentemente de outras abordagens , esta técnica não é proveniente das técnicas de software pipelining . Nas arquiteturas obtidas o controle das operações é distribuído , tornando desnecessária uma máquina de estados finitos para controlar a ordem das operações , o que permitiu a obtenção de implementações eficientes . A especificação de um bloco de hardware é feita por meio de uma linguagem de domínio específico ( LALP ) , especialmente concebida para suportar a aplicação das técnicas . Embora a sintaxe da linguagem lembre C , ela contém certas construções que permitem intervenções do programador para garantir ou relaxar dependências de dados , conforme necessário , e assim otimizar o desempenho do hardware 
 Os múltiplos trabalhos de sistemas multiagentes musicais realizados nos últimos_anos  demonstram o interesse crescente na pesquisa de sistemas de composição e de performance musical que utilizem a tecnologia de agentes computacionais , sendo que apresentam um interesse maior por aqueles sistemas que integram técnicas de composição algorítmica , componentes de vida artificial e interatividade . Observamos também que a maioria dos trabalhos existentes apresentam muitas limitações em termos de escopo e flexibilidade , normalmente apresentando codificação musical simbólica e a resolução de um único problema , sendo que a motivação é mais técnica do que musical . Nesse contexto , surgem arcabouços voltados à criação de sistemas multiagentes musicais , como o Ensemble e o Interactive Swarm Orchestra , oferecendo flexibilidade para a modelagem e implementação de sistemas desse tipo , diversificando tanto os tipos de aplicação , tendo um propósito composicional ou performático , como os tipos de codificação musical que podem ser utilizados . Partimos da aparição dessas ferramentas para estudar o agente musical a partir de uma perspectiva interna , focando nos seus raciocínios , que são processos que definem o comportamento do agente no ambiente virtual do sistema e que são fundamentais para determinar e melhorar o seu valor composicional . Os arcabouços estudados se diferenciam por permitir a utilização de áudio como possível formato de codificação musical , o aproveitamento da espacialização sonora e a exploração da interatividade nos aplicativos , seja esta apenas entre agentes computacionais ou entre agentes e usuários humanos . Pretendemos portanto , nessa pesquisa , abordar sistemas com essas características . Através de extensões nos arcabouços e estudos de caso com motivação estética pretendemos dar continuidade a esses projetos e ao mesmo tempo validar e divulgar a sua utilização entre os potenciais usuários das ferramentas , como compositores , músicos interessados em performance e outros entusiastas dos sistemas musicais interativos 
 O problema da localização consiste em estimar a posição de um robô com relação a algum referencial externo e é parte essencial de sistemas de navegação de robôs e veículos_autônomos  . A localização baseada em odometria visual destaca-se em relação a odometria de encoders na obtenção da rotação e direção do movimento do robô . Esse tipo de abordagem é também uma escolha atrativa para sistemas de controle de veículos_autônomos  em ambientes urbanos , onde a informação visual é necessária para a extração de informações_semânticas  de placas , semáforos e outras sinalizações . Neste contexto este trabalho propõe o desenvolvimento de um sistema de odometria visual utilizando informação visual de uma câmera monocular baseado em reconstrução 3D para estimar o posicionamento do veículo . O problema da escala absoluta , inerente ao uso de câmeras monoculares , é resolvido utilizando um conhecimento prévio da relação métrica entre os pontos da imagem e pontos do mundo em um mesmo plano 
 Neste trabalho , apresentamos o desenvolvimento de ferramentas no programa iGeom - Geometria Interativa na Internet , para ensino-aprendizagem de Geometria , dando destaque aos recursos que facilitam a integração e uso deste programa , principalmente em ambientes de educação a distância via Internet 
 Atualmente , este tipo de programa é bastante conhecido e a Geometria que ele possibilita é usualmente denominada Geometria Dinâmica . Em poucas palavras , um programa de Geometria Dinâmica é a implementação computacional da régua e do compasso , permitindo que os objetos construídos sejam movidos mantendo-se às propriedades da construção 
 Dentre os principais recursos desenvolvidos , destacamos a autoria e a validação automática de exercícios e a comunicação com servidores , que podem ser utilizados para integrarem o iGeom em sistemas gerenciadores de cursos pela Web . Deste modo , se integrado a um sistema gerenciador , estes recursos podem ser utilizados para facilitar a tarefa do professor , que poderá criar exercícios diretamente pela Web e não precisará avaliar pessoalmente as respostas de cada aluno , e também para que o aluno saiba de imediato se sua solução está dentro do esperado pelo 
 Neste trabalho foi estudado um problema de dimensionamento de lotes em uma indústria química brasileira , cujo_objetivo  era determinar o tamanho dos lotes dos produtos para atender às demandas , minimizando os custos produtivos . Os itens podem ser produzidos em máquinas_paralelas  distintas , através de diferentes processos , e devem ser armazenados em taques cativos , exclusivos a um produto , ou multipropósitos , compartilhado entre produtos , desde que não simultaneamente . Foram propostos dois modelos matemáticos de programação inteira_mista  para representar o problema , o primeiro apresentava uma função objetivo compreendendo o preço das matérias-primas consumidas nas reações , os gastos com a estocagem de produtos e o custo de descarte de produtos quando os tanques de armazenamento não tiverem capacidade suficiente para armazená-los , já o segundo estendendo este modelo para considerar custos de preparação de máquina . Experimentos computacionais com os modelos propostos , utilizando instâncias geradas a partir dos dados fornecidos pela empresa , mostraram que o software de otimização empregado foi capaz de resolver poucas instâncias , após uma hora de processamento . Portanto , foram propostas heurísticas construtivas do tipo LP-and-fix e relax-and-fix , além de heurísticas de melhoria do tipo fix-and-optimize . Após serem realizados testes com essas heurísticas , constatou-se que algumas proporcionaram a obtenção de soluções factíveis de boa_qualidade  , quando comparadas às obtidas pelo software , sendo ainda capazes de resolver um maior número de 
 Fluxos de dados são usualmente caracterizados por grandes quantidades de dados gerados continuamente em processos síncronos ou assíncronos potencialmente infinitos , em aplicações como : sistemas meteorológicos , processos industriais , tráfego de veículos , transações financeiras , redes de sensores , entre outras . Além_disso  , o comportamento dos dados tende a sofrer alterações significativas ao longo do tempo , definindo data streams evolutivos . Estas alterações podem significar eventos temporários ( como anomalias ou eventos extremos ) ou mudanças relevantes no processo de geração da stream ( que resultam em alterações na distribuição dos dados ) . Além_disso  , esses conjuntos de dados podem possuir características espaciais , como a localização geográfica de sensores , que podem ser úteis no processo de análise . A detecção dessas variações de comportamento que considere os aspectos da evolução_temporal  , assim como as características espaciais dos dados , é relevante em alguns tipos de aplicação , como o monitoramento de eventos climáticos extremos em pesquisas na área de Agrometeorologia . Nesse contexto , esse projeto de mestrado_propõe  uma técnica para auxiliar a análise espaço-temporal em data streams multidimensionais que contenham informações espaciais e não espaciais . A abordagem adotada é baseada em conceitos da Teoria de Fractais , utilizados para análise de comportamento temporal , assim como técnicas para manipulação de data streams e estruturas de dados hierárquicas , visando permitir uma análise que leve em consideração os aspectos espaciais e não espaciais simultaneamente . A técnica desenvolvida foi aplicada a dados agrometeorológicos , visando identificar comportamentos distintos considerando diferentes sub-regiões definidas pelas características espaciais dos dados . Portanto , os resultados deste trabalho incluem contribuições para a área de mineração de dados e de apoio a pesquisas em Agrometeorologia 
 Este trabalho apresenta um sistema de auxilio à navegação_autônoma  para veículos terrestres com foco em ambientes estruturados em um cenário agrícola . É gerada a estimativa das posições dos obstáculos baseado na fusão das detecções provenientes do processamento dos dados de duas câmeras , uma estéreo e outra térmica . Foram desenvolvidos três_módulos  de detecção de obstáculos . O primeiro módulo utiliza imagens monoculares da câmera estéreo para detectar novidades no ambiente através da comparação do estado atual com o estado anterior . O segundo módulo utiliza a técnica Stixel para delimitar os obstáculos acima do plano do chão . Por fim , o terceiro módulo utiliza as imagens térmicas para encontrar assinaturas que evidenciem a presença de obstáculo . Os módulos de detecção são fundidos utilizando a Teoria de Dempster-Shafer que fornece a estimativa da presença de obstáculos no ambiente . Os experimentos foram executados em ambiente agrícola real . Foi executada a validação do sistema em cenários bem iluminados , com terreno irregular e com obstáculos diversos . O sistema apresentou um desempenho satisfatório tendo em vista a utilização de uma abordagem baseada em apenas três_módulos  de detecção com metodologias que não tem por objetivo priorizar a confirmação de obstáculos , mas sim a busca de novos obstáculos . Nesta dissertação são apresentados os principais componentes de um sistema de detecção de obstáculos e as etapas necessárias para a sua concepção , assim como resultados de experimentos com o uso de um veículo real 
 Atualmente há uma demanda crescente por aplicações_hipermídia  baseadas na WWW ( World_Wide  Web ) , conhecidas como WIS ( Web Information Systems ) . Esse novo tipo de aplicação apresenta requisitos adicionais aos sistemas de software clássicos , o que resulta na necessidade de investigar modelos mais adequados para apoiar o seu desenvolvimento . Em especial , os sistemas para apoio ao EaD ( Ensino a Distância ) baseados na Web apresentam características e requisitos ainda mais específicos . Os modelos atuais para modelagem e especificação de hiperdocumentos não são completamente adequados para representar características deste domínio , como a necessidade de acompanhamento dos aprendizes e a realização de avaliações diagnósticas e formativas . Isso motivou a proposta de um modelo para apoiar WISs voltados especificamente para EaD , denominado MDE ( Modelo para Documentos Educacionais ) , que estende o modelo HMBS ( Hyperdocument Model Based on Statecharts ) para a descrição de conteúdo nesse domínio . O MDE adota como modelo formal subjacente uma variante da técnica Statecharts , cuja estrutura e semântica operacional possibilitam especificar a estrutura organizacional e a semântica navegacional de hiperdocumentos complexos . Adicionalmente , foi integrada ao MDE a técnica de mapeamento conceitual , que acrescenta um significado educacional aos grafos hierárquicos . Dessa forma , o modelo apresenta como pontos fortes a possibilidade de captar várias informações relevantes do comportamento do usuário no estudo do material disponível on-line e a disponibilização desses dados ao instrutor , como importante apoio à avaliação formativa . Como prova de conceito , foi desenvolvido o protótipo de um ambiente para autoria e oferecimento de cursos denominado ATEnA ( Ambiente para Treinamento , Ensino e Aprendizagem ) . Esta tese apresenta também avaliações conceituais e práticas do modelo e do protótipo desenvolvidos 
 O problema integrado de programação de produção e dimensionamento de lotes em ambiente fowshop consiste em estabelecer tamanhos de lotes de produção e alocar máquinas para processá-los dentro de um horizonte de planejamento , em uma linha de produção com máquinas dispostas em série . O problema considera que a demanda deve ser atendida sem atrasos , que a capacidade das máquinas deve ser respeitada e que as preparações de máquinas são dependentes da sequência de produção e preservadas entre períodos do horizonte de planejamento . O objetivo é determinar uma programação de produção visando minimizar os custos de preparação de máquina , de produção e de estoque . Um modelo matemático da literatura é apresentado assim como procedimentos para obtenção de limitantes inferiores . Além_disso  , abordamos o problema por meio de distintas versões da metaheurística Times Assíncronos ( A-Teams ) . Os procedimentos propostos foram comparados com heurísticas da literatura baseadas em Programação Inteira Mista ( MIP ) . As metodologias desenvolvidas e os resultados obtidos são apresentados nesta 
 A robótica_móvel  é uma área de pesquisa que está obtendo grande atenção da comunidade_científica  . O desenvolvimento de robôs_móveis  autônomos , que sejam capazes de interagir com o ambiente , aprender e tomar decisões corretas para que suas tarefas sejam executadas com êxito é o maior desafio em robótica_móvel  . O desenvolvimento destes sistemas inteligentes e autônomos consiste em uma área de pesquisa multidisciplinar considerada recente e extremamente promissora que envolve ; por exemplo , inteligência_artificial  , aprendizado de máquina , estimação estatística e sistemas embarcados . Dentro desse contexto , esse trabalho aborda o problema de navegação e monitoramento de ambientes utilizando robôs_móveis  . Dada uma representação do ambiente ( mapa topológico ) e uma lista com urgências de cada uma das regiões do mapa , o robô deve estimar qual o percurso mais eficiente para monitorar esse ambiente . Uma vez que a urgência de cada região não visitada aumenta com o tempo , o trajeto do robô deve se adaptar a essas alterações . Entre as diversas aplicações práticas desse tipo de algoritmo , destaca-se o desenvolvimento de sistemas de segurança móveis 
 Uma forma de extrair e organizar o conhecimento , que tem recebido muita atenção nos últimos_anos  , é por meio de uma representação estrutural dividida por tópicos hierarquicamente relacionados . Uma vez construída a estrutura hierárquica , é necessário encontrar descritores para cada um dos grupos obtidos pois a interpretação destes grupos é uma tarefa complexa para o usuário , já que normalmente os algoritmos não apresentam descrições conceituais simples . Os métodos encontrados na literatura consideram cada documento como uma bag-of-words e não exploram explicitamente o relacionamento existente entre os termos dos documento do grupo . No entanto , essas relações podem trazer informações importantes para a decisão dos termos que devem ser escolhidos como descritores dos nós , e poderiam ser representadas por regras de associação . Assim , o objetivo deste trabalho é avaliar a utilização de regras de associação para apoiar a identificação de descritores para agrupamentos hierárquicos . Para isto , foi proposto o método SeCLAR ( Selecting Candidate Labels using Association Rules ) , que explora o uso de regras de associação para a seleção de descritores para agrupamentos hierárquicos de documentos . Este método gera regras de associação baseadas em transações construídas à partir de cada documento da coleção , e utiliza a informação de relacionamento existente entre os grupos do agrupamento hierárquico para selecionar candidatos a descritores . Os resultados da avaliação_experimental  indicam que é possível_obter  uma melhora significativa com relação a precisão e a cobertura dos métodos 
 As recentes pesquisas na área de Gestão de Processos de Negócio ( GPN ) vêm contribuindo para aumentar a eficiência nas organizações . A GPN pode ser compreendida como o conjunto de métodos , técnicas e ferramentas computacionais desenvolvidas para amparar os processos de negócios . Tipicamente , a GPN é fundamentada por modelos de processos . Esses modelos , além de permitirem a automação da configuração e execução , aumentam a capacidade de análise dos processos de negócio . Apesar de auxiliar os especialistas de negócio nas diferentes fases envolvidas no ciclo de vida de um processo de negócio ( projeto , configuração , implantação/execução e a análise ) , os modelos definidos em linguagens específicas de domínio , como a BPMN ( Business Process Model and Notation ) , não são os mais apropriados para amparar a fase de análise . De formal geral , esses modelos não possuem uma semântica operacional formalmente definida ( o que limita o seu uso para a verificação e validação dos processos ) e nem mecanismos para quantificar o comportamento modelado ( o que impossibilita a análise de desempenho ) . Neste trabalho de doutorado , nós desenvolvemos um arcabouço que ampara e automatiza os principais passos envolvidos na análise de desempenho de processos de negócio via modelagem analítica . Nós estudamos a viabilidade da aplicação de três formalismos Markovianos na modelagem de processos de negócio : as Redes de Petri Estocásticas , as Álgebras de Processo Estocásticas e as Redes de Autômatos Estocásticos ( SAN , do inglês Stochastic Automata Networks ) . Escolhemos SAN como formalismo base para o método proposto neste trabalho . Nosso arcabouço é constituído por : ( i ) uma notação para enriquecer modelos de processos de negócio descritos em BPMN com informações sobre o seu gerenciamento de recursos , e ( ii ) um algoritmo que faz a conversão automática desses modelos não-formais de processos para modelos estocásticos em SAN . Com isso , somos capazes de capturar o impacto causado pela contenção de recursos no desempenho de um processo de negócio . A partir de um modelo em SAN gerado com o nosso arcabouço , podemos predizer variados índices de desempenho que são boas aproximações para o desempenho esperado do processo de negócio no mundo_real  
 A disponibilização de técnicas efetivas para a Visualização Exploratória e Mineração Visual de conjuntos de dados volumosos e de alta dimensionalidade ainda representa um grande desafio para os pesquisadores . Apesar das muitas contribuições e dos avanços recentes , técnicas convencionais de visualização direta - em que cada elemento do conjunto de dados é mapeado em um elemento visual na tela - ainda são bastante limitadas nesse contexto . Neste trabalho técnicas de visualização exploratória foram revisitadas , e são enriquecidas com algumas estratégias que permitem gerar representações_visuais  mais efetivas para apoiar processos interativos de exploração de grandes_conjuntos  de dados de alta dimensionalidade . As técnicas introduzidas apresentam baixa complexidade computacional , de modo a não prejudicar ou inviabilizar a interação do usuário com as visualizações , na busca de padrões como agrupamentos e regiões de alta densidade nos dados , e podem ser caracterizadas como quatro contribuições distintas : ( i ) primeiramente , é apresentada uma nova_técnica  de visualização multi-dimensional , denominada VizJD , que projeta os dados em três dimensões espaciais e assim ameniza o efeito indesejado da excessiva sobreposição dos marcadores , típica de estratégias bidimensionais , como o RadViz e Star Coordinates , oferecendo uma alternativa de baixo custo para a projeção de dados multidimensionais no espaço 3D ; ( ii ) técnicas de visualização direta , como Coordenadas Paralelas , RadViz e o próprio VizJD , são enriquecidas com informações de frequência e densidade computadas a partir dos dados , as quais são mapeadas visualmente buscando reduzir o efeito prejudicial de uma excessiva sobreposição dos marcadores , atenuar os efeitos indesejados da presença de ruído nos dados e ainda realçar a presença de padrões nos dados ; ( iii ) é introduzido um procedimento interativo para apoiar a identificação de agrupamentos nos dados , o qual é apoiado nas visualizações enriquecidas com informação de frequência e densidade introduzidas anteriormente : ( iv ) finalmente , é apresentada uma estratégia simples para configurar a disposição dos atributos de dados mapeados na visualização , a qual resulta em visualizações de melhor qualidade e mais informativas no caso de dados de alta dimensionalidade . As propostas apresentadas são comparadas a trabalhos_relacionados  existentes na literatura , e avaliadas por meio de sua aplicação a vários conjuntos de dados reais e sintéticos , de vários tamanhos e dimensionalidades . Os resultados indicam que , apesar das estratégias propostas não eliminarem os problemas associados à oclusão e congestionamento visual na visualização de grandes_conjuntos  de dados , elas oferecem soluções efetivas e de baixo custo que ampliam a escalabilidade visual de técnicas tradicionais , viabilizando tarefas de exploração e mineração que seriam impossíveis , ou muito dificultadas , sem esses recursos 
 Nesse trabalho foi desenvolvido o método VBSeg , um método de segmentação semiautomático de corpos vertebrais , que utiliza superpixels para aumentar a eficiência de técnicas de segmentação de imagens já estabelecidas na literatura , sem perder qualidade do resultado final . Experimentos mostraram que o uso de superpixels melhorou o resultado da segmentação dos corpos vertebrais em até 18 % , além de aumentar a eficiência desses métodos , deixando a execução dos algoritmos de segmentação pelo menos 38 % mais rápida . Além_disso  , o método desenvolvido possui baixa dependência do nível de especialidade do usuário e apresentou resultados comparáveis ao método Watershed , um método bem estabelecido na área de segmentação de imagens . Contudo , o método VBSeg segmentou 100 % dos corpos vertebrais das imagens analisadas , enquanto que o método Watershed deixou de segmentar 44 % dos corpos 
 Reconciliação é o processo de prover uma visão consistente de dados provenientes de várias fontes de dados . Embora existam na literatura trabalhos voltados à proposta de soluções de reconciliação baseadas em colaboração assíncrona , o desafio de reconciliar dados quando vários usuários colaborativos trabalham de forma assíncrona sobre as mesmas cópias locais de dados , compartilhando somente eventualmente as suas decisões de integração particulares , tem recebido menos atenção . Nesta tese de doutorado investiga-se esse desafio , por meio da proposta do modelo AcCORD ( Asynchronous COllaborative data ReconcIliation moDel ) . AcCORD é um modelo colaborativo assíncrono para reconciliação de dados no qual as atualizações dos usuários são mantidas em um repositório de operações na forma de dados de procedência . Cada usuário tem o seu próprio repositório para armazenar a procedência e a sua própria cópia das fontes . Ou seja , quando inconsistências entre fontes importadas são detectadas , o usuário pode tomar decisões de integração para resolvê-las de maneira autônoma , e as atualizações que são executadas localmente são registradas em seu próprio repositório . As atualizações são compartilhadas entre colaboradores quando um usuário importa as operações dos repositórios dos demais usuários . Desde que diferentes usuários podem ter diferentes pontos de vista para resolver o mesmo conflito , seus repositórios podem_estar  inconsistentes . Assim , o modelo AcCORD também inclui a proposta de diferentes políticas de reconciliação multiusuário para resolver conflitos entre repositórios . Políticas distintas podem ser aplicadas por diferentes usuários para reconciliar as suas atualizações . Dependendo da política aplicada , a visão final das fontes importadas pode ser a mesma para todos os usuários , ou seja , um única visão global integrada , ou resultar em distintas visões locais para cada um deles . Adicionalmente , o modelo AcCORD também incorpora um método de propagação de decisões de integração , o qual tem como objetivo evitar que um usuário tome decisões inconsistentes a respeito de um mesmo conflito de dado presente em diferentes fontes , garantindo um processo de reconciliação multiusuário mais efetivo . O modelo AcCORD foi validado por meio de testes de desempenho que avaliaram as políticas propostas , e por entrevistas a usuários que avaliaram não somente as políticas propostas mas também a qualidade da reconciliação multiusuário . Os resultados obtidos demonstraram a eficiência e a eficácia do modelo proposto , além de sua flexibilidade para gerar uma visão integrada ou distintas visões locais . As entrevistas realizadas demonstraram diferentes percepções dos usuários quanto à qualidade do resultado provido pelo modelo AcCORD , incluindo aspectos_relacionados  à consistência , aceitabilidade , corretude , economia de tempo e satisfação 
 Esta dissertação apresenta recursos voltados para o desenvolvimento de aplicações de reconhecimento de fala e avaliação de pronúncia . São quatro as contribuições aqui discutidas . Primeiro , um conversor grafema-fonema híbrido para o Português Brasileiro , chamado Aeiouadô , o qual utiliza regras de transcrição fonética e Classification and Regression Trees ( CART ) para inferir os fones da fala . Segundo , uma ferramenta de correção automática baseada em aprendizado de máquina , que leva em conta erros de digitação de origem fonética , que é capaz de lidar com erros contextuais e emprega as transcrições geradas pelo Aeiouadô . Terceiro , um método para a extração de sentenças foneticamente-ricas , tendo em vista a criação de corpora de fala , baseado em algoritmos gulosos . Quarto , um protótipo de um sistema de reconhecimento e correção de fala não-nativa , voltado para o Inglês falado por aprendizes brasileiros 
 Um dos padrões mais importantes que ocorrem em ecossistemas é a relação espécie-área , que relaciona o número de espécies em um ecossistema com a sua área disponível . O estudo dessa relação é fundamental para entender-se a biodiversidade e o impacto de políticas ambientais de preservação de espécies , de modo que é possível analisar desde os tamanhos das reservas necessários para a conservação das espécies e até verificar o impacto da intervenção humana em habitats naturais . Assim sendo , várias estratégias matemáticas e computacionais foram desenvolvidas para prever e entender esse padrão ecológico em modelos ecológicos . Todavia , muitas abordagens são simuladas em ambientes homogêneos e regulares , porém , sabe-se que , em cada ecossistema , há regiões com acidentes geográficos , variações de altitudes , vegetação e clima . Dessa forma , nesse trabalho , estamos interessados em estudar a influência de diferentes ambientes no processo de evolução das espécies . Para isso , consideramos modelos ecológicos que utilizam características geográficas para colonização e , comportamentos individuais como dispersão , mutação , acasalamento . Com isso , foi possível simular a propagação das espécies em diferentes topologias e analisar como ocorreu a dinâmica em cada uma delas . Assim , verificamos que a topologia regular e a dispersão homogênea dos indivíduos são duas características que maximizam a diversidade de espécies . E por outro_lado  , a formação de regiões mais densas e interações heterogêneas , contribuem para a diminuição da quantidade de espécies , apesar de em alguns_casos  , ajudarem na velocidade de propagação e colonização 
 O operador de Divisão ( ÷ ) da Álgebra Relacional permite representar de forma simples consultas com o conceito de para todos , e por isso é requerido em diversas aplicações reais . Entretanto , evidencia-se neste trabalho de mestrado que a divisão não atende às necessidades de diversas aplicações atuais , principalmente quando estas analisam dados complexos , como imagens , áudio , textos longos , impressões digitais , entre outros . Analisando o problema verifica-se que a principal limitação é a existência de comparações de valores de atributos intrínsecas à Divisão Relacional , que , por definição , são efetuadas sempre por identidade ( = ) , enquanto objetos complexos devem geralmente ser comparados por similaridade . Hoje , encontram-se na literatura propostas de operadores relacionais com suporte à similaridade de objetos complexos , entretanto , nenhuma trata a Divisão Relacional . Este trabalho de mestrado_propõe  investigar e estender o operador de Divisão da Álgebra Relacional para melhor adequá-lo às demandas de aplicações atuais , por meio de suporte a comparações de valores de atributos por similaridade . Mostra-se aqui que a Divisão por Similaridade é naturalmente adequada a responder consultas diversas com um conceito de elementos candidatos e exigências descrito na monografia , envolvendo dados complexos de aplicações reais de alto impacto , com potencial por exemplo , para apoiar a agricultura , análises de dados genéticos , buscas em bibliotecas digitais , e até mesmo para controlar a qualidade de produtos manufaturados e a identificação de novos clientes em indústrias . Para validar a proposta , propõe-se estudar as duas primeiras aplicações citadas 
 Dada uma grande base de dados de dimensionalidade moderada a alta , como identificar padrões úteis nos objetos de dados ? Nesses casos , a redução de dimensionalidade é essencial para superar um fenômeno conhecido na literatura como a maldição da alta dimensionalidade . Embora existam algoritmos capazes de reduzir a dimensionalidade de conjuntos de dados na escala de Terabytes , infelizmente , todos falham em relação à identificação/eliminação de correlações não lineares entre os atributos . Este trabalho de Mestrado trata o problema explorando conceitos da Teoria de Fractais e processamento paralelo em massa para apresentar Curl-Remover , uma nova_técnica  de redução de dimensionalidade bem adequada ao pré-processamento de Big Data . Suas principais_contribuições  são : ( a ) Curl-Remover elimina correlações lineares e não lineares entre atributos , bem como atributos irrelevantes ; ( b ) não depende de supervisão do usuário e é útil para tarefas analíticas em geral não apenas para a classificação ; ( c ) apresenta escalabilidade linear tanto em relação ao número de objetos de dados quanto ao número de máquinas utilizadas ; ( d ) não requer que o usuário sugira um número de atributos para serem removidos , e ; ( e ) mantêm a semântica dos atributos por ser uma técnica de seleção de atributos , não de extração de atributos . Experimentos foram executados em conjuntos de dados sintéticos e reais contendo até 1,1 bilhões de pontos , e a nova_técnica  Curl-Remover apresentou desempenho_superior  comparada a dois algoritmos do estado da arte baseados em PCA , obtendo em média até 8 % a mais em acurácia de resultados 
 As metodologias ágeis e em especial a Programação eXtrema ( XP ) surgem como um contraponto aos métodos tradicionais de desenvolvimento de software . Nos encontramos em um momento no qual considera-se aceitável encontrar defeitos em programas de computador , até mesmo naqueles sistemas pelos quais temos que pagar muito dinheiro . Melhorar o ensino de técnicas para que equipes possam colaborar no desenvolvimento de software de qualidade é essencial para que esta área do conhecimento alcance a maturidade que esperamos . O ensino de XP é uma tarefa relativamente complexa pois exige que pessoas passem por uma mudança cultural , para aceitar seus valores , princípios e práticas . Diferentes organizações precisam adaptar a metodologia para que ela funcione bem em seu contexto local . Encontrar maneiras de facilitar o ensino e a adoção das práticas ágeis é fundamental para melhorar a qualidade do software desenvolvido no país . Este trabalho pesquisa o ensino de XP em contextos acadêmicos , governamentais e industriais . Três estudos de caso foram conduzidos e analisados para sugerir padrões que podem auxiliar o ensino da metodologia por um educador em qualquer contexto 
 Esta tese propõe um modelo de navegação exploratória para a infra-estrutura da Web Semântica , denominado Navigation and Exploration Model ( NAVE ) . O modelo NAVE foi desenvolvido com base na literatura de information searching , nos níveis de atividades de information seeking , e na estratégia de orienteering . O objetivo é facilitar o projeto e desenvolvimento de sistemas de navegação exploratória . O NAVE é descrito por meio de uma representação gráfica dos estágios e decisões do processo de navegação e suas respectivas técnicas de suporte à navegação , além de recomendações . Um sistema , denominado de Exploratory Navigation System ( ENS ) , foi desenvolvido para avaliar a viabilidade de utilizar o modelo NAVE em aplicações reais . O sistema ENS é composto de diversas ferramentas de navegação que permitem ao usuário escolher a ferramenta adequada , ou a melhor combinação de ferramentas , provavelmente ajustada ao seu nível de habilidade e conhecimento , à sua preferência , e ao tipo de informação que ele está procurando no momento . O sistema permite ao usuário priorizar de maneiras diferentes as suas escolhas de ferramentas em cada passo de uma estratégia de orienteering , subjacente ao modelo NAVE . Essas ferramentas podem apresentar vantagens complementares no contexto de uma tarefa de information searching . O sistema ENS foi avaliado utilizando uma abordagem tanto qualitativa quanto quantitativa , que serviram para refinar as questões de pesquisa e explorar o modelo NAVE . Primeiro , um estudo de usabilidade foi conduzido que combinou vários métodos , como questionários , think-aloud , entrevistas , e registro da interação do usuário . Esse estudo forneceu informações com relação às ferramentas e o modelo NAVE subjacente , as quais foram consideradas no seu desenvolvimento . Segundo , um estudo_experimental  foi conduzido para comparar o ENS com uma abordagem de busca por palavra-chave . Os resultados forneceram indicações estatísticas de que os participantes tiveram desempenho_superior  utilizando o 
 Ruído pode ser definido como um exemplo em um conjunto de dados que aparentemente é inconsistente com o restante dos dados existentes , pois não segue o mesmo padrão dos demais . Ruídos em conjuntos de dados podem reduzir o desempenho das técnicas de Aprendizado de Máquina ( AM ) empregadas e aumentar o tempo de construção da hipótese induzida , assim como sua complexidade . Dados são geralmente coletados por meio de medições realizadas em um domínio de interesse . Nesse sentido , nenhum conjunto de dados é perfeito . Erros de medições , dados incompletos , errados , corrompidos ou distorcidos , falhas humanas ou dos equipamentos utilizados , dentre muitos outros fatores , contribuem para a contaminação dos dados , e isso é particularmente verdadeiro para dados com elevada dimensionalidade . Sendo_assim  , a detecção de ruídos é uma tarefa crítica , principalmente em ambientes que exigem segurança e confiabilidade , uma vez que a presença desses pode indicar situações que degradam o desempenho do sistema ou a segurança e confiabilidade das informações . Algoritmos para a detecção e remoção de ruídos podem aumentar a confiabilidade de conjuntos de dados ruidosos . Nesse âmbito , esse trabalho_investiga  técnicas de detecção de ruído baseadas em distância , em que a remoção de ruídos é feita em uma etapa de pré-processamento , aplicadas a problemas de classificação de dados de Expressão Gênica , caracterizados pela presença de ruídos , elevada dimensionalidade e complexidade . O objetivo é melhorar o desempenho das técnicas de AM empregadas para solucioná-los . Por fim , combinações de técnicas de detecção de ruído são implementadas de modo a analisar a possibilidade de melhorar , ainda mais , o desempenho obtido 
 A simulação_numérica  de escoamentos de fluidos em uma grande variedade de aplicações requer a utilização de técnicas numéricas de alta eficência e recursos_computacionais  de alto_desempenho  . O objetivo deste trabalho é iniciar uma investigação de escoamentos de fluido durante o enchimento de compartimentos de reservatórios . Uma abordagem inicial foi tratar problemas de escoamento em um canal , rebuscando a geometria do domínio para contemplar problemas mais complexos . Este trabalho apresenta o desenvolvimento e os resultados obtidos de um método_numérico  para simulação de escoamento de fluido incompressível em um domínio tridimensional , onde as equações de Navier-Stokes são desenvolvidas em uma formulação euleriana e discretizadas pelo método de elementos finitos . Os termos convectivos destas equações foram tratados pelo método semi-lagrangeano e o método de Galerkin foi utilizado para discretização espacial , um método baseado em decomposição LU foi utilizado para desacoplar as componentes de velocidade e pressão , sendo esta última calculada utilizando-se uma aproximação hidrostática . O domínio tridimensional foi representado por uma malha manipulada por uma estrutura de dados topológica , formada por células que definem elementos prismáticos lineares . Foram realizados_experimentos  sob várias alterações na geometria do domínio e também sob diferentes condições iniciais . Os resultados mostraram uma boa aproximação do método , quando analisado comparativamente a uma solução 
 O escoamento em torno de corpos tem sido objeto de estudo de muitos pesquisadores e é muito explorado experimental e computacionalmente , devido a sua grande aplicabilidade na engenharia . No entanto , simular computacionalmente este tipo de escoamento requer uma atenção especial ao escolher o tipo malha a ser utilizado . Em muitos_casos  faz-se necessário o uso de uma malha que se adapte ao contorno do obstáculo , o que pode ocasionar um aumento no esforço computacional . Um maneira de contornar este problema é a utilização do Método das Fronteiras Imersas , que possibilita o uso de malha cartesiana na simulação computacional do escoamento em torno de obstáculos . Isso é possível através da adição de um termo forçante nas equações que modelam o escoamento , e assim as forças que agem sobre o contorno do corpo são transferidas diretamente para a malha . O objetivo deste trabalho de mestrado foi implementar o método das Fronteiras Imersas e simular o escoamento em torno de um cilindro circular em repouso , movimentando-se na mesma direção do escoamento , na direção perpendicular ao escoamento , ou rotacionando em torno do próprio eixo . As simulações computacionais possibilitaram a captura do fenômeno de Atrelagem Síncrona , caracterizado pela sincronia entre a frequência de desprendimento natural de vórtices e a frequência de oscilação do mesmo . O Método das Fronteiras Imersas mostrou um ótimo desempenho quando comparado a resultados experimentais e numéricos encontrados na 
 Neste trabalho , uma nova_técnica  de reconstrução tridimensional é apresentada . Esta técnica oferece uma família de modelos , governada por um parâmetro chamado β , e que pode ser obtida a partir de um mesmo conjunto de seções planares . Este fato faz possível múltiplas opções para o problema da correspondência , tarefa_difícil  de ser realizada por outros métodos de reconstrução . Além da flexibilidade na conexão entre regiões de níveis consecutivos , o algoritmo produz LP-variedades e respeita a condição de reexemplificação 
 Neste trabalho especificam-se os recursos lingüísticos e computacionais necessários à implementação de um realizador monosentencial para o português a partir de uma linguagem de representação semântica . Para desenvolver este gerador necessita-se de uma teoria lingüística que descreva os recursos da língua em questão , e também de um formalismo computacional que represente e manipule estes recursos lingüísticos em um ambiente computacional . Nesta dissertação , com o intuito de atender a estes requisitos , será proposto o uso de teorias lingüísticas baseadas em restrições e o uso de um sistema de geração baseado em unificação . A linguagem de representação semântica utilizada é a UNL ( Universal Networking Language ) . Sendo_assim  , o gerador aqui proposto deverá converter , uma por vez , cada uma fins sentenças da UNL em orações gramaticalmente corretas do português 
 UMA tendência no projeto de sistemas hipemiidia é o desenvolvimento de sistemas que sejam abertos , extensíveis e distribuídos entre diferentes usuários . Na última_década  vários sistemas hipermidia abertos foram apresentados na literatura . Nesse contexto , técnicas formais consistem de ferramentas muito úteis para especificação de aplicações_hipermídia  ( e por conseguinte , de aplicações em sistemas hipermídia abertos ) por possibilitar que determinada solução especificada seja precisa , não ambígua , independente de sua implementação e reutilizável . Além_disso  , modelos formais adequados podem oferecer abordagens sistemáticas e confiáveis para a análise e verificação de propriedades estruturais e dinâmicas dessas aplicações . Este projeto de mestrado tem como objetivo verificar a adequação do modelo formal XHMBS ( eXtended Hypertext Model Based ou Statecharts ) em relação à especificação de aplicações em sistemas hiperrnídia abertos e propor extensões a esse modelo para permitir a especificação de tais aplicações 
 Neste trabalho de mestrado foram desenvolvidas duas arquiteturas de controlador PID ( Proporcional , Integral e Derivativo ) digital para controle de motores CC . O controlador PID foi desenvolvido para ser integrado a um sistema mais complexo de gerenciamento de um robô . O controlador PID permite que se obtenha respostas mais rápidas e precisas do sistema a ser controlado , que , no caso deste trabalho , é um motor . Por ser relativamente simples de implementar e também por possuir parâmetros de fácil sintonia , o PID é o tipo de controle mais utilizado para esta classe de aplicação . Para o desenvolvimento do controlador foram utilizadas metodologias e ferramentas avançadas para projeto de hardware , tendo como alvo da implementação , os dispositivos reconfiguráveis FPGA ( Field Programmable Gate Arrays ) . Os resultados de desempenho apresentados pelas arquiteturas e a flexibilidade de projeto proporcionados pela implementação em FPGAs indicam que a metodologia escolhida é adequada para esta 
 Um data warehouse geográco ( DWG ) é um banco de dados multidimensional , orientado a assunto , integrado , histórico , não-volátil e geralmente organizado em níveis de agregação . Além_disso  , também armazena dados espaciais em uma ou mais dimensões ou em pelo menos uma medida numérica . Visando oferecer suporte à tomada de decisão , é possível realizar em DWGs consultas SOLAP ( spatial online analytical processing ) , isto é , consultas analíticas multidimensionais ( e.g. , drill-down , roll-up , drill-across ) com predicados espaciais ( e.g. , intersecta , contém , está contido ) denidos para range queries e junções espaciais . Um desafio no processamento dessas consultas é recuperar , de forma eficiente , dados espaciais e convencionais em DWGs muito volumosos . Na literatura , existem poucos índices voltados à indexação de DWGs , e ainda assim nenhum desses índices dedica-se a indexar consultas SOLAP drill-across e com junção espacial . Esta dissertação visa suprir essa limitação , por meio da proposta de estratégias para o processamento dessas consultas complexas . Para o processamento de consultas SOLAP drill-across foram propostas duas estratégias , Divide e Única , além da especicação de um conjunto de diretrizes que deve ser seguido para o projeto de um esquema de DWG que possibilite a execução dessas consultas e da especicação de classes de consultas . Para o processamento de consultas SOLAP com junção espacial foi proposta a estratégia SJB , além da identicação de quais características o esquema de DWG deve possuir para possibilitar a execução dessas consultas e da especicação do formato dessas consultas . A validação das estratégias propostas foi realizada por meio de testes de desempenho considerando diferentes congurações , sendo que os resultados obtidos foram contrastados com a execução de consultas do tipo junção estrela e o uso de visões materializadas . Os resultados mostraram que as estratégias propostas são muito eficientes . No processamento de consultas SOLAP drill-across , as estratégias Divide e Única mostraram uma redução no tempo de 82,7 % a 98,6 % com relação à junção estrela e ao uso de visões materializadas . No processamento de consultas SOLAP com junção espacial , a estratégia SJB garantiu uma melhora de desempenho na grande maioria das consultas executadas . Para essas consultas , o ganho de desempenho variou de 0,3 % até 99,2 
 Nesta tese é apresentado um esquema novo de alta_resolução  upwind ( denominado TDPUS-C3 ) para reconstrução de fluxos numéricos para leis de conservação não lineares e problemas relacionados em DFC . O esquema é baseado nos critérios de estabilidade CBC e TVD e desenvolvido utilizando condições de diferenciabilidade 'C POT . 3 ' . Além_disso  , é realiozada a implementação da associação do esquema TDPLUS-C3 com a modelagem de turbulência RNG '\kappa - \epsilon ' . O propósito é obter soluções numéricas de sistemas hiperbólicos de leis de conservação para dinâmica dos gases e equações de Navier-Stokes para escoamento incompreensível de fluidos newtonianos e não newtonianos ( viscoelásticos ) . Fazendo o uso do esquema TDPUS-C3 , a precisão global dos métodos_numéricos  é verificada acessando o erro em problemas teste ( benchmark ) 1D e 2D . Um estudo comparativo entre os resultados do esquema TDPUS-C3 e os esquemas upwind convencionais para leis de conservação hiperbólicas complexas é também realizado . A Associação das modelagens numéricas ( upwinding mais RNG '\kappa - \epsilon ' ) é , então , examinada na simulação de escoamentos turbulentos de fluidos newtonianos envolvendo superfícies_livres  móveis , usando a metodologia URANS . No geral , em termos do comportamento global , concordância satisfatória é 
 Redes de Sensores Sem Fio são um tipo especial de rede ad-hoc que são posicionadas em uma região para monitorar fenômenos físicos . Considerando que os sensores dessas redes são independentes e possuem um raio de cobertura pequeno , é comum a utilização de um grande número de sensores para monitorar uma área grande . Um problema nesses tipos de redes é garantir que o máximo de dados capturados por esses sensores sejam coletados e transmitidos até uma estação base para que possam ser analisados por usuários . Uma abordagem para resolver esse problema é por meio da utilização de sensores especiais chamados cluster heads . Esses sensores são posicionados estrategicamente para coletar a informação de um grupo de sensores e transmiti-la para a estação base . Assim surge a necessidade de agrupar esses sensores . Nesse trabalho é proposta uma técnica híbrida baseada no algoritmo de agrupamento de dados K-Médias e em detecção comunidades em redes complexas . Esse algoritmo , chamado de QK-Médias , tenta aproveitar as vantagens das duas_abordagens  em duas etapas . Primeiro a rede é quebrada em comunidades usando uma técnica de detecção de comunidades . Em seguida essas comunidades são quebradas em subcomunidades de tal forma que os cluster heads consigam gerenciar . Os resultados obtidos a partir do agrupamento de sensores utilizando o QK-Médias mostram que é possível diminuir o número de mensagens perdidas na rede utilizando menos cluster heads que algoritmos tradicionais de agrupamento em redes de sensores sem 
 Escrever aplicações concorrentes é comumente tido como uma tarefa_difícil  e propensa a erros . Isso é particularmente verdade para aplicações escritas nas linguagens de uso mais disseminado , como C++ e Java , que oferecem um modelo de programação concorrente baseado em memória_compartilhada  e travas . Muitos consideram que o modo de se programar concorrentemente nessas linguagens é inadequado e dificulta a construção de sistemas livres de problemas como condições de corrida e deadlocks . Por conta disso e da popularização de processadores com múltiplos núcleos , nos últimos_anos  intensificou-se a busca por ferramentas mais adequadas para o desenvolvimento de aplicações concorrentes . Uma alternativa que vem ganhando atenção é o modelo de atores , proposto inicialmente na década de 1970 e voltado especificamente para a computação concorrente . Nesse modelo , cada ator é uma entidade isolada , que não compartilha memória com outros atores e se comunica com eles somente por meio de mensagens assíncronas . A implementação mais bem sucedida do modelo de atores é a oferecida por Erlang , a linguagem que ( provavelmente ) explorou esse modelo de forma mais eficiente . A linguagem Scala , surgida em 2003 , roda na JVM e possui muitas semelhanças com Java . No entanto , no que diz_respeito  à programação concorrente , os criadores de Scala buscaram oferecer uma solução mais adequada . Assim , essa linguagem oferece uma biblioteca que implementa o modelo de atores e é fortemente inspirada nos atores de Erlang . O objetivo deste trabalho é explorar o uso do modelo de atores na linguagem Scala , especificamente no caso de aplicações distribuídas . Aproveitando o encapsulamento imposto pelos atores e a concorrência inerente ao modelo , propomos uma plataforma que gerencie a localização dos atores de modo totalmente transparente ao desenvolvedor e que tem o potencial de promover o desenvolvimento de aplicações eficientes e escaláveis . Nossa infraestrutura oferece dois serviços principais , ambos voltados ao gerenciamento da localização de atores : distribuição automática e migração . O primeiro deles permite que o programador escreva sua aplicação pensando apenas nos atores que devem ser instanciados e na comunicação entre esses atores , sem se preocupar com a localização de cada ator . É responsabilidade da infraestrutura definir onde cada ator será executado , usando algoritmos configuráveis . Já o mecanismo de migração permite que a execução de um ator seja suspensa e retomada em outro computador . A migração de atores possibilita que as aplicações se adaptem a mudanças no ambiente de execução . Nosso sistema foi construído tendo-se em mente possibilidades de extensão , em particular por algoritmos que usem o mecanismo de migração para melhorar o desempenho de uma aplicação 
 A tecnologia de componentes de software é propícia para encapsular questões técnicas de implementação e favorecer o reúso entre aplicações , o que é particularmente relevante no desenvolvimento de aplicações colaborativas na Web . Este trabalho utiliza a plataforma Groupware Workbench nesse contexto . A aplicação social Arquigrafia foi a principal motivadora dessa evolução . O Arquigrafia é um ambiente colaborativo para o estudo de arquitetura e compartilhamento de imagens fortemente baseado em colaboração e inteligência coletiva . Como o conceito de inteligência coletiva é muito amplo e mal definido , foi realizada uma análise de domínio e uma classificação das técnicas e seus usos nos sistemas atuais . Também foi feito o mapeamento e a implementação das funcionalidades do Arquigrafia em componentes do Groupware Workbench e executada uma avaliação da plataforma em quatro vertentes , sendo elas : arquitetura de componentes ; suporte à colaboração ; arquitetura técnica ; e percepção dos desenvolvedores . Limitações tecnológicas e conceituais foram identificadas , como por exemplo , o modelo de mapeamento objeto-relacional e questões ligadas à flexibilidade . Essas limitações e colocações foram tratadas e avaliadas na plataforma , resultando em melhorias na arquitetura dos componentes e na simplificação do código . O Groupware Workbench no geral mostrou-se viável para o desenvolvimento de uma aplicação colaborativa real na Web 2.0 
 Dispositivos móveis do tipo smartphones e tablets possibilitam uma interação rica e intuitiva , abrindo novas possibilidades para usuários e desenvolvedores . Entre as características que os tornam populares estão a alta capacidade de processamento , a melhoria constante das alternativas de interação com usuários e a possibilidade de captura de imagens e vídeos , além da comodidade de acessar serviços e informações em dispositivos tão portáteis . Além_disso  , ao longo dos últimos_anos  tem-se observado uma cultura de geração de conteúdo por parte de usuários , ao mesmo tempo que dispositivos_móveis  se tornam populares reprodutores de mídias e adquirem um papel_importante  na produção de conteúdo multimídia . Da mesma forma que novas mídias podem ser capturadas é também possível enriquece-las com anotações . Anotações são comuns em documentos textuais e recentemente tornaram-se populares em imagens , sendo possível encontrar diversos aplicativos para anotação em dispositivos_móveis  . No entanto , aplicativos para anotação em vídeo em dispositivos_móveis  ainda são raros . Explorando esta oportunidade , o principal objetivo deste trabalho foi investigar alternativas de interação entre usuários e seus dispositivos_móveis  considerando o cenário de anotação em vídeos . Para investigar os problemas relacionados foi desenvolvido um aplicativo executável em tablets e smartphones com soluções singulares para o contexto proposto . Com foco na interação humano-computador foram realizadas três avaliações , sendo uma avaliação de usabilidade com usuários , uma avaliação heurística com especialistas e uma avaliação colaborativa com uma usuária da área de terapia ocupacional . Resultados de avaliações auxiliaram na definição de novos requisitos e na identificação de problemas que se aplicam não apenas ao contexto de anotações em vídeo , mas em aplicações para dispositivos_móveis  , de forma geral . Assim , as principais_contribuições  deste trabalho consistem em soluções relacionadas à autoria multimídia , à captura de interações para sistemas de anotação em vídeo em dispositivos_móveis  e à usabilidade para aplicativos 
 Com o aumento do uso e da complexidade de sistemas web , o desenvolvimento de tais sistemas com qualidade exige a adoção de uma abordagem sistemática e bem definida . Assim , a engenharia web é uma disciplina essencial que considera , além de características da engenharia de software , fatores inerentes aos sistemas web , como a multiplicidade de perfis de usuários . A engenharia web é apoiada por processos , métodos , técnicas e ferramentas que são elementos fundamentais para o desenvolvimento de sistemas web , os quais devem ser adequados para fornecer suporte às ações inerentes ao projeto e à implementação . Esses elementos devem ser selecionados , combinados e tecnicamente implementados de modo a produzir um sistema web acessível e usável . Nesta tese é proposto um processo de desenvolvimento que possui fases gerais bem definidas para a inserção de requisitos de acessibilidade e usabilidade no desenvolvimento de sistemas web , garantindo o seu uso pela maioria das pessoas e facilitando seus meios de acesso . Um estudo de caso foi realizado para verificar a efetividade da aplicação do processo formalizado , o qual possibilitou o desenvolvimento de um sistema acadêmico de agendamento de bancas . Considerando-se a dificuldade prática de avaliar diretamente o processo , foram realizados um experimento controlado e um estudo de viabilidade comparando o sistema acadêmico desenvolvido com outro sistema de mesmo propósito e funcionalidades , mas desenvolvido de maneira ad-hoc . Por meio das avaliações realizadas nos dois sistemas de agendamento de bancas , indiretamente avaliou-se o processo formalizado e foram encontrados fortes indícios sobre a efetividade do processo proposto . Adicionalmente , foi criado um instrumento de medição objetivo e quantitativo das características de acessibilidade e usabilidade de um sistema web . Foi também criado um método para avaliar , comparar e melhorar a acessibilidade e a usabilidade de sistemas web existentes . Tanto o instrumento de medição quanto o método de avaliação podem ser aplicados , independentemente , a qualquer sistema web 
 Corrida de carros e um gênero popular de jogos eletrônicos e um domínio com vários desafios a serem explorados no âmbito da Inteligência_Artificial  ( IA ) , tendo recebido atenção crescente nos últimos_anos  . Naturalmente , um desses desafios e criar pilotos virtuais capazes de aprender sozinhos a correr nas pistas . Neste projeto de mestrado , nos adaptamos e aplicamos técnicas de Aprendizagem por Reforço ( Reinforcement Learning ) no desenvolvimento de um agente completamente autônomo capaz de correr em pistas de vários formatos dentro do simulador TORCS . Esse jogo de código_aberto  possui um sistema de física muito elaborado e permite a criação de módulos de IA para controlar os carros , sendo assim um ambiente de testes frequentemente adotado para pesquisas nesse contexto . O objetivo do nosso agente e encontrar ações de controle do acelerador e freio a fim de gastar o menor tempo possível em cada volta . Para atingir tal meta , ele coleta dados na primeira volta , gera um modelo do circuito , segmenta e classifica cada trecho da pista e , finalmente , da voltas no percurso ate atingir um comportamento consistente . Além das questões relacionadas a aprendizagem , este trabalho explora conceitos de Sistemas de Controle , em especial controladores PID ( Proporcional , Integrativo , Derivativo ) , usados para a implementação da heurística do manejo do volante . Também abordamos os fundamentos de alguns assistentes de direção , tais como ABS ( Anti-lock Braking System ) e controle de estabilidade . Esses princípios são de grande_importância  para tornar o agente capaz de guiar o carro dentro de um ambiente com simulação física tão próxima a realidade . Nesse ponto e no emprego do sensoriamento para a aquisição de dados , nosso trabalho flerta com a área de Robótica Móvel . Por fim , avaliamos o desempenho de nosso piloto virtual comparando seus resultados com os de controladores baseados em outras técnicas 
 Anotações têm sido associadas a documentos em todas as gerações de sistemas hipermídia . Este trabalho explora o uso de anotações como hiperdocumentos de primeira classe baseados em sua semântica . Nesse contexto , anotações são entidades próprias , na forma de hipertexto , possuindo seus próprios atributos e operações . A Web Semântica é uma extensão da Web atual na qual é dado um significado bem definido à informação , permitindo que informações sejam compreensíveis não só por humanos , mas também por computadores . Este trabalho possui como objetivo prover um serviço aberto , o GroupNote , de suporte a anotações colaborativas como hiperdocumentos de primeira classe na Web Semântica . Para prover esse serviço foram realizadas a modelagem conceitual e a definição e implementação de uma API , a API GroupNote . Como um estudo de caso do serviço GroupNote foi construída a aplicação WebNote , uma ferramenta que permite que usuários tenham seu próprio repositório de anotações na Web 
 Atualmente , os sistemas robóticos têm sido necessários para uma diversidade de novos produtos , como em robôs de serviço e em robôs para ambientes perigosos . Como consequência , um aumento da complexidade desses sistemas e observada , exigindo também atenção considerável para a qualidade e a produtividade . Vale destacar que a complexidade de parte desses sistemas decorre , parcial ou totalmente , da necessidade de se utilizar diversos robôs para compor a solução . Em outra perspectiva , arquiteturas de referência surgiram como um tipo especial de arquitetura de software que consegue agregar conhecimento de domínios específicos , facilitando o desenvolvimento , padronização e evolução de sistemas de software . Nessa perspectiva , arquiteturas de referência foram também propostas para o domínio de robótica e de têm sido consideradas como um elemento importante para o desenvolvimento de sistemas para esse domínio . No entanto , existe uma falta de arquiteturas de referência especializadas em sistemas multirrobóticos de serviço . Assim , a principal_contribuição  desse trabalho e o apoio ao desenvolvimento de sistemas multirrobóticos de serviço . Para isso , o principal resultado , aqui apresentado , e o estabelecimento da SiMuS , uma arquitetura de referência que agrega o conhecimento e a experiência de como organizar sistemas multirrobóticos de serviço , visando a escalabilidade , evolução e reuso . Para o estabelecimento dessa arquitetura , foi utilizado o ProSA-RA , um processo que sistematiza o projeto , representação e avaliação de arquiteturas de referência . Resultados alcançados por uma inspeção , por meio de checklist , e estudo de caso conduzidos evidenciam que a SiMuS e uma arquitetura de referência viável e reusável para o desenvolvimento de sistemas multirrobóticos de 
 Neste trabalho é apresentada e discutida uma ferramenta que auxilia a construção de Sistemas Tutores Inteligentes - STIs - no domínio da Matemática , denominada TOOTEMA - TOOI ( ferramenta ) para Tutores de Ensino de MAtemática . O TOOTEMA é um sistema que tem por objetivo ajudar na tarefa de criar e gerenciar o Módulo do Domínio do ARQTEMA ( uma arquitetura genérica para Sistemas Tutores em Matemática ) , acompanhando o autor ( professor ) na estruturação de grande quantidade de informações . Ao usuário-autor , a ferramenta de autoria fornece facilidades de organização do material instrucional , provê recursos de representação gráfica , visando a consistência e a qualidade do material , além de reduzir o tempo de construção de STI em subdomínios diferentes e , principalmente , de envolver mais diretamente o autor leigo na criação do sistema 
 Nesta dissertação , propomos um novo método de relaxamento para resolver o problema de roteamento de dados em redes de comutação . Este problema pode ser formulado como um problema de multifluxo a critério convexo . Este algoritno resolve subproblemas de simples fluxos e pode ser implementado em paralelo . No primeiro capítulo , relembramos alguns resultados da teoria dos grafos . No segundo , apresentamos o problema de roteamento de dados , bem como sua formulação_matemática  . O terceiro apresenta um método primal de relaxamento . O quarto é dedicado à apresentação dos resultados computacionais e à analise destes . Na última parte , apresentamos a conclusão e perspectivas de trabalhos futuros 
 Esta dissertação apresenta o projeto e implementação de um sistema gerador de sfubs , para utilização no ambiente computacional do Laboratório de Sistemas Digitais do ICMSC/USP , em plataformas DOS . O gerador de sfubs é uma ferramenta de auxílio ao desenvolvimento de aplicações distribuídas e tem como função criar automaticamente os procedimentos sfubs , a partir da definição dos serviços que serão executados remotamente . Esse software de apoio libera os programadores da implementação de rotinas que envolvam os protocolos básicos de comunicação . Os procedimentos sfubs são responsáveis pela comunicação entre os processos clientes e os serviços oferecidos . Com esses procedimentos , os processos cliente e servidor podem ser compilados e executados separadamente , em máquinas diferentes . Dentre as vantagens do sistema gerador de sfubs , a mais importante , e que está bem caracterizada neste trabalho , é o ganho de produtividade nos projetos de aplicações distribuídas , tornando extremamente atrativa sua adoção nesses projetos . O sistema proposto está implementado na linguagem `` C '' e pode ser utilizado facilmente para a geração de aplicações distribuídas envolvendo equipamentos compatíveis com a linha IBM-PC , executando DOS versão 3 ou superior 
 É comum , em muitas áreas de investigação científica , a existência de vários modelos de regressão não lineares que podem ser usados para elucidar um mesmo fenômeno . Estando o pesquisador diante de vários modelos alternativos , como escolher qual fornece melhor ajuste ? Essa é uma questão de interesse aos estatísticos e muitas estratégias clássicas e Bayesianas de discriminação tem sido propostas na literatura . Nesta dissertação , considerando os modelos não lineares de crescimento sigmóide : Logístico , Gompertz , Tipo-Weibull , Morgan-Mercer-Flodin e Richards , apresentamos uma análise_Bayesiana  e algumas estratégias ( clássicas e Bayesianas ) que podem ser usadas em problemas de discriminação de modelos alternativos . Sob o ponto de vista clássico , a discriminação é conduzida com base em conceitos de não linearidade , uma vez que o `` melhor modelo possível '' dentre todos os propostos é aquele que apresenta o comportamento mais próximo do comportamento linear . No contexto Bayesiano , considerando um conjunto de dados , usando uma priori não informativa de Jeffreys , o método de Laplace para aproximar as integrais de interesse e a técnica proposta por Gelfand e Dey ( 1994 ) procedemos a discriminação usando as estratégias : Fator de Bayes , critério baseado no conceito de entropia , Pseudo Fator de Bayes e o Fator de Bayes a Posteriori 
 Máquina de Estado Finito ( MEF ) é uma técnica de especificação usada para modelar o aspecto comportamental de sistemas e tem sido amplamente utilizada em diversas_áreas  de aplicação : protocolos de comunicação , telefonia , controle de processos , entre outros . O objetivo deste trabalho consiste em estudar os conceitos fundamentais utilizados no teste e validação de modelos baseados em MEF , com ênfase nos critérios DS [ GONE70 ] , UIO [ SABN88 ] , W [ CHOW78 ] e Wp [ FUJI91 ] . Dentro deste contexto foi desenvolvida tuna ferramenta , denominada MGASET , que visa a apoiar a aplicação de alguns critérios estudados , fornecendo facilidades para verificação de minimalidade , determinismo , especificação completa , conectividade forte , Máquina de Mealy e alcançabilidade do estado inicial de uma MEF ; para gerar seqüência de sincronização , seqüência distingüivel , seqüências únicas de entrada/saída e conjunto de caracterizacão ; e gerar , inicialmente , seqüências de teste baseada no critério W 
 Esta tese aborda o problema de obtenção de um índice de carga ou de desempenho adequado para utilização no escalonamento de processos em sistemas computacionais heterogêneos paralelos/distribuídos 
 Uma ampla revisão bibliográfica com a correspondente análise crítica é apresentada . Essa revisão é a base para a comparação das métricas existentes para a avaliação do grau de heterogeneidade/homogeneidade dos sistemas computacionais . Uma nova métrica é proposta neste trabalho , removendo as restrições identificadas no estudo comparativo realizado . Resultados de aplicações dessa nova métrica são apresentados e discutidos 
 Esta tese propõe também o conceito de heterogeneidade/homogeneidade temporal que pode ser utilizado para futuros aprimoramentos de políticas de escalonamento empregadas em plataformas computacionais heterogêneas paralelas/distribuídas 
 Um novo índice de desempenho ( Vector for Index of Performance - VIP ) , generalizando o conceito de índice de carga , é proposto com base em uma métrica Euclidiana . Esse novo índice é aplicado na implementação de uma política de escalonamento e amplamente testado através de modelagem e simulação . Os resultados obtidos são apresentados e analisados estatisticamente . É demonstrado que o novo índice leva a bons_resultados  de modo geral e é apresentado um mapeamento mostrando as vantagens e desvantagens de sua adoção quando comparado às métricas tradicionais 
 As características irrelevantes , presentes em bases de dados de diversos_domínios  , deterioram a acurácia de predição de classificadores induzidos por algoritmos de aprendizado de máquina . As bases de dados geradas por uma língua eletrônica são exemplos típicos onde a demasiada quantidade de características irrelevantes e redundantes prejudicam a acurácia dos classificadores induzidos . Para lidar com este problema , duas_abordagens  podem ser utilizadas . A primeira é a utilização de métodos para seleção de subconjuntos de características . A segunda abordagem é por meio de ensemble de classificadores . Um ensemble deve ser constituído por classificadores diversos e acurados . Uma forma efetiva para construção de ensembles de classificadores é por meio de seleção de características . A seleção de características para ensemble tem o objetivo adicional de encontrar subconjuntos de características que promovam acurácia e diversidade de predição nos classificadores do ensemble . Algoritmos genéticos são técnicas promissoras para seleção de características para ensemble . No entanto , a busca genética , assim como outras estratégias de busca , geralmente visam somente a construção do ensemble , permitindo que todas as características ( relevantes , irrelevantes e redundantes ) sejam utilizadas . Este trabalho apresenta uma abordagem baseada em algoritmos genéticos para construção de ensembles de redes neurais_artificiais  com um conjunto reduzido das características totais . Para melhorar a acurácia dos ensembles , duas_abordagens  diferenciadas para treinamento de redes neurais foram utilizadas . A primeira baseada na interrupção precoce do treinamento com o algoritmo back-propagation e a segunda baseada em otimização multi-objetivo . Os resultados obtidos comprovam a eficácia do algoritmo proposto para construção de ensembles de redes neurais acurados . Também foi constatada sua eficiência na redução das características totais , comprovando que o algoritmo proposto é capaz de construir um ensemble utilizando um conjunto reduzido de características 
 A linguagem_natural  corresponde ao meio mais convencional de comunicação entre as pessoas . O desejo que os seres_humanos  possuem de se comunicar com as máquinas é evidenciado por pesquisas , que têm sido realizadas desde o final da década de 70 , com o objetivo de ter requisições expressas em linguagem_natural  executadas pelas máquinas . Alguns trabalhos na literatura têm sido_propostos  com esse fim , entretanto a maioria deles considera requisições expressas apenas em Inglês . Uma maneira de flexibilizar a utilização de várias línguas em sistemas que utilizam linguagem_natural  é por meio de uma interlíngua , pois essa é uma representação intermediária e processável por máquina das informações contidas em diversas línguas naturais . O trabalho descrito nesta tese propõe que requisições imperativas em linguagem_natural  sejam convertidas para a interlíngua UNL ( Universal Networking Language ) e executadas por meio da ativação dos componentes de software apropriados . Para atingir esse objetivo , este trabalho propõe a Arquitetura OntoMap ( Ontology-based Semantic Mapping ) , que utiliza ontologias para realizar o mapeamento semântico entre UNL e componentes de software e para realizar a busca pelos componentes mais apropriados para executar as requisições . A Arquitetura OntoMap conta com ( i ) um serviço para converter requisições em linguagem_natural  para UNL ; ( ii ) uma ontologia de alto nível , chamada Ontologia InterComp ( Interlíngua-Componentes ) , que juntamente com regras e inferência , fornece informações_semânticas  a respeito dos componentes que podem ser utilizados para executar a requisição ; ( iii ) uma Ontologia de Componentes , que relaciona dados das interfaces dos componentes com informações_semânticas  do domínio de aplicação dos mesmos ; e ( iv ) um Módulo de Busca que utiliza as informações_semânticas  inferidas e a Ontologia de Componentes para encontrar os componentes apropriados para executar as requisições expressas em linguagem_natural  . Este trabalho propõe ainda um processo para utilizar a Arquitetura OntoMap em diversos_domínios  de aplicação e com diferentes conjuntos de componentes . Esse processo foi instanciado considerando componentes desenvolvidos para o domínio de gerenciamento de 
 O planejamento da produção é uma atividade que avalia decisões para um melhor uso dos recursos disponíveis , visando satisfazer aos objetivos produtivos da empresa ao longo de um horizonte de planejamento . Este trabalho enfoca o problema de dimensionamento de lotes com restrições de capacidade ( PDLC ) , que é uma das tarefas centrais envolvidas no planejamento da produção . O PDLC visa determinar o tamanho dos lotes a serem produzidos em períodos de tempo de um horizonte de planejamento . Os PDLC estudados neste trabalho contemplam duas características importantes : a presença de múltiplos itens e a existência de tempos de preparação para as máquinas . Além_disso  , são consideradas restrições de capacidade e situações onde o atraso para atender a demanda é permitido ( backlogging ) . Alguns dos modelos estudados permitem que a preparação do ambiente de produção para um dado item possa ser mantida de um período para o seguinte , o que propiciaria a economia de até uma preparação a cada_período  . Esta característica é chamada de preservação de preparação ( carry-over ) . Também existem situações onde a preparação de uma máquina começa em um período e termina no período seguinte . Na literatura , esta característica é chamada de set-up crossover . Este trabalho tem três metas centrais : a ) avaliar diferentes configurações do software comercial ILOG CPLEX 11 para a solução dos PDLC estudados ; b ) estudar a influência na solução dos PDLC quando se acrescenta a possibilidade de atraso na demanda , de preservação de preparação e de set-up crossover ; c ) aplicar local branching para resolver os problemas estudados . Para resolver as instâncias propostas , foram utilizados o software comercial ILOG CPLEX 11 e um programa em C++ que foi desenvolvido neste trabalho . Foram utilizados exemplos encontrados na literatura para avaliar as propostas , e bons_resultados  foram 
 No desenvolvimento de códigos paralelos , a biblioteca PETSc se destaca como uma ferramenta prática e útil . Com o uso desta ferramenta , este trabalho apresenta um estudo sobre resolvedores de sistemas_lineares  aplicados a escoamentos_incompressíveis  de fluidos em microescala , além de uma análise de seu comportamento em paralelo . Após um estudo dos diversos aspectos dos métodos de fronteira imersa , é apresentado um método de fronteira imersa paralelo de primeira_ordem  . Na sequência , é apresentada uma proposta de melhoria na precisão do método , baseada na minimização da distância entre a condição de contorno exata e aproximada , no sentido de mínimos quadrados . O desenvolvimento de uma ferramenta paralela eficiente é demonstrado na solução_numérica  de problemas envolvendo escoamentos_incompressíveis  de fluidos viscosos com fronteiras 
 Este trabalho apresenta uma técnica de diferenças_finitas  para resolver a equação_constitutiva  Fluido de Segunda Ordem para escoamentos tridimensionais com superfície_livre  . As equações governantes são resolvidas pelo método de diferenças_finitas  em uma malha_deslocada  3D . A superfície_livre  é modelada por células marcadoras ( Marker-and-Cell ) e as condições de contorno a superfície_livre  são empregadas . O método_numérico  apresentado neste trabalho foi validado pela comparação entre as soluções numéricas obtidas para o escoamento em um tubo com a solução analítica correspondente para Fluidos de Segunda Ordem . Ao fazer refinamento de malha , a convergência do método_numérico  foi verificada . Resultados_numéricos  da simulação do problema do inchamento do extrudado para números de Deborah De ' < OU = ' 0:3 são 
 Atualmente , diversas_áreas  de aplicação necessitam de mecanismos mais efetivos para analisar dados provenientes de naturezas distintas . Tipicamente , esses dados são abstratos , não estruturados e possuem uma natureza multidimensional ( e.g. , coleções de documentos ) . Dados que não possuem uma natureza multidimensional podem ser representados como tal por meio da aplicação de algoritmos extratores de características ( e.g. , coleções de imagens ) . Assim , técnicas de visualização de informação projetadas para interpretar dados multidimensionais podem ser aproveitadas para analisar dados não estruturados . Esta tese empregou técnicas de visualização de informação para construir mapas de similaridade a partir de dados multidimensionais como uma forma de representação desses dados , uma vez que as técnicas para construilos tem evoluído com a expansão dos campos de aplicação . Novas técnicas para coordenação de múltiplas visões foram desenvolvidas para permitir a exploração de conjuntos de dados , a partir de mapas de similaridade gerados por diferentes técnicas de construção de mapas , diferentes parâmetros ou ainda diferentes conjuntos de dados . As técnicas de coordenação desenvolvidas são baseadas em identificador , em distância , em tópicos , na identificação de tópicos em coleções que evoluem no tempo , e em uma técnica que combina o mapeamento de diferentes técnicas de coordenação . Esta tese também apresenta aplicações das técnicas de coordenação desenvolvidas e das ferramentas construídas para análise de coleções de documentos , coleções de imagens e dados volumétricos , empregando coordenações de mapas de similaridade . As técnicas de coordenação desenvolvidas são apoiadas por um modelo de coordenação que estende um modelo previamente proposto na literatura . O modelo estendido permite a configuração de técnicas de coordenação durante a exploração , admitindo diferentes_tipos  de mapeamentos . Uma característica importante do modelo é permitir o desenvolvimento de mapeamentos dinâmicos para técnicas de coordenação , isto é , mapeamentos que podem mudar o comportamento de acordo com a interação do usuário . Como resultado desta tese , está disponível um arcabouço para visualização coordenada de múltiplos mapas de similaridade , composto por um modelo , um conjunto de técnicas e um conjunto de ferramentas que efetivamente permitem a análise visual de conjuntos de dados 
 Esta dissertação apresenta a construção de um protótipo de servidor Web distribuído , baseado no modelo de servidor Web com diferenciação de serviços ( SWDS ) e a implementação e avaliação de algoritmos de seleção , utilizando o conceito de controle de congestionamento para requisições HTTP . Com isso , além de implementar uma plataforma de testes , este trabalho também avalia o comportamento de dois algoritmos de controle de congestionamento . Os dois algoritmos estudados são chamados de Drop Tail e RED ( Random Early Detection ) , no qual são bastante difundidos na literatura científica e aplicados em redes de computadores . Os resultados obtidos demostram que , apesar das particularidades de cada algoritmo , existe uma grande relação entre tempo de resposta e a quantidade de requisições 
 Pessoas portadoras de deficiências encontram enormes dificuldades ao tentarem entrar no mercado de trabalho . De fato , sobretudo em países em desenvolvimento , esta parcela significativa da população representa uma fração ínfima dos trabalhadores empregados . Dentre as iniciativas que tentam reverter este quadro , destaca-se a criação de Centros de Trabalhadores Deficientes ( CTDs ) , empresas sem fins lucrativos que empregam pessoas portadoras de deficiências , geralmente em linhas de produção . Um dos fins últimos dos CTDs é expor os trabalhadores a situações encontradas em uma gama diversa de contextos produtivos , de modo que eles possam , eventualmente , vir a compor o quadro de empresas convencionais . A organização e planejamento da operação de CTDs envolve uma série de dificuldades . Questões ligadas à ergonomia do trabalho ou ao gerenciamento de qualidade , por exemplo , adquirem características particulares neste ambiente . Da mesma forma , problemas clássicos de balanceamento de linhas de produção ganham novas particularidades devido , sobretudo , à enorme heterogeneidade existente entre os trabalhadores . Neste contexto , nos interessamos por problemas referentes ao balanceamento da linha de produção com trabalhadores deficientes , onde se busca obter a maior eficiência produtiva dadas as habilidades específicas de cada trabalhador . De maneira mais precisa , o problema de balanceamento de linhas de produção em CTDs , conhecido na literatura como problema de balanceamento e designação de trabalhadores em linhas de produção ( ALWABP , na sigla em inglês ) consiste em alocar tarefas e trabalhadores a estações de trabalho , de modo a minimizar o gargalo produtivo e levando em consideração que cada tarefa tem um tempo de duração que depende do trabalhador escolhido para sua execução . Isto dá ao problema um caráter de dupla alocação , aumentando seu caráter combinatório e , consequentemente , sua dificuldade de resolução . Nesta dissertação , estudamos uma variedade de técnicas de resolução do ALWABP . Os objetivos deste estudo são , primeiramente , obter métodos diversos para resolução do problema que sejam eficazes tanto em termos do tempo computacional necessário para sua utilização como em termos da qualidade da solução obtida . Dentre as abordagens propostas e testadas encontram-se versões de algoritmos com diferentes complexidades , indo desde heurísticas construtivas e estratégias de busca monotônica em vizinhança até meta-heurísticas como GRASP e Busca Tabu . A variedade de técnicas desenvolvidas permitiu a resolução de um problema ainda mais complexo que o ALWABP , que consiste em programar a linha para diversos períodos produtivos , levando em consideração a rotação de tarefas entre os trabalhadores . Deste modo , os trabalhadores podem ser expostos ao maior número de tarefas possível ( atendendo , assim , o fim de treinamento almejado no ambiente dos CTDs ) . Para resolução do problema de rotação de tarefas , as técnicas desenvolvidas foram utilizadas em um esquema de otimização híbrido que faz_uso  de um pool de soluções ( obtidas pelos métodos heurísticos ) que são integradas através de modelos de otimização_linear  inteira_mista  . Os resultados obtidos sugerem que as técnicas desenvolvidas são eficientes e flexíveis para o problema ALWABP e que a sua integração permite a obtenção de soluções eficientes para o problema de rotação de tarefas . Deste modo , esta dissertação_propõe  um esquema completo para o balanceamento de linhas de produção em 
 Startups de software enfrentam um mercado muito exigente : elas devem entregar soluções altamente inovativas no menor período de tempo possível . Recursos são limitados e tempo para alcançar o mercado é pequeno . Então , é extremamente importante coletar os requisitos certos e que eles sejam precisos . Entretanto , os requisitos de software geralmente não são claros e as startups fazem um grande esforço para identificar quais serão implementados . Esse contexto afeta como as atividades de engenharia de requisitos são executadas nessas organizações . Este trabalho procura compreender o estado-da-prática da engenharia de requisitos em startups de software . Usando uma abordagem iterativa , dezessete entrevistas foram realizados em três diferentes estágios com fundadores e/ou gestores de diferentes startups de software brasileiras operando em diferentes setores e com diferentes estágios de maturidade . Os dados foram analisados usando técnicas de teoria fundamentada como codificação aberta e axial através da comparação contínua . Como resultado , um modelo_conceitual  do estado-da-prática da engenharia de requisitos em startups de software foi desenvolvido consistindo da suas influências do contexto ( fundadores , gerente de desenvolvimento de software , desenvolvedores , modelo de negócio , mercado e ecossistema ) e descrição das atividades ( time de produto ; levantamento ; análise , validação e priorização ; e documentação ) . Técnicas oriundas de metodologias de desenvolvimento de software e desenvolvimento de startups também são apresentadas e seu uso em no contexto de startups é analisado . Finalmente , a partir de uma analogia de maus_cheiros  presente na literatura de desenvolvimento de software , algumas más práticas e maus comportamentos identificados em startups de software são apresentados e algumas sugestões de solução são propostas 
 Muito embora trilhas sonoras desempenhem um papel essencial na experiência criada por jogos digitais , elas sofrem de diversas restrições de projeto causadas por limitações tecnológicas . Isso afeta principalmente efeitos em tempo real , que são uma demanda natural na mídia interativa dos jogos . Desenvolvedores precisam optar entre implementar uma solução própria caso a caso , investir em software proprietário , ou simplesmente negligenciar a trilha sonora por falta de opção melhor . Além_disso  , mesmo as melhores ferramentas comerciais trabalham apenas com áudio baseado em amostras , o que é uma das principais causas das ditas restrições de projeto . Portanto , esta dissertação_propõe  VORPAL , um middleware livre para áudio em jogos que foca em áudio procedural mas mantém compatibilidade com áudio baseado em amostras como uma ferramenta mais acessível e adequada para a composição de trilhas sonoras em tempo real para jogos digitais . O middleware , inspirado em seus antecessores comerciais , é dividido em duas principais componentes de software : um motor de áudio e um kit de criação de trilhas sonoras . O primeiro é constituído por uma biblioteca de programação nativa em C++ , com a qual jogos e motores de jogos podem se ligar para reproduzir e controlar , em tempo real , peças da trilha sonora criadas usando a outro componente , que é um kit de blocos de construção providos como abstrações de Pure Data . Projetistas de som profissionais foram entrevistados e depois trabalharam em parceria com os autores para validar a tecnologia proposta , o que levou ao desenvolvimento de um jogo de prova conceitual chamado Sound Wanderer , que demonstra as possibilidades e limitações do middleware VORPAL 
 O objetivo deste projeto de Mestrado foi o estudo , a análise e o desenvolvimento de técnicas de Realimentação de Relevância ( RR ) para melhorar a respostas de consultas por similaridade que empregam técnicas de recuperação de imagens por conteúdo ( do inglês content-based image retrieval - CBIR ) . A motivação para o desenvolvimento deste projeto veio do iRIS ( internet Retrieval of Images System ) , que é um protótipo de servidor Web para o processamento de consultas por similaridade , em construção no GBdI ( Grupo de Bases de Dados e Imagens ) do ICMC-USP . O iRIS pode ser integrado a PACS ( Picture and Archiving and Communication System ) permitindo que estes possam recuperar imagens por semelhança . A principal restrição do uso de sistemas que incorporam CBIR é a descontinuidade semântica ( semantic gap ) , que credita-se principalmente à utilização de características de baixo_nível  para descrever as imagens . As características mais utilizadas são baseadas em cor , textura e forma , e geralmente não conseguem mapear o que o usuário deseja/esperar recuperar , gerando um descontentamento do usuário em relação ao sistema . Entretanto , se sistema permitir a iteração do usuário na classificação do conjunto resposta e usar estas informações no processo de refinamento , as consultas podem ser re-processadas e os resultados tendem a atender a expectativa do usuário . Esse é o propósito das técnicas de realimentação de relevância . Este projeto desenvolveu duas técnicas de realimentação de relevância ( RR ) : o RF Projection e o RF Multiple Point Projection . O ganho com a aplicação dessas técnicas foi expressivo , alcançando 29 % a mais de precisão sobre a consulta original já na primeira iteração e 42 % após 5 iterações . Os experimentos_realizados  com usuários mostraram que em média são executadas 3 iterações para chegar a um resultado satisfatório . Pelos resultados apresentados nos experimentos , podemos afirmar que RR é uma poderosa ferramenta para impulsionar o uso dos sistemas CBIR e aprimorar as consultas por similaridade 
 Em muitos problemas de simulação de fenômenos físicos ou fenômenos de engenharia , o uso das malhas é um componente muito importante . Uma malha é uma aproximação de uma dada geometria por um conjunto de elementos mais simples , tais como triângulos e quadriláteros ( caso bidimensional ) ou tetraedros , prismas , pirâmides e hexaedros ( caso tridimensional ) . Nesse texto , as malhas de interesse são as não-estruturadas e compostas por triângulos . A escolha de uma malha é fortemente influenciada pelo desempenho e precisão dos resultados da simulação . O desempenho depende do número de elementos a serem processados , ou seja , quanto maior for a área coberta por cada elemento da malha , menos elementos são necessários , por conseguinte , mais rápida será a simulaçao . A precisão nos resultados da simulação está relacionada tanto com o formato quanto com o tamanho dos elementos . Diferente do desempenho , quanto menor forem os elementos , mais precisos serão os resultados . O formato dos elementos também influencia a precisão , em geral , elementos mais próximos dos equiláteros são preferidos . Como é possível observar , desempenho e precisão são requisitos conflitantes e geralmente é necessário fazer uma ponderação entre eles . Para um determinado grupo de aplicações , o melhor compromisso entre desempenho e precisão é conseguido com elementos finos , longos e corretamente alinhados sobre o domí ? nio onde a malha está definida . São as chamadas malhas anisotrópicas . Além_disso  , um método de refinamento anisotrópico pode melhorar ainda mais a precisão dos resultados . O principal objetivo desse trabalho é desenvolver métodos de refinamento de malhas anisotrópicas , usando como base , e tendo como ponto de partida , os métodos de refinamento Delaunay isotrópicos , a saber , os métodos de refinamento Delaunay de Jim Ruppert [ 13 ] e de Paul Chew [ 6 ] , e também realizar a simplificação Delaunay proposto por Olivier Devillers [ 8 
 Este trabalho apresenta uma implementação de VPN utilizando-se dos circuitos reprogramáveis do tipo FPGA ( Field Programmable Gate Array ) que são a base da computação reconfigurável . VPNs utilizam criptografia para permitir que a comunicação seja privada entre as partes . Assim , todo o custo_computacional  decorrente desta prática é executado em nível de hardware , procurando-se atingir um alto_desempenho  e voltado para as aplicações de sistemas embutidos . O uso desta solução , VPN por hardware , será na interligação de um robô ( em desenvolvimento no Laboratório de Computação Reconfigurável - LCR do Instituto de Ciências_Matemáticas  e de Computação da Universidade de São Paulo ) ao seu servidor de configuração e tarefas , através de linhas privadas . O emprego de uma VPN em robótica permitirá a utilização de um sistema de comunicação , com ou sem fio , e toda a infra-estrutura da Internet para a comunicação com o robô ( e no futuro entre os robôs ) a qualquer distância de forma segura e confiável . O hardware_reconfigurável  utilizado para a VPN deste trabalho proporciona flexibilidade no modo de implementação , possibilitando que o sistema seja adequado para satisfazer situações que exijam alto_desempenho  . Além_disso  , a arquitetura proposta possibilita que parte das operações sejam executadas em software ( no caso , foi utilizado o sistema_operacional  ? Clinux e ferramentas para se estabelecer a VPN ) e parte das operações executadas em hardware ( um coprocessador criptográfico AES ) . As principais ferramentas de software são o conjunto ipsec-tools que foram desenvolvidas para serem executadas com o IPSec nativo do Kernel e devidamente portadas para o ? 
 Visando contribuir com o estado-da-arte de sistemas bioinspirados em formigas na robóotica , neste trabalho é abordado o problema do controle de um grupo de robôs para a solução coletiva das tarefas de exploração do ambiente e localização de objetos . Para isso , são utilizados algoritmos inspirados em colônias de formigas . O objetivo deste trabalho , portanto , é o desenvolvimento de um sistema de controle de navegação baseado em colônia de formigas para um time de robôs , de maneira que os robôs resolvam esses problemas utilizando estratégias de controle individuais e simples . Esse sistema tem como base a utilização de marcadores ou feromônios artificiais , que podem ser depositados pelos robôs para marcar determinadas posiçôes do 
 Problemas de corte de estoque consistem em arranjar peças menores , em tamanhos e quantidades especificados , dentro de peças maiores . Tais problemas têm sido investigados intensamente nas últimas_décadas  , acrescidos de novas características e novos métodos de solução . Nesta tese abordamos o problema de corte de estoque multiperíodo que surge imerso no planejamento e programação da produção em empresas que têm um estágio de produção caracterizado pelo corte de peças . As demandas dos itens ocorrem em períodos diversos de um horizonte de planejamento finito , sendo possível antecipar ou não a produção de itens . Os objetos disponíveis em estoque não utilizados em um período ficam disponíveis no próximo período , juntamente com novos objetos adquiridos ou produzidos pela própria empresa . Um modelo de otimização_linear  inteira de grande porte é proposto , cujo_objetivo  pondera o custo das perdas nos cortes , os custos de estocagem de objetos e itens . O método simplex com geração de colunas foi especializado para resolver a relaxação linear do modelo proposto . Foram realizados_experimentos  computacionais com problemas de corte de estoque unidimensional e bidimensional . Tais experimentos mostram que ganhos efetivos podem ser obtidos usando-se o modelo de corte de estoque multiperíodo , quando comparado com a solução lote-por-lote , tipicamente utilizada na prática . Porém , na prática , a solução relaxada é de pouca , ou nenhuma , utilidade . Assim , nesta tese , desenvolvemos dois procedimentos de arredondamento da solução do problema multiperíodo , baseado em horizonte rolante , ou seja , determinamos uma solução inteira factível apenas para o primeiro período , a qual será , de fato , implementada . Enfim , concluímos que o modelo para o problema de corte de estoque multiperíodo permite flexibilidade na análise de uma solução a ser implementada e , portanto , é uma ferramenta que permite ao gerente de produção uma visão global do problema para auxiliá-lo na tomada de 
 Em um sistema computacional , os dados são armazenados na unidade de armazenamento , segundo alguma lógica , em estruturas denominadas arquivos . O Sistema de Arquivos é o responsável por estruturar , identificar , acessar , proteger e gerenciar esses arquivos , além de agir como um elo de ligação entre o usuário e o dispositivo , traduzindo comandos de alta abstração ( oriundos do usuário ) em comandos de baixo_nível  , compreensível a unidade de armazenamento . O presente_trabalho  visa a implementação de um sistema de arquivos para aplicação em dispositivos_móveis  baseado em computação reconfigurável . Tal sistema servirá de suporte para as aplicações que necessitem armazenar e/ou restaurar grande volume de dados , como a aquisição de imagens digitalizadas de câmeras CMOS . Este sistema também será utilizado como uma ferramenta inicial para o desenvolvimento de um módulo de armazenamento em uma placa baseada em computação reconfigurável a ser utilizada para fins didáticos . O sistema de arquivos implementado foi a FAT16 e o dispositivo de armazenamento de massa utilizado foram os cartões de memória SD-Secure Digital e 
 Malhas dinâmicas são comumente utilizadas em problemas de simulação sobre dominios cuja geometria varia com o tempo . Sempre que o domínio onde a malha está definida é alterado , as molas são acionadas movimentando os vértices para que estes se conformem com a nova descrição do domínio . Os tipos de molas mais utilizadas são : as longitudinais , as torcionais e as semi-torcionais . Nesta tese uma nova mola é proposta , a mola altura , que além de evitar sobreposição de elementos , é conceitualmente simples e fácel de ser implementada . Outra contribuição desse trabalho é o mecanismo de vértices ativos , que permite economia de processamento durante a resolução da malha dinâmica . Quando a fronteira do domínio sofre grandes alterações , o processo dinâmico pode não ter êxito na correção da malha . Para contornar esse problema , a fronteira deve ser alterada aos poucos . Uma nova estratégia para realizar grandes deformações em pequenos passos é introduzida nesta tese . Em algumas aplicações , o movimento da fronteira da malha pode comprometer células da própria fronteira . A correção da fronteira e um processo delicado , já que em muitos_casos  ele implica em alterar a descrição do domínio . Um novo método para efetuar a correção da fronteira é apresentado neste trabalho . Ele é baseado em malhas dinâmicas e utiliza um novo conceito de molas , as molas conservativas . Todas as contribuições citadas acima tiveram aplicação prática na industria aeronáutica , sendo utilizadas na implementação de uma metodologia inovadora para acoplar um simulador de escoamento de fluidos tridimensional com uma ferramenta de projeto inverso de aerofólios que roda em um contexto bidimensional . O outro assunto abordado e o remalhamento de triangulações superficiais . Foi proposto um novo método , chamado ANTS ( Anisotropic Triangulations on Surfaces ) que produz triangulações anisotrópicas de qualidade sobre superfícies descrevendo objetos com geometria complexa . O método ANTS é caracterizado por efetuar o remalhamento diretamente na triangulação inicial , isto é , ele não faz_uso  de qualquer tipo de parametrização , seja global ou local . O processo de remalhamento é feito por meio de quatro operadores : inserção , remoção e movimento de vértices e alternância de arestas . Os operadores de inserção e remoção de vértices possibilitam controlar a densidade de vértices no domínio , permitindo que nós sejam inseridos em regiões com densidade baixa ou eliminados onde a densidade é alta . A qualidade dos triângulos é controlada por meio dos operadores de movimento de vértices e de alternância ( flipping ) de arestas . O operador de movimento é utilizado no núcleo do processo de remalhamento . Para evitar que o remalhamento danifique a forma original da superfície , as quinas e os córneres são detectados no inicio do processo e preservados durante o remalhamento . A densidade de vértices sobre o domínio é controlada por uma função de espalhamento . Tal função pode ser passada como entrada para o ANTS ou calculada pelo próprio método . O ANTS foi aplicado com êxito em diversos exemplos gerando malhas de boa_qualidade 
 Técnicas para gerar malhas triangulares ou tetraedrais a partir de imagens , ou assumem como entrada uma imagem pré-processada , ou geram uma malha sem distinguir as diferentes estruturas contidas na imagem . O pré-processamento e a ausência de estruturas bem definidas podem apresentar dificuldades na utilização das malhas geradas em algumas aplicações , tais como simulações numéricas . Neste trabalho , apresentamos uma nova_técnica  que elimina a necessidade do pré-processamento embutindo a segmentação dentro do processo de geração de malha . Além disto , a técnica proposta considera critérios de qualidade nas malhas geradas , mostrando-se apropriada para aplicações de simulação_numérica  assim como modelagem de imagens com 
 A expansão e a popularização da World_Wide  Web têm incentivado o desenvolvimento de aplicações Web . É crescente a exigência por aplicações Web cada vez mais complexas , cujo desenvolvimento deve ser feito com qualidade e rapidez . Para orientar o ciclo de vida dessas aplicações , diversos métodos de desenvolvimento Web têm sido criados , além de ferramentas de apoio a sua utilização . O emprego de padrões de software no desenvolvimento de aplicações pode aumentar a produtividade e a qualidade . Com o objetivo de minimizar erros e facilitar a utilização , ferramentas de apoio à utilização de padrões têm sido desenvolvidas . No entanto , há uma carência por ambientes e ferramentas que apóiem o emprego de padrões durante as etapas do desenvolvimento de aplicações . Algumas das principais atividades para a garantia da qualidade de software são as de VV & T ( Verificação , Validação e Teste ) . Algumas iniciativas de associação de teste a padrões têm sido estudadas com o intuito de minimizar o tempo despendido em VV & T . Neste trabalho é apresentado o ambiente Peônia com o intuito de apoiar o emprego de padrões de software durante as etapas de um processo de desenvolvimento . Para isso , os padrões de software podem ser previamente associados a essas etapas para que possam ser sugeridos durante a execução de um projeto . Além_disso  , o ambiente Peônia oferece a possibilidade de associar requisitos de teste a padrôes de software , para auxiliar nas atividades de VV & T . Também é proposto um Método Para Desenvolvimento Utilizando Padrões de Software , formalizando a técnica empregada no ambiente Peônia durante a execução de projetos , onde é incentivada a utilização de padrões de software na criação de artefatos e execução de fases e atividades . O método proposto estimula usuários a empregar padrões de software durante as etapas do processo de desenvolvimento , independentemente de utilizar , ou não , o ambiente 
 Este trabalho avalia de maneira detalhada diferentes parâmetros para a definição de níveis de homogeneidade e heterogeneidade em sistemas computacionais_distribuídos  . O objetivo é analisar a eficiência da métrica GH - Grau de Heterogeneidade - em relação a diferentes perspectivas . Métricas encontradas na literatura e benchmarks de código_aberto  ( reconhecidos pela comunidade_científica  ) são utilizados para quantificar a heterogeneidade do sistema computacional . A métrica GH também é empregada no AMIGO , um ambiente de escalonamento real , para analisar a sua utilização em algoritmos de escalonamento de processos . Os principais resultados obtidos neste trabalho são : a comprovação da estabilidade da métrica GH para determinar o grau de heterogeneidade de plataformas computacionais distribuídas , o uso da métrica GH com sucesso em um ambiente de escalonamento real e o desenvolvimento de um algoritmo de escalonamento adaptativo . Sub-produtos deste trabalho são : um levantamento dos principais benchmarks com código_aberto  e livre disponíveis na literatura , os quais podem ser utilizados em trabalhos futuros no grupo de pesquisa e a continuidade do desenvolvimento do ambiente de escalonamento 
 O TESTE baseado em modelos visa a possibilitar a derivação de conjuntos de casos de teste a partir de especificações formais , tais como Máquinas de Estados_Finitos  . Os conjuntos de teste podem ser obtidos tanto pelos métodos clássicos de geração quanto por alguma abordagem ad hoc . Procura-se obter um conjunto de teste que consiga detectar todos os possíveis defeitos de uma implementação e possua tamanho reduzido para que a sua aplicação seja factível . Por questões de ordem prática , pode não ser possível a aplicação de todo o conjunto de teste gerado . Desse modo , um subconjunto de casos de teste deve ser selecionado , ou seja , uma minimização do conjunto de teste deve ser realizada . No entanto , é fundamental que a minimização reduza o custo de aplicação dos testes , mas mantenha a efetividade em revelar defeitos . Neste trabalho , propõe-se um algoritmo de minimização de conjuntos de teste para Máquinas de Estados_Finitos  . O algoritmo baseia-se em condições de suficiência para que a completude em relação à detecção de defeitos seja mantida . O algoritmo foi utilizado em dois diferentes contextos . Utilizou-se o algoritmo com conjuntos de teste gerados de forma aleatória para verificar a minimização obtida . O algoritmo também foi utilizado para reduzir o esforço em se obter um conjunto completo em relação à detecção de 
 Até pouco tempo acreditava-se que a maioria das moléculas de RNA estava relacionada à tradução de proteínas . Porém , descobriu-se que outros_tipos  de moléculas de RNA que não são traduzidas estão presentes em muitos organismos diferentes e afetam uma variedade de processos moleculares , são os chamados RNAs não-codificantes ( ncRNAs ) . Apesar de sua importância funcional , os métodos biológicos e computacionais para a detecção e caracterização de RNAs não-codificantes ainda são imprecisos e incompletos . A identificação de novas espécies de ncRNAs é difícil através de procedimentos experimentais e as técnicas computacionais existentes são lentas . O objetivo deste trabalho foi obter uma ferramenta mais eficiente para a comparação de uma seqüência de RNA não-codificante contra um banco de seqüências . Para isso foi proposto e implementado um modelo para identificação computacional de ncRNAs com apoio dos pacote Viena e Infernal e foram realizados_experimentos  para 
 O Teste de software é uma atividade de garantia da qualidade que tem por finalidade diminuir o número de defeitos do software . Esta atividade contribui para redução do custo de manutenção e para a melhora da qualidade do software , durante o processo de desenvolvimento . Isso tem motivado a investigação e proposta de estratégias , técnicas , critérios e ferramentas de teste para diferentes paradigmas de desenvolvimento , tais como procedimental , orientado a objetos e orientado a aspectos . Vários estudos_experimentais  têm sido desenvolvidos para avaliar e comparar critérios de teste . Grande parte desses experimentos foram realizados com programas construídos sob um mesmo paradigma ou desconsiderando a influência do mesmo sobre os resultados . Entretanto , é importante avaliar o impacto de um paradigma específico sobre a atividade de teste uma vez que alguns defeitos podem_estar  relacionados ao seu uso . Este trabalho apresenta um estudo_experimental  realizado para caracterizar e avaliar o custo de aplicação e a dificuldade de satisfação de critérios de teste , comparando dois paradigmas : o orientado a objetos e o procedimental . O estudo considera critérios de teste funcionais e estruturais e utiliza um conjunto de programas do domínio de Estrutura de Dados . Os termos e fases do processo de experimentação controlada foram usados , à medida em que estes se mostraram adequados , para definir e executar o presente estudo . Os objetivos com a execução dessa pesquisa foram obter resultados iniciais sobre as questões investigadas bem como gerar artefatos que sirvam de base para a definição e condução de futuros experimentos e a criação de pacotes de laboratório . Além_disso  , pretende-se apoiar , por meio dos materiais gerados , o treinamento e o ensino da atividade do teste de 
 Desde a definção do problema de obtenção de regras de associação , vários algoritmos eficientes foram introduzidos para tratá-lo . Entretanto , ainda hoje o problema apresenta várias dificuldades práticas para os mineradores , como a determinação de limiares adequados de suporte mínimo e confiança mínima , a manipulação de grandes_conjuntos  de regras , e a compreensão de regras ( especialmente aquelas contendo muitos itens ) . Para tratar estes problemas , pesquisadores têm investigado a aplicação de técnicas interativas , sumarização ( de conjuntos de regras ) e representações_visuais  . Entretanto , nenhuma abordagem na qual os usuários podem entender e controlar o processo por meio da interação com o algoritmo analítico ao longo de sua execução foi introduzida . Neste trabalho , é introduzida uma abordagem interativa para extração e exploração de regras de associação que insere o usuário no processo por meio de : execução interativa do Apriori ; seleção interativa de itemsets freqüentes ; extração de regras baseada em itemsets e orientada por agrupamentos de itemsets similares ; e exploração de regras aos pares . Para validar a abordagem , foram realizados diversos estudos , apoiados pelo Sistema 'I IND.2 ' E , com o objetivo de : comparar a abordagem interativa , sob diversos aspectos , com uma abordagem convencional de obtenção de regras de associação ; avaliar o efeito de variar alguns parâmetros do processo nos resultados finais ; e mostrar a aplicação dos recursos oferecidos em situações reais e com usuários reais . Os resultados indicam que a abordagem apresentada é adequada , tanto em cenários exploratórios quanto em cenários em que há um direcionamento inicial para o processo , à execução de certas tarefas de extração de regras de associação , pois : provém recursos capazes de evitar execuções inteiras do algoritmo antes que os resultados sejam analisados ; gera conjuntos de regras mais compactos ; preserva a cobertura de itemsets ; favorece a reformulação de tarefas ou a formulação de novas tarefas ; e provê meios para comparação visual de regras , aumentando o poder de análise do 
 O comportamento dinâmico das redes sem fio as torna muito peculiares e de difícil análise . No entanto , algumas destas redes , como as de sensores com funcionamento intermitente , redes periódicas ou cíclicas e as do sistema de satélites de órbita baixa têm um comportamento dinâmico relativamente previsível , pois as variações da topologia da rede no tempo são quase que determinísticas . Recentemente , um modelo teórico -- grafos evolutivos -- foi proposto com o intuito de capturar o comportamento dinâmico destas redes e formalizar algoritmos de roteamento de custo mínimo , além de outros . Os algoritmos e idéias obtidos com este modelo são teoricamente muito eficientes , mas , no entanto , antes deste trabalho não existiam estudos do uso destes modelos em situações práticas . Assim , o objetivo deste trabalho é analisar a aplicabilidade da teoria de grafos evolutivos na construção de protocolos de roteamento eficientes em cenários realistas . Foram implementados dois protocolos de roteamento para redes móveis ad hoc baseados nos algoritmos de grafos evolutivos , são eles : Jornada que Chega Mais Cedo e Jornada Mais Curta . Extensivas simulações foram realizadas utilizando o simulador de redes NS2 e os resultados foram comparados com outros quatro protocolos clássicos para este tipo de rede : AODV , DSR , OLSR e DSDV . Os resultados preliminares mostram que este recente modelo tem muito potencial para ser uma ferramenta poderosa no desenvolvimento e análise de algoritmos para redes dinâmicas com comportamento previsível . No entanto , foram apontados alguns aspectos que precisam ser melhores estudados para que estes algoritmos possam ser utilizados em situações reais 
 Um importante mecanismo da arquitetura orientada a serviços é o componente de registro de serviços . Ele permite a interação entre provedores e clientes , oferecendo um meio de acesso aos serviços desenvolvidos e publicados no registro . Nesta dissertação é proposto o desenvolvimento de um registro de serviços para dar_apoio  à publicação , busca e classificação de serviços_Web  , em particular , àqueles relacionados a ferramentas de teste de software . Uma limitação comum dos serviços de registro refere-se às buscas realizadas , pois são basicamente sintáticas e podem trazer resultados pouco relacionados aos interesses do usuário . Para resolver este problema uma ontologia de teste foi adaptada e incorporada ao registro com o objetivo de oferecer facilidades de busca e agregar informação semântica nos serviços registrados . Uma arquitetura genérica baseada em serviços para o domínio de engenharia de software é apresentada e instanciada para o domínio de teste de software com o objetivo de auxiliar no entendimento e implementação do registro de serviços proposto . Também são apresentados exemplos de ferramentas de teste publicadas no registro e um exemplo de busca e interação com o serviço de teste JaBUTiWS , previamente publicado no registro , que tem por objetivo apoiar o teste_estrutural  de componentes e 
 Este trabalho apresenta a avaliação de diferentes algoritmos de roteamento utilizados na camada lógica ponto a ponto ( P2P ) adotada por um Metaescalonador que provê Qualidade de Serviços ( QoS ) na Computação em Nuvem . Experimentos mostram a superioridade de três algoritmos de roteamento P2P ( BCR , Chord e Pastry ) em relação à utilização de Round Robin , analisando-se o tempo de resposta e a variabilidade entre os resultados obtidos em diferentes testes . Os experimentos consideram , além dos algoritmos de roteamento , a influência do número de usuários e do tipo de serviço requisitado e como esses fatores interagem entre si . É apresentado ainda um estudo sobre a melhor métrica a ser adotada para representar as informações da rede . As métricas consideradas foram latência e número de saltos . Os resultados obtidos permitem determinar , com base nos objetivos especificados , qual o impacto dos sistemas P2P utilizados pelo metaescalonador na busca e descoberta de serviços em relação à forma como a qualidade de serviços é 
 Navegação autônoma é um tópico abrangente cuja atenção por parte da comunidade de robôs_móveis  vemaumentando ao longo dos anos . O problema consiste em guiar um robô de forma inteligente por um determinado percurso sem ajuda humana . Esta dissertação apresenta um sistema de navegação para ambientes abertos baseado em visão estéreo . Uma câmera estéreo é utilizada na captação de imagens do ambiente e , utilizando o mapa de disparidades gerado por um método estéreo semi-global , dois métodos de detecção de obstáculos são utilizando para segmentar as imagens em regiões navegáveis e não navegáveis . Posteriormente esta classificação é utilizada em conjunto com um método de desvio de obstáculos , resultando em um sistema completo de navegação_autônoma  . Os resultados obtidos por está dissertação incluem a avaliação de dois métodos estéreo , esta sendo favorável ao método estéreo empregado ( semi-global ) . Foram feitos testes visando avaliar a qualidade e custo_computacional  de dois métodos para detecção de obstáculos , um baseado em plano e outro baseado em cone . Tais testes deixaram claras as limitações de ambos os métodos e levaram a uma implementação paralela do método baseado em cone . Utilizando uma unidade de processamento gráfico , a versão paralelizada do método baseado em cone atingiu um ganho no tempo computacional de aproximadamente dez vezes . Por fim , os resultados demonstrarão o sistema completo em funcionamento , onde a plataforma robótica utilizada , um veículo elétrico , foi capaz de desviar de pessoas e cones alcançando seu objetivo 
 Atualmente as projeções de demanda e ganho tornam-se variáveis importantes no processo de tomada de decisões para investimentos envolvendo custo e capital , em pesquisa de mercado envolvendo produtos de consumo , em pesquisa de populações e em qualquer outro tipo de previsões que tenham a ver com ganhos ou demandas futuras , por exemplo o volume de água que é preciso para ser gerada determinada quantidade de energia consumido por uma população através de um sistema de operação e planejamento de um sistema hidroelétrico , etc . Em resposta desse interesse muitos estudos examinaram a possibilidade de gerar previsões usando séries_temporais  , ajustando modelos mediante a metodologia de Box e Jenkins , porém estas séries sugeriam variabilidade maior em diferentes níveis , violando deste modo a suposição de variância constante na formulação dos modelos ARIMA . Nestas situações , é comum na prática , contemplar uma extensão destes modelos , assumindo que alguma transformação da série obedeça um modelo ARIMA , frequentemente são usadas transformações de Box e Cox , porém as previsões destas séries transformadas afeta as interpretações em quanto à série original . Uma abordagem combinada de métodos clássicos e bayesianos é apresentada no tratamento destas transformações , os quais estimam junto com os parâmetros do modelo a potência desta transformação , apresentamos também uma alternativa para examinar a estrutura das auto-covariâncias através do Polinómio de Hermite . A pergunta que surge é , se a incorporação destas transformações resulta numa melhora nas previsões . No caso particular apresentamos resultados em processos auto-regressivos . É feita uma aplicação destes métodos em séries de vazões medias mensais no Reservatório de Furnas 
 Este trabalho apresenta a implementação e a utilização da migração de processos SPMD ( Single Program Multiple Data ) , a qual realiza somente a transferência dos dados , que estão sendo manipulados pelo processo , para realizar a migração . Seu principal objetivo foi o estudo do impacto do balanceamento de carga no desempenho de aplicações , desenvolvidas utilizando o modelo de programação SPMD . Depois de realizados testes com aplicações SPMD reais , em sistemas computacionais_distribuídos  utilizando a migração de processos SPMD , foi possível verificar que ganhos de desempenho podem ser alcançados , tanto na migração de processos quanto no tempo de execução de aplicações paralelas SPMD 
 Context : Systematic literature review ( SLR ) is a methodology used to aggregate all relevant evidence of a specific research question . One of the activities associated with the SLR process is the selection of primary studies . The process used to select primary studies can be arduous , particularly when the researcher faces large volumes of primary studies . Another activity associated with an SLR is the presentation of results of the primary studies that meet the SLR purpose . The results are generally summarized in tables and an alternative to reduce the time consumed to understand the data is the use of graphic representations . Systematic mapping ( SM ) is a more open form of SLR used to build a classification and categorization scheme of a field of interest . The categorization and classification activities in SM are not trivial tasks , since they require manual effort and domain of knowledge by reviewers to achieve adequate results . Although clearly crucial , both SLR and SM processes are time-consuming and most activities are manually conducted . Objective : The aim of this research is to use Visual Text Mining ( VTM ) to support different activities of SLR and SM processes , e.g. , support the selection of primary studies , the presentation of results of an SLR and the categorization and classification of an SM . Method : Extensions to the SLR and SM processes based on VTM were proposed . A series of case studies were conducted to demonstrate the usefulness of the VTM techniques in the selection , review , presentation of results and categorization context . Results : The findings have showed that the application of VTM is promising in terms of providing positive support to the study selection activity and that visual representations of SLR data have led to a reduction in the time taken for their analysis , with no loss of data comprehensibility . The application of VTM is relevant also in the context of SM . Conclusions : VTM techniques can be successfully employed to assist the SLR and SM 
 A simulação de escoamentos com superfícies_livres  vem ganhando importância ao longo dos últimos_anos  devido às várias aplicações práticas em que esse tipo de escoamento está envolvido . Dentre os métodos_numéricos  existentes para a simulação de escoamentos , temos o GENSMAC , que é uma técnica numérica para simular_escoamentos  newtonianos com superfícies_livres  . A implementação de métodos semi-implícitos para a discretização temporal das equações de Navier-Stokes permitiu uma relaxação significativa na restrição no passo temporal , reduzindo consideravelmente o custo_computacional  na simulação de escoamentos com Re 1 . Mas , mesmo no caso dos métodos semi-implícitos , o passo temporal não pode aumentar além de certos limites , bem aquém daquele da restrição CFL , sem provocar sérios problemas de precisão nos resultados numéricos e consequente aparecimento de resultados não físicos . Portanto , mesmo na formulação semi-implícita , uma restrição no passo temporal é aplicada . Neste trabalho , analisamos e implementamos no sistema FREEFLOW2D uma estratégia de adaptação do passo temporal de maneira a garantir a estabilidade e a precisão utilizando o maior passo temporal possível . A eficiência e robustez da técnica incorporada à formulação implícita do GENSMAC são demonstradas na solução de problemas bidimensionais complexos com superfícies_livres  e baixo número de Reynolds , incluindo os problemas do inchamento do extrudado e jet 
 Com base em métodos de extração de características de imagens e extração de vocabulários de textos , podemos empregar técnicas para posicionamento de dados multidimensionais no plano para mapear conjuntos de dados em espaços visuais , auxiliando usúarios na interpretação e análise dos dados . Alguns desses métodos constroem árvores de similaridade , impondo uma hierarquia sobre as relações entre as características_extraídas  dos dados . Em um ambiente de análise exploratória , é natural que se procurem métodos e técnicas capazes de manipular e interagir com os dados de forma rápida e eficiente . Nesse contexto , o trabalho visa implementar e aplicar técnicas conhecidas de navegação e interação em árvores no contexto de visualizações baseadas em posicionamento de pontos no plano . Em particular as técnicas NJ e MST , implementadas e utilizadas com sucesso na ferramenta PEx-Image , tornaram-se pontos chave para o auxílio na exploração dos dados através das apresentações radial e hiperbólica , implementadas juntamente com ferramentas de exploração . Este trabalho implementa e apresenta a capacidade exploratória dessas duas formas de apresentação de árvores sobre as visualizações NJ e MST 
 Este trabalho de mestrado apresenta um estudo de Grades Computacionais e Simulações Distribuídas sobre a técnica MRIP . A partir deste estudo foi possível propor e implementar o protótipo de uma ferramenta para Gerenciamento de Experimento em Ambiente de Grade , denominada Grid Experiments Manager - GEM , organizada de forma modular podendo ser usada como um programa ou integrada com outro software , podendo ser expansível para vários middlewares de Grades Computacionais . Com a implementação também foi possível avaliar o desempenho de simulações sequenciais com aquelas executadas em cluster e em uma Grade Computacional de teste , sendo construído um benchmark que possibilitou repetir a mesma carga de trabalho para os sistemas sobre avaliação . Com os testes foi possível verificar um ganho alto no tempo de execução , quando comparadas as execuções sequenciais e em cluster , obteve-se eficiência em torno de 197 % para simulações com tempo de execução baixo e 239 % para aquelas com tempo de execução maior ; na comparação das execuções em cluster e em grade , obteve-se os valores para eficiência de 98 % e 105 % , para simulações pequenas e grandes , 
 A área de melhoria de processos de software ( MPS ) tem sido investigada sistematicamente , dadas as evidências de que a qualidade do processo pode influenciar significativamente na qualidade do produto_final  . Modelos e guias com boas práticas para a MPS têm sido sintetizados . Ainda assim , a literatura carece de trabalhos que estabeleçam estratégias de como implementar as boas práticas sugeridas por tais modelos e guias na indústria . Em paralelo , o desenvolvimento distribuído de software ( DDS ) tornou-se uma realidade , aumentando a complexidade e a importância do processo de desenvolvimento de software e demandando estratégias que permitam uma MPS também distribuída . O objetivo deste trabalho é investigar estratégias e mecanismos que possam promover uma MPS distribuída e colaborativa . A ColabSPI , uma estratégia colaborativa e distribuída para MPS , é proposta para apoiar a evolução estruturada do processo ; o tratamento de propostas de melhorias de processo ; e a comunicação e participação dos desenvolvedores na MPS . Durante a investigação , fatores que podem influenciar a MPS foram identificados e a relação entre eles foi mapeada , tanto a partir da literatura quanto a partir de um estudo em campo . Dois contextos foram explorados durante a investigação : ( i ) a MPS em uma organização de grande porte , com unidades distribuídas ; e ( ii ) a MPS no desenvolvimento de software_livre  ( SL ) . Contribuições para a MPS no desenvolvimento de SL foram geradas no contexto do projeto internacional Qualipso , com a co-autoria do Qualipso Open Source Maturity Model ( OMM ) e a adequação de ColabSPI para evoluir o OMM . Em ambiente industrial , estudos_experimentais  foram conduzidos para validar a aplicação de ColabSPI e revelaram que algumas práticas do desenvolvimento de software , inclusive do DDS e do desenvolvimento de SL , podem ser aplicadas com sucesso à MPS , trazendo ganhos de eficácia e eficiência para a melhoria de um processo de desenvolvimento de software . A estratégia proposta está sendo base também para a definição do ambiente de MPS do processo Demoiselle , um processo livre para desenvolvimento de 
 Neste trabalho apresenta-se uma avaliação do desempenho da implementação de memória transacional da linguagem Clojure , utilizada como biblioteca de sincronização para uso em conjunto com outras aplicações dentro da máquina virtual de Java . É implementada uma camada de interface entre as estruturas de dados de Clojure e o benchmark STMBench7 e são discutidos alguns aspectos que geram sobrecarga no desempenho 
 Neste projeto estudamos algoritmos de aproximação para o problema da árvore de Steiner com coleta de prêmios . Trata-se de uma generalização do problema da árvore de Steiner , onde é dado um grafo com custos positivos nas arestas e penalidades positivas nos vértices . O objetivo é encontrar uma subárvore do grafo que minimize a soma dos custos das arestas mais a soma das penalidades dos vértices que não pertencem à subárvore . Em 2009 , os autores Archer , Bateni , Hajiaghayi e Karloff obtiveram pela primeira vez um algoritmo com fator de aproximação estritamente menor do que 2 . Além de analisarmos este algoritmo , estudamos também a implementação de algoritmos 2-aproximação para o problema da árvore de Steiner e da árvore de Steiner com coleta de prêmios 
 Este trabalho apresenta um método empírico para previsão de sobrecargas transientes em sistemas computacionais por meio de modelagem dinâmica . A técnica , baseada em aproximações lineares e invariantes no tempo , tem como objetivo identificar a capacidade de um sistema computacional absorver variações na carga de trabalho . Experimentalmente , a identificação dessa capacidade do sistema pode ser feita por meio de técnicas de avaliação de desempenho , em que a abordagem prevalente é a estimação da capacidade estática em regime estacionário de operação , observando-se o desempenho sob_demanda  constante . Entretanto , essa avaliação não considera o regime transiente do sistema , i.e durante o período de restabelecimento ao regime estacionário após uma perturbação , e durante o qual , o esforço exigido pode ser bastante diverso , e potencialmente acima daquele apurado sob condições de regime estacionário . A proposta deste trabalho é a formulação de uma metodologia para avaliação de desempenho em regime transiente em sistemas computacionais sob carga de trabalho variável e que forneça informação para o dimensionamento de recursos e políticas de controle de admissão que evitem sobrecargas por efeitos transitórios . A metodologia baseia-se na parametrização de um modelo dinâmico a partir de ensaios experimentais , considerando perturbações bruscas e de longa duração , e os resultados são avaliados por comparação das predições do modelo em relação aos objetivos por simulação ou aferição 
 Sistemas de visão_computacional  ( SVCs ) são interessantes ferramentas para a navegação de robôs . Este trabalho propõe um SVC baseado em redes neurais multi-camadas para o controle de um robô_móvel  . Dois módulos principais compõe o SVC : módulo de visão e módulo de controle de navegação . O primeiro é dividido em três partes : pré-processamento , segmentação e reconhecimento das imagens . este módulo processa as imagens ( obtidas por uma câmera ) que podem conter diversos com cores diferentes , retornando a posição ou a forma e a posição de um dos objetos , o qual corresponde a cor especificada . O processamento das imagens é realizada por redes neurais multi-camadas . O módulo de controle é responsável por validar os resultados do módulo de visão e conduzir o robô utilizando os dados provenientes do módulo de visão . O objetivo do sistema proposto é capacitar um robô a realizar as tarefas : seguir um objeto de cor determinada , seguir um objeto de cor e forma determinada ou ainda navegar pelo ambiente seguindo um objeto de cor determinada evitando obstáculos . Vários experimentos são apresentados , em um ambiente real , utilizando o robô . Pioneer I , para mostrar as vantagens e desvantagens do sistema proposto 
 Neste trabalho é desenvolvido um método_numérico  para resolver escoamentos viscoelásticos com superfícies_livres  . As equações governantes para escoamentos governados pelo modelo de Maxwell juntamente com as condições de contorno para escoamentos bidimensionais usando coordenadas cartesianas são apresentadas . As equações são resolvidas utilizando o método de diferenças_finitas  numa malha_deslocada  . Também , é desenvolvida uma formulação para o cálculo do tensor não-Newtoniano em contornos rígidos . As condições de contorno na superfície_livre  são discutidas em detalhes . Resultados_numéricos  mostrando a convergência do método_numérico  desenvolvido nesse trabalho são apresentados . Finalmente , são apresentados resultados numéricos que mostram que a técnica numérica empregada nesse trabalho é capaz de simular_escoamentos  viscoelásticos governados pelo modelo de Maxwell . Em particular , os seguintes problemas são simulados : jato oscilante , inchamento do extrudado e uma gota de fluido viscoelástico incidindo sobre uma superfície rígida 
 Sistemas de informações geográficas permitem armazenar , manipular e armazenar , manipular e analisar dados espaciais e aspectos descritivos desses dados . A análise de dados espaciais pode ser realizada por meio de técnicas de extração de regras de associação , ou seja , regras que descrevem relacionamentos entre os dados . Porém , a mineração de regras de associação não considera as relações topológicas existentes entre dados georreferenciados . Para solucionar esse problema , Koperski and Han ( 1995 ) e Malerba et_al  . ( 2001 ) propuseram um processo de extração de regras integrado ao algoritmo de mineração e utilizavam predicados lógicos para representar as regras . Como alternativa a essa solução , este trabalho propõe pré-processar os dados referenciados para encontrar relações topológicas em separado e aplicar um algoritmo de mineração de regras de associação disponí ? vel pela comunidade acadêmica . As regras geradas devem apresentar características descritivas dos dados e relações topológicas . Para atingir esse objetivo foi especificado um processo de extração de regras em dados georreferenciados e implementado um módulo de pré-processamento que extrai relações topológicas . O módulo foi avaliado por meio de um estudo de caso utilizando o sistema de informação geográfica da cidade de Jaboticabal , no contexto de planejamento urbano . As regras encontradas foram analisadas por um especialista utilizando as medidas de suporte e confiança . Além_disso  , uma análise sobre o tempo de processamento e consumo de memória para encontrar as relações topológicas foi realizada , mostrando que é possível extrair padrões utilizando o processo e o módulo proposto neste trabalho 
 Esta dissertação tem por objetivo apresentar a proposta de uma política de escalonamento para grids computacionais . Essa política , intitulada Dynamic Max-Min2x , é orientada ao escalonamento de aplicações cujas tarefas não realizam comunicação entre si e visa a redução do tempo de resposta dessas aplicações através da utilização de atribuição dinâmica de tarefas e replicação das mesmas . Experimentos , feitos através de simulação , mostram que o tempo_médio  de resposta de aplicações utilizando-se a Dynamic Max-Min2x é inferior ao de outras políticas da literatura . Análises dos resultados desses experimentos apontam que esse tempo tende a ser mais atrativo principalmente quando as tarefas necessitam de muito processamento e quando há grande variação de carga no sistema , caracteristicas comuns em grids computacionais . Além_disso  , esta dissertação apresenta a implementação de um framework utilizando-se o Globus Toolkit , onde é possível a inserção de políticas de escalonamento para a submissão inteligente de tarefas em um grid computacional 
 Este trabalho apresenta uma análise de comportamento de índices de carga relacionados ao uso e à atividade de memória . São descritos 11 índices que refletem direta ou indiretamente a carga de memória . Esses índices podem ser obtidos facilmente no sistema peracional GNU/Linux através do sistema de arquivos /proc . Uma ferramenta de monitoramento foi criada para facilitar a análise de comportamento , podendo também servir para fornecer informações de carga para políticas de escalonamento . Os valores de cada índice foram monitorados durante a execução de uma carga de trabalho composta por aplicações reais que utilizam altas quantidades de memória . A partir dos resultados é possível descobrir a utilidade de cada índice , indicando como eles podem ser usados para auxiliar políticas de escalonamento a avaliar a carga de memória de uma máquina . Métricas de avaliação de carga originárias da combinação de mais de um índice são descritas . Essas métricas foram criadas em casos em que a combinação de mais de um índice permitia representar a carga de memória com mais precisão do que índices usados isoladamente . As métricas e índices pesquisados proporcionam formas precisas de representar a carga de memória em vários níveis , desde níveis de baixa utilização até níveis de saturação da memória principal com sintomas de ocorrência de thrashing 
 Na literatura da área da análise de sobrevivência existem os modelos tradicionais , ou sem fração de cura , e os modelos de longa duração , ou com fração de cura . Recentemente tem sido proposto um modelo mais geral , conhecido como o modelo com fatores de risco latentes com esquemas de ativação . Nesta tese são deduzidas novas propriedades que possuem a função de sobrevivência , a função de taxa de risco e o valor esperado , quando e considerado o modelo com fatores de risco latentes . Estas propriedades são importantes , já que muitos outros modelos que tem aparecido na literatura recentemente podem ser considerados como casos particulares do modelo com fatores de risco latentes . Além disto , são propostos novos modelos de sobrevivência e estes são aplicados a conjuntos de dados reais . Também é realizado um estudo de simulação e uma análise de sensibilidade , para mostrar a qualidade destes 
 Em diversos_domínios  , além das informações sobre os objetos ou entidades que os compõem , existem , também , informaçõoes a respeito das relações entre esses objetos . Alguns desses domínios são , por exemplo , as redes de co-autoria , e as páginas_Web  . Nesse sentido , é natural procurar por técnicas de classificação que levem em conta estas informações . Dentre essas técnicas estão as denominadas classificação baseada em grafos , que visam classificar os exemplos levando em conta as relações existentes entre eles . Este trabalho aborda o desenvolvimento de métodos para melhorar o desempenho de classificadores baseados em grafos utilizando estratégias de ensembles . Um classificador ensemble considera um conjunto de classificadores cujas predições individuais são combinadas de alguma_forma  . Este classificador normalmente apresenta um melhor desempenho do que seus classificadores individualmente . Assim , foram desenvolvidas três técnicas : a primeira para dados originalmente no formato proposicional e transformados para formato relacional baseado em grafo e a segunda e terceira para dados originalmente já no formato de grafo . A primeira técnica , inspirada no algoritmo de boosting , originou o algoritmo KNN Adaptativo Baseado em Grafos ( A-KNN ) . A segunda ténica , inspirada no algoritmo de Bagging originou trê abordagens de Bagging Baseado em Grafos ( BG ) . Finalmente , a terceira técnica , inspirada no algoritmo de Cross-Validated Committees , originou o Cross-Validated Committees Baseado em Grafos ( CVCG ) . Os experimentos foram realizados em 38 conjuntos de dados , sendo 22 conjuntos proposicionais e 16 conjuntos no formato relacional . Na avaliação foi utilizado o esquema de 10-fold stratified cross-validation e para determinar diferenças estatísticas entre classificadores foi utilizado o método proposto por Demsar ( 2006 ) . Em relação aos resultados , as três técnicas melhoraram ou mantiveram o desempenho dos classificadores bases . Concluindo , ensembles aplicados em classificadores baseados em grafos apresentam bons_resultados  no desempenho 
 O presente_trabalho  investiga as diculdades enfrentadas por decientes visuais ao ingressarem em um curso de nvel superior em Música , onde a troca de informacão musical escrita é frequente e se dá por meio de partituras impressas em tinta , e a conversão deste material para braille demanda conhecimentos específicos e disponibilidade de recursos . Igualmente problemática , a produção musical do aluno cego é feita em braille , seja para tomar nota de aulas como para realizar tarefas de disciplinas como Contraponto , Harmonia e Análise Musical , ou mesmo para a realização de exames . Claramente , esse material deve passar por um processo de conversão para que o professor possa avaliar o aluno , entre outros motivos . O foco_principal  da pesquisa realizada é a analise da musicografia braille sob a ótica das possibilidades de se produzir transcrições automáticas entre partituras em braille e tinta , a fim de prover recursos tecnológicos direcionados a solução deste problema . Para tanto , foi desenvolvido um aplicativo capaz de receber informação musical em braille e converê-la para o formato MusicXML , adequado para a leitura a partir de outros aplicativos de notação musical e , consequentemente , a impressão deste material em tinta . Este programa esta sendo distribudo como software_livre  sob licenca LGPL , contrapondo-se as suas alternativas hoje existentes no mercado . O aplicativo desenvolvido foi utilizado e avaliado por usuarios decientes visuais e com visão normal por meio de um questionário . Os dados foram então analisados , buscando mapear as diferenças nas experiências de uso e verificar necessidades de melhorias e novas_funcionalidades  , buscando com isso o aprofundamento nas questões pertinentes ao problema e dando suporte a novas pesquisas relativas ao assunto 
 A presença de onda de choque e vórtices de pequena escala exigem métodos_numéricos  mais sofisticados para simular_escoamentos  compressíveis em velocidades altas . Alguns desses métodos produzem resultados adequados para regiões com função suave , embora os mesmos não possam ser utilizados diretamente em regiões com função descontínua , resultando em oscilações espúrias . Dessa forma , métodos foram desenvolvidos para solucionar esse problema , apresentando um bom_desempenho  para regiões com função descontínua ; entretanto , estes possuem termos de alta dissipação . Para evitar os problemas encontrados , foram desenvolvidos os métodos híbridos , onde dois métodos com características ideais para cada região são combinados através de uma função detectora que analisa numericamente a variação de uma quantidade em uma região através de fórmulas que envolvem derivadas . Um detector de descontinuidades foi desenvolvido a partir da revisão bibliográfica de diversos métodos_numéricos  híbridos existentes , sendo avaliadas as principais desvantagens e limitações de cada um . Diversas comparações entre o novo detector e os detectores de descontinuidades já desenvolvidos foram realizadas através da aplicação em funções unidimensionais e bidimensionais . Finalmente , o método híbrido foi aplicado para a solução das equações de Euler unidimensionais e bidimensionais 
 Neste trabalho a família de métodos de pontos_interiores  barreira logarítmica é desenvolvida para o problema de regressão pela norma Lp e a estrutura matricial resultante é explorada objetivando uma implementação eficiente . Apresentamos alguns conceitos sobre métodos de pontos_interiores  necessários para o desenvolvimento do método e descrevemos um método de convergência quadrática previamente conhecido . Uma implementação em Matlab dos métodos de pontos_interiores  desenvolvidos é comparada com uma implementação do método quadrático existente , obtendo desempenho computacional superior 
 Esta tese apresenta uma nova metodologia para animação de objetos líquidos em imagens . Contrariamente às técnicas existentes , este método é baseado em um modelo físico , o que proporciona efeitos realísticos . A perspectiva da imagem é obtida com a intervenção do usuário , por um esquema simples de calibração da câmera , o qual permite a projeção da camada da imagem a ser animada sobre um plano horizontal no espaço tridimensional . As equações de águas rasas conduzem a simulação e as informações de altura são projetadas de volta ao espaço da imagem utilizando traçado de raios . Além_disso  , efeitos de refração e iluminação são aplicados durante este estágio , resultando em animações realísticas e 
 Esta dissertação flexibiliza a suposição de normalidade , dispondo de distribuições assimétricas em modelos de crescimento . Propõe uma abordagem_bayesiana  para ajuste de modelos não lineares quando a suposição de normalidade para os erros não é razoável e/ou apresentam heteroscedasticidade . Assim , adota-se as distribuições skew-normal e skew-t para as situações em que é necessário modelar dados com caudas mais pesadas ou mais leves que a normal e assimétricos ; sendo que é considerado também a presença de heteroscedasticidade . Diferentes funções são utilizadas na estrutura multiplicativa para modelar a variância . Com esse objetivo , métodos de inferência na abordagem_bayesiana  são desenvolvidos para estimar os parâmetros dos modelos de regressão não linear com os erros seguindo as distribuições citadas anteriormente . A metodologia visa aplicação à curvas de crescimento para dados de 
 Esta tese apresenta um estudo sobre análise de séries_temporais  , a qual foi conduzida baseada na seguinte hipótese : séries_temporais  influenciadas por ruído aditivo podem ser decompostas em componentes estocásticos e determinísticos que ao serem modelados individualmente permitem obter um modelo híbrido de maior acurácia . Essa hipótese foi confirmada em duas etapas . Na primeira , desenvolveu-se uma análise formal usando o teorema de amostragem proposto por Nyquist-Shannon , provando que IMFs ( Intrinsic Mode Functions ) extraídas pelo método EMD ( Empirical Mode Decomposition ) podem ser combinadas de acordo com suas intensidades de frequência para formar os componentes estocásticos e determinísticos . Considerando essa prova , duas_abordagens  de decomposição de séries foram desenvolvidas e avaliadas em aplicações sintéticas e reais . Resultados experimentais confirmaram a importância de decompor séries_temporais  e modelar seus componentes estocásticos e determinísticos , provando a segunda parte da hipótese . Além_disso  , notou-se que a análise individual desses componentes possibilita detectar padrões e extrair importantes informações implícitas em séries_temporais  . Essa tese apresenta ainda duas novas medidas . A primeira é usada para avaliar a acurácia de modelos utilizados para predizer observações . A principal vantagem dessa medida em relação às existentes é a possibilidade de avaliar os valores individuais de predição e o comportamento global entre as observações preditas e experadas . A segunda medida permite avaliar a influência dos componentes estocásticos e determinísticos sobre as séries_temporais  . Finalmente , essa tese apresenta ainda resultados obtidos por meio de uma revisão_sistemática  da literatura , a qual coletou importantes trabalhos_relacionados  , e dois novos métodos para geração de dados substitutos , permitindo investigar a presença de processos Gaussianos lineares e não-lineares , independente da influência de comportamento 
 Recentemente , diversas tecnologias de análise de expressão_gênica  têm sido introduzidas . Os miroarrays estão entre as mais utilizadas . Dentre suas aplicações mais comuns , pode-se destacar a classificação de amostras de tecido , essencial para a identificação correta do tipo de câncer . Esta classificação é realizada com a ajuda de algoritmos de AMáquina ( AM ) , como as Máquinas de Vetores de Suporte , ou simplesmente SVMs . Uma particularidade dos dados de expressão_gênica  é que a quantidade de amostras utilizadas pelo algoritmo de aprendizado é , normalmente , muitas_vezes  inferior à quantidade de características consideradas , o que pode deteriorar o desempenho dos algoritmos de AM e dificultar a compreensão dos dados . Neste contexto , o presente_trabalho  visa à comparação de diversas técnicas de seleção de características ( SC ) em SVMs aplicadas a dados microarrays . Além_disso  , durante a pesquisa , foram desenvolvidas 2 novas técnicas de SC baseadas em algoritmos genéticos . Os experimentos demonstram que a maioria das técnicas testadas é capaz de reduzir sobremaneira a dimensionalidade dos dados de expressão_gênica  sem prejudicar o desempenho das SVMs 
 Este trabalho apresenta parte de um sistema de simulação integrado para escoamento de fluido incompressível bidimensional em malhas não estruturadas denominado UmFlow-2D . O sistema consiste de três_módulos  : um módulo modelador , um módulo simulador e um módulo visualizador . A parte do sistema apresentado neste trabalho é o módulo simulador . Este módulo , implementa as equações de Navier-Stokes . As equações governantes são discretizadas pelo método de diferenças_finitas  generalizadas e os termos convectivos pelo método semi-lagrangeano . Um método de projeção é empregado para desacoplar as componentes da velocidade e pressão . O gerenciamento da malha , não estruturada é feito pela estrutura de dados SHE . Os resultados numéricos obtidos pelo UmFlow-2D são comparados com soluções analíticas e soluções numéricas de outros trabalhos 
 A maioria dos sistemas embarcados hoje desenvolvidos utilizam complexos sistemas eletrônicos integrados em um único chip , os Systems-on-a-Chip ( SoC ) . A análise do comportamento de uma aplicação em execução , ou seja , o profiling nesses sistemas não é uma tarefa trivial em virtude da complexidade dos SoCs e pela restrição de ferramentas de profiling adequadas . Neste contexto , este trabalho apresenta o P2L , uma ferramenta de profiling que se baseia em métricas de nível de instrução e função para o processador LEON3 . O P2L fornece estatísticas detalhadas de uso do processador , memórias e barramento de programas em execução sem uso de instrumentação . A ferramenta é composta por um componente em hardware e drivers e aplicativos em software . Os resultados mostram que o P2L fornece medidas com erro inferior a 1 % e overhead desprezível quando comparado ao tempo de execução nativa do programa e ao do profiler GNU gprof 
 Nas últimas_décadas  , com o crescimento acelerado na geração e armazenamento de dados , houve um aumento na necessidade de criação e gerenciamento de grandes bases de dados . Logo , a utilização de técnicas de mineração de dados adequadas para descoberta de padrões e informações úteis em bases de dados é uma tarefa de interesse . Em especial , bases de séries_temporais  têm sido alvo de pesquisas em áreas como medicina , economia e agrometeorologia . Em mineração de dados , uma das tarefas mais exploradas é a classificação . Entretanto , é comum em bases de séries_temporais  , a quantidade e complexidade de dados extrapolarem a capacidade humana de análise manual dos dados , o que torna o processo de supervisão dos dados custoso . Como consequência disso , são produzidos poucos dados rotulados , em comparação a um grande volume de dados não rotulados disponíveis . Nesse cenário , uma abordagem adequada para análise desses dados é a classificação semissupervisionada , que considera dados rotulados e não rotulados para o treinamento do classificador . Nesse contexto , este trabalho de mestrado_propõe  1 ) uma metodologia de análise de dados obtidos a partir de séries_temporais  de imagens de satélite ( SITS ) usando tarefas de mineração de dados e 2 ) uma técnica baseada em grafos para classificação semissupervisionada de séries_temporais  extraídas de imagens de satélite . A metodologia e a técnica de classificação desenvolvidas são aplicadas na análise de séries_temporais  de índices de vegetação obtidas a partir de SITS , visando a identificação de áreas de plantio de cana-de-açúcar . Os resultados obtidos em análise experimental , realizada com apoio de especialistas no domínio de aplicação , indicam que a metodologia_proposta  é adequada para auxiliar pesquisas em agricultura . Além_disso  , os resultados do estudo comparativo mostram que a técnica de classificação semissupervisionada desenvolvida supera métodos de classificação supervisionada consolidados na literatura e métodos correlatos de classificação semissupervisionada 
 Estudos atuais na área de Interação Humano-Computador evidenciam a importância de se considerar aspectos emocionais na interação com sistemas computacionais . Acredita-se que ao permitir agentes artificiais identificar emoções de usuários , em uma interação humano-computador , torna-se possível induzir e despertar emoções a fim de estimulá-los em suas atividades . Um dos grandes desafios dos pesquisadores em Interação humano-computador é prover sistemas capazes de reconhecer , interpretar e reagir de modo inteligente e sensível às emoções do usuário , para atender aos requisitos do maior número possível de indivíduos ; um dos caminhos que se apresenta é o desenvolvimento de sistemas flexíveis . O principal objetivo de se promover essa interação emotiva é contribuir para o aumento da coerência , consistência e credibilidade das reações e respostas computacionais providas durante a interação humana via interface humano-computador . Nesse contexto , surge a oportunidade de explorar sistemas computacionais capazes de identificar e inferir o estado_emocional  do usuário em tempo de execução . Este projeto tem como objetivo desenvolver e avaliar um modelo que possa : i . ) identificar o estado_emocional  do usuário ; ii . ) prover um mecanismo de persuasão com vistas a mudar o estado_emocional  do usuário ( com um estudo de caso em player de música ) e ; iii . ) explorar a abordagem flexível na persuasão ( de acordo com o estado_emocional  particular de cada usuário ) através de mecanismos persuasivos que poderão variar entre um player de música , jogos e/ou vídeos . Assim , ao longo do estudo , o modelo baseado em Comitê de Classificação se mostrou eficiente na identificação das emoções básicas ( alegria , aversão , medo , neutro , raiva , surpresa e tristeza ) com média de acurácia superior a 80 % e , ainda , observou-se a satisfação dos usuários mediante a aplicação do modelo com o player de música 
 Protocolos de autenticação e de estabelecimento de chaves são peças fundamentais em implementações de segurança para comunicação de dispositivos eletrônicos . Em aplicações que envolvam dispositivos com poder_computacional  restrito ( tais como smartphones ou tablets ) comunicando-se com um servidor , é primordial a escolha de protocolos eficientes e que necessitem de uma infraestrutura mais simples . Neste trabalho selecionamos e implementamos protocolos de acordo de chave seguros nos modelos de criptografia de chave_pública  baseado em identidade ( ID-based ) e sem certificado ( Certificateless ) em plataformas com processadores ARM . Comparamos tempos de execução , utilização de memória e uso do canal de comunicação 
 A Plataforma Lattes é uma excelente base de dados de pesquisadores para a sociedade brasileira , adotada pela maioria das instituições de fomento , universidades e institutos de pesquisa do País . Entretanto , é limitada quanto à exibição de dados sumarizados de um grupos de pessoas , como por exemplo um departamento de pesquisa ou os orientandos de um ou mais professores . Diversos projetos já foram desenvolvidos propondo soluções para este problema , alguns inclusive desenvolvendo ontologias a partir do domínio de pesquisa . Este trabalho tem por objetivo integrar todas as funcionalidades destas ferramentas em uma única solução , a SOS Lattes . Serão apresentados os resultados obtidos no desenvolvimento desta solução e como o uso de ontologias auxilia nas atividades de identificação de inconsistências de dados , consultas para construção de relatórios consolidados e regras de inferência para correlacionar múltiplas bases de dados . Além disto , procura-se por meio deste trabalho contribuir com a expansão e disseminação da área de Web Semântica , por meio da criação de uma ferramenta capaz de extrair dados de páginas_Web  e disponibilizar sua estrutura semântica . Os conhecimentos adquiridos durante a pesquisa poderão ser úteis ao desenvolvimento de novas ferramentas atuando em diferentes ambientes 
 A análise de redes_sociais  permite estudar a maneira como são estabelecidas as conexões entre indivíduos e como estas evoluem ao longo do tempo . A coautoria é uma das formas mais estudadas e bem documentadas de colaboração científica . Existem muitos aspectos de redes de colaboração científica , os quais podem ser rastreados de forma confiável através da análise de redes de colaboração usando métodos bibliométricos . Diversos esforços em diferentes áreas de pesquisa tentam analisar , entender , explicar e predizer o comportamento de sistemas modelados através de redes_sociais  . Nestes estudos , os indivíduos são modelados como vértices de um grafo , enquanto as relações entre eles são representadas por arestas . Atualmente , o estudo de redes de colaboração científica é importante e necessário para apoiar o planejamento estratégico , implementação e gestão dos programas de pesquisa científica . Neste trabalho , apresentamos um modelo de análise de redes científicas baseado em Grafos Relacionais com Atributos ( ARG ) . O modelo proposto permite representar as redes de colaboração científica incluindo atributos individuais dos pesquisadores e atributos dos trabalhos colaborativos de pares de pesquisadores . Os dados correspondem às produções científicas de pesquisadores cadastrados na plataforma Lattes e extraídas automaticamente usando a ferramenta scriptLattes . Na primeira etapa , foi implementado o cálculo automatizado da taxa de internacionalização de cada pesquisador , a qual mostra a proporção entre o número de publicações internacionais e o número total de publicações . Esta medida junto com a produção científica individual discretizada em diversos grupos fazem parte das informações armazenadas nos vetores de atributos dos vértice dos ARGs . Por outro_lado  os vetores de atributos das arestas armazenam informações dos trabalhos colaborativos discretizados segundo a classificação da CAPES . Adicionalmente , neste trabalho foram exploradas duas aplicações relacionadas à ( i ) predição de trabalhos colaborativos futuros e à ( ii ) influência dos pesquisadores na rede de colaboração . O resultado da predição de vínculos foi usado para determinar a influência dos pesquisadores na redes de colaboração . A influência tem sido explorada com base na variação da predição de ligações com a presença ou a ausência do pesquisador na rede . Nossa proposta foi avaliada considerando diferentes testes sobre redes de coautoria científica de diversos grupos de pesquisadores . Os resultados obtidos são promissores para a análise de redes_sociais  em geral 
 A detecção de objetos é uma área de extrema importância para sistemas de visão_computacional  . Em especial , dado o aumento constante da utilização de dispositivos_móveis  , torna-se cada vez mais importante o desenvolvimento de métodos e aplicações capazes de serem utilizadas em tais aparelhos . Neste sentido , neste trabalho propõe-se o estudo e implementação de um aplicativo para dispositivos_móveis  capaz de detectar , em tempo real , objetos existentes em ambientes internos com uma aplicação para auxiliar um usuário a se localizar dentro do local . O aplicativo depende somente das capacidades do próprio aparelho e , portanto , procura ser mais flexível e sem restrições . A detecção de objetos é realizada por casamento de grafos-chave entre imagens de objetos pré-escolhidas e a imagem sendo capturada pela câmera do dispositivo . Os grafos-chave são uma generalização do método de detecção de pontos-chave tradicional e , por levarem em consideração um conjunto de pontos e suas propriedades estruturais , são capazes de descrever e detectar os objetos de forma robusta e eficiente . Para realizar a localização , optou-se por detectar placas existentes no próprio local . Após cada detecção , aplica-se um simples , mas bastante eficaz , sistema de localização baseado na comparação da placa detectada com uma base de dados de imagens de todo o ambiente . A base foi construída utilizando diversas câmeras colocadas sobre uma estrutura móvel , capturando sistematicamente imagens do ambiente em intervalos regulares . A implementação é descrita em detalhes e são apresentados resultados obtidos por testes reais no ambiente escolhido utilizando um celular Nokia N900 . Tais resultados são avaliados em termos da precisão da detecção e da estimativa de localização , bem como do tempo decorrido para a realização de todo o processo 
 O principal objetivo deste trabalho 6 estudar o comportamento dos zeros de polinômios ortogonais e similares . Inicialmente , consideramos uma relação entre duas sequências ele polinômios ortogonais , onde as medidas associadas estão relacionadas entre si . Usamos esta relação para estudar as propriedades de monotonicidade dos zeros dos polinômios ortogonais relacionados a uma medida obtida através da generalização da medida associada a uma outra sequência de polinômios ortogonais . Apresentamos , como exemplos , os polinômios ortogonais obtidos a partir da generalização das medidas associadas aos polinômios de Jacobi , Laguerre e Charlier . Em urna segunda etapa , consideramos polinômios gerados por uma certa relação de recorrência de três termos com o objetivo de encontrar limitantes , em termos dos coeficientes da relação de recorrência , para as regiões onde os zeros estão localizados . Os zeros são estudados através do problema de autovalor associado a uma matriz de Hessenberg . Aplicações aos polinômios de Szegó , polinômios para-ortogonais e polinômios com coeficientes complexos não-nulos são consideradas 
 Neste trabalho são apresentadas as equações governantes para um fluido Oldroyd-B juntamente com as condições de contorno para escoamentos viscoelásticos axissimétricos com superfícies_livres  . Um método_numérico  para simular_escoamentos  com superfícies_livres  é apresentado e as equações resultantes são resolvidas utilizando o método de diferenças_finitas  numa malha_deslocada  . São desenvolvidas formulações para o cálculo do tensor extra-tensão em contornos rígidos e no eixo de simetria . As condições de contorno na superfície_livre  são discutidas em detalhes . Os termos convectivos são aproximados pelo método 'upwind ' de alta ordem CUBISTA ( 'A convergent and universally bounded interpolation scheme for the treatment of advection ' ) . O fluido é modelado utilizando a técnica 'Marker-and-Cell ' o que permite visualizar e localizar a superfície_livre  do fluido . Para evitar ondulações , a superfície_livre  é suavizada pela técnica TSUR ( 'Trapezoidal Surface Removal ' ) . O método_numérico  descrito neste trabalho foi implementado no sistema de simulação Freeflow-AXI e validado comparando os resultados numéricos do escoamento em um tubo com a respectiva solução analítica . Resultados_numéricos  incluem : simulação do inchamento do extrudado , gota incidindo contra uma superfície rígida e a simulação do 'splashing drop ' para vários números de Reynolds e de Weissenberg 
 Esta monografia apresenta o desenvolvimento e os resultados obtidos da implementação de um método_numérico  para simular_escoamentos  multifásicos em malhas dinâmicas não estruturadas . As equações de Navier-Stokes são desenvolvidas em uma formulação Lagrangeana-Euleriana arbitrária e são aproximadas utilizando-se o método de elementos finitos . Um método de projeção baseado em decomposição LU é utilizado para desacoplar aceleração e pressão . A interface que define a fronteira livre entre os fluidos imiscíveis é representada explicitamente por vértices e arestas da triangulação , e a tensão interfacial é calculada através de uma distribuição baseada na discretização do gradiente de uma função Heaviside . 0 movimento da malha é computado através da composição entre a velocidade do escoamento e uma velocidade elástica , calculada utilizando-se um filtro Laplaciano a partir da posição dos vértices . O controle da malha dinâmica é feito através de inserção e remoção de pontos baseando-se em triangulações localmente Delaunay , para se manter a qualidade dos elementos . Adicionalmente , é proposto um esquema de distribuição de pontos através da estimativa do erro baseado na Hessiana das velocidades . São apresentadas validações para escoamentos monofásicos e multifásicos , com comparações teóricas e corroboração por outros métodos , ilustrando o bom_desempenho  do método proposto . Adicionalmente , é mostrada uma aplicação a problemas de escoamentos de bolhas , comparando-se os resultados obtidos com resultados de outras técnicas numéricas 
 Neste trabalho de mestrado foi desenvolvido o projeto de uma máquina_paralela  dedicada para solução de sistemas de equações lineares . Este é um problema presente em uma grande variedade de aplicações científicas e de engenharia e cuja solução torna-se uma tarefa computacionalmente intensiva , a medida em que o número de incógnitas aumenta . Implementou-se uma Arquitetura Sistólica unidimensional , conectada numa topologia em anel , que mapeia métodos de solução iterativos . Essa classe de arquiteturas paralelas apresenta características de simplicidade , regularidade e modularidade que facilitam implementações em hardware , sendo muito utilizadas em sistemas de computação dedicados à solução de problemas específicos , que possuem como características básicas a grande demanda computacional e a necessidade de respostas em tempo real . Foram adotadas metodologias e ferramentas avançadas para projeto de hardware que aceleram o ciclo de desenvolvimento e para a implementação foram utilizados circuitos reconfiguráveis FPGAs ( Field Programmable Gate Arrays ) . Os resultados de desempenho são apresentados e avaliados apontado a melhor configuração da arquitetura para atingir um speedup em relação a implementações em máquinas seqüenciais . Também são discutidas as vantagens e desvantagens deste tipo de abordagem e metodologia na solução de problemas que possuem requisitos de tempo 


 Com o advento da Internet , as empresas puderam mostrar-se para o mundo 
 A possibilidade de colocar um negócio na World_Wide  Web ( WWW ) criou 
 novo tipo de dado que as empresas podem utilizar para melhorar ainda 
 seu conhecimento sobre o mercado : a sequência de cliques que um 
 efetua em um site . Esse dado pode ser armazenado em uma espécie de 
 Warehouse para ser analisado com técnicas de descoberta de conhecimento 
 bases de dados . Assim , há a necessidade de se realizar pesquisas para 
 como retirar conhecimento a partir dessas sequências de cliques . Neste 
 são discutidas e analisadas algumas das técnicas utilizadas para 
 esse objetivo . é proposta uma ferramenta onde os dados dessas sequências 
 cliques são mapeadas para o formato_atributo-valor  utilizado pelo Sistema Discover 
 um sistema sendo desenvolvindo em nosso Laboratório para o 
 e execução de experimentos relacionados aos algoritmos de 
 utilizados durante a fase de Mineração de Dados do processo de descoberta 
 conhecimento em bases de dados . Ainda , é proposta a utilização do 
 de Programação Lógica Indutiva chamado Progol para extrair 
 relacional das sessões de sequências de cliques que caracterizam a 
 de usuários com as páginas visitadas no site . Experimentos iniciais com 
 utilização de uma sequência de cliques real foram realizados usando Progol 
 algumas das facilidades já implementadas pelo Sistema Discover 
 O avanço tecnológico dos laboratórios de biologia molecular tem proporcionado um grande aumento no volume de seqüências de nucleotídeos armazenadas em bancos de dados biológicos , introduzindo o desafio de pesquisar eficientemente estes dados . Neste contexto , a árvore de sufixo é um método de acesso utilizado por muitas aplicações que envolvem pesquisa em dados biológicos . Entretanto , o custo de construção das árvores de sufixo é alto devido ao tamanho da estrutura de indexação gerado e à necessidade da árvore de sufixo caber em memória principal para ser construída com complexidade linear em relação ao tempo . Esta dissertação_propõe  o Perseus , uma nova_técnica  para tratar árvores de sufixo persistentes . A técnica Perseus apresenta os seguintes diferenciais . Ela introduz uma abordagem que realiza a construção de árvores de sufixo persistentes cujos tamanhos podem exceder a capacidade da memória principal . Além_disso  , ela provê um algoritmo que constrói árvores de sufixo por meio do particionamento destas árvores somente quando necessário . Esta construção também permite que o usuário escolha quais subseqüências de uma seqüência devem ser indexadas , de acordo com os requisitos particulares de suas aplicações . Por fim , a técnica proposta também introduz um algoritmo de casamento exato que permite a busca por uma seqüência de consulta em árvores de sufixo que podem_estar  particionadas . A validação do Perseus foi realizada por meio de testes de desempenho considerando genomas de vários organismos , os quais possuem diferentes ordens de magnitude de tamanho . Os resultados obtidos foram comparados com a técnica Trellis+ , a qual representa o estado da arte nesta linha de pesquisa . Os testes indicaram que o Perseus construiu árvores de sufixo mais rapidamente do que o Trellis+ , reduzindo o tempo total gasto na construção em até 24 % . Perseus também criou árvores de sufixo mais compactas , atingindo uma redução média de 27 % no espaço de memória secundária utilizado . Já com relação ao tempo total gasto no processamento de consultas , Perseus sempre produziu os melhores_resultados  , respondendo consultas em média 49 % mais rápido do que o seu principal concorrente . Com relação à indexação de subseqüências escolhidas pelo usuário , comparando os resultados obtidos com o Trellis+ , os testes mostraram que Perseus proveu uma redução no tempo de construção de árvores de sufixo de 97 % na média e uma redução no tempo gasto no processamento de consultas de genes de 93 % na 
 A extração de termos em coleções textuais , que é uma atividade da etapa de Pré-Processamento da Mineração de Textos , pode ser empregada para diversos fins nos processos de extração de conhecimento . Esses termos devem ser cuidadosamente extraídos , uma vez que os resultados de todo o processo dependerão , em grande parte , da `` qualidade '' dos termos obtidos . A `` qualidade '' dos termos , neste trabalho , abrange tanto a representatividade dos termos no domínio em questão como sua compreensibilidade . Tendo em vista sua importância , neste trabalho , avaliou-se o efeito do uso de diferentes técnicas de simplificação de termos na compreensibilidade e representatividade dos termos em coleções textuais na Língua Portuguesa . Os termos foram extraídos seguindo os passos da metodologia apresentada neste trabalho e as técnicas utilizadas durante essa atividade de extração foram a radicalização , lematização e substantivação . Para apoiar tal metodologia , foi desenvolvida uma ferramenta , a ExtraT ( Ferramenta para Extração de Termos ) . Visando garantir a `` qualidade '' dos termos extraídos , os mesmos são avaliados objetiva e subjetivamente . As avaliações subjetivas , ou seja , com o auxílio de especialistas do domínio em questão , abrangem a representatividade dos termos em seus respectivos documentos , a compreensibilidade dos termos obtidos ao utilizar cada técnica e a preferência geral subjetiva dos especialistas em cada técnica . As avaliações objetivas , que são auxiliadas por uma ferramenta desenvolvida ( a TaxEM - Taxonomia em XML da Embrapa ) , levam em consideração a quantidade de termos extraídos por cada técnica , além de abranger tambéem a representatividade dos termos extraídos a partir de cada técnica em relação aos seus respectivos documentos . Essa avaliação objetiva da representatividade dos termos utiliza como suporte a medida CTW ( Context Term Weight ) . Oito coleções de textos reais do domínio de agronegócio foram utilizadas na avaliaçao experimental . Como resultado foram indicadas algumas das características positivas e negativas da utilização das técnicas de simplificação de termos , mostrando que a escolha pelo uso de alguma dessas técnicas para o domínio em questão depende do objetivo principal pré-estabelecido , que pode ser desde a necessidade de se ter termos compreensíveis para o usuário até a necessidade de se trabalhar com uma menor quantidade de 
 A Qualidade de Serviço oferece aos Sistemas de Tempo Real garantias das restrições temporais de aplicações tais como comércio_eletrônico  , vídeo conferência , telemedicina , entre outras que necessitam de confiabilidade e desempenho para efetivação dos seus serviços . Contribuições nessas áreas têm focado a provisão de qualidade de serviço em termos absolutos , descritos num limite máximo de média de tempo de atendimento , destinado às aplicações Web . Todavia , o controle de qualidade baseado em tempo_médio  de resposta durante todo o período de serviço prestado , não restringe os tempos de resposta quanto a sua dispersão . Um valor médio razoável pode ser resultado da combinação de tempos de resposta elevados e pequenos . O objetivo deste trabalho é reduzir a dispersão dos atrasos em sistemas escalonados com a política EBS ( Exigency Based Scheduling ) , definidos pela diferença positiva entre o tempo de resposta da requisição do usuário e o valor médio contratado , em um modelo de contrato determinado por janelas deslizantes de operações . A EBS é uma política de escalonamento que define um modelo de contratos de tempo_médio  de resposta avaliados durante todo o tempo de serviço . Avalia-se a dispersão dos atrasos , pois , é preferível que haja a possibilidade dos tempos de resposta ficarem substancialmente abaixo da média contratada . Os resultados são obtidos utilizando métodos de simulação dos algoritmos desenvolvidos durante o 
 Em uma de suas definções , a Terminologia representa o conjunto de princípios e métodos adotados no processo de gestão e criação de produtos terminológicos , tais como glossários e dicionários de termos . A sistematização desses métodos envolve a aplicação de ferramentas computacionais específicas e compatíveis com as tarefas terminológicas , contribuindo para o desenvolvimento desses produtos e a difusão de conhecimento especializado . Entretanto , principalmente no Brasil , a combinação da Terminologia e Informática é incipiente , e dentre as atividades do trabalho terminológico é comum a utilização de várias ferramentas não especializados para esse fim . Isso torna o trabalho dos terminólogos muito moroso , pois esse trabalho geralmente é feito por uma equipe multidisciplinar que deve ter acesso , a todo o momento , à versão mais atual das várias etapas da geração de um produto terminológico . Além_disso  , deixa o gerenciamento dos dados mais complicado , pois não existe um padrão de entrada e saída definido para os programas . Apoiado nos pressupostos da Teoria Comunicativa da Terminologia ( TCT ) , este trabalho apresenta a proposta de desenvolvimento e avaliação do e- Termos , um Ambiente ColaborativoWeb composto por seis módulos de trabalho bem definidos , cujo propósito é automatizar as tarefas de gestão e criação de produtos terminológicos . Cada módulo do e-Termos possui a responsabilidade de abrigar tarefas inerentes ao processo de criação das terminologias , sendo atreladas a eles diferentes ferramentas de apoio lingüístico , que possuem a função de dar suporte às atividades de Processamento de Língua Natural envolvidas nesse processo . Além delas , há também ferramentas colaborativas , designadas para dar_apoio  às necessidades comunicacionais e de interação da equipe de trabalho . Particularmente com relação ao processo de avaliação proposto , uma de suas características é a capacidade de ser executado em um tempo curto , viabilizando a avaliação controlada de vários grupos , mas executada no ambiente de trabalho do público alvo . As principais_contribuições  desta pesquisa são o aspecto colaborativo instanciado na prática terminológica , a criação flexível da Ficha Terminológica , a possibilidade didática de uso para o ensino de terminologia , lexicografia e tradução e o processo de avaliação para sistemas colaborativos desenvolvido para o e-Termos , que combina Cenários de Uso e um Questionário de Pesquisa . Utilizando tecnologias Web e da área de Computer Supported Collaborative Work ( CSCW ) para o desenvolvimento da sua arquitetura computacional colaborativa , o e-Termos apresenta-se como um ambiente inovador para a pesquisa terminolóogica assistida por computador , pois automatiza um método prático que exp~oe os postulados da terminologia de orientação descritiva e evidencia todas as etapas do processo de criação de produtos terminológicos com o inédito diferencial colaborativo . Para certificar este êxito , o e-Termos tem recebido um número crescente de novas propostas de projeto , tendo até Agosto de 2009 mais de 130 usuários cadastrados , alocados em 68 diferentes projetos 
 A Programação Orientada a Aspectos ( POA ) é uma técnica contemporânea de desenvolvimento de software fortemente baseada no princípio da separação de interesses . Ela tem como objetivo tratar de problemas de modularização de software por meio da introdução do aspecto como uma nova unidade de implementação que encapsula comportamento relacionado aos interesses_transversais  do software . A despeito dos benefícios que podem ser alcançados com o uso da POA , seus mecanismos de implementação representam novas potenciais fontes de defeitos que devem ser tratados durante a fase de teste de software . Nesse contexto , o teste de mutação consiste em um critério de seleção de testes baseado em defeitos que tem sido bastante investigado para demonstrar a ausência de defeitos pré-especifiados no software . Acredita-se que o teste de mutação seja uma ferramenta adequada para lidar com as particularidades de técnicas de programação contemporâneas como a POA . Entretanto , até o presente momento , as poucas iniciativas para adaptar o teste de mutação para o contexto de programas orientados a aspectos ( OA ) apresentam cobertura limitada em relação aos tipos de defeitos simulados , ou ainda requerem adequado apoio automatizado e avaliações . Esta tese visa a mitigar essas limitações por meio da definição de uma abordagem abrangente de teste de mutação para programas OA escritos na linguagem AspectJ . A tese inicia como uma investigação da propensão a defeitos de programas OA e define uma taxonomia de defeitos para tais programas . A taxonomia inclui uma variedade de tipos de defeitos e serviu como base para a definição de um conjunto de operadores de mutação para programas OA . Suporte automatizado para a aplicação dos operadores também foi disponibilizado . Uma série de estudos quantitativos mostra que a taxonomia de defeitos proposta é suficiente para classificar defeitos encontrados em vários sistemas OA . Os estudos também mostram que os operadores de mutação propostos são capazes de simular defeitos que podem não ser relevados por conjuntos de teste pré-existentes , não derivados para cobrir mutantes . Além_disso  , observou-se que o esforço requerido para evoluir tais conjuntos de teste de forma a torná-los adequados para os requisitos gerados pelos 
 O teste baseado em modelos tem como objetivo auxiliar a atividade de testes , gerando conjuntos de casos de teste a partir de modelos , como Máquinas de Estados_Finitos  ( MEFs ) . Diversos métodos de geração de conjuntos de caso de teste têm sido_propostos  ao longo das últimas_décadas  , com algumas contribuições recentes . Dentre esses trabalhos , há os que geram seqüências de verificação que são conjuntos de caso de teste formados por uma única seqüência e que são capazes de detectar os defeitos de uma implementação cujo comportamento pode ser modelado a partir de uma MEF . Neste trabalho é proposto um algoritmo de geração de seqüências de verificação que tem a finalidade de gerar seqüências menores que as seqüências geradas pelos métodos existentes . O algoritmo , que é baseado na técnica de algoritmos genéticos e nas condições de suficiência para a completude de casos de teste , consiste basicamente em criar novas seqüências a partir de seqüências menores . Por meio de mutações , novas seqüências são geradas pelo algoritmo . As condições de suficiência são utilizadas para determinar quais seqüências geradas são seqüências de verificação . Também são apresentados neste trabalho os estudos_experimentais  realizados para determinar o comportamento do algoritmo diante de diferentes 
 Esta tese apresenta novos metodos baseados na teoria dos fractais e em tecnicas de mineração de dados para dar suporte ao monitoramento agrícola em escala regional , mais especicamente areas com plantações de cana-de-açucar que tem um papel_importante  na economia brasileira como uma alternativa viavel para a substituição de combustíveis fósseis . Uma vez que o clima tem um grande impacto na agricultura , os agrometeorologistas utilizam dados climáticos associados a índices agrometeorológicos e mais recentemente dados provenientes de satélites para apoiar a tomada de decisão . Neste sentido , foi proposto um método que utiliza a dimensão_fractal  para identicar mudanças de tendências nas séries climáticas juntamente com um módulo de análise estatística para definir quais atributos são responsáveis por essas alterações de comportamento . Além_disso  , foram propostos dois métodos de medidas de similaridade para auxiliar na comparação de diferentes regiões agrícolas representadas por múltiplas variáveis provenientes de dados meteorológicos e imagens de sensoriamento remoto . Diante da importância de se estudar os extremos climáticos que podem se intensicar dado os cenários que preveem mudanças globais no clima , foi proposto o algoritmo CLIPSMiner que identifica padrões relevantes e extremos em séries climáticas . CLIPSMiner também permite a identificação de correlação de múltiplas séries considerando defasagem de tempo e encontra padrões de acordo com parâmetros que podem ser calibrados pelos usuários . A busca por padrões de associação entre séries foi alcançada por meio de duas_abordagens  distintas . A primeira delas integrou o cálculo da correlação de dimensão_fractal  com uma técnica para tornar os valores contínuos das séries em intervalos discretos e um algoritmo de regras de associação gerando o método Apriori-FD . Embora tenha identificado padrões interessantes em relação a temperatura , este método não conseguiu lidar de forma apropriada com defasagem temporal . Foi proposto então o algoritmo CLEARMiner que de forma não-supervisionada minera padrões em uma série associando-os a padrões em outras séries considerando a possibilidade de defasagem temporal . Os métodos propostos foram comparados a técnicas similares e avaliados por um grupo composto por meteorologistas , agrometeorologistas e especialistas em sensoriamento remoto . Os experimentos_realizados  mostraram que a aplicação de técnicas de mineração de dados e fractais contribui para melhorar a análise dos dados agrometeorológicos e de satélite auxiliando no trabalho de pesquisadores , além de se configurar como uma ferramenta importante para apoiar a tomada de decisão no 
 Nesta dissertação de mestrado , sumarizamos alguns conceitos básicos de geometria diferencial e estudamos a conexão existente entre geometria diferencial e modelos estatísticos . Assim , calculamos medidas geométricas associadas aos modelos estatísticos e estudamos os efeitos de uma boa parametrização nas inferências obtidas . Na nova parametrização , verificamos se a precisão dos resultados da inferência melhoram e quais são as relações existentes com as medidas geométricas . Várias aplicações são consideradas , especialmente com modelos para dados de sobrevivência censurados ou no e modelos não-lineares . Também estudamos os efeitos de uma parametrização em inferência Bayesiana , especialmente usando algoritmos de simulação de amostras MCMC ( Monte_Carlo  em Cadeias de Markov ) 
 Encontra-se em desenvolvimento no Laboratório de Sistemas_Distribuídos  e Programação Concorrente do ICMC-USP um ambiente para escalonamento de processos denominado AMIGO ( DynA Mica ! FlexIble SchedulinG EnvirOnment ) . Esse ambiente é organizado em duas camadas principais . Uma camada inferior possui os módulos responsáveis pelo escalonamento e pelo relacionamento entre as aplicações dos usuários , as políticas de escalonamento disponíveis e o ambiente de passagem de mensagens . Uma camada superior composta por uma interface_gráfica  , na qual o usuário tem acesso a várias opções para configurar o ambiente e para configurar o modo que as políticas de escalonamento serão usadas . Este trabalho tem como objetivo o desenvolvimento da interface_gráfica  da camada superior do AMIGO , que permite cadastrar classes de software , o hardware utilizado , as políticas de escalonamento disponíveis e as métricas a serem utilizadas para o monitoramento das políticas de escalonamento consideradas pelo AMIGO . A interface_gráfica  desenvolvida neste trabalho foi projetada considerando dois módulos : Front End ( desenvolvido em TCL/TK , é responsável por captar os dados definidos pelo usuário e ativar a execução da função solicitada ) e Back End ( desenvolvido em C e responsável por executar as funcionalidades das tarefas definidas pelo usuário ) 
 O presente_trabalho  de mestrado é parte de um projeto em andamento ICMC-USP denominado Power Vis que desenvolve pesquisa nas áreas de visualização científica , sonificação de dados e em aplicações reais das técnicas desenvolvidas . A visualização científica é uma área da computação gráfica que abrange um conjunto de técnicas para a geração de representações gráficas a partir de um conjunto de dados . Algumas vezes ses conjuntos de dados formam imagens complexas e de difícil entendimento , o que to : na difícil sua apresentação em uni ambiente bi-dimensional como a tela do computador . Uma alternativa para amenizar tais problemas é o uso de som como apoio ás técnicas visuais . A área da computação responsável pelo estudo de mapeamentos sonoros de conjuntos de dados com o objetivo de exibir algum tipo de informação é conhecida como sonificação . Este trabalho apresenta uma evolução de um trabalho em sonificação desenvolvido anteriormente ( SVoI ) para apoio à visualização de dados científicos volumétricos . Os processos visuais apoiados pela sonificação neste trabalho são a geração de isosuperfícies e o rendering volume métrico direto . A versão aqui implementada , além de reorganizar o código e aumentar sua funcionalidade , implementa duas estratégias para a evolução do sistema : o acesso à suas funções pela internet usando Java , e a distribuição de algumas funções de geração de sons e imagens usando COBRA 
 Uma pane impo tante , da proposta dos agentes de software , é o princípio que esses agentes podem funcionar de foJma mais eficiente quando trabalham em grupos . Para que a cooperação entre agentes tenha suc so , é requerida comunicação entre eles . Para que essa comunicação seja possível precisa-se de uma Linguagem de Comunicação entre Agentes ( em inglês Agent Communication Langu e , ou ACL ) . Dentro de uma ACL , toma-se importante a forma como são comunicadas as mensagins , isto é , se as mensagens expressam adequadamente seu propósito sob um ponto de vista sesr4tico . O objetivo dest trabalho é a especificação de urna nova ACL , chamada UCL Universal Communication Language , que se preocupa com a descrição da estrutura das mensagens , com o modelo semântico e ct , m suporte a protocolos para interação enfie agentes ( de software ou humanos ) . Além_disso  , importante explorar , no contexto deste trabalho , a utilização do padrão XML ( Extensible Markup Language ) , para atribuir à linguagem UCL meios para uma fácil integração à Internet . Por isso a linguÀigem UCL foi implementada no padrão XML . Foi desenvolvid também um protótipo de enconverter-deconverter , que serviu como ferramenta para experi4entação e teste da proposta de linguagem apresentada . Esse protótipo permite a conversão de nglês para UCL e de UCL para inglês , ele usou ferramentas e programas abertos , estando disponífel para uso de todos sob a GPL ( ONU Public License ) 
 Nesta dissertação estamos interessados na análise_Bayesiana  de dados de sobrevivência médicos com observações censuradas e na presença de uma ou mais covariáveis . Em muitos_casos  , podemos assumir um modelo de regressão paramétrica para os dados como uma alternativa aos modelos de regressão não paramétricas . Em casos especiais , podemos precisar de uma regressão paramétrica com uma distribuição mais geral para os dados de sobrevivência como uma mistura de distribuições . Um dos objetivos principais do projeto é relacionado ao uso de misturas de distribuições paramétricas para a variável erro nos modelos log-linerares . No contexto Bayesiano utilizamos técnicas de simulação de Monte_Carlo  em Cadeias de Markov ( MCMC ) e destacamos a utilização dos softwares `` Ox `` e `` VVinBugs '' como grandes alternativas para os problemas referente ao tempo de execução do§ algoritmos computacionais 
 A descoberta de conhecimento em bases de dados é uma área de pesquisa que tem crescido rapidamente , e cujo desenvolvimento tem . sido dirigido ao benefício das necessidades práticas . sociais e econômicas . entre outras . Entretanto . mesmo em bases de dados podem_estar  presentes campos importantes que não são considerados na análise , como aqueles preenchidos com informações textuais . Essas informações não estão presentes apenas em tabelas , mias também em diversos documentos que são disponibilizados e acessados através da Internet . Para tornar a descoberta de conhecimento em textos possível , tem-se associado estratégias de preparação/pré-proccssamento de textos e técnicas utilizadas no processo Data_Mining  . Essa associação vem_sendo  denominada de processo Text Mining . Tendo em vista a importância da preparação dos textos . neste trabalho foi projetado e construído um módulo de pré-processamento de textos , com a finalidade de tornar possível a realização do processo Text Mining considerando textos em português . Devido ao fato de que muitas estratégias e/ou ferramentas estarem disponíveis somente para o inglês , a possibilidade de realizar a descoberta de padrões em textos em português pode significar o alcance de novos horizontes era termos de oportunidades e informações desconhecidas 
 A Robótica é uma área de pesquisa que tem evoluído muito nas décadas , e tem recebido grande atenção pela comunidade_científica  . As principais limitações dos sistemas robóticos atuais estão no controle dessas máquinas complexas . É importante que sejam desenvolvidos algoritmos inteligentes e suficiente para controlar os robôs de maneira apropriada , e que esses algoritmos sejam executados de maneira eficiente . A utilizaçõa de hardware para a execução desses algoritmos é uma alternativa interessante para se conseguir um bom_desempenho  . Essa dissertação apresenta a implementação de neural artificial , utilizada no reconhecimento de gestos por robôs_móveis  . Para tanto , é utilizada a tecnologia FPGA , que pérmite o desenvolvimento de hardware com rapidez , eficiência e baixo custo . Após o desenvolvimento , são apresentadas comparações de desempenho e conclusões sobre as implementações 
 Sistemas Reativos caracterizam-se por reagir continuamente a estímulos externos e internos e controlar atividades humanas . A ocorrência de falhas nesses sistemas pode resultar em grandes prejuízos . Dessa forma , o uso de métodos e técnicas rigorosas para a especificação do comportamento desse tipo de sistema é essencial , buscando-se evitar inconsistências e ambigüidades no modelo . Redes de Petri é uma das técnicas que têm sido usadas para a especificação de sistemas reativos . Teste e validação são atividades essenciais na produção dessa classe de sistemas 
 Por isso , o critério_Análise  de Mutantes , um critério de teste baseado em erros normalmente aplicado ao teste de programas , tem sido explorado no contexto de teste de especificações de sistemas reativos . É necessário o desenvolvimento de ferramentas que apóiem sua utilização , visto que a aplicação manual do critério é impraticável . O objetivo deste trabalho é a implementação da ferramenta Proteum-RS/PN , que apóia a aplicação do critério_Análise  de Mutantes para validar especificações baseadas em Redes de Petri 
 O Problema de Minimização do Número Máximo de Pilhas Abertas ( MOSP , do inglês minimization of open stacks problem ) é um problema de otimização combinatória da família NP-Difícil que vem recebendo grande atenção na literatura especializada . Este trabalho apresenta novas contribuições em termos de modelos e técnicas de resolução para o problema . A primeira parte deste trabalho lidou com modelos matemáticos , sendo analisados os modelos existentes que se baseiam em programação inteira_mista  . Variações de um modelo da literatura foram propostas , com o objetivo de tentar diminuir o tempo de execução necessário para se obter uma solução exata com a utilização de pacotes comerciais . Os resultados mostraram que as propostas são capazes de acelerar a solução de algumas classes de instâncias mas , que de maneira geral , métodos baseados em relaxação linear encontram dificuldade em provar a otimalidade devido à baixa qualidade dos limitantes inferiores . Uma outra contribuição deste trabalho foi o desenvolvimento de um modelo conjunto para o problema MOSP e para o problema de minimização da duração de pedidos ( MORP , do inglês minimization of order spread problem ) . Este modelo propõe um framework unificado em que os dois problemas podem ser resolvidos ao mesmo tempo , tendo suas funções objetivo individuais ponderadas através de pesos definidos pelo usuário . A segunda parte do trabalho voltou-se para o desenvolvimento de métodos heurísticos para o MOSP . Duas estratégias de solução foram desenvolvidas . O primeiro método propõe uma transformação heurística entre o problema MOSP e o clássico problema do caixeiro viajente ( TSP , do inglês traveling salesman problem ) . A partir de uma representação em grafo do MOSP , o TSP é definido por meio de uma regra de atribuição de distâncias baseadas nos graus dos nós . Nos testes computacionais , a estratégia proposta mostrou-se eficiente em relação às heurísticas específicas para o MOSP , obtendo a solução ótima do MOSP em 80,42 % das instâncias testadas e sendo competitiva em termos de tempo computacional com algumas das melhores heurísticas da literatura . O segundo método heurístico proposto utilizou a ideia de decomposição . De fato , neste método , um corte no grafo associado ao problema original divide-o em problemas menores , que são resolvidos . A solução global é obtida através da junção das soluções dos subproblemas e , em alguns_casos  , é possível demonstrar a otimalidade da solução obtida . Testes computacionais indicam a validade da proposta e apontam caminhos para pesquisas 
 Um Jogo Markoviano Alternado ( Alternating Markov Game - AMG ) é uma extensão de um Processo de Decisão Markoviano ( Markov Decision Process - MDP ) para ambientes multiagentes . O modelo AMG é utilizado na tomada de decisão sequencial de n agentes quando são conhecidas as probabilidades de transição das ações a serem tomadas por cada agente . Nesse trabalho estamos interessados em AMGs com probabilidades de transição de estados imprecisas , por exemplo , quando elas são dadas na forma de intervalos de probabilidades . Apresentamos um novo modelo de AMG , que chamamos de Jogo Markoviano Alternado com Probabilidades Imprecisas ( Alternate Markov Game with Imprecise Probabilities - AMGIP ) que permite que as imprecisões nas probabilidades de transições de estados sejam dadas na forma de parâmetros sujeitos a restrições_lineares  que estende trabalhos anteriores em que a imprecisão é dada por intervalos de probabilidades ( AMG-INTERVAL ) . Dizemos que a imprecisão representa escolhas da Natureza . A imprecisão desses modelos implica no valor do jogo ser dado por uma função intervalar . Existem diversas formas de calcular a solução do jogo , que depende do comportamento da Natureza e dos critérios de preferência dos jogadores diante das escolhas da Natureza . Assim , neste trabalho discutimos diversas soluções para o AMG-IP e AMG-INTERVAL . Também como resultado do estudo das relações existentes entre os MDPs e os AMGs , propomos um novo modelo chamado de AMG-ST ( Alternating Markov Game with Set-valued Transition ) , capaz de modelar a incerteza do modelo MDP-ST ( Markovian Decision Process with Set-valued Transition ) como um jogo entre o agente e a Natureza , isto é , um jogo em que a Natureza faz o papel de um dos jogadores 
 O problema do caixeiro viajante ( PCV ) é um problema clássico de otimização que possui diversas variações , aplicações e instâncias . Encontrar a solução ótima para muitas instâncias desse problema é geralmente muito difícil devido o alto custo_computacional  . Vários métodos de otimização , conhecidos como meta-heurísticas ( MHs ) , são capazes de encontrar boas soluções para o PCV . Muitos algoritmos baseados em diversas MHs têm sido_propostos  e investigados para diferentes variações do PCV . Como não existe um algoritmo universal que encontre a melhor solução para todas as instâncias de um problema , diferentes MHs podem prover a melhor solução para diferentes instâncias do PCV . Desse modo , a seleção a priori da MH que produza a melhor solução para uma dada instância é uma tarefa_difícil  . A pesquisa desenvolvida nesta tese investiga o uso de abordagens de meta-aprendizado para selecionar as MHs mais promissoras para novas instâncias de PCV . Essas abordagens induzem meta-modelos preditivos a partir do treinamento das técnicas de aprendizado de máquina em um conjunto de meta-dados . Cada meta-exemplo , em nosso conjunto de meta-dados , representa uma instância de PCV descrita por características ( meta-atributos ) do PCV e pelo desempenho das MHs ( meta-atributo alvo ) para essa instância . Os meta-modelos induzidos são usados para indicar os valores do meta-atributo alvo para novas instâncias do PCV . Vários experimentos foram realizados durante a investigação desta pesquisa e resultados importantes foram 
 Com a heterogeneidade de tecnologias de comunicação sem fio presentes na borda de redes de acesso , serviços providos na Internet podem ser acessados de forma quasi ubíqua através de dispositivos_móveis  ou portáteis . O acesso a esses serviços , contudo , está associado a atrasos e rupturas frequentes na comunicação devido a razões inerentes à mobilidade do dispositivo , como : i ) perda de sinal em locais onde há pouca ou nenhuma cobertura de acesso móvel ; ii ) erros no quadro de dados durante a transmissão e , consequentemente , perdas de pacotes , que podem ser ocasionados por interferência no sinal ou enfraquecimento deste pelo distanciamento do dispositivo em relação à Estação Base ; iii ) mudanças de endereços IP durante transmissões em andamento causadas pela migração do dispositivo entre diferentes redes . Como consequência , aplicações falham com a ruptura de comunicações orientadas a conexão . Tratar a mobilidade de forma transparente à aplicação é um dos desafios da Computação Móvel e Ubíqua que vem_sendo  pesquisado ao longo da última_década  . Soluções foram propostas para operarem desde a Camada de Enlace à Aplicação . Muitas delas , entretanto , exigem modificações na pilha de protocolos TCP/IP e adição de infraestrutura específica de rede no suporte à comunicação fim-a-fim . Além de elevar o custo das etapas de implantação e manutenção , estratégias intrusivas e dependentes de infraestrutura adicional podem não apresentar desempenho satisfatório . Nesse contexto , propomos tratar a mobilidade no nível da própria aplicação através de Sessões de Comunicação que não falham com atrasos e desconexões . Operando somente nos nós-fim e de modo transparente às Camadas adjacentes de Aplicação e Transporte , as sessões não requerem infraestrutura adicional para intermediar ou controlar a comunicação entre pares , tampouco modificações em protocolos legados da pilha TCP/IP . O conceito de Sessões Tolerantes a Rupturas é implementado através de uma API de propósito_geral  em sistemas Linux que estende a interface de Sockets . A API é , na prática , uma camada transparente sobre o Socket que provê Ciência de Mobilidade à aplicação através de mecanismos para : acompanhar a localização de nós ao longo da duração de uma sessão ; detectar rupturas nas transmissões causadas pela mobilidade do nó ou de seu par remoto ; suspender e retomar sessões de forma eficiente , segura e confiável . Experimentos conduzidos em ambientes emulados e reais com equipamentos de uso comercial mostram a eficiência das sessões . Além de introduzir baixa degradação na vazão fim-a-fim , rupturas na transmissão podem ser detectadas em microssegundos e sessões suspensas são reabertas em milissegundos . Com um desempenho_superior  a solução de mobilidade geral da Camada IP , as sessões não necessitam de adaptações de software em equipamentos de 
 Estudamos o comportamento da familia aresta-triangulo de grafos aleatorios exponenciais ( ERG ) usando metodos de Monte_Carlo  baseados em Cadeias de Markov . Comparamos contagens de subgrafos e correlacoes entre arestas de ergs as de Grafos Aleatorios Binomiais ( BRG , tambem chamados de Erdos-Renyi ) . E um resultado teorico conhecido que para algumas parametrizacoes os limites das contagens de subgrafos de ERGs convergem para os de BRGs , assintoticamente no numero de vertices [ BBS11 , CD11 ] . Observamos esse fenomeno em grafos com poucos ( 20 ) vertices em nossas simulacoes 
 Reúso de software , principalmente quando apoiado por ferramentas computacionais , é uma forma de enfrentar os constantes desafios da Engenharia de Software em aumentar a produtividade e qualidade no desenvolvimento de software . Diversos ambientes de apoio ao reúso de software têm sido_propostos  , em sua maioria com objetivos específicos , por exemplo , repositórios de componentes , ferramentas para geração de aplicações e ferramentas para configuração de linhas de produtos . Entretanto , a integração desses ambientes é , muitas_vezes  , deficiente e deixada por conta do próprio desenvolvedor . Por outro_lado  , Sistemas-de-Sistemas ( SoS ) constituem uma forma de integrar sistemas independentes e seus relacionamentos , formando um todo maior que a soma das partes . Dessa forma , o SoS permite alcançar objetivos complexos que não poderiam ser facilmente alcançados individualmente pelos seus sistemas constituintes . Em um SoS , comportamentos emergentes podem surgir a qualquer momento ou terem sido previamente implementados . Por serem recentes , os conceitos de SoS ainda apresentam uma vasta gama de tópicos em aberto , entre eles sua aplicação a ambientes de reúso de software . Assim , neste trabalho , investigou-se como integrar ambientes de reúso com base nos conceitos de SoS . O SoS resultante , denominado SoS-Reúso , visa facilitar a implementação de comportamentos emergentes relacionados ao reúso de software . Com isso , pode-se potencializar a atividade de reúso , facilitando a busca por ativos reusáveis em diferentes sistemas constituintes . Para colocar em prática essa ideia , desenvolveu-se um simulador do SoS-Reúso , denominado P-SoS-Reúso . Realizou-se um estudo qualitativo da usabilidade e adequação funcional do P-SoS-Reúso , a fim de obter do usuário feedbacks em relação ao seu uso . A abordagem utilizada no desenvolvimento do SoS-Reúso poderá ser adaptada futuramente para integrar outros_tipos  de sistemas , os quais poderiam se beneficiar da abordagem baseada em SoS 
 Com o avanço tecnológico dos últimos_anos  passou a ser normal vermos microprocessadores com múltiplos núcleos ( cores ) . A expectativa é de que o crescimento da quantidade de núcleos passe a ser maior do que o crescimento da velocidade desses núcleos . Assim , além de se preocuparem em otimizar algoritmos sequenciais , os programadores começaram a dar mais atenção às possibilidades de aproveitamento de toda a capacidade oferecida pelos diversos cores . Existem alguns modelos de programação que permitem uma abordagem concorrente . O modelo de programação concorrente mais adotado atualmente é o baseado em threads , que utiliza memória_compartilhada  e é adotado em Java . Um outro modelo é o baseado em troca de mensagens , no qual as entidades computacionais ativas são denominadas atores . Nesse trabalho , estudamos a linguagem Scala e seu modelo de atores . Além_disso  , implementamos em Scala uma versão paralela de um algoritmo de classicação que simula o sistema imunológico dos animais , o AIRS paralelo , e comparamos seu desempenho com a versão em Java 
 O objetivo do presente_trabalho  de pesquisa é reduzir o nível de atenção requerido para o uso do sistema Borboleta por meio de reconhecimento de fala na navegação através das funcionalidades do sistema , permitindo ao profissional dar maior atenção ao paciente . A metodologia de desenvolvimento do projeto inclui uma revisão bibliográfica para definir o estado da arte da área , uma pesquisa sobre o software disponível para reconhecimento de fala , uma coleta de dados dos comandos do sistema em português brasileiro para treinar e testar o sistema , uma etapa de projeção e desenvolvimento para definir a arquitetura de integração com o Borboleta , e uma fase de testes para medir a precisão do sistema e seus níveis de usabilidade e aceitação por parte do usuário 
 Os sistemas criptográficos de chave_pública  amplamente_utilizados  hoje em dia tem sua segurança baseada na suposição da intratabilidade dos problemas de fatoração de inteiros e do logaritmo discreto , sendo que ambos foram demonstrados inseguros sob o advento dos computadores quânticos . Sistemas criptográficos baseados em Multivariáveis Quadráticas ( MQ ) utilizam como base o problema MQ , que consiste em resolver um sistema de equações polinomiais multivariáveis quadráticas sobre um corpo finito . O problema MQ foi provado como sendo NP-completo e até hoje não se conhece algoritmo , nem mesmo quântico , de tempo polinomial que possa resolver o problema , fazendo com que sistemas criptográficos baseados nesta primitiva mereçam ser investigados e desenvolvidos como reais candidatos a proverem nossa criptografia pós-quântica . Durante a CRYPTO'2011 Sakumoto , Shirai e Hiwatari introduziram dois novos protocolos de identificação baseados em polinômios multivariáveis quadráticos , os quais chamamos de MQID-3 e MQID-5 , e que em especial e pela primeira vez , tem sua segurança reduzida apenas ao problema MQ . Baseados nestas propostas iremos apresentar uma versão aprimorada do protocolo MQID-3 na qual teremos uma redução da comunicação necessária em aproximadamente 9 % 
 Nos anos recentes , suavização de superfícies é um assunto de intensa pesquisa em processamento geométrico . Muitas das abordagens para suavização de malhas usam um esquema de duas etapas : filtragem de normais seguido de um passo de atualização de vértices para corresponder com as normais filtradas . Neste trabalho , propomos uma adaptação de tais esquemas de duas etapas para superfícies representadas por nuvens de pontos . Para isso , exploramos esquemas de pesos para filtrar as normais . Além_disso  , investigamos três métodos para estimar normais , analisando o impacto de cada método para estimar normais em todo o processo de suavização da superfície . Para uma análise quantitativa , além da comparação visual convencional , avaliamos a eficácia de diferentes opções de implementação usando duas medidas , comparando nossos resultados com métodos de suavização de nuvens de pontos encontrados a 
 As equações diferencias parciais tem origem na modelagem do problemas nas ciências e engenharia , tais como a equação do calor , equação da onda , equação de Poisson , entre outras . Para muitas destas equações não é tão simples obter uma técnica analítica para achar sua solução e nestes casos é necessário uso de soluções aproximadas obtidas pelo computador . Existem técnicas tradicionais para solução_numérica  de uma grande classe de equações_diferenciais  , mas quando esta equação está na forma implícita , muitas destas técnicas já não podem ser aplicadas . Frequentemente as equações_diferenciais  parciais de segunda_ordem  tem maior estudo que as equações de primeira_ordem  sendo uma das razões que os modelos envolvem derivadas de segunda_ordem  . No caso das equações_diferenciais  parciais de primeira_ordem  implícitas a não linearidade em alguns_casos  não permite determinar uma solução de forma simples . O trabalho desenvolvido faz uma revisão do método das características para estabelecer as condições necessárias e suficientes , que permitam encontrar uma solução , ao mesmo tempo evidencia a complexidade de determinar uma solução clássica . Dentro das aplicações existentes relacionadas com as Equações Diferenciais Parciais Implícitas de Primeira Ordem , podemos mencionar a Equação cinemática e a Equação de Hamilton-Jacobi que podem-se associar com o movimento de partículas . Para a solução de uma Equação Diferencial Implícita de Primeira Ordem o método das características tem uma estrutura de solução que permite resolver a equação de forma analítica e numérica , desde que se verifique o Teorema de Cauchy . O objetivo deste trabalho de mestrado é obter um método_numérico  para a solução de equações_diferenciais  parciais de primeira_ordem  implícitas . Nós propomos um método_numérico  do tipo previsor-corretor que resolve uma EDP de primeira_ordem  implícita , utilizando o sistema característico em conjunto com as condições de banda , para reduzir o erro global nas iterações 
 Este trabalho faz parte de um projeto maior , o electronic Physician Annotation Device ( ePAD ) . O ePAD permite a criação de uma base de conhecimento médico usando anotações semânticas sobre lesões em imagens radiológicas , usando uma plataforma Web . Essas anotações servirão para identificar , acompanhar e reason sobre lesões tumorais em pesquisas médicas ( especialmente sobre câncer ) . A informação adquirida e persistida pelo sistema permite avaliação automática por computadores ; recuperação de imagens hospitalares e outros serviços relacionados a exames médicos . O ePAD é um desenvolvimento conjunto de grupos de pesquisas do ICMC-USP e do Department of Radiology da Stanford University . O principal trabalho , apresentado neste texto , é um novo conjunto de funcionalidades na Web para adicionar a marcação de lesões em imagens radiológicas em três dimensões ao ePAD . Elas permitirão a obtenção de dados mais precisos acerca de medidas tridimensionais de lesões como volume , posição e cálculo de maior diâmetro . O objetivo é facilitar o trabalho dos profissionais de radiologia na análise de diagnósticos e acompanhamento de lesões produzindo um acompanhamento mais acurado da evolução de doenças como o câncer . Anotações podem ser conectadas a lesões e conter informações_semânticas  , usando termos biomédicos da ontologia RadLex . Essas novas_funcionalidades  são baseadas em HTML5 , com o auxílio de WebGL para visualização e manipulação de objetos 3D . As maiores contribuições deste trabalho são o software para visualização de séries de imagens radiológicas em 3D usando planos ortogonais , os protótipos de vídeo mostrando as três possíveis interfaces para marcação de lesões em 3D , a pesquisa com radiologistas ( usando os protótipos de vídeo ) para determinar que o cursor esférico era a melhor interface para marcar lesões em 3D e o protótipo dessa interface no ePAD . Este trabalho contou com a ajuda de usuários do Department . of Radiology da Stanford University e do Hospital das Clínicas da Universidade de São Paulo em Ribeirão Preto 
 Sistemas biológicos podem ser representados por redes que armazenam não apenas informações de conectividade , mas também informações de características de seus nós . No contexto biomolecular , esses nós podem representar proteínas , metabólitos , entre outros_tipos  de moléculas . Cada molécula possui características anotadas e armazenadas em bases de dados como o Gene Ontology . A comparação visual dessas redes depende de ferramentas que permitam o usuário identificar diferenças e semelhanças entre as anotações feitas sobre as moléculas ( atributos ) e também sobre as interações conhecidas ( conexões ) . Neste trabalho de mestrado , buscou-se desenvolver técnicas que facilitem a comparação desses atributos sobre as moléculas , tentando manter no processo a visualização das redes em que essas moléculas estão inseridas . Como resultado , obteve-se a ferramenta VisPipeline-MultiNetwork , que permite comparar até seis redes , utilizando operações de conjuntos sobre as redes e sobre seus atributos . Dessa forma , diferentemente da maioria das ferramentas conhecidas para a visualização de redes biológicas , o VisPipeline-MultiNetwork permite a criação de redes cujos atributos são derivados das redes originais por meio de operações de união , intersecção e valores exclusivos . A comparação visual das redes é feita pela visualização do resultado dessas operações de conjuntos sobre as redes , por meio de um método de comparação lado-a-lado . Já a comparação dos atributos armazenados nos nós das redes é feita por meio de diagramas de Venn . Para auxiliar este tipo de comparação , a técnica InteractiVenn foi desenvolvida , em que o usuário pode interagir com um diagrama de Venn efetuando operações de união entre conjuntos . Essas operações de união aplicadas sobre os conjuntos são também aplicadas sobre as respectivas formas no diagrama . Esta característica da técnica a diferencia das outras ferramentas de criação de diagramas de Venn . Integrando essas funcionalidades , o usuário é capaz de comparar redes sob diversas perspectivas . Para exemplificar a utilização do VisPipeline-MultiNetwork , dois casos no contexto biomolecular foram estudados . Adicionalmente , uma ferramenta web para a comparação de listas de cadeias de caracteres por meio de diagramas de Venn foi desenvolvida . Ela também implementa a técnica InteractiVenn e foi denominada InteractiVenn website 
 Projeções Multidimensionais são úteis para gerar visualizações adequadas para apoiar a análise exploratória de uma grande variedade de dados complexos e de alta dimensionalidade . Tarefas de análise visual de dados têm se beneficiado de projeções para explorar dados textuais , de imagens , de sensores , entre outros . Porém , diferentes técnicas de projeção e diferentes parametrizações de uma mesma técnica produzem resultados distintos para um mesmo conjunto de dados , pois as técnicas adotam estratégias distintas para representar os dados originais em um espaço cuja dimensionalidade permite sua visualização . Atualmente , ainda há poucos recursos para avaliar a qualidade dessas projeções e , em geral , as soluções existentes avaliam propriedades específicas , demandando grande esforço do analista para uma avaliação mais abrangente . Neste trabalho , introduzimos um arcabouço para computar medidas de avaliação de projeções com enfoque em análise de vizinhanças e de agrupamentos . Para elaborar este arcabouço , foi realizado um estudo de percepção para entender melhor como os usuários observam as projeções e foi conduzida uma investigação de representações dos dados capazes de favorecer a identificação de vizinhanças e de agrupamentos . Os padrões identificados no estudo de percepção auxiliaram a validar a representação dos dados , em que foi proposto um modelo de grafo , chamado de Extended Minimum Spanning Tree ( EMST ) , capaz de capturar características condizentes com as observações dos participantes no estudo . O grafo EMST também foi validado por meio de dois estudos comparativos de identificação de vizinhanças e de agrupamentos . Com base neste arcabouço foram propostas cinco medidas de qualidade de projeções_multidimensionais  , duas delas para avaliar características relacionadas à separação visual das classes , e outras três para avaliar a preservação de propriedades do espaço original , especificamente a separação das classes , as vizinhanças e os agrupamentos . As medidas são ilustradas por meio de sua aplicação a conjuntos de dados artificiais , favorecendo a sua interpretação , bem como a conjuntos de dados reais , evidenciando a sua potencial utilidade em cenários reais . Também é apresentada uma comparação das novas medidas de preservação de vizinhanças com medidas similares descritas na literatura , permitindo identificar diferenças e semelhanças entre elas 
 Esta dissertação apresenta o programa computacional EQMOD , o qual determina a expressão do erro de truncamento presente na discretização de equações_diferenciais  parciais . Esta expressão é obtida a partir da equação modificada associada à discretização , cujo processo de obtenção envolve extensas manipulações algébricas , dificultando seu cálculo manual . Porém , ele se presta à automatização em um computador munido de uma linguagem de programação simbólica . Para isso , escolheu-se a linguagem de programação do pacote Mathematica . São mostrados vários exemplos que demonstram a funcionalidade e aplicabilidade do sistema desenvolvido 
 A Visualização Científica ( VisC ) é uma área da computação gráfica que ajuda cientistas e outros profissionais a capturar o significado dos seus dados . Essa área tem como objetivo a geração de imagens a partir de um conjunto de dados ( geralmente obtidos a partir de dispositivos de detecção ou simulações numéricas ) , as quais são interpretados por usuários que desejam ganhar entendimento do fenômeno que gerou os dados . Algumas vezes esses conjuntos de dados formam imagens complexas , dificultando o seu entendimento , ou possuem muita informação , tornando difícil a sua apresentação no ambiente bidimensional da tela do computador . Nesses casos o som é uma boa alternativa de representação em apoio às técnicas visuais . Sonificação é a área da computação que tratqa do mapeamento sonoro de conjuntos de dados com a intenção de exibir informações . Esta dissertação mostra um estudo sobre o uso de som para o desenvolvimento de um sistema de sonificação para visualização . Esse sistema compreende a implementação de funções de som para visualização volumétrica baseada em superfícies desenvolvidas anteriormente em SSound , no ambiente orientado a objetos de uma biblioteca de visualização de baixo custo e ampla utilização ( o VTK Visualization Toolkit ) . Os resultados desta dissertação identificam a utilidade do som integrado com a visualização para representar e reforçar dados , além de uma série de recursos para suportar a interpretação de informações complexas 
 A presença de computadores e outros recursos tecnológicos no ambiente educacional sempre causou polêmica . Quando as máquinas de calcular entraram nas salas de aula , temia-se que as crianças e , em pouco tempo , os adultos perdessem a capacidade de realizar cálculos às custas de seu próprio raciocínio ( preocupação antecipada em um conto de ficção de Isaac Asimov , A Sensação de Poder , publicado pela primeira vez em 1957 ) . O cenário atual nos mostra não o receio , mas a necessidade de oferecer recursos tecnológicos aos estudantes , o mais cedo possível . Neste contexto , focalizamos o sistema Interland ( Moreira , Nunes & Pimentel , 1995 ) , que propõe um ambiente de auxílio ao aprendizado no sistema World-Wide Web , com facilidades para exploração de recursos interativos e de multimídia . O objeto deste trabalho é a interface do professor no sistema Interland : o Teaching Assistant ou T.A . O sistema T.A . implementa um conjunto de recursos de autoria com modelos e ferramentas para a produção de documentos relacionados a disciplinas . Os modelos de páginas contribuem para uma menor necessidade de decisões de projeto por parte do professor e menos problemas de autoria , embutindo conceitos importantes na produção de documentos para a World-Wide Web . Essa preocupação também tem em vista evitar problemas para os usuários das páginas em hipertexto ( os alunos ) . Um dos pontos principais do sistema T.A . é permitir a introdução de recursos multimídia ( no estágio atual , trechos de vídeo selecionados pelo professor ) disponíveis em uma base de dados , eliminando a necessidade de se aprender a manipular outros programas específicos para esse fim . Os protótipos já desenvolvidos oferecem modelos de documentos , e permitem a introdução de interações simples e trechos de vídeo em páginas , de maneira transparente para o usuário 
 indiscutível o grande interesse , tanto dos pesquisadores quanto do mercado de informática , na tecnologia de sistemas de Banco de Dados Distribuído . Entretanto , a distribuição introduz um novo nível de complexidade aos projetos de banco de dados , sua utilização implica em decisões quanto a alocação de dados e programas ao longo da rede de computadores . A maioria dos trabalhos na área busca o desenvolvimento de métodos iterativos para solução otimizada de estratégias de consulta eiou alocação de dados . São poucas as abordagens semânticas do problema . Este trabalho propõe um método prático para alocação de dados , sem considerar qualquer modelo quantitativo ; visa-se o enfoque qualitativo da distribuição de dados . Reduz-se a generalização do problema em prol de uma análise pragmática da questão . A base do trabalho são os sistemas de informação da área operacional de uma Empresa de Telecomunicação . O objetivo da proposta é definir o esquema de fragmentação de um banco de dados centralizado através da análise e classificação dos subsistemas de manutenção dos dados , para depois determinar o Esquema de Alocação usando o Modelo Relacional . Por fim , o diaarama conceituai , concebido através cio Modelo Entidade-Relacionamento Estendido , é revisto para incorporar a representação da distribuição aqui desenvolvida 
 Bons códigos importam , mas como saber quando a qualidade está baixa ? Maus cheiros de código , ou anomalias , auxiliam desenvolvedores na identificação de trechos de código problemáticos , porém a maioria dos maus_cheiros  catalogados são voltados para práticas e tecnologias tradicionais , criadas entre as décadas de 70 a 90 , como orientação a objetos e Java . Ainda há dúvidas sobre maus_cheiros  em tecnologias que surgiram na última_década  , como o Android , principal plataforma móvel em 2017 com mais de 86 % de participação de mercado . Alguns pesquisadores derivaram maus_cheiros  Android relacionados à eficiência e à usabilidade . Outros notaram que maus_cheiros  específicos ao Android são muito mais frequentes nos aplicativos do que maus_cheiros  tradicionais . Diversas pesquisas concluíram que os componentes Android mais afetados por maus_cheiros  tradicionais são Activities e Adapters , que pertencem à camada de apresentação . Notou-se também que em alguns aplicativos , códigos da camada de apresentação representam a maior_parte  do código do projeto . Vale ressaltar que a camada de apresentação Android também é composta por arquivos XML , chamados de recursos , usados na construção da interface do usuário ( User Interface - UI ) , porém nenhuma das pesquisas citadas os considerou em suas análises . Nesta dissertação , investigamos a existência de maus_cheiros  relacionados à camada de apresentação Android considerando inclusive os recursos . Fizemos isso através de dois questionários e um experimento de código online , totalizando a participação de 316 desenvolvedores . Nossos resultados mostram a existência de uma percepção comum entre desenvolvedores sobre más práticas no desenvolvimento da camada de apresentação Android . Nossas principais_contribuições  são um catálogo com 20 maus_cheiros  da camada de apresentação Android e uma análise estatística da percepção de desenvolvedores sobre os 7 principais maus_cheiros  catalogados . Nossas contribuições servirão a pesquisadores como ponto de partida para a definição de heurísticas e implementação de ferramentas automatizadas e a desenvolvedores como auxílio na identificação de códigos problemáticos , ainda que de forma_manual  
 Este trabalho propõe um ambiente denominado SIATE ( Sistema Inteligente de Apoio ao Treinamento e Ensino ) , que integra características de ambientes de Prática/Treinamento , Hipennídia , Sistemas Tutores e Sistemas Híbridos de Conhecimento , é voltado para o ensino e tem como característica marcante a liberdade , por parte do estudante , de explorar um domínio qualquer , podendo recorrer , quando necessário , ao auxílio de um Sistema Tutor e de um Sistema Híbrido de Conhecimento com conhecimento especialista nesse domínio . São enfatizados aqui o projeto e o desenvolvimento de um Sistema Tutor que oferece apoio pedagógico ao ambiente , bem como o desenvolvimento dos recursos hipermídia desse ambiente 
 Este trabalho envolveu estudos nas áreas de Modelagem Geométrica , Técnicas de Modelagem e Interação Usuário-Computador e descreve a implementação de um Modelador de Moldes Tridimensional . Esse sistema tem o objetivo de prover o Simulador de Escoamento de Fluidos FREEFLOW 3D de uma ferramenta na qual o usuário possa construir o molde 3D onde um fluido será injetado , de uma forma fácil e intuitiva . O Modelador utiliza como técnicas de modelagem Varreduras e Curvas e Superfícies Livres NURBS . Para a interação com o usuário utiliza técnicas bidimensionais que auxiliam à defmição das faces geradoras necessárias aos algoritmos de modelagem . Já os moldes tridimensionais criados são representados internamente pela sua fronteira ( Boundary Representation ) , usando uma estrutura do tipo semi-aresta ( halfedge ) 
 A construção de ferramentas para a correção automática de textos tem ganhado destaque , seguindo a evolução e eficiência dos processadores de textos ao qual elas são incorporadas . E , junto às técnicas tradicionais simbolistas de implementação de tais ferramentas , através do uso de regras de produção , surgem aplicações com técnicas até então não usuais na . área de lingüística computacional , como o uso de Redes_Neurais  Artificiais . O trabalho proposto tem por objetivo fazer um estudo comparativo do uso das técnicas conexionista e simbolista na revisão automática de erros gramaticais da língua_portuguesa  . Usando como estudo de caso as regras gramaticais da crase , tomamos como exemplo de modelo tradicional de implementação o revisor gramatical ReGra , e de outro_lado  , implementamos dois modelos de redes neurais ( um modelo backpropagation e um modelo Elman ) , para detectar erros com relação ao uso da crase , tanto em casos de presença incorreta , quanto de ausência . A meta deste estudo não é apontar qual método é o mais eficiente em termos gerais , mesmo porque acreditamos que isto não seja possível . Queremos , sim , observar o desempenho de ambos os métodos quanto ao problema determinado , visando assim uma maior integração entre eles , aproveitando suas melhores potencialidades 
 Os processos de recuperação e aprendizado de casos , que exercem um papel fundamental , em sistemas de Raciocínio_Baseado  em Casos , não são fáceis de serem desenvolvidos . Estes dois processos são bastante dependentes . Os casos devem ser recuperados rapidamente da memória para o sistema de Raciocínio_Baseado  em Casos ser eficiente . Isto implica em estruturas mais elaboradas para armazenálos , organizá-los e recuperá-los . Quando um conhecimento novo é incorporado ao sistema ( aprendizado ) , a reorganização dos casos na memória torna-se muito complexa devido justamente à estas estruturas . O principal objetivo deste trabalho é a integração de Raciocínio_Baseado  em Casos e Redes_Neurais  . Neste trabalho , uma Rede_Neural  , modelo ART1 , é utilizada para auxiliar na recuperação e aprendizado de casos em um sistema de Raciocínio_Baseado  em Casos 
 Nesta dissertação de mestrado , apresentamos análises Clássica e Bayesiana para os principais modelos de Estratégia tipo I , estratégia de modelos de confiabilidade de software que modelam os tempos entre falhas do software . Na análise Clássica , estimadores pontuais e intervalos de confiança são encontrados usando métodos assintóticos . Na análise_Bayesiana  , considerando densidades a priori informativas para os parâmetros dos modelos , determinamos os resumos a posteriori , utilizando os métodos de simulação Gibbs Sampling e Metrópolis Hastings . Em particular , consideramos diferentes densidades a priori para os parâmetros do modelo de Jelinski e Moranda ( 1972 ) ( um dos primeiros modelos de confiabilidade de software desenvolvido ) e verificamos a consequência de uma reparametrização para esse modelo . Também , apresentamos a técnica das distribuições preditivas condicionais ordenadas ( CPO ) para selecionar o melhor modelo dentre os modelos analisados . Finalizamos , ilustrando os métodos propostos através de um exemplo 
 Apresentamos neste trabalho , um estudo do modelo de confiabilidade de software de Jelinski e Moranda ( 1972 ) . Enfocamos , a importância da análise Bayesianapara a correção da instabilidade do estimador de máxima_verossimilhança  de N ( número de erros do software ) , e a ortogonalização de Cox e Reid ( 1987 ) para a realização de inferência Bayesiana sobre a taxa de falhas A. Também , caractenzamos a existência da densidade a posteriori de { quando admitimos densidades a priori não-informativas e impróprias aos parâmetros do modelo . Destacamos o comportamento dos métodos de aproximação de Monte_Carlo  em cadeias de Markov , para a obtenção da distribuição a posteriori de N quando esta é imprópria 
 Neste trabalho estudam-se problemas de dimensionamento de lotes com aplicações específicas em uma fundição automatizada . Quando várias ligas são consideradas na programação , o problema é modelado com um problema de programação linear_inteira  mista ; caso contrário como um problema de programação linear . Algumas hipóteses são assumidas sobre o gargalo e heurísticas são desenvolvidas . Mostra-se também como a fundição pode ser integrada a um sistema de múltiplos estágios , tais como usinagem e montagem . Experiências computacionais sobre o desempenho dos métodos são relatadas 
 O risco de crédito a pessoas físicas tem sido avaliado empiricamente ou por sistemas de credit score . No entanto , com o crescimento do mercado de crédito ao varejo , o assunto passou a merecer maior preocupação em razão da elevação das taxas de inadimplência , que vem causando enormes prejuízos aos doadores de recursos . Redes_Neurais  Artificiais ( RNA ) podem ser treinadas utilizando grandes quantidades de exemplos significativos . Utilizando esta técnica , as avaliações podem ser modeladas através de exemplos encontrados nos históricos dos clientes das aplicações de crédito . Contudo , a topologia e os parâmetros de aprendizado das RNA precisam ser apropriadamente estabelecidos para que a rede funcione eficientemente . Para resolver este tipo de problema , recentemente vêm_sendo  utilizados Algoritmos Genéticos ( AG ) , algoritmos baseados em mecanismos genéticos e de seleção natural , que podem ser utilizados para encontrar as arquiteturas mais eficientes . O objetivo deste projeto é investigar como o projeto de RNA pode se beneficiar de AG para a determinação de sua arquitetura e o comportamento de RNA como técnica para análise de risco crédito financeiro . Para avaliar os modelos desenvolvidos , foram utilizados dois conjuntos de dados diferentes , compostos de informações sobre aplicações reais de crédito . O primeiro deles é constituído por dados de aplicações de cartão de crédito , cujo_objetivo  do modelo é `` imitar '' a avaliação humana . O segundo é constituído por dados de clientes de crédito bancário e seus históricos de pagamento com o objetivo de prever o comportamento de futuros clientes 
 Aprendizado de máquina figura-se como uma área de pesquisa que visa a desenvolver métodos computacionais capazes de aprender com a experiência . Embora uma grande quantidade de técnicas de aprendizado de máquina foi proposta e aplicada , com sucesso , em sistemas reais , existem ainda inúmeros problemas desafiantes que necessitam ser explorados . Nos últimos_anos  , um crescente_interesse  em técnicas baseadas em redes complexas ( grafos de larga_escala  com padrões de conexão não triviais ) foi verificado . Essa emergência é explicada pelas inerentes vantagens que a representação em redes complexas traz , sendo capazes de capturar as relações espaciais , topológicas e funcionais dos dados . Nesta tese , serão investigadas as possíveis vantagens oferecidas por redes complexas quando utilizadas no domínio de aprendizado de máquina . De fato , será mostrado que a abordagem por redes realmente proporciona melhorias nos aprendizados supervisionado , semissupervisionado e não supervisionado . Especificamente , será reformulada uma técnica de competição de partículas para o aprendizado não supervisionado e semissupervisionado por meio da utilização de um sistema dinâmico estocástico não linear . Em complemento , uma análise analítica de tal modelo será desenvolvida , permitindo o entendimento evolucional do modelo no tempo . Além_disso  , a questão de confiabilidade de dados será investigada no aprendizado semissupervisionado . Tal tópico tem importância prática e é pouco estudado na literatura . Com o objetivo de validar essas técnicas em problemas reais , simulações computacionais em bases de dados consagradas pela literatura serão conduzidas . Ainda nesse trabalho , será proposta uma técnica híbrica de classificação supervisionada que combina tanto o aprendizado de baixo como de alto nível . O termo de baixo_nível  pode ser implementado por qualquer técnica de classificação tradicional , enquanto que o termo de alto nível é realizado pela extração das características de uma rede construída a partir dos dados de entrada . Nesse contexto , aquele classifica as instâncias de teste segundo qualidades físicas , enquanto que esse estima a conformidade da instância de teste com a formação de padrões dos dados . Os estudos aqui desenvolvidos mostram que o método proposto pode melhorar o desempenho de técnicas tradicionais de classificação , além de permitir uma classificação de acordo com o significado semântico dos dados . Enfim , acredita-se que este estudo possa gerar contribuições relevantes para a área de aprendizado de máquina 
 O problema de minimização com restrições_lineares  e importante , não apenas pelo problema em si , que surge em várias áreas , mas também por ser utilizado como subproblema para resolver problemas mais gerais de programação não-linear . GENLIN e um método eficiente para minimização com restrições_lineares  para problemas de pequeno e médio porte . Para que seja possível a implementação de um método similar para grande porte , é necessário ter um método eficiente , também para grande porte , para projeção de pontos no conjunto de restrições_lineares  . O problema de projeção em um conjunto de restrições_lineares  pode ser escrito como um problema de programação quadrática convexa . Neste trabalho , estudamos e implementamos métodos esparsos para resolução de problemas de programação quadrática convexa apenas com restrições de caixa , em particular o clássico método Moré-Toraldo e o `` método '' NQC . O método Moré-Toraldo usa o método dos Gradientes Conjugados para explorar a face da região factível definida pela iteração atual , e o método do Gradiente Projetado para mudar de face . O `` método '' NQC usa o método do Gradiente Espectral Projetado para definir em que face trabalhar , e o método de Newton para calcular o minimizador da quadrática reduzida a esta face . Utilizamos os métodos esparsos Moré-Toraldo e NQC para resolver o problema de projeção de GENLIN e comparamos seus 
 A interação com aplicações Web está se tornando cada vez mais presente no dia-a-dia das pessoas , sendo útil para a disponibilização de recursos que permitem a realização de serviços , estudos ou entretenimento . Porém muitos dos recursos disponibilizados apresentam barreiras em relação à acessibilidade , impedindo que usuários finais , com algum tipo de deficiência , possam utilizá-los de forma eficiente . Como solução para esse problema , têm sido propostas diretrizes para o desenvolvimento de aplicações Web acessíveis . Entretanto , apesar da existência dessas diretrizes , o cenário atual ainda mostra que há dificuldades em relação ao desenvolvimento considerando requisitos de acessibilidade . Neste sentido , esta tese está calcada na criação de uma abordagem que permita o apoio efetivo a boas práticas para desenvolvimento Web , a partir da aproximação de tais diretrizes ao ambiente das equipes de desenvolvimento . Para tanto , são definidas atividades separadas em 3 eixos de preocupações : Treinamento em Acessibilidade , Gerência de Decisões e Desenvolvimento e Ferramental . Para validação da concepção inicial da proposta desta tese , estudos de caso são detalhados , demonstrando como o desenvolvimento Web pode ser beneficiado com melhor apoio ferramental , que incluem o uso de técnicas de modelagem e de padrões para geração de código acessível . São ainda verificados os benefícios da colaboração de experiências e treinamento da equipe de desenvolvimento , para tanto foi desenvolvida a ferramenta AccessibilityUtil . Posteriormente é descrita a abordagem e validada em um experimento controlado em que existiram grupos de participantes utilizando a abordagem e outros que não fizeram uso da mesma . Com isso , observou-se que , com a utilização da abordagem , houve melhor apoio a boas práticas de desenvolvimento em relação à aplicação de requisitos de acessibilidade , o que ocasiona em melhores_resultados  de qualidade para as aplicações Web 
 A criação de recursos linguístico-computacionais de base , como é o caso dos léxicos computacionais , é um dos focos da área de Processamento de Línguas_Naturais  ( PLN ) . Porém , a maioria dos recursos léxicos computacionais existentes é específica da língua inglesa . Dentre os recursos já desenvolvidos para a língua inglesa , tem-se a VerbNet , que é um léxico com informações_semânticas  e sintáticas dos verbos do inglês , independente de domínio , construído com base nas classes verbais de Levin , além de possuir mapeamentos para a WordNet de Princeton ( WordNet ) . Considerando que há poucos_estudos  computacionais sobre as classes de Levin , que é a base da VerbNet , para línguas diferentes do inglês , e dada a carência de um léxico para o português nos moldes da VerbNet do inglês , este trabalho teve como objetivo a criação de um recurso léxico para o português do Brasil ( chamado VerbNet.Br ) , semelhante à VerbNet . A construção manual destes recursos geralmente é inviável devido ao tempo gasto e aos erros inseridos pelo autor humano . Portanto , há um grande esforço na área para a criação destes recursos apoiada por técnicas computacionais . Uma técnica reconhecida e bastante usada é o uso de aprendizado de máquina em córpus para extrair informação linguística . A outra é o uso de recursos já existentes para outras línguas , em geral o inglês , visando à construção de um novo recurso alinhado , aproveitando-se de atributos multilíngues/cross-linguísticos ( cross-linguistic ) ( como é o caso da classificação verbal de Levin ) . O método proposto neste mestrado para a construção da VerbNet.Br é genérico , porque pode ser utilizado para a construção de recursos semelhantes para outras línguas , além do português do Brasil . Além_disso  , futuramente , será possível estender este recurso via criação de subclasses de conceitos . O método para criação da VerbNet.Br é fundamentado em quatro etapas : três automáticas e uma manual . Porém , também foram realizados_experimentos  sem o uso da etapa manual , constatando-se , com isso , que ela pode ser descartada sem afetar a precisão e abrangência dos resultados . A avaliação do recurso criado foi realizada de forma intrínseca qualitativa e quantitativa . A avaliação qualitativa consistiu : ( a ) da análise manual de algumas classes da VerbNet , criando um gold standard para o português do Brasil ; ( b ) da comparação do gold standard criado com os resultados da VerbNet.Br , obtendo resultados promissores , por volta de 60 % de f-measure ; e ( c ) da comparação dos resultados da VerbNet.Br com resultados de agrupamento de verbos , concluindo que ambos os métodos apresentam resultados similares . A avaliação quantitativa considerou a taxa de aceitação dos membros das classes da VerbNet.Br , apresentando resultados na faixa de 90 % de aceitação dos membros em cada classe . Uma das contribuições deste mestrado é a primeira versão da VerbNet.Br , que precisa de validação linguística , mas que já contém informação para ser utilizada em tarefas de PLN , com precisão e abrangência de 44 % e 92,89 % , 
 Esta dissertação apresenta o projeto de construção do sistema INTEMA : um gerador de explicações e um ambiente de interação para Sistemas Tutores Inteligentes ( sTIs ) em Matemática . o INTEMA é parte de uma arquitetura genérica de STIs em Matemática que permite a troca de domínio , sem necessidade de alterar a geração de explicações . As explicações geradas têm por objetivo justificar o comportamento do sistema e informar os usuários , de maneira individualizada , sobre as características e funções do sistema . O estilo de interação fornecido pelo sistema é o de manipulação direta . São discutidos em detalhes os três_módulos  principais do INTEMA : planejador , realizador e interface . Um exemplo de interação com o sistema no domínio de Lógica de Primeira Ordem é apresentado 
 Áreas cultivadas com cana-de-açúcar podem sofrer o ataque do fungo Puccinia melanocephala e variedades suscetíveis desenvolvem uma doença conhecida por ferrugem da cana-de-açúcar . Por afetar , geralmente , áreas imensas , os prejuízos são grandes . Atualmente , a avaliação da doença é feita por especialistas que percorrem as áreas plantadas analisando visualmente as folhas e atribuindo à região um determinado grau de infecção . Esse modelo pode ser considerado subjetivo pois , dependendo da experiência e acuidade visual do especialista , a avaliação de uma mesma área pode apresentar resultados divergentes . Diante desta situação , este trabalho apresenta uma abordagem para automatizar o processo de identificação e avaliação , criando alternativas para minimizar os prejuízos . Este trabalho apresenta um método para classificação dos níveis de infecção da ferrugem por meio da análise de imagens aéreas de canaviais , adquiridas por um aeromodelo . Dessas fotos são extraídas características baseadas nas cores , as quais são classificadas por meio de uma rede_neural  backpropagation . Além_disso  , foi implementado um método para segmentação de imagens_digitais  de folhas de cana-de-açúcar infectadas com o intuito de corroborar a avaliação manual feita por especialistas . Os resultados mostram que o método é eficaz na discriminação dos três níveis de infecção disponíveis , além disso , indicam que este pode ser igualmente eficiente na discriminação dos nove níveis de infecção da escala adotada 
 A análise de dados contendo informações sequenciais é um problema de crescente_interesse  devido à grande quantidade de informação que é gerada , entre outros , em processos de monitoramento . As séries_temporais  são um dos tipos mais comuns de dados sequenciais e consistem em observações ao longo do tempo . O algoritmo k-Nearest Neighbor - Time Series Prediction kNN-TSP é um método de previsão de dados temporais . A principal vantagem do algoritmo é a sua simplicidade , e a sua aplicabilidade na análise de séries_temporais  não-lineares e na previsão de comportamentos sazonais . Entretanto , ainda que ele frequentemente encontre as melhores previsões para séries_temporais  parcialmente periódicas , várias questões relacionadas com a determinação de seus parâmetros continuam em aberto . Este trabalho , foca-se em dois desses parâmetros , relacionados com a seleção de vizinhos mais próximos e a função de previsão . Para isso , é proposta uma abordagem simples para selecionar vizinhos mais próximos que considera a similaridade e a distância temporal de modo a selecionar os padrões mais similares e mais recentes . Também é proposta uma função de previsão que tem a propriedade de manter bom_desempenho  na presença de padrões em níveis diferentes da série_temporal  . Esses parâmetros foram avaliados empiricamente utilizando várias séries_temporais  , inclusive caóticas , bem como séries_temporais  reais referentes a variáveis ambientais do reservatório de Itaipu , disponibilizadas pela Itaipu Binacional . Três variáveis limnológicas fortemente correlacionadas são consideradas nos experimentos de previsão : temperatura da água , temperatura do ar e oxigênio dissolvido . Uma análise de correlação é realizada para verificar se os dados previstos mantem a correlação das variáveis . Os resultados mostram que , o critério de seleção de vizinhos próximos e a função de previsão , propostos neste trabalho , são 
 O processo de mineração de dados tem como objetivo encontrar o conhecimento implícito em um conjunto de dados para auxiliar a tomada de decisão . Do ponto de vista do usuário , vários problemas podem ser encontrados durante a etapa de pós-processamento e disponibilização do conhecimento_extraído  , como a enorme_quantidade  de padrões gerados por alguns algoritmos de extração e a dificuldade na compreensão dos modelos extraídos dos dados . Além do problema da quantidade de regras , os algoritmos tradicionais de regras de associação podem levar à descoberta de conhecimento muito específico . Assim , pode ser realizada a generalização das regras de associação com o intuito de obter um conhecimento mais geral . Neste projeto é proposta uma metodologia interativa que auxilie na avaliação de regras de associação generalizadas , visando melhorar a compreensibilidade e facilitar a identificação de conhecimento interessante . Este auxílio é realizado por meio do uso de técnicas de visualização em conjunto com a aplicação medidas de avaliação objetivas e subjetivas , que estão implementadas no módulo de visualização de regras de associação generalizados denominado RulEE-GARVis , que está integrado ao ambiente de exploração de regras RulEE ( Rule Exploration Environment ) . O ambiente RulEE está sendo desenvolvido no LABIC-ICMC-USP e auxilia a etapa de pós-processamento e disponibilização de conhecimento . Neste contexto , também foi objetivo deste projeto de pesquisa desenvolver o Módulo de Gerenciamento do ambiente de exploração de regras RulEE . Com a realização do estudo dirigido , foi possível verificar que a metodologia_proposta  realmente facilita a compreensão e a identificação de regras de associação generalizadas 
 Aplicações de sistemas computacionais emergentes atribuindo requisitos de resposta na forma de tempo de resposta requerem uma abordagem de sistemas de tempo real . Nesses sistemas , a qualidade de serviço é expressa como garantia das restrições temporais . Um amplo leque de técnicas para provisão de QoS encontram-se na literatura . Estas técnicas são baseadas tanto na diferenciação de serviço ( QoS relativa ) , quanto na especificação de garantia de desempenho ( QoS absoluta ) . Porém , a integração de QoS relativa e absoluta em nível de aplicação não tem sido tão explorada . Este trabalho realiza o estudo , a análise e a proposta de um método de escalonamento de tempo real em um ambiente simulado , baseado em contratos virtuais adaptativos e modelo re-alimentado . O objetivo é relaxar as restrições temporais dos usuários menos exigentes e priorizar usuários mais exigentes , sem degradar a qualidade do sistema como um todo . Para tanto , estratégias são exploradas em nível de escalonamento para o cumprimento dos contratos especificados por requisitos de tempo_médio  de resposta . Os resultados alcançados com o emprego do método proposto sinalizam uma melhoria em termos de qualidade de serviço relativa e absoluta e uma melhor satisfação dos usuários . Este trabalho também propõe uma extensão para os modelos convencionalmente estudados nesse contexto , ampliando a formulação original de duas classes para n classes de 
 Ainda pouco se sabe sobre as causas do transtorno do espectro autista ( TEA ) e seus efeitos na funcionalidade cerebral , porém , diversas pesquisas apontam que a condição esteja relacionada à uma conectividade diferenciada entre regiões do cérebro . A conectividade córtico-cerebelar tem sido tema de pesquisas nas últimas_décadas  em decorrência de novos achados que indicam que esta conectividade está relacionada ao aprendizado e refinamento de diversas funcionalidades do córtex . Acredita-se que uma falha na conectividade córtico-cerebelar poderia estar relacionada à falhas em funções sensorimotoras , cognitivas e emocionais . A investigação de regiões cuja conectividade córtico-cerebelar está alterada no TEA contribui para uma melhor compreensão deste transtorno . Assim , o objetivo deste trabalho é identificar regiões do cérebro cuja conectividade funcional com o cerebelo seja diferente entre indivíduos com desenvolvimento típico ( DT ) e diagnosticados com TEA . Para isto , utilizamos imagens de ressonância_magnética  funcional ( fMRI ) de 708 indivíduos em estado de repouso ( 432 DT e 276 TEA ) com idades entre 6 e 58 anos coletados pelo consórcio ABIDE . Os dados foram pré-processados e divididos conforme regiões anatômicas do cérebro que foram adotadas como regiões de interesse ( ROIs ) . Para determinar a conectividade funcional de cada região do córtex com o cerebelo , aplicamos o método de análise de componentes principais ( PCA ) nas ROIs do cerebelo e utilizamos um modelo regressão linear para cada ROI do córtex , sendo a série_temporal  da ROI do córtex a variável resposta e as componentes principais ( PCs ) do cerebelo as variáveis preditoras . Em seguida , identificamos as regiões com conectividade funcional diferente entre indivíduos com DT e diagnosticados com TEA através de um modelo linear que inclui como covariáveis , idade , gênero e local de coleta do dado . Identificamos cinco regiões do córtex que apresentam reduzida conectividade funcional com o cerebelo nos indivíduos com TEA , sendo elas : ( i ) giro fusiforme direito , ( ii ) giro pós-central direito , ( iii ) giro temporal superior direito e ( iv ) giro temporal médio direito e ( v ) esquerdo . Todas as cinco regiões são parte do sistema sensorimotor , e estão relacionadas à funções ligadas à sintomas característicos do quadro de TEA , como : sensibilidade à estímulos sensoriais , dislexia , prosopagnosia ( dificuldade para reconhecer faces ) , dificuldade de compreensão de linguagem e dificuldade de reconhecimento de emoções em faces . Nossos resultados mostram que existem regiões do sistema sensorimotor que apresentam conectividade funcional com o cerebelo atipicamente reduzida em TEA , como corroborado por estudos de imageamento com tarefa específica e como hipotetizado por estudos de conectividade estrutural . Nós acreditamos que a conectividade córtico-cerebelar reduzida dessas regiões esteja prejudicando o processamento e aprendizado de funções sensorimotoras , levando ao surgimento de sintomas típicos do TEA 
 O crescimento explosivo no número de usuários de Internet levou arquitetos de software a reavaliarem questões relacionadas à escalabilidade de serviços que são disponibilizados em larga_escala  . Projetar arquiteturas de software que não apresentem degradação no desempenho com o aumento no número de acessos concorrentes ainda é um desafio . Neste trabalho , investigamos o impacto do sistema_operacional  em questões relacionadas ao desempenho , paralelização e escalabilidade de jogos interativos multi-usuários . Em particular , estudamos e estendemos o jogo interativo , multi-usuário , QuakeWorld , disponibilizado publicamente pela id Software sob a licença GPL . Criamos um modelo de paralelismo para a simulação distribuída realizada pelo jogo e o implementamos no servidor do QuakeWorld com adaptações que permitem que o sistema_operacional  gerencie de forma adequada a execução da carga de trabalho gerada 
 Veículos Aéreos não Tripulados ( VANTs ) são aeronaves que voam sem tripulação e são capazes de realizar diversos tipos de missões , como vigilância , coleta de dados topográficos e monitoramento ambiental . Este é um domínio que tem muito a ganhar com a aplicação da abordagem de Linha de Produtos de Software ( LPS ) , uma vez que é rico em variabilidades e cada modelo de VANT tem também muitas partes comuns . Neste trabalho é apresentada uma infraestrutura tecnológica e de configuração de ativos em Simulink , gerenciados pelas ferramentas Pure : :variant e Hephaestos para uma LPS de VANTs . Um conjunto de padrões para especificação de variabilidades em Simulink é proposto , bem como uma extensão para a ferramenta Hephaestus . Uma comparação entre as ferramentas Pure : :variants e Hephaestus é 
 Atualmente , os principais indicadores sobre a educação básica no Brasil são calculados a partir dos resultados apurados pelo Censo Escolar , fonte que apresenta inconsistências nos resultados , principalmente na obtenção das Taxas do Fluxo Escolar e nos totais de matrículas dos alunos . O modelo de Fluxo Escolar tem como função descrever o movimento dos alunos dentro do Sistema de Ensino , reconstruindo a evolução dos mesmos nas séries do ensino fundamental ao longo dos anos mediante as Taxas de Transição . É desenvolvida uma metodologia baseados numa proposta demográfica , onde é mostrada a dinâmica do processo de transição das séries que regulam o fluxo dos alunos medindo as taxas de promoções , repetências e desistências , de forma a estimar o fluxo dos estudantes para uma coorte hipotética de uma determinada idade . O presente_trabalho  tenta encontrar uma ponte entre a explicação da atual realidade educacional do país com a teoria e as ferramentas estatísticas , as quais tentam conseguir uma melhor visão da dimensão do desempenho do Sistema Educativo . O ponto central do trabalho trata das estimativas e previsões de matrículas que possam explicar esta dimensão , a traves da estimação das chamadas taxas do Fluxo Escolar . É descrito o tratamento de Cadeias de Markov Discretas mediante a Inferência Clássica , dado que o número de alunos aprovados , repetentes e evadidos podem ser vistos como tal . São calculados os Estimadores de Máxima Verossimilhança das taxas e são mostradas as propriedades asintóticas das mesmas e encontrada a distribuição asintótica com correlação serial , dado que os dados tem certa medida de dependência entre pares sucessivos de observações , são construídos testes de hipóteses e é feita uma abordagem_Bayesiana  considerando que o número de alunos aprovados , repetentes e evadidos tem uma distribuição multinomial sendo os parâmetros as probabilidades de transição ( taxas de transição do fluxo ) . É feita a análise considerando primeiro como priori não informativa a priori de Jeffrey , logo é considerada como priori conjugada uma distribuição beta multivariada , conhecida também como distribuição de Dirichlet , esta distribuição pode ser interpretada como contendo informação equivalente ao número total de matrículas . As taxas do fluxo são estimadas usando função de perda quadrática . Também é considerado como estimador das taxas a moda da posteriori , em ausência de uma função de perda específica . São desenhadas propostas futuras dando alguns tópicos de Inferência Bayesiana para Processos estocâsticos . As taxas do Fluxo escolar são calculadas para diversas regiões do país e é considerado um modelo hierárquico com parâmetros comuns para cada região . Dessa forma o presente estudo busca contribuir no sentido de apresentar e discutir as possibilidades do modelo de Profluxo e , ao mesmo tempo , propor uma metodologia que combine os resultados deste método com as tendências demográficas e a teoria estatística , de maneira a se ter um quadro mais fidedigno na demanda por ensino fundamental no Brasil para os anos seguintes 
 Neste trabalho foi feita uma aplicação das Equações Diferenciais Estocásticas á teoria da Precificação de Opções . Esta teoria teve grande impulso com o trabalho [ Black & Scholes , 73 ] , Black e Scholes em seu trabalho entre outra premissas feitas consideraram que os log-retornos dos ativos tinha uma distribuição normal . Aqui neste trabalho foram considerados três modelos , um deles é a difusão log- normal utilizada por Black-Scholes os outros dois modelos são a difusão linear e o processo de Ornstein-Uhlenbeck . Para estes três modelos foram determinadas as Medidas Martingales Equivalentes , isto foi feito utilizando o Teorema de Cameron-Martin- Girsanov , veja [ Friedman , 75 ] , Também foram analisadas versões discretas destes modelos obtidas pela aproximação de Euler , veja [ Kloeden & Platen , 95 ] . O objetivo foi comparar os resultados obtidos com os modelos contínuos com os resultados obtidos com os modelos discretos . Também se fez uma análise dos estimadores dos parâmetros dos modelos contínuos . Nesta análise foi utilizada a abordagem Clássica e a abordagem_Bayesiana  . Primeiramente se fez uma comparação das estimativas obtidas por estas duas_abordagens  e posteriormente uma análise do comportamento assintótico desses estimadores 
 O problema de corte de estoque consiste em cortar unidades maiores ( objetos ) em unidades menores ( itens ) de maneira a satisfazer uma demanda e otimizar algum critério , por exemplo , minimizar a perda gerada pelos padrões de corte ( um padrão descreve como arranjar itens dentro de um objeto ) . O problema dc sequenciamento de padrões de corte consiste em determinar uma sequência , na qual os padrões serão processados a fim de otimizar algum critério , por exemplo , minimizar o número máximo de pilhas abertas ( itens que ainda serão cortados de um ou mais padrões na sequência ) durante o corte dos padrões . Em alguns processos industriais os problemas de geração e sequenciamento de padrões de corte não podem ser resolvidos de forma independente pois , em geral , uma boa solução para o problema de corte ( isto é , com pequena perda de material ) não corresponde a uma boa solução para o problema de sequenciamento ( isto é , com um pequeno número de pilhas abertas ) e vice-versa . Existe , na verdade , um trade-off entre os objetivos desses dois problemas . Neste trabalho três abordagens heurísticas são apresentadas para resolver de forma integrada os problemas de geração e sequenciamento de padrões . Os resultados computacionais apresentados mostram que as abordagens geram boas soluções e são eficazes para analisar o trade-off entre esses dois problemas 
 Os Sistemas de Gerenciamento de Banco de Dados ( SGBDs ) existentes são muito sofisticados , eficientes e rápidos na recuperação de informações envolvendo dados de tipos tradicionais , tais como números , texto , etc. , mas existem muitas limitações em se tratando de recuperar informações quando os tipos de dados são mais complexos , isto é , dados multi-dimensionais . Considerando os problemas existentes com a indexação e recuperação de dados multi-dimensionais , este trabalho propõe um sistema híbrido que combina um modelo de Redes_Neurais  da família ART , ART2-A , com uma estrutura de dados , Slim-Tree , que é um método de acesso a dados no espaço métrico . Esta proposta é uma alternativa para realizar o processo de agrupamento de dados de forma `` inteligente '' tal que os dados pertencentes aos agrupamentos ( clusters ) possam ser recuperados a partir da Slim-Tree correspondente . O sistema híbrido proposto é capaz de realizar consultas do tipo : busca por abrangência e dos k-vizinhos mais próximos , o que não é característica comum das redes neurais_artificiais  . Além disto , os experimentos_realizados  mostram que o desempenho do sistema foi igual ou superior ao desempenho obtido pela Slim-Tree 
 As redes complexas surgiram como uma nova e importante maneira de representação e abstração de dados capaz de capturar as relações espaciais , topológicas , funcionais , entre outras características presentes em muitas bases de dados . Dentre as várias abordagens para a análise de dados , destacam-se a classificação e a detecção de outliers . A classificação de dados permite atribuir uma classe aos dados , baseada nas características de seus atributos e a detecção de outliers busca por dados cujas características se diferem dos demais . Métodos de classificação de dados e de detecção de outliers baseados em redes complexas ainda são pouco estudados . Tendo em vista os benefícios proporcionados pelo uso de redes complexas na representação de dados , o presente_trabalho  apresenta o desenvolvimento de um método baseado em redes complexas para detecção de outliers que utiliza a caminhada aleatória e um índice de dissimilaridade . Este método possibilita a identificação de diferentes_tipos  de outliers usando a mesma medida . Dependendo da estrutura da rede , os vértices outliers podem ser tanto aqueles distantes do centro como os centrais , podem ser hubs ou vértices com poucas ligações . De um modo geral , a medida proposta é uma boa estimadora de vértices outliers em uma rede , identificando , de maneira adequada , vértices com uma estrutura diferenciada ou com uma função especial na rede . Foi proposta também uma técnica de construção de redes capaz de representar relações de similaridade entre classes de dados , baseada em uma função de energia que considera medidas de pureza e extensão da rede . Esta rede construída foi utilizada para caracterizar mistura entre classes de dados . A caracterização de classes é uma questão importante na classificação de dados , porém ainda é pouco explorada . Considera-se que o trabalho desenvolvido é uma das primeiras tentativas nesta 
 Neste projeto é proposta uma avaliação e comparação de diretrizes e a adequação de técnicas que permitam não somente a criação de Web_services  seguros , mas também a validação dos serviços utilizados para determinar se a aplicação possui as características almejadas relacionadas ao desempenho e à segurança . Neste sentido , é primordial analisar as principais especificações de segurança empregadas em Web_services  no contexto atual , bem como avaliar os algoritmos criptográficos e o comprimento das chaves utilizadas . Os resultados obtidos permitem determinar , com base nos objetivos especificados , qual o impacto dos mecanismos de segurança utilizados no desempenho da 
 Tomografia computadorizada por emissão de fóton único ( SPECT ) é uma modalidade da medicina nuclear baseada na medida da distribuição espacial de um radionuclídeo . Esta técnica é amplamente utilizada em cardiologia para avaliar problemas de perfusão miocárdica , relacionados ao fluxo sanguíneo nas artérias coronárias . As imagens SPECT proporcionam melhor separação das regiões do miocárdio e facilitam a localização e a definição dos defeitos de perfusão . Um dos grandes desafios em estudos SPECT é a eficiente apresentação da informação , uma vez que um único estudo pode gerar imagens de centenas de cortes a serem analisados . Para resolver este problema , são utilizados mapas polares ( também conhecidos como gráficos Bulls Eye ) . Mapas polares são construídos a partir de cortes tomográficos do ventrículo esquerdo e apresentam as informações dos exames de forma sumarizada , em uma imagem bidimensional . Essa dissertação apresenta um método para segmentação do ventrículo esquerdo em estudos SPECT do miocárdio e a construção de mapas polares . A segmentação do ventrículo esquerdo é realizada para facilitar o processo de geração_automática  de mapas polares . O método desenvolvido utiliza a transformada watershed , no contexto do paradigma de Beucher-Meyer . Para visualização dos resultados , foi desenvolvida uma aplicação , chamada Medical Image Visualizer ( MIV ) . O MIV será disponibilizado como projeto Open Source , podendo ser livremente utilizado e/ou modificado pela comunidade de usuários , desenvolvedores e pesquisadores 
 Um dos problemas encontrados em sistemas de Processamento de Línguas_Naturais  ( PLN ) é a dificuldade de se identificar que elementos textuais referem-se à mesma entidade . Esse fenômeno , no qual o conjunto de elementos textuais remete a uma mesma entidade , é denominado de correferência . Sistemas de resolução de correferência podem melhorar o desempenho de diversas aplicações do PLN , como : sumarização , extração de informação , sistemas de perguntas e respostas . Recentemente , pesquisas em PLN têm explorado a possibilidade de identificar os elementos correferentes em múltiplos documentos . Neste contexto , este trabalho tem como foco o desenvolvimento de um método aprendizado não supervisionado para resolução de correferência em múltiplos documentos , utilizando como língua-alvo o português . Não se conhece , até o momento , nenhum sistema com essa finalidade para o português . Os resultados dos experimentos feitos com o sistema sugerem que o método desenvolvido é superior a métodos baseados em concordância de cadeias de 
 Sistemas de recuperação de imagens por conteúdo ( Content-based image retrieval { CBIR ) e de classificação dependem fortemente de vetores de características que são extraídos das imagens considerando critérios visuais específicos . É comum que o tamanho dos vetores de características seja da ordem de centenas de elementos . Conforme se aumenta o tamanho ( dimensionalidade ) do vetor de características , também se aumentam os graus de irrelevâncias e redundâncias , levando ao problema da `` maldição da dimensionalidade '' . Desse modo , a seleção das características relevantes é um passo primordial para o bom funcionamento de sistemas CBIR e de classificação . Nesta tese são apresentados novos métodos de seleção de características baseados em algoritmos genéticos ( do inglês genetic algorithms - GA ) , visando o aprimoramento de consultas por similaridade e modelos de classificação . A família Fc ( `` Fitness coach '' ) de funções de avaliação proposta vale-se de funções de avaliação de ranking , para desenvolver uma nova abordagem de seleção de características baseada em GA que visa aprimorar a acurácia de sistemas CBIR . A habilidade de busca de GA considerando os critérios de avaliação propostos ( família Fc ) trouxe uma melhora de precisão de consultas por similaridade de até 22 % quando comparado com métodos wrapper tradicionais para seleção de características baseados em decision-trees ( C4.5 ) , naive bayes , support vector machine , 1-nearest neighbor e mineração de regras de associação . Outras contribuições desta tese são dois métodos de seleção de características baseados em filtragem , com aplicações em classificação de imagens , que utilizam o cálculo supervisionado da estatística de silhueta simplificada como função de avaliação : o silhouette-based greedy search ( SiGS ) e o silhouette-based genetic algorithm search ( SiGAS ) . Os métodos propostos superaram os métodos concorrentes na literatura ( CFS , FCBF , ReliefF , entre outros ) . É importante também ressaltar que o ganho em acurácia obtido pela família Fc , e pelos métodos SiGS e SiGAS propostos proporcionam também um decréscimo significativo no tamanho do vetor de características , de até 90 
 O ensino de fundamentos de programação não é uma tarefa trivial muitos estudantes têm dificuldades em compreender os conceitos abstratos de programação e possuem visões erradas sobre a atividade de programação . Uma das iniciativas que tem sido investigada a fim de amenizar os problemas associados refere-se ao ensino conjunto de conceitos básicos de programação e de teste de software . A introdução da atividade de teste pode ajudar o desenvolvimento das habilidades de compreensão e análise nos estudantes . Além_disso  , aprendendo teste mais cedo os alunos podem se tornar melhores testadores e desenvolvedores . Seguindo esta tendência , em trabalhos anteriores foram investigados alguns mecanismos de apoio ao ensino integrado de fundamentos de programação e teste . Dentre os mecanismos investigados destaca-se a proposição de um ambiente de apoio para submissão e avaliação automática de trabalhos práticos dos alunos , baseado em atividades de teste de software PROGTEST . Em sua primeira versão , a PROGTEST foi integrada à ferramenta JABUTISERVICE , que apoia o teste_estrutural  de programas escritos em Java . O presente projeto de mestrado visou a dar continuidade aos trabalhos já realizados , tendo como principal objetivo a identificação e integração de diferentes ferramentas de teste ao ambiente PROGTEST , explorando tanto técnicas e critérios de teste diferenciados como linguagens de programação distintas . O ambiente PROGTEST também foi aplicado e validado em diferentes cenários de ensino , considerando diferentes linguagens e técnicas de teste . Em linhas gerais , os resultados evidenciam a viabilidade da aplicação do ambiente em cenários de ensino e 
 Um sistema de aprendizado_supervisionado  é um programa capaz de realizar decisões baseado na experiência contida em casos resolvidos com sucesso . As regras de classificação induzidas por um sistema de aprendizado podem ser analisadas segundo dois critérios : a complexidade dessas regras e o erro de classificação sobre um conjunto independente de exemplos 
 Sistemas de aprendizado têm sido desenvolvidos na prática utilizando diferentes paradigmas incluindo estatística , redes neurais , bem como sistemas de aprendizado simbólico proposicionais e relacionais . Diversos métodos de aprendizado podem ser aplicados à mesma amostra de dados e alguns deles podem desempenhar melhor que outros . Para uma dada aplicação , não existem garantias que qualquer um desses métodos é necessariamente o melhor . Em outras_palavras  , não existe uma análise matemática que possa determinar se um algoritmo de aprendizado irá desempenhar melhor que outro . Desta forma , estudos_experimentais  são necessários 
 Neste trabalho nos concentramos em uma tarefa de aprendizado conhecida como classificação ou predição , na qual o problema consiste na construção de um procedimento de classificação a partir de um conjunto de casos no qual as classes verdadeiras são conhecidas , chamado de aprendizado_supervisionado  . O maior objetivo de um classificador é ser capaz de predizer com sucesso a respeito de novos casos . A performance de um classificador é medida em termos da taxa de erro . Técnicas experimentais para estimar a taxa de erro verdadeira não somente provêem uma base para comparar objetivamente as performances de diversos algoritmos de aprendizado no mesmo conjunto de exemplos , mas também podem ser uma ferramenta poderosa para projetar um classificador . As técnicas para estimar a taxa de erro são baseadas na teoria estatística de resampling 
 Um ambiente chamado AMPSAM foi implementado para ajudar na aplicação dos métodos de resampling em conjuntos de exemplos do mundo_real  . AMPSAM foi projetado como uma coleção de programas independentes , os quais podem interagir entre si através de scripts pré-definidos ou de novos scripts criados pelo usuário . O ambiente utiliza um formato padrão para arquivos de exemplos o qual é independente da sintaxe de qualquer algoritmo . AMPSAM também inclui ferramentas para particionar conjuntos de exemplos em conjuntos de treinamento e teste utilizando diferentes métodos de resampling . Além do método holdout , que é o estimador de taxa de erro mais comum , AMPSAM suporta os métodos n-fold cross-validation --_-  incluindo o leaning-one-out --_-  e o método bootstrap . As matrizes de confusão produzidas em cada iteração utilizando conjuntos de treinamento e teste podem ser fornecidas a um outro sistema implementado chamado SMEC . Este sistema calcula e mostra graficamente algumas das medidas descritivas mais importantes relacionadas com tendência central e dispersão dos dados 
 Este trabalho também relata os resultados experimentais a respeito de medidas do erro de classificação de três classificadores proposicionais e relacionais bem conhecidos , utilizando ambos os sistemas implementados , em diversos conjuntos de exemplos freqüentemente utilizados em pesquisas de Aprendizado de Máquina 
 Em Aprendizado de Máquina , a abordagem supervisionada normalmente necessita de um número significativo de exemplos de treinamento para a indução de classificadores precisos . Entretanto , a rotulação de dados é freqüentemente realizada manualmente , o que torna esse processo demorado e caro . Por outro_lado  , exemplos não-rotulados são facilmente obtidos se comparados a exemplos_rotulados  . Isso é particularmente verdade para tarefas de classificação de textos que envolvem fontes de dados on-line tais como páginas de internet , email e artigos científicos . A classificação de textos tem grande_importância  dado o grande volume de textos disponível on-line . Aprendizado semi-supervisionado , uma área de pesquisa relativamente nova em Aprendizado de Máquina , representa a junção do aprendizado_supervisionado  e não-supervisionado , e tem o potencial de reduzir a necessidade de dados rotulados quando somente um pequeno conjunto de exemplos_rotulados  está disponível . Este trabalho descreve o algoritmo de aprendizado semi-supervisionado co-training , que necessita de duas descrições de cada exemplo . Deve ser observado que as duas descrições necessárias para co-training podem ser facilmente obtidas de documentos textuais por meio de pré-processamento . Neste trabalho , várias extensões do algoritmo co-training foram implementadas . Ainda mais , foi implementado um ambiente computacional para o pré-processamento de textos , denominado PreTexT , com o objetivo de utilizar co-training em problemas de classificação de textos . Os resultados experimentais foram obtidos utilizando três conjuntos de dados . Dois conjuntos de dados estão relacionados com classificação de textos e o outro com classificação de páginas de internet . Os resultados , que variam de excelentes a ruins , mostram que co-training , similarmente a outros algoritmos de aprendizado semi-supervisionado , é afetado de maneira bastante complexa pelos diferentes aspectos na indução dos modelos 

 A literatura referente a testes de hipótese em modelos auto-regressivos que apresentam uma possível raiz unitária é bastante vasta e engloba pesquisas oriundas de diversas_áreas  . Nesta dissertação , inicialmente , buscou-se realizar uma revisão dos principais resultados existentes , oriundos tanto da visão clássica quanto da bayesiana de inferência . No que concerne ao ferramental clássico , o papel do movimento browniano foi apresentado de forma detalhada , buscando-se enfatizar a sua aplicabilidade na dedução de estatísticas assintóticas para a realização dos testes de hipótese relativos à presença de uma raíz unitária . Com relação à inferência bayesiana , foi inicialmente conduzido um exame detalhado do status corrente da literatura . A seguir , foi realizado um estudo comparativo em que se testa a hipótese de raiz unitária com base na probabilidade da densidade a posteriori do parâmetro do modelo , considerando as seguintes densidades a priori : Flat , Jeffreys , Normal e Beta . A inferência foi realizada com base no algoritmo Metropolis-Hastings , usando a técnica de simulação de Monte_Carlo  por Cadeias de Markov ( MCMC ) . Poder , tamanho e confiança dos testes apresentados foram computados com o uso de séries simuladas . Finalmente , foi proposto um critério bayesiano de seleção de modelos , utilizando as mesmas distribuições a priori do teste de hipótese . Ambos os procedimentos foram ilustrados com aplicações empíricas à séries_temporais  macroeconômicas 
 Esta dissertação apresenta o Modelo de Componentes CORBA , que é parte da especificação CORBA 3.0 . Esse modelo representa uma das mudanças mais significativas em relação às versões anteriores de CORBA . O desenvolvimento de sistemas distribuídos é uma tarefa complexa , envolvendo fatores como a organização de conjuntos de objetos que devem ser implantados e executados em servidores distintos . Aqui estes conjuntos são denominados componentes , conceito já existente em outras especificações , como a dos Enterprise Java Beans . O texto da especificação dos Componentes CORBA , apesar de conter exemplos , mostra-se de difícil compreensão para o desenvolvedor de sistemas . Este trabalho se propõe a facilitar a tarefa de entendimento e utilização de Componentes CORBA , procurando ser didático e ilustrativo o suficiente para que diferentes perfis de leitores possam compreender os diversos conceitos apresentados , seja como uma visão geral , ou ainda como auxílio no desenvolvimento de componentes 
 A computação ubíqua , uma das mais recentes áreas da Ciência da Computação , tem como objetivo tornar os serviços computacionais tão intrínsecos a um determinado ambiente que se tornam transparentes para seus usuários . Este trabalho se insere nesse contexto , tanto buscando apoiar as atividades cotidianas de um usuário em particular quanto provendo flexibilidade de comunicação entre um conjunto de usuários de modo geral . Investigando os problemas associados , desenvolvemos um sistema de anotações visando a captura e o acesso a informações públicas em experiências ao vivo , tais como aulas presenciais , utilizando , para tanto , dispositivos pessoais digitais ( ou PDAs - Personal Digital Assistants ) . Apesar dos PDAs apresentarem vantagens como portabilidade e baixo consumo de energia , a limitação de sua tela representa problemas para usuários , que , geralmente , têm dificuldade em visualizar e interagir com uma quantidade de informações que extrapola o tamanho da tela desse dispositivo . Neste trabalho , implementamos um sistema que simula uma área de anotações maior do que a tela dos PDAs e elaboramos um mecanismo de rolagem de textos para favorecer a escrita de anotações . Para avaliar esse mecanismo e o impacto da utilização de uma área maior na orientação espacial do usuário , conduzimos dois experimentos e analisamos seus resultados . Por fim , acoplamos essas características no sistema desenvolvido e o integramos à infra-estrutura do Projeto InCA-SERVE , em utilização pelo grupo de hipermídia do ICMC-USP 
 Um dos principais focos de pesquisa na área de computação ubíqua é o suporte à construção de aplicações de captura e acesso . Tais aplicações automatizam o processo de captura de experiências ao vivo e a geração de hiperdocumentos associados para o acesso à informação que foi capturada . Aplicações de captura e acesso apresentam requisitos que dificultam a sua construção e evolução , tal como a demanda por suporte a variados dispositivos durante a captura assim como diversos formatos para os documentos resultantes e um alto grau de automação na geração desses documentos . Nesse contexto , o projeto InCA-SERVE foi proposto para suportar o desenvolvimento de tais aplicações por meio de um conjunto de infra-estruturas e de serviços . O objetivo do trabalho reportado nesta dissertação é complementar o projeto InCA-SERVE com relação à visualização da informação capturada . Baseada em requisitos de aplicações de captura e acesso em particular , e em conceitos estabelecidos de metodologias de projeto hipermídia em geral , uma nova infra-estrutura chamada wVIEW foi desenvolvida . wVIEW suporta a geração_automática  de aplicações Web que permitem a geração dinâmica de documentos de apresentação para o conteúdo capturado por aplicações de captura e acesso 
 Este trabalho tem por objetivo desenvolver ferramentas que permitam gerar em fones-de-ouvido o efeito psicoacústico de fontes sonoras locomovendo-se no espaço , por meio da auralização do sinal monofônico original . Embora a auralização binaural possa ser feita empregando variações de atraso ( chamadas ITD interaural time difference , ou diferença de tempo interaural ) e de intensidade ( chamadas ILD interaural level difference , ou diferença de nível interaural ) entre os canais , melhores_resultados  psicoacústicos podem ser obtidos ao se utilizar filtros digitais conhecidos como HRTFs ( head related transfer functions , ou funções de transferência relativas à cabeça ) . Uma HRTF insere no sinal monofônico informações que possibilitam ao sistema auditivo identificá-lo como proveniente de uma direção específica , direção esta que é única para cada HRTF . Para posicionar uma fonte estática em uma direção específica , bastaria , então , filtrar o sinal original pela HRTF da direção desejada . Se , no entanto , for desejável que a fonte se locomova em uma trajetória contínua , um número infinitamente grande de filtros seria necessário . Como eles são , normalmente , obtidos empiricamente , um número arbitrariamente alto deles não está disponível . Disso surge a necessidade de técnicas de interpolação de HRTFs , que possibilitem gerar os filtros intermediários não disponíveis . Este trabalho apresenta três novas técnicas de interpolação de HRTFs , para assim alcançar o objetivo de auralizar fontes sonoras móveis : a interpolação triangular , que é uma técnica de interpolação linear baseada na técnica de panorama sonoro VBAP ( vector-based amplitude panning , ou panorama sonoro baseado em vetores ) ; o método das movimentações discretas , que busca explorar o limiar de percepção do nosso sistema auditivo para , com isso , gerar uma técnica extremamente barata computacionalmente ; e a interpolação espectral , que altera continuamente as estruturas das HRTFs para gerar filtros interpolados . São apresentadas também as implementações feitas dessas novas técnicas desenvolvidas , bem como os testes numéricos realizados para medir sua eficácia 
 O método de Kaczmarz é um algoritmo iterativo que soluciona sistemas_lineares  do tipo Ax = b através de projeções sobre hiperplanos bastante usado em aplicações que envolvem a Tomografia Computadorizada . Recentemente voltou a ser destaque após a publicação de uma versão aleatória apresentada por Strohmer e Vershynin em 2009 a qual foi provada possuir taxa de convergência esperada exponencial . Posteriormente , Eldar e Needell em 2011 sugeriram uma versão modificada do algoritmo de Strohmer e Vershynin , na qual a cada iteração é selecionada a projeção ótima a partir de um conjunto aleatório , utilizando para isto o lema de Johnson-Lindenstrauss . Nenhum dos artigos mencionados apresenta uma técnica para a escolha do parâmetro de relaxação , entretanto , a seleção apropriada deste parâmetro pode ter uma influência substancial na velocidade do método . Neste trabalho apresentamos uma metodologia para a escolha do parâmetro de relaxação , bem como implementações paralelas do algoritmo de Kaczmarz utilizando as ideias de Eldar e Needell . Nossa metodologia para seleção do parâmetro utiliza uma nova generalização dos resultados de Strohmer e Vershynin que agora leva em consideração o parâmetro λ de relaxação e , a partir daí , obtemos uma estimativa da taxa de convergência como função de λ. Escolhemos então , para uso no algoritmo , aquele que otimiza esta estimativa . A paralelização dos métodos foi realizada através da plataforma CUDA e se mostrou muito promissora , pois conseguimos , através dela , um ganho significativo na velocidade de 
 O sequenciamento de pequenos RNAs surgiu recentemente como uma tecnologia inovadora na descoberta de microRNAs ( miRNA ) . Essa tecnologia tem facilitado a descoberta de milhares de miRNAs em um grande número de espécies . No entanto , apesar dos benefícios dessa tecnologia , ela apresenta desafios , como a necessidade de construir uma biblioteca de pequenos RNAs , além do genoma . Diferentemente , métodos computacionais ab_initio  buscam diretamente no genoma regiões prováveis de conter miRNAs . A maioria desses métodos usam modelos preditivos capazes de distinguir entre os verdadeiros ( positivos ) e pseudo precursores de miRNA - pre-miRNA - ( negativos ) , os quais são induzidos utilizando técnicas de mineração de dados . No entanto , a aplicabilidade de métodos ab_initio  da literatura atual é limitada pelas altas taxas de falsos positivos e/ou por outras dificuldades computacionais , como o elevado tempo necessário para calcular um conjunto de atributos . Neste trabalho , investigamos como os principais aspectos envolvidos na indução de modelos preditivos de pre-miRNA afetam o desempenho preditivo . Particularmente , avaliamos a capacidade discriminatória de conjuntos de atributos propostos na literatura , cujos custos computacionais e a composição variam amplamente . Os experimentos computacionais foram realizados utilizando dados de sequências positivas e negativas de 45 espécies , cobrindo espécies de oito filos . Os resultados mostraram que o desempenho preditivo de classificadores induzidos utilizando conjuntos de treinamento com 1608 ou mais vetores de atributos calculados de sequências humanas não diferiram significativamente , entre os conjuntos de atributos que produziram as maiores acurácias . Além_disso  , as diferenças entre os desempenhos preditivos de classificadores induzidos por diferentes algoritmos de aprendizado , utilizando um mesmo conjunto de atributos , foram pequenas ou não significantes . Esses resultados inspiraram a obtenção de um conjunto de atributos menor e que pode ser calculado até 34 vezes mais rapidamente do que o conjunto de atributos menos custoso produzindo máxima acurácia , embora a acurácia produzida pelo conjunto proposto não difere em mais de 0.1 % das acurácias máximas . Quando esses experimentos foram executados utilizando vetores de atributos calculados de sequências de outras 44 espécies , os resultados mostraram que os conjuntos de atributos que produziram modelos com as maiores acurácias utilizando vetores calculados de sequências humanas também produziram as maiores acurácias quando pequenos conjuntos de treinamento ( 120 ) calculados de exemplos de outras espécies foram utilizadas . No entanto , a análise destes modelos mostrou que a complexidade de aprendizado varia amplamente entre as espécies , mesmo entre aquelas pertencentes a um mesmo filo . Esses resultados mostram que a existência de características espécificas em pre-miRNAs de certas espécies sugerida em estudos anteriores pode estar correlacionada com a complexidade de aprendizado . Consequentemente , a acurácia de modelos induzidos utilizando um mesmo conjunto de atributos e um mesmo algoritmo de aprendizado varia amplamente entre as espécies . i Os resultados também mostraram que o uso de exemplos de espécies filogeneticamente mais complexas pode aumentar o desempenho preditivo de espécies menos complexas . Por último , experimentos computacionais utilizando técnicas de ensemble mostraram estratégias alternativas para o desenvolvimento de novos modelos para predição de pre-miRNA com maior probabilidade de obter maior desempenho preditivo do que estratégias atuais , embora o custo_computacional  dos atributos seja inferior . Uma vez que a descoberta de miRNAs envolve a análise de milhares de regiões genômicas , a aplicação prática de modelos preditivos de baixa acurácia e/ou que dependem de atributos computacionalmente custosos pode ser inviável em análises de grandes genomas . Neste trabalho , apresentamos e discutimos os resultados de experimentos computacionais investigando o potencial de diversas estratégias utilizadas na indução de modelos preditivos para predição ab_initio  de pre-miRNAs , que podem levar ao desenvolvimento de ferramentas ab_initio  de maior aplicabilidade 
 Este trabalho descreve experimentos_realizados  com Redes_Neurais  Artificiais e algoritmos de aprendizado simbólico . Também são investigados dois algoritmos de extração de conhecimento de Redes_Neurais  Artificiais . Esses experimentos são realizados com três bases de dados com o objetivo de comparar os desempenhos obtidos . As bases de dados utilizadas neste trabalho são : dados de falência de bancos brasileiros , dados do jogo da velha e dados de análise de crédito . São aplicadas sobre os dados três técnicas para melhoria de seus desempenhos . Essas técnicas são : partição pela menor classe , acréscimo de ruído nos exemplos da menor classe e seleção de atributos mais relevantes . Além da análise do desempenho obtido , também é feita uma análise da dificuldade de compreensão do conhecimento_extraído  por cada método em cada uma das bases de dados 
 Esta dissertação de mestrado aborda formulações computacionais e algoritmos para a busca e extração de padrões em cadeias biológicas . Em particular , o presente texto concentra-se nos dois problemas a seguir , considerando-os sob as distâncias de Hamming e Levenshtein : a ) como determinar os locais nos quais um dado padrão ocorre de modo aproximado em uma cadeia fornecida ; b ) como extrair padrões que ocorram de modo aproximado em um número significativo de cadeias de um conjunto fornecido . O primeiro problema , para o qual já existem diversos algoritmos polinomiais , tem recebido muita atenção desde a década de 60 , e ganhou novos ares com o advento da biologia computacional , nos idos dos anos 80 , e com a popularização da Internet e seus mecanismos de busca : ambos os fenômenos trouxeram novos obstáculos a serem superados , em razão do grande volume de dados e das bastante justas restrições de tempo inerentes a essas aplicações . O segundo problema , de surgimento um pouco mais recente , é intrinsicamente desafiador , em razão de sua complexidade computacional , do tamanho das entradas tratadas nas aplicações mais comuns e de sua dificuldade de aproximação . Também é de chamar a atenção o seu grande potencial de aplicação . Neste trabalho são apresentadas formulações adequadas dos problemas abordados , assim como algoritmos e estruturas de dados essenciais ao seu estudo . Em especial , estudamos a extremamente versátil árvore dos sufixos , assim como uma de suas generalizações e sua estrutura irmã : o vetor dos sufixos . Grande parte do texto é dedicada aos filtros baseados em q-gramas para a busca aproximada de padrões e algumas de suas mais recentes variações . Estão cobertos os algoritmos bit-paralelos de Myers e Baeza-Yates-Gonnet para a busca de padrões ; os algoritmos de Sagot para a extração de padrões ; os algoritmos de filtragem de Ukkonen , Jokinen-Ukkonen , Burkhardt-Kärkkäinen , entre outros 
 Com o aumento da expectativa de vida em quase todos os países do mundo , inclusive no Brasil , o número de pessoas idosas vem crescendo de forma considerável . Consequentemente , as doenças degenerativas comuns às pessoas com idades avançadas também tendem a aumentar correspondentemente , tornando doenças , como o Alzheimer , cada vez mais comuns . O cuidado a esses idosos é de extrema importância . Infelizmente , um dos problemas enfrentados pela sociedade , é que o número de cuidadores nesse contexto não segue um aumento proporcional , resultando então em cuidadores altamente sobrecarregados . Acredita-se que o uso de tecnologia para algumas tarefas dos cuidadores ( e.g . registro do dia a dia ) , possa amenizar essa sobrecarga , mas , o que se tem visto é a carência de ferramentas tecnológicas que possam auxiliar nas atividades de cuidado ao paciente idoso . Apesar da importância de estudos nessa direção , poucas iniciativas de pesquisas abordam questões relacionadas às ferramentas tecnológicas para apoiar cuidadores de idosos , sem que essa os traga uma sobrecarga ainda maior . Uma das tarefas que pode ser feita com o auxílio da tecnologia é o registro de informações diárias do idoso , como por exemplo , alimentação , medicação e higiene . A partir das informações coletadas , é possível gerar visualizações que podem potencializar as análises feitas pelos médicos , e consequentemente , prover colaboração entre os cuidadores e os diferentes profissionais de saúde ( e.g . psicólogos , nutricionistas , terapeutas ocupacionais , enfermeiros ) . Com o objetivo de entender melhor as reais necessidades dos cuidadores na coleta das informações do dia a dia , foram conduzidos alguns estudos que consistiam , basicamente , na realização de entrevistas e aplicação de questionários avaliativos . Diante dos resultados obtidos nesses estudos , foi desenvolvida uma ferramenta nomeada Day2Day que utiliza técnicas de visualização de informação e conceitos colaborativos como solução . A fim de avaliar se o Day2Day atingia os objetivos propostos , foi possível implantar a ferramenta em dois ambientes reais distintos , um lar para idosos e duas residências particulares . No total , sete profissionais de saúde fizeram uso da ferramenta por aproximadamente 25 dias , gerando informações diárias de oito idosos . Os resultados dessa avaliação indicam que o Day2Day atende em conformidade com as expectativas propostas , como por exemplo , a redução do tempo gasto para registro das informação 
 Este projeto apresenta o desenvolvimento de um conjunto de algoritmos que objetivam o escalonamento de tarefas em grades móveis , com foco no problema de conectividade e tolerância às falhas . Algoritmos de aprendizado de máquina e algoritmos estocásticos são aplicados na organização de recursos da grade durante o processo de escalonamento de tarefas . Além_disso  , uma tolerância a falhas foi considerada em conjunto ao algoritmo proposto e o desempenho do algoritmo foi avaliado por meio de simulação . Os resultados mostram que a queda da quantidade de falhas é significativa com a utilização dos algoritmos desenvolvidos , principalmente quando há uma grande quantidade de tarefas e poucos recursos disponíveis . O aumento do tempo de execução é inevitável neste caso , porém é controlável pelas variáveis definidas nos algoritmos 
 Neste trabalho , estudamos o problema a recoloração convexa de grafos , denotado por RC . Dizemos que uma coloração dos vértices de um grafo G é convexa se , para cada cor tribuída d , os vértices de G com a cor d induzem um subgrafo conexo . No problema RC , é dado um grafo G e uma coloração de seus vértices , e o objetivo é recolorir o menor número possível de vértices de G tal que a coloração resultante seja convexa . A motivação para o estudo deste problema surgiu em contexto de árvores filogenéticas . Sabe-se que este problema é NP-difícil mesmo quando G é um caminho . Mostramos que o problema RC parametrizado pelo número de mudanças de cor é W [ 2 ] -difícil mesmo se a coloração inicial usa apenas duas cores . Além_disso  , provamos alguns resultados sobre a inaproximabilidade deste problema . Apresentamos uma formulação inteira para a versão com pesos do problema RC em grafos arbitrários , e então a especializamos para o caso de árvores . Estudamos a estrutura facial do politopo definido como a envoltória convexa dos pontos inteiros que satisfazem as restrições da formulação proposta , apresentamos várias classes de desigualdades que definem facetas e descrevemos os correspondentes algoritmos de separação . Implementamos um algoritmo branch-and-cut para o problema RC em árvores e mostramos os resultados computacionais obtidos com uma grande quantidade de instâncias que representam árvores filogenéticas reais . Os experimentos mostram que essa abordagem pode ser usada para resolver instâncias da ordem de 1500 vértices em 40 minutos , um desempenho muito superior ao alcançado por outros algoritmos propostos na literatura 
 Apresentamos um novo esquema de acordo de chaves criptográficas hierárquico , não Interativo e seguro contra comprometimento de múltiplos nós . Esquemas para Acordo de chaves criptográficas ( KAS - Key Agreement Scheme ) , são usados quando duas ou mais entidades desejam compartilhar uma chave secreta única , afim de para realizar uma comunicação segura por meio de um protocolo de criptografia simétrico . O acordo de chaves proposto possui as seguintes características : Não interativo : Chaves compartilhadas são calculadas sem interação dos nós participantes ; Chaves Públicas sem certificados ( Certificateless ) : Para o cálculo da chave compartilhada o nó utiliza sua chave secreta e a chave_pública  do destinatário , que é certificada pela identidade do destinatário ; Hierárquico : Permite que seja utilizado um gerenciamento hierárquico , para concessão , revogação e distribuição de chaves ; e Resistente : Permite segurança do sistema mesmo quando nós dentro da hierarquia são comprometidos em qualquer ordem e quantidade . Este trabalho é uma nova abordagem do artigo `` Strongly-Resilent and Non-Interactive Hierarchical Key-Agreement in MANETs '' onde substituímos o uso de sistemas baseados na identidade por sistemas sem certificado , eliminando a custódia de chaves em todos os níveis hierárquicos , aumentando a segurança do sistema quanto ao comprometimento de nós . É apresentado ainda uma discussão_sobre  a segurança do esquema proposto e de acordos de chaves não interativos 
 A tecnologia tem estado presente cada vez mais no dia-a-dia e trouxe avanços notáveis para a área de visão_computacional  . Uma das áreas de visão , a detecção de características humanas , sempre foi importante para tarefas de reconhecimento , vigilância , controle e outras . Há um grande potencial de uso na área de acessibilidade , podendo ser benéfica para um grupo de pessoas com necessidades especiais , para proporcionar uma maior interação com o ambiente e com as pessoas . Uma dessas características , os lábios , é útil para o reconhecimento visual e/ou auditivo da fala e pode ser usada para aplicações em acessibilidade de deficientes auditivos e surdos , como por exemplo para a leitura labial . Com a crescente popularização dos dispositivos_móveis  e avanços no hardware , além do custo cada vez mais acessível , torna-se possível a implementação de métodos rápidos e eficientes para detecção e seu posterior rastreamento . Assim , é possível o uso em tempo real nos dispositivos_móveis  . Esta dissertação descreve um sistema desenvolvido para a detecção e rastreamento da região dos lábios nesse contexto . A detecção da região dos lábios é feita pelo algoritmo de Viola-Jones , com o diferencial do uso de conjuntos de imagens sintéticas para o treinamento e geração do detector proposto . O rastreamento é baseado no Camshift com modificações , um método de rastreamento de objeto por kernel . A implementação é descrita em detalhes e são mostrados resultados obtidos por sequências gravadas em um dispositivo móvel . As sequências são capturadas em uma configuração não frontal , o que pode ser útil em aplicações específicas . Métricas baseadas no número de acertos , de erros e de falsos positivos são usadas para avaliar os resultados , além da taxa de quadros por segundo 
 Capturar e analisar pacotes de dados que trafegam pelas redes são tarefas essenciais para os administradores de redes . Estas tarefas ajudam na detecção de anomalias nos sistemas e na verificação do estado atual da rede . Existem várias aplicações que desempenham este papel para o sistema_operacional  GNU/Linux . Estes programas também exportam informações para os usuários e outras aplicações de várias maneiras . Entretanto , não exportam estas informações de forma hierárquica . Esta pesquisa propõe uma arquitetura alternativa aos sistemas atuais . Nossa arquitetura exporta pacotes de dados em uma estrutura hierárquica de arquivos e diretórios . Além_disso  , por se tratar de uma arquitetura modular , filtros adicionais , desenvolvidos por terceiros , podem ser adicionados ao sistema . A arquitetura proposta acompanha uma implementação de referência : o sistema de arquivos virtuais netsfs ( Network Statistics File System ) , que funciona em espaço de núcleo ( kernel space ) . A arquitetura e o sistema de arquivos netsfs , propostos nesta pesquisa , apresentam um método alternativo para exibir os pacotes de redes . Os resultados mostraram uma aparente melhoria no que diz_respeito  à vazão da rede 
 A análise automática da similaridade entre imagens depende fortemente de descritores que consigam caracterizar o conteúdo das imagens em dados compactos e discriminativos . Esses dados extraídos e representados em um vetor-de-características tem o objetivo de representar as imagens nos processos de mineração e análise para classificação e/ou recuperação . Neste trabalho foi explorado o uso de dicionários visuais e contexto para representar e recuperar as características locais das imagens utilizando formalismos estendidos com alto poder descritivo . Esta tese apresenta em destaque três novas propostas que contribuem competitivamente com outros trabalhos da literatura no avanço do estado-da-arte , desenvolvendo novas metodologias para a caracterização de imagens e para o processamento de consultas por similaridade . A primeira proposta estende a modelagem Bag-of-Visual-Words , permitindo codificar a interação entre palavras-visuais e suas disposições espaciais na imagem . Para tal fim , três novas abordagem são apresentadas : ( i ) Weighted Histogram ( WE ) ; ( ii ) Bunch-of-2-grams e ( iii ) Global Spatial Arrangement ( GSA ) . Cada uma dessas técnicas permitem extrair_informações  semanticamente complementares , que enriquecem a representação final das imagens descritas em palavras-visuais . A segunda proposta apresenta um novo descritor , chamado de Bag-of-Salience-Points ( BoSP ) , que caracteriza e analisa a dissimilaridade de formas ( silhuetas ) de objetos explorando seus pontos de saliências . O descritor BoSP se apoia no uso de um dicionário de curvaturas e em histogramas espaciais para representar sucintamente as saliências de um objeto em um único vetor-de-características de tamanho fixo , permitindo recuperar formas usando funções de distâncias computacionalmente rápidas . Por fim , a terceira proposta apresenta um novo modelo de consulta por similaridade , denominada Similarity Based on Dominant Images ( SimDIm ) , baseada no conceito de Imagens Dominantes , que é um conjunto que representa , de uma maneira mais diversificada e reduzida , toda a coleção de imagens da base de dados . Tal conceito permite dar mais eficiência quando se deseja analisar o contexto da coleção , que é o objetivo da proposta . Os experimentos_realizados  mostram que os métodos propostos contribuem de maneira efetiva para caracterizar e quantificar a similaridade entre imagens por meio de abordagens estendidas baseadas em dicionários visuais e análise contextual , reduzindo a lacuna semântica existente entre a percepção humana e a descrição computacional 
 A atividade de teste é essencial para a garantia de qualidade do software e deveria ser empregada durante todo o processo de desenvolvimento . Entretanto , o esforço para a sua aplicação e o alto custo envolvido , comprometem sua utilização de maneira adequada . Durante o processo de desenvolvimento ágil , onde o tempo é um fator crítico , otimizar a atividade de testes sem afetar a qualidade é uma tarefa desafiadora . Apesar do crescente_interesse  em pesquisas sobre testes no contexto de métodos ágeis , poucas evidências são encontradas sobre avaliação do esforço para elaboração , evolução e manutenção dos testes nesse contexto . Este trabalho propõe uma abordagem para predição de defeitos desenvolvida para o contexto do desenvolvimento ágil e , portanto , considerando as características deste processo de desenvolvimento . Essa abordagem pode ser aplicada quando se considera ou não o desenvolvimento dirigido a testes . A abordagem permite priorizar a execução dos testes com base em uma lista de arquivos que apresentam maior probabilidade de apresentarem defeitos . A abordagem proposta foi avaliada por meio de um estudo de caso conduzido em um ambiente real de desenvolvimento . Como resultado obtido , observou-se que a abordagem melhorou a qualidade do projeto desenvolvido , sem aumentar o esforço durante a atividade de teste de software 
 Cadeias de suprimentos podem ter operações seguindo diferentes estratégias de distribuição e a utilização de cada uma dessas estratégias pode resultar em diferentes operações e custos . A estratégia de cross-docking auxilia na redução dos custos de distribuição de produtos , consolidando cargas , e a redução de tempo e custos de armazenamento , uma vez que o tempo máximo de estoque permitido pela estratégia é de cerca de 24 horas . O objetivo deste trabalho é apresentar um modelo para o problema de cross-docking , em que cargas são entregues e reorganizadas de forma a atender a outras cargas que são coletadas e garantir que as janelas de tempo para início das operações sejam atendidas . Devido à falta de instâncias para o problema disponíveis na literatura , buscou-se gerar um benchmark e disponibilizá-las à comunidade_científica  . Uma vez que o problema é de difícil solução exata , um método heurístico para a resolução do problema foi desenvolvido . Os resultados mostraram que o modelo proposto resulta em boas soluções quando comparado ao modelo da literatura . O estudo de calibração do software IBM CPLEX mostrou que a calibração dos parâmetros pode resultar em melhores soluções e , por fim , a matheurística se mostrou competitiva com o CPLEX , principalmente para cenários em que a proporção de entregas e coletas diverge 
 As qualidades humanas somente são desenvolvidas na sociedade através da interação com os outros . Desde que a criança nasce , ela vê , ouve , toca , e sente as coisas . Portanto , estímulos visuais e auditivos são naturais para o homem , e estes estímulos também estão presentes em vídeos . Pode ser que seja esse o motivo pelo qual os vídeos causam grande impacto na sociedade . Uma evidência da popularidade e do crescimento de vídeos na sociedade é a grande quantidade de vídeos carregados no YouTube ou o grande número de vídeos postados no Facebook . Além_disso  , o surgimento dos smartphones tem aumentado a quantidade de usuários que não só assistem vídeos , mas também fazem vídeos . Mas o quê acontece com as pessoas que não podem acessar estes conteúdos por terem alguma deficiência ou doença temporária ? Para evitar que pessoas não compreendam o conteúdo do vídeo , é importante fornecer um vídeo com acessibilidade , assim , por exemplo , cerca de 23,9 % do total da população de Brasil , declaradas com deficiência , serão beneficiadas . Diretrizes e leis para promover a acessibilidade para vídeos foram desenvolvidas . Existem também pesquisas sobre reprodutores de vídeo acessíveis , melhoramento da legenda , recursos sobre a língua de sinais e implementações para enriquecer conteúdos . Apesar do processo de produção de vídeo estar bem definido , as práticas e pesquisas atuais não consideram acessibilidade em seus estágios . Não se explica como autores amadores podem criar conteúdo alternativo considerando as diretrizes ou como incorporar essas diretrizes de acessibilidade no processo de produção de vídeo . Esta tese propõe um método , chamado Video4All , para a autoria de conteúdo alternativo dentro de um processo de produção de vídeo acessível . O Video4All é composto por um conjunto de atividades , incluindo diretrizes para conteúdo alternativo e avaliação desse conteúdo . Estudos de caso foram realizados para verificar a eficácia da aplicação do método proposto por autores amadores , avaliando o seu conteúdo alternativo criado . Assim , ao processo foi incorporada acessibilidade em todos os estágios , considerando-se estudos de caso que relataram as dificuldades que os usuários têm durante a interação com um reprodutor de vídeo acessível . Adicionalmente , foi desenvolvido um método de avaliação do conteúdo alternativo para obter uma medida de qualidade , de forma que os autores entendam melhor o nível de acessibilidade do conteúdo alternativo . O processo de produção de vídeo acessível pode ser utilizado por qualquer autor amador para melhorar a acessibilidade nos seus vídeos . O Video4All também auxilia na criação e avaliação de conteúdo alternativo 
 Um dos principais problemas da Biologia é tentar explicar o processo evolutivo das espécies existentes e de que forma essas espécies se relacionam em termos de ancestrais comuns . A determinação dessas relações evolutivas dá-se o nome de filogenia ou reconstrução de árvores filogenéticas . A reconstrução de árvores filogenéticas têm sido importante para uma variedade de problemas , tais como : taxonomia , virologia , filogenômica , alinhamento múltiplo de sequências , entre outras . Um problema fundamental em filogenia consiste no fato das espécies ancestrais que existiram no passado não poderem ser observadas diretamente . Assim , é necessário buscar mecanismos para , analisando os organismos atuais , recuperar informações a respeito das relações de parentesco com os organismos ancestrais hipotéticos . Neste sentido , as técnicas filogenéticas buscam determinar os ancestrais hipotéticos que melhor representam um processo evolutivo que explique as espécies existentes . Os Algoritmos_Evolutivos  ( AEs ) têm mostrado resultados significativos em filogenia . Por outro_lado  , a reconstrução de árvores filogenéticas é um problema de Projeto de Redes ( PR ) para o qual novas abordagens evolutivas têm sido desenvolvidas recentemente buscando o aumento de eficiência computacional . Este trabalho_investiga  a aplicação dessas novas abordagens para filogenia 
 A identificação de unia planta exige , pelos padrões de taxionomia vegetal , a análise de folhas , flores e frutos . O projeto TreeVis surge com uma proposta de auxiliar na identificação de espécies vegetais , por meio do uso de métodos biométricos , a partir da análise de alguns atributos de uma folha . A contribuição inicial deste trabalho de mestrado , para o projeto TreeVis , está obtenção de classificadores por meio do uso de assinaturas de contorno , sob o domínio da frequência , possibilitando a composição de diversos tipos de assinaturas e classificadores para uma mesma espécie . Devido à baixa eficiência obtida por métodos de classificação como distância mínima , optou-se pelo uso de redes neurais . Essa abordagem evidenciou a necessidade de solução de dois problemas : o grande número de possibilidades de composição de sinais o que ocasionaria um grande esforço computacional para a obtenção de todas respectivas redes neurais ; e o reduzido número das amostras utilizadas no trabalho - o qual comprometeria as etapas de treinamento e teste de uma rede_neural  . Para a solução desses problemas , foram desenvolvidos dois métodos : o primeiro método identifica e seleciona as assinaturas que apresentam um maior potencial de sucesso em obter um classificador por meio de redes neurais , solucionando o problema e desperdício de esforço computacional ; o segundo método possibilita a geração de amostras artificiais de folhas através da combinação dos espectros de frequência do contorno das amostras reais por meio operadores genéticos de cross-over e mutação . Solucionadas as duas questões , foram obtidas diversas redes neurais , através da indicação das assinaturas de melhor potencial e treinadas com amostras artificiais . Do total de 31 classes , 7 foram descartadas da tentativa de obtenção de classificadores por não apresentarem nenhuma assinatura com potencial de classificação - conforme indicação do método desenvolvido . Das 24 espécies restantes , foram obtidos classificadores para 18 espécies ( 75 % ) com taxas médias de 85 % de acerto . A execução deste trabalho necessitou do desenvolvimento de um arcabouço para a automatização da geração , treinamento e teste das redes neurais 
 Frente à grande demanda por software e à forte competitividade existente no mercado atual , o alcance de alguns objetivos , tais como aumento de qualidade e de produtividade , redução dos prazos de entrega e dos custos globais do desenvolvimento de software , tem sido apresentado como bastante importante para o sucesso de uma organização de desenvolvimento de sistemas . Neste cenário , o reúso sistemático de componentes de software tem exercido o papel de grande colaborador para o alcance desses objetivos . Para a concretização de sua prática efetiva . porém , o reúso vêm enfrentando barreiras de diversas origens , como questões técnicas e organizacionais . Assim , é importante que a introdução do reúso aborde tais questões de maneira integrada e gradativa , respeitando o tempo natural da organização para a absorção de mudanças e cuidando para que seu impacto seja reduzido . Desta maneira , o principal objetivo deste projeto foi o desenvolvimento de uma estratégia que sistematizasse o processo de introdução do reúso em uma organização , considerando mudanças organizacionais , técnicas e processuais , assim como sua sequência de execução , suas consequências e dificuldades . Para criar e validar esta estratégia , foi desenvolvido um estudo de caso no Laboratório de Inteligência Computacional do Instituto de Ciências_Matemáticas  e de Computação da Universidade de São Paulo . Neste estudo de caso , foi também selecionado um conjunto mínimo de atividades e requisitos importantes para viabilização do reúso e desenvolvida a especificação de um ambiente , denominado RaCooMN [ Reusable Components Management ENvironment ) , que automatiza muitas das atividades propostas na estratégia referentes ao armazenamento e gerenciamento dos componentes 
 A importância do planejamento de sistemas hidrotérmicos é largamente reconhecida . A questão constitui-se em um complicado problema de otimização que envolve aspectos não-lineares e estocásticos , e que se torna mais complexo quando várias usinas são consideradas em conjunto , em virtude da ocorrência de interações . Apesar de se conhecerem formalizações teóricas do problema e da solução , a complexidade acarreta sérios entraves computacionais e muitas_vezes  consideram-se apenas cenários simplificados , como o determinístico ou estocástico com um modelo de reservatório equivalente . Este trabalho tem como objetivo conciliar a facilidade numérica de problemas determinísticos com a consideração indireta da natureza estocástica do problema . Com esta finalidade , propõe-se o uso de um funcional de custo que reflete o risco de vertimento a cada_período  . Outra linha estudada neste trabalho foi a de rastreamento de alvo utilizando a técnica de controle Linear Quadrático ( LQ ) . A eficácia das estratégias propostas é avaliada através de diversos casos de estudo , incluindo comparação com o planejamento determinístico 
 Este trabalho introduz uma técnica chamada Answer Set Programming Probabilístico ( PASP ) , que permite a modelagem de teorias complexas e a verificação de sua consistência em relação a um conjunto de dados estatísticos . Propomos métodos de resolução baseados em uma redução para o problema da satisfazibilidade probabilística ( PSAT ) e um método de redução de Turing ao ASP 
 Obter uma representação sucinta e representativa de imagens médicas é um desafio que tem sido perseguido por pesquisadores da área de processamento de imagens médicas com o propósito de apoiar o diagnóstico auxiliado por computador ( Computer Aided Diagnosis - CAD ) . Os sistemas CAD utilizam algoritmos de extração de características para representar imagens , assim , diferentes extratores podem ser avaliados . No entanto , as imagens médicas contêm estruturas internas que são importantes para a identificação de tecidos , órgãos , malformações ou doenças . É usual que um grande número de características sejam extraídas das imagens , porém esse fato que poderia ser benéfico , pode na realidade prejudicar o processo de indexação e recuperação das imagens com problemas como a maldição da dimensionalidade . Assim , precisa-se selecionar as características mais relevantes para tornar o processo mais eficiente e eficaz . Esse trabalho desenvolveu o método de seleção supervisionada de características FSCoMS ( Feature Selection based on Compactness Measure from Scatterplots ) para obter o ranking das características , contemplando assim , o que é necessário para o tipo de imagens médicas sob análise . Dessa forma , produziu-se vetores de características mais enxutos e eficientes para responder consultas por similaridade . Adicionalmente , foi desenvolvido o extrator de características k-Gabor que extrai características por níveis de cinza , ressaltando estruturas internas das imagens médicas . Os experimentos_realizados  foram feitos com quatro bases de imagens médicas do mundo_real  , onde o k-Gabor sobressai pelo desempenho na recuperação por similaridade de imagens médicas , enquanto o FSCoMS reduz a redundância das características para obter um vetor de características menor do que os métodos de seleção de características convencionais e ainda com um maior desempenho em recuperação de 
 A tecnologia atual permite armazenar grandes quantidades de dados , no entanto sua exploração e compreensão resultam em um enorme desafio devido não só ao tamanho dos conjuntos produzidos mas também sua complexidade . Nesse sentido a visualização de informação vem se mostrando um recurso extremamente poderoso para ajudar a interpretar e extrair informação útil desse universo de dados . Dentre as abordagens existentes , as tecnicas de projeção_multidimensional  estão emergindo como um instrumento de visualização importante em aplicações que implicam a análise visual de dados de alta dimensão devido ao poder analítico que essas oferecem na exploração de relações de similaridade e correlação de dados abstratos . Contudo , os resultados obtidos por tais técnicas estão intimamente ligados à qualidade do espaço de características que descrevem os dados sendo processados . Se o espaço for bem formado e refletir as relações de similaridade esperadas por um usuário , os resultados nais serão satisfatórios . Caso contrário pouca utilidade terão as representações_visuais  geradas . Neste projeto de mestrado técnicas de projeção_multidimensional  são empregadas , para , não somente explorar conjuntos de dados multidimensionais , mas também para servir como um guia em um processo que visa `` moldar '' espaços de características . A abordagem proposta se baseia na combinação de projeções de amostras e mapeamentos locais , permitindo ao usuário de forma interativa transformar os atributos dos dados por meio da modicação dessas projeções . Mais especicamente , as novas relações de similaridade criadas pelo usuário na manipulação das projeções das amostras são propagadas para o espaço de característica que descreve os dados , transformando-o em um novo espaço que reflita essas relações , ou seja , o ponto de vista do usuário sobre as semelhanças e diferenças presentes nos dados . Resultados experimentais_mostram  que a abordagem desenvolvida nesse projeto pode com sucesso transformar espaços de características com base na manipulação da projeção de pequenas amostras , melhorando a coesão e separação de grupos . Com base no ferramental criado , um sistema de recuperação de imagens por conteúdo e sugerido , mostrando que a abordagem desenvolvida pode ser bastante útil nesse tipo de 
 O Problema dos K Menores Caminhos é uma generalização do Problema do Menor Caminho , em que desejamos encontrar os K caminhos de menor_custo  entre dois vértices de um grafo . Estudamos e implementamos algoritmos que resolvem esse problema em grafos dirigidos , com peso nos arcos e que permitem apenas caminhos sem repetição de vértices na resposta . Comparamos seus desempenhos utilizando grafos do 9th DIMACS Implementation Challenge . Identificamos os pontos fortes e fracos de cada algoritmo , e propusemos uma variante híbrida dos algoritmos de Feng e de Pascoal . Essa variante proposta obteve desempenho_superior  aos algoritmos base em alguns grafos , e resultado superior a pelo menos um deles na grande maioria dos testes 
 Esta dissertação de mestrado aborda o problema de balanceamento de carga em ambientes paralelos virtuais , com aplicações desenvolvidas em PVM-W95 . São considerados no desenvolvimento deste trabalho algoritmos de balanceamento estáticos e dinâmicos , incluindo métodos de distribuição e de migração de tarefas . O trabalho apresenta uma vasta revisão bibliográfica que cobre os principais métodos e algoritmos de balanceamento de cargas em ambientes distribuídos e paralelos , propostos nos últimos_anos  . Os métodos e algoritmos relevantes para o domínio de aplicação considerado são cuidadosamente investigados . Os algoritmos estudados são implementados como parte dos programas de aplicação e o estudo desenvolvido abrange fundamentalmente programas com alto processamento numérico . Os resultados obtidos_mostram  claramente que o método gradiente e os algoritmos globais são os mais eficientes para os programas aplicativos desenvolvidos e para o ambiente paralelo virtual adotado 
 Este trabalho discute o potencial e , principalmente , os problemas decorrentes do uso de sistemas hipermídia tradicionais ( i.é redes de páginas manipuladas por browser ) em aplicações de ensino & aprendizagem . Argumenta-se que um dos grandes problemas dos sistemas hipermídia consiste na usual falta de qualquer controle sobre a navegação do usuário , que pode refletir negativamente no processo de aprendizagem . O trâbalho propõe , e implementa no protótipo SASHE , o uso de recursos extremamente simples , como a associação de atributos aos nós do hiperdocumento , bem como a exploração da característica de aninhamento ( ou composição ) de nós , já usual nos modelos conceituais de hipermídia , para modelar aplicações_hipermídia  que atendam a algumas das necessidades tanto do autor quanto do estudante . Tais contribuições consistem em : a ) oferecer mais recursos ao usuário-estudante , além das funções de controle ; isto se dá através de um conjunto botões da interface para o acesso a informações contextuais , de controle e estratégicas ( busca de nós alternativos de acordo com dificuldade associada e nó atual ) ; b ) oferecer mais recursos ao usuário-autor , no sentido de fornecer-lhe ferramentas de autoria de hiperdocumentos `` qualificados '' para aplicações no ensino ( independente do domínio de conhecimento ) , bem como de possibilitar a criação de roteiros que imprimem uma forma de controle `` flexível '' : ao selecionar cada nó que fará pafte de um roteiro , o autor pode definir um `` grau de liberdade '' associado , correspondente a um contexto da hierarquia que engloba aquele nó . O autor , ao definir os limites de `` fuga '' do roteiro , indica quais informações periféricas podem contribuir à leitura do estudante , sem interferirem negativamente nos objetivos do autor 
 Nesta dissertação de mesûado , exploramos os modelos estatísticos de confiabilidade de softrvare que utilizam os processos de Poisson homogêneo e não homogêneo para modelagem dos dados de falhas . Para modelar os instantes de falhas , escolhemos a classe de modelos de estatísticas de ordem proposta por Yang ( 1994 ) . Propomos uma extensão desses modelos , considerando a distribuição gaussiana inversa , para modelar a função de valor médio dos processos de Poisson não homogêneo . Os métodos considerados para fazet inferências pam os parâmetros de interesse são os métodos Bayesianos . Exploramos , ainda , o uso de algorinnos de Metropolis com etapas Gibbs para desenvolver a inferência Bayesiana . Tendo em vista a verificação das suposições dos modelos de estatísticas de ordem , desenvolvemos e incorporamos algumas técnicas Bayesianas de diagnóstico . Baseamos a seleção de modelos nos valores de predição ordenados . A metodologia desenvolvida neste trabalho é exemplificada com conjuntos de dados introduzidos por Jelinski e Moranda ( 1972 ) e Goel ( 1985 ) 
 Este trabalho apresenta um método_numérico  para resolver escoamentos axisimétricos não-Newtonianos com superfícies_livres  . A metodologia empregada é uma extensão do código bidimensional GENSMAC para problemas axisimétricos . O código GENSMAC é uma técnica numérica que utiliza o método das particulas marcadoras ( `` marker-and-cell '' ) para simular_escoamentos  incompressíveis transientes . As equações governantes são resolvidas usando o método de diferenças_finitas  numa malha diferenciada ( `` staggered grid '' ) . O fluido é representado por partículas marcadoras , as quais permitem a localização e visualização da superfície_livre  do fluido . Vários exemplos que demonstram a aplicação dessa nova_técnica  são apresentados . Em particular , a simulação de enchimento de moldes , do `` die-swell '' e do `` splashing drop '' são apresentados 
 Nesta dissertação de mestrado , apresentamos uma abordagem clássica e Bayesiana para o modelo de Captura-Recaptura proposto por Nayak ( 1988 ) para obter informação sobre o número de falhas , N , em um sistema de confiabitidade . A probabilidade de corrigir um elro , p , é assumida ser conhecida e desconhecida . Mostramos que o e.m.v . de N depende não somente das freqüências dos erros detectados , como também dos tempos entre falhas . A 'sensibilidade ' da distribuição a posteriori de N com respeito a p e a influência dos tempos entre falhas , são considerados através da distância da variação total e divergência de Kullback-Leibler . Também mostramos que a correção por recaptura é uma condição necessária para a existência da distribuição a posteriori de N quando utilizamos uma priori imprópria 
 Este trabalho estuda implicações do tempo na modelagem de dados e o tratamento a ele dispensado nos Modelos de Dados construídos para este fim . Busca-se soluções à representação do tempo em Sistemas de Informações apoiados em BD relacionais . Para este fim foram pesquisados conceitos de tempo , modelos de dados temporais e outras soluções propostas na literatura . A base desta pesquisa é resolver situações práticas de um projeto de Sistema de Informações ( estudo de caso em uma Empresa de Telefonia ) onde o tratamento de tempo é imprescindível . são propostas soluções que incorporam o tratamento de tempo no Modelo Entidade-Relacionamento Estendido , as quais podem ser generalizadas para atender problemas com características semelhantes 
 Uma placa de dimensões ( L , W ) deve ser cortada para produzir m peças de dimensões ( li , wi ) , às quais estão associados valores de utilidade vi e um limite máximo bi ( problema restrito ) , i =1 , ... , m. Os cortes são do tipo guilhotinado e limitados a 2-estágios . O problema consiste em maximizar o valor de utilidade total . Neste trabalho estendemos a abordagem clássica de Glmore e Gomory para problemas irrestritos , onde apresentamos um modelo matemático de otimização inteira não-linear , e propomos métodos de solução baseados na relaxação lagrangeana e heurísticas . Resultados computacionais são apresentados ao final 
 Os sistemas de banco de dados têm sofrido um processo de constante evolução , e ultimamente , diversas novas frentes têm se aberto . Uma frente importante é a da orientação a objetos , com a criação dos SGBD00- Sistemas Gerenciadores de Banco de Dados Orientado a Objetos . Dentre os vários SGBDOO existentes temos : 02 , GemStone , Object Store , MRO-Modelo de Representação de Objetos , entre outros . Este trabalho apresenta um estudo de diversos sistemas . Em cada um deles foi analisado interfaces , controle de versões , gerenciamento de armazenamento , linguagem de manipulação de esquemas , arquitetura do sistema , entre outros.Além deste estudo foram analisadas características de áudio . De acordo com a acústica - uma parte da fisica - o som é o efeito produzido no aparelho auditivo pelas vibrações das moléculas de um meio transmissor . Este meio transmissor normalmente é o ar , mas também existem outros meios possíveis [ Gome931.Foram verificadas as várias formas de gravação de áudio , entre elas a digitalização e a síntese . No processo de gravação foram analisados alguns formatos de arquivo para armazenamento , permitindo assim o balizamento da escolha . Dentre os formatos estudados estão : MOD , WAV , SBI e SBK , SNG e MIDI.Com base nestes estudos realizados , este trabalho apresenta uma avaliação das técnicas para a armazenagem e recuperação de áudio em bases de dados . O modelo de dados que serve de suporte para essa avaliação é o MIRO sendo que o GEO é a implementação do MIRO . Foi feito o tratamento de som em nível de esquema para o modelo e em nível de interface para o GEO . O tratamento de som permite o armazenamento e a recuperação de músicas , usando o formato MIDI 
 Avaliação de processo é a primeira etapa para a melhoria de qualidade de processo de software . Através da melhoria de processo , é possível obter-se um produto de software com maior qualidade , atualmente um fator de diferenciação entre as empresas que desenvolvem software . Este trabalho apresenta um modelo de avaliação de processo de software ( MAPPE ) , apropriado à empresas de pequeno_porte  , visto que os modelos de avaliação e melhoria mais utilizados ( CMM , SPICE e ISO 9000-3 ) , devido à complexidade , são inadequados para essas empresas . para a elaboração do modelo , utilizou-se parcialmente a estrutura do modelo SplCE , que usa para a avaliação , um modelo padrão de processo de software , que no caso deste trabalho é um modelo adequado à pequena empresa . O Modelo de Avaliação de Processo de Software para pequena Empresa - MAPPE , proposto neste trabalho , pode mostrar as deficiências dos processos . de software atuais das empresas , indicando atividades que precisam ser melhoradas e possibilitando assim a obtenção de produtos de software com maior qualidade 
 Métodos para extração de características têm como objetivo selecionar , a partir de um conjunto de dados , características que representem informações relevantes ou que sejam básicas para diferenciar uma classe de objetos de outras . Neste trabalho , são apresentadas duas metodologias que podem ser usadas para extração de características . A primeira utiliza métodos estatísticos clássicos como Análise de Componentes Principais ( PCA ) , Análise Discriminante Linear ( LDA ) e Análise de Cluster . A segunda consiste na utilização de arquiteturas de Redes_Neurais  Artificiais ( RNA ) que implementam os mesmos métodos estatísticos . O desempenho dos modelos de RNA apresentados são avaliados , considerando-se a utilização destes na extração de características de um pequeno conjunto de dados e , para investigar a aplicabilidade desses modelos na área de processamento de imagens , uma das redes que implementa PCA é utilizada na tarefa de compressão de algumas imagens médicas . Os resultados obtidos pela rede PCA são comparados com outros provenientes da aplicação da análise PCA clássica e do padrão JPEG ( Joint Photographic Experts Group ) para o mesmo conjunto de imagens 
 Este trabalho apresenta um método de engenharia_reversa  , denominado FUSIONRE/ I ( Fusion - Reverse Engineering / Interface ) , para auxiliar a atividade de manutenção de software . O método é baseado nos conceitos e idéias do método FUSION-RE de engenharia_reversa  orientada a objetos . O método FUSION-RE/I fornece mecanismos para abstrair Visões Funcionais ( Modelos de Análise de Sistemas de acordo com o Método Fusion ) e Visões Estruturais ( Quadro de Operações-Procedimentos de Implementação ) a partir de aspectos operacionais e de dados disponíveis via interface usuário-computador . O método FUSION-RE/I foi desenvolvido a partir da experiência da aplicação de engenharia_reversa  na ferramenta Proteum V1.1 C , uma ferramenta de teste desenvolvida no ICMSC-USP , que apoia o critério_Análise  de Mutantes para programas escritos na linguagem C 
 Gerenciamento de configuração de software é uma das primeiras exigências para se ter um processo de software com qualidade . Entretanto , essa atividade é difícil de ser implantada , principalmente em empresas de pequeno_porte  . Este trabalho propõe um plano de implantação de gerenciamento de configuração de software destinado a essas empresas . Para isso foi realizada uma pesquisa empírica --_-  utilizando o método Goal/ Question/ Metric ( GQM ) para identificar como as empresas de pequeno de porte realizam gerenciamento de configuração . O resultado mostrou que elas não realizam gerenciamento de configuração e , por isso , as atividades contidas no padrão IEEE Std 828-1990 foram consideradas como base para o plano proposto . O plano detalha os procedimentos técnicos e administrativos necessários para ( a ) a execução das atividades de identificação da configuração , ( b ) relato de situação e ( c ) controles de versões e mudanças . O controle de versões é apoiado pela ferramenta de controle de versões de baixo custo Quma Version Control System ( QVCS ) --_-  uma exigência necessária para a implantação do plano em empresas de pequeno_porte  . Esse plano permite que essas empresas dêem o primeiro passo na direção da melhoria de qualidade de processos de software 
 Nesse trabalho , é projetada e implementada uma modificação do módulo de acesso à rede do conjunto de protocolos TCP/IP para , transparentemente às aplicações , aumentar a largura de banda de redes locais típicas , como ethernet , através do uso de múltiplos dutos . Um estudo detalhado dos serviços disponíveis pelos vários protocolos constituintes da tecnologia internet e das interfaces de acesso de programa é feita . Uma avaliação da pilha de protocolos é descrita , mostrando qual a melhor posição para se fazer a divisão do fluxo de dados . A modificação ao nível do driver , mostrou-se a mais conveniente , provendo um modo simples de implementação e instalação . A escolha do duto a ser utilizado por um determinado pacote é feita pela avaliação do número de porta que aparece no header TCP ou UDP . Alguns testes foram realizados de modo a demonstrar a eficiência do sistema 
 Com o crescente aumento na área de Aplicações Multimídia e Redes de Computadores , devido ao grande progresso nas industrias de hardware e soþ , vare , tornou-se possível a utilização de áudio e vídeo como mecanismo de comunicação em redes de computadores . Esses recursos contribuíram para uma maior consistência e um melhor aproveitamento na assimilação da mensagem a ser transmitida . Este trabalho , apresenta a implementação de um Sistema de Videoconferência utilizando os algoritmos de compressão ADPCM ( para áudio ) e H26l ( para vídeo ) . O sistema de videoconferência foi implementado e testado sobre redes TCP/IP . Os resultados são apresentados e discutidos no fim 
 O avanço tecnológico em muito tem dificultado a atualização das ferramentas de gerenciamento de redes , principalmente por não existirem propostas concretas para uma arquitetura operacional . Este trabalho tem o propósito de suprir tais deficiências , apresentando uma avançada arquitetura operacional para sistemas de gerenciamento de redes , que possui como características a facilidade e flexibilidade de incorporação de novas tecnologias e adequação a diferentes plataformas . Os conceitos apresentados foram aplicados ao protótipo da nova ferramenta de gerenciamento de redes MultView , estendendo suas capacidades operacionais . Para tal foi dada especial atenção às tecnologias que apresentam-se como tendência , tais como Java , HTTP , CORBA , SSL , SNNIP , entre outras . O protótipo foi projetado para ser o núcleo de um sistema de gerenciamento de redes com recursos de adição , remoção e controle de módulos de aplicação em tempo real . Para tal foi definida uma interface de aplicação para ser utilizada como base na criação dos módulos a serem integrados ao sistema . A gestão das atividades de controle operacional é feita através de uma interface própria , codificada em HTML , e que pode ser acessada local ou remotamente com ajuda de um navegador Web qualquer . A ferramenta foi inteiramente codificada em linguagem Java , utilizando-se o pacote de desenvolvimento de domínio público JDK 1.0.2 , nas plataformas Solaris e Windows NT 
 É crescente o número de sistemas de software complexos e críticos . As técnicas de melhoria e suporte ao processo de software tem sido reconhecidas como importantes meios de aumentar a qualidade de software , bem como de reduzir os custos e esforços de desenvolvimento . Neste contexto , um assistente inteligente pode apoiar a execução de processos de software baseado nas técnicas da inteligência_artificial  . Este trabalho apresenta uma aplicação das técnicas de planejamento para apoiar o processo de desenvolvimento de software baseado no método FUSION . O trabalho mostra a aplicação das técnicas de planejamento em processos práticos e de escala real em contraste aos pequenos exemplos normalmente encontrados na literatura . A abordagem proposta consiste de uma biblioteca de operadores mais dois algoritmos para reconhecer e montar planos . A maioria dos operadores foram refinados até o terceiro ou quarto nível de modo a encontrar um nível de descrição adequado para os desenvolvedores . Um protótipo de um assistente inteligente foi desenvolvido e simulado com o processo de desenvolver a aplicação clássica ATM . Os resultados enfatizam as vantagens de formalizar ações , heurísticas e regras de consistência em métodos de desenvolvimento de software . A formalização desses aspectos aumenta a produtividade de desenvolvedores inexperientes pois eles podem reutilizar conhecimentos que estão disponíveis apenas nas mentes de desenvolvedores experientes . Além_disso  , vantagens da utilização da abordagem declarativa na representação de processos reais são indicadas . Essas vantagens incluem a flexibilidade na definição e modificação da ordem dos processos . As dificuldades na definição da biblioteca de operadores são apontadas 
 O suporte a Atributos Multivalorados em Sistemas de Bases de Dados Orientados a Objetos é um tema estudado nos diversos modelos existentes atualmente . A necessidade de um tratamento especial para este tipo de atributos foi o objeto de estudo deste trabalho . A forma rígida como os diversos modelos de dados orientados a objetos oferecem suporte aos atributos multivalorados exige que as aplicações assumam o trabalho de compatibilizar as suas necessidades com a forma como são suportados estes atributos . No decorrer deste trabalho é feita uma anáLlise de como os diversos modelos existentes dão suporte a estes atributos e é apresentada uma nova maneira de suportáJos . Assim permite-se a criação de um novo tipo de atributo , o de estrutura , para que um conjunto de valores possa ser estruturado de acordo com a necessidade da aplicação , sem que o seja feito de forma invariável pelo gerenciador . Esse novo tipo de atributo , o de estrutura , permite , por exemplo , que um mesmo conjunto de valores tenha estruturas diferentes de apresentação . Esse trabalho abrange , de forma mais aprofundada , a aplicação deste conceito em modelos de dados orientados a objetos como o Modelo de Representação de Objetos e o SIRruS 
 Raciocínio_Baseado  em Casos ( RBC ) é um paradigma de Inteligência_Artificial  ( IA ) que , em essência , busca utilizar uma experiência prévia para entender e resolver um problema novo . O objetivo central deste trabalho é apresentar em detalhes esse paradigma de IA , analisar seus principais temas e propor uma metodologia para o desenvolvimento de um sistema de RBC . Além_disso  , a metodologia_proposta  é avaliada através da implementação dos principais algoritmos relacionados à : representação de conhecimento utilizando casos , indexação e armazenamento de casos , métricas de similaridade para recuperação e adaptação de casos 
 O crescente aumento na utilização da tecnologia de redes de computadores tem provocado uma demanda por ferramentas que auxiliem o seu gerenciamento . Paralelamente , os progressos nas indústrias de hardware e software estão dando origem a uma nova linhagem de aplicações que fazem_uso  de diversos meios , como áudio , imagens estáticas , vídeo , gráficos e texto para facilitar a interação entre usuários . Este trabalho apresenta uma experiência de introduzir funcionalidades multimidia em um Sistema de Gerenciamento de Redes , através do desenvolvimento de um sistema de captura , transmissão e apresentação de áudio e vídeo , utilizado para auxiliar a comunicação entre administradores e usuários de uma rede . O Sistema de Áudio e Vídeo implementado foi incorporado ao MultView ( o Sistema de Gerenciamento de Redes em desenvolvimento no ICMSC-USP ) e tem mostrado o ganho em qualidade que sistemas como este acrescentam às aplicações onde são empregados 
 Um processo para o desenvolvimento de frameworks para sistemas de informação baseados na Web é proposto . Esse processo é composto pelos subprocessos de engenharia_reversa  de sistemas baseados na Web , de criação de uma linguagem de padrões e de construção e instanciação do framework . O subprocesso de engenharia_reversa  utiliza sistemas presentes na Web para derivar um modelo do domínio de aplicação . O desenvolvimento da linguagem de padrões é baseado no modelo do domínio e a construção do framework utiliza essa linguagem de padrões como base de todo o processo . Os produtos resultantes do uso desse processo para o domínio dos leilões virtuais , a Linguagem de Padrões LV e o Framework Qd+ , também são apresentados 
 Esta dissertação de mestrado apresenta a implementação e validação de mecanismos de negociação no módulo de controle de admissão de uma arquitetura de servidor web com diferenciação de serviços - SWDS . Foram propostos dois algoritmos , um deles denominado algoritmo de negociação forçada e o outro algoritmo de negociação com a participação do cliente , ambos detalhados ao longo deste trabalho . Verificou-se que a técnica de negociação melhorou o atendimento aos clientes pertencentes a uma determinada classe de serviço para que esses tivessem uma qualidade de serviço garantida . Também foi objeto de estudo a criação de uma métrica envolvendo parâmetros relativos à QoS em servidores_web  . Os resultados alcançados com desenvolvimento deste trabalho sinalizam para uma melhoria em termos de qualidade de serviços sendo , portanto , mais uma técnica que contribui para a construção de protótipos de servidores_web  com diferenciação de serviços num futuro não muito distante 
 A Programação Orientada a Aspectos ( POA ) é uma técnica de desenvolvimento que apoia a separação de interesses_transversais  . Na POA , adendos são aplicados a pontos de junção do sistema por meio de uma construção especial chamada descritor de conjuntos de junção ( ou simplesmente conjunto de junção ) . Esse mecanismo apoia a modularização de comportamentos transversais , entretanto , como as interações adicionadas não ficam explícitas no código-fonte , é difícil assegurar que estão corretas . Para lidar com esse problema , nesta tese é proposta uma abordagem rigorosa de teste_estrutural  de integração para programas orientados a aspectos . É definido um modelo de fluxo de controle e de dados baseado no bytecode Java chamado Grafo Def-Uso baseado em conjuntos de junção ( ou PointCut-based Def-Use graph , PCDU ) que modela as regiões de execução de um programa escrito em AspectJ que são afetadas por um conjunto de junção . Sobre este modelo são definidos três critérios de teste : todos-nós-baseados-em-conjunto-de-junção , todas-arestas-baseadas-em-conjunto-de-junção e todos-usos-baseados-em-conjunto-de-junção , que requerem a cobertura de todos os comandos , condicionais e pares def-uso no contexto de cada ponto de junção selecionado . Para automatizar o uso do modelo e critérios propostos , é implementada uma ferramenta chamada JaBUTi/PC-AJ . Além_disso  , para validar a abordagem proposta , são conduzidos estudos teóricos e experimentais que procuram avaliar os critérios tanto do ponto de vista do custo de aplicação quanto do ponto de vista da eficácia em encontrar defeitos . Os estudos oferecem indícios da aplicabilidade e da eficácia dos critérios para encontrar defeitos diretamente relacionados com a 
 Apesar de as EDPS que modelam leis de conservação e problemas em dinâmica dos fluídos serem bem estabelecidas , suas soluções numéricas continuam ainda desafiadoras . Em particular , há dois desafios associados à computação e ao entendimento desses problemas : um deles é a formação de descontinuidades ( choques ) e o outro é o fenômeno turbulência . Ambos os desafios podem ser atribuídos ao tratamento dos termos advectivos não lineares nessas equações de transporte . Dentro deste canário , esta tese apresenta o estudo do desenvolvimento de um novo esquema `` upwind '' de alta_resolução  e sua associação com modelagem da turbulência . O desempenho do esquema é investigado nas soluções da equação de advecção 1D com dados iniciais descontínuos e de problemas de Riemann 1D para as equações de Burgers , Euler e águas rasas . Além_disso  , são apresentados resultados numéricos de escoamentos_incompressíveis  2D e 3D no regime laminar a altos números de Reynolds . O novo esquema é então associado à modelagem 'capa ' - 'epsilon ' da turbulência para a simulação_numérica  de escoamentos_incompressíveis  turbulentos 2D e 3D com superfícies_livres  móveis . Aplicação , verificação e validação dos métodos_numéricos  são também 
 O Problema da Mochila Compartimentada é uma extensão do Problema da Mochila , em que os itens solicitados são divididos em classes , de modo que a mochila deve ser subdividida em compartimentos , os quais têm capacidades limitadas e são carregados com itens da mesma classe . Além_disso  , a construção de um compartimento tem um custo fixo e ocasiona uma perda no espaço da mochila . O objetivo consiste em maximizar a soma dos valores dos itens , descontado o custo fixo de inclusão de compartimentos . Neste trabalho , são abordados dois métodos de solução . A primeira abordagem é uma heurística , que consiste na combinação de duas heurísticas da literatura . A segunda abordagem é o método Geração de Colunas , que além de fornecer um novo limitante superior para o Problema da Mochila Compartimentada , ao final do método o problema mestre foi resolvido com as variáveis definidas como inteiras , obtendo uma solução factível . Em ambos os métodos , o modelo não-linear é decomposto em dois modelos lineares , no qual , um gera compartimentos e o outro os seleciona . Os resultados obtidos com as duas_abordagens  foram comparados com um limitante superior e se mostraram bastante 
 As proteínas são moléculas presentes nos seres vivos e essenciais para a vida deles . Para entender a função de uma proteína , devese conhecer sua estrutura tridimensional ( o posicionamento correto de todos os seus átomos no espaço ) . A partir da estrutura de uma proteína vital de um organismo causador de uma doença é possível desenvolver fármacos para o tratamento da doença . Para encontrar a estrutura de uma proteína , métodos biofísicos , como Cristalografia de Raio-X e Ressonância Nuclear Magnética têm sido empregados . No entanto , o uso desses métodos tem restrições práticas que impedem a determinação de várias estruturas de proteínas . Para contornar essas limitações , métodos computacionais para o problema de predição da estrutura da proteína ( PSP , Protein Structure Prediction ) têm sido investigados . Várias classes de métodos computacionais têm sido desenvolvidas para o problema de PSP . Entre elas , as abordagens ab_initio  são muito importantes , pois não utilizam nenhuma informação prévia de outras estruturas de proteínas para fazer o PSP , apenas a sequência de aminoácidos da proteína e o gráfico de Ramachandran são empregados . O PSP ab_initio  é um problema combinatorial que envolve relativamente grandes instâncias na prática , por exemplo , as proteínas em geral têm centenas ou milhares de variáveis para determinar . Para vencer esse entrave , metaheurísticas como os Algoritmos Genéticos ( AGs ) têm sido investigados . As soluções geradas por um AG são avaliadas pelo cálculo da energia potencial da proteína . Entre elas , o cálculo da interação da energia de van der Waals é custoso computacionalmente tornando o processo evolutivo do AG muito lento mesmo para proteínas pequenas . Este trabalho_investiga  técnicas para reduzir significativamente o tempo de execução desse cálculo . Basicamente , foram propostas modificações de técnicas de paralelização utilizando MPI e OpenMP para os algoritmos resultantes . Os resultados mostram que o cálculo pode ser 1.500 vezes mais rápido para proteínas gigantes quando aplicadas as técnicas investigadas neste 
 O filtro de Kalman é amplamente conhecido e utilizado em aplicações , em virtude de apresentar diversas propriedades interessantes . Este trabalho aborda uma das características mais importantes , a estabilidade do filtro de Kalman aplicado a sistemas_lineares  discretos com saltos Markovianos . Sistemas desta classe são muito empregados em problemas práticos . Neste trabalho mostramos que o conceito de controlabilidade fraca e detetabilidade estocástica são condições suficientes para estabilidade do filtro de Kalman com relação a condição inicial . No que se refere a estabilidade no sentido mais usual , apresentamos resultados parciais , dependentes de uma condição adicional sobre a cadeia de Markov , bem como uma conjectura . O estudo da estabilidade do filtro de Kalman é relevante , pois filtros instáveis oferecem estimativas de baixa qualidade . O tema tem interesse teórico inerente e é bastante relevante para aplicações.O filtro de Kalman é amplamente conhecido e utilizado em aplicações , em virtude de apresentar diversas propriedades interessantes . Este trabalho aborda uma das características mais importantes , a estabilidade do filtro de Kalman aplicado a sistemas_lineares  discretos com saltos Markovianos . Sistemas desta classe são muito empregados em problemas práticos . Neste trabalho mostramos que o conceito de controlabilidade fraca e detetabilidade estocástica são condições suficientes para estabilidade do filtro de Kalman com relação a condição inicial . No que se refere a estabilidade no sentido mais usual , apresentamos resultados parciais , dependentes de uma condição adicional sobre a cadeia de Markov , bem como uma conjectura . O estudo da estabilidade do filtro de Kalman é relevante , pois filtros instáveis oferecem estimativas de baixa qualidade . O tema tem interesse teórico inerente e é bastante relevante para 
 Este trabalho estuda a observabilidade e controlabilidade para uma classe de sistema dinâmico markoviano com saltos nos parâmetros , e uma coleção de matrizes de observabilidade e controlabilidade associadas . São explorados alguns resultados de invariância , bem como certas propriedades envolvendo essas matrizes . Uma dessas propriedades , relacionada com a coleção de matrizes de observabilidade é conhecida na literatura desta classe de sistemas , mas não há uma prova disponível . Esses resultados de invariancia foram estendidos para o estudo da controlabilidade e sua respectiva coleção de matrizes associada , obtendo assim uma propriedade análoga ao caso da observabilidade . Os resultados obtidos são importantes para validar outros resultados existentes que se baseiam na propriedade 
 A automação de tarefas como descoberta , composição e invocação de Serviços_Web  é um requisito importante para o sucesso da Web Semântica . Nos casos de insucesso na busca por um serviço , por não existir disponível um serviço completo que atenda plenamente a requisição do usuário , uma possibilidade de contorno é compor o serviço procurado a partir de elementos básicos que atendam parcialmente a requisição inicial e que se completem . A composição de Serviços_Web  pode ser realizada de forma_manual  ou de forma automática . Na composição manual , o desenvolvedor de Serviços_Web  pode tirar proveito da sua expertise sobre os serviços envolvidos na composição e sobre o resultado que se deseja alcançar . Esta tese aborda problemas e apresenta contribuições relacionadas ao processo de composição automática de Serviços_Web  . A composição automática de Serviços_Web  requer que os serviços sejam descritos e publicados de forma a modelar o conhecimento ( semântica explícita ) que o desenvolvedor utiliza para realizar a composição manual . A descoberta automática baseada nas descrições semânticas do serviço é também um passo crucial na direção da composição automática , pois é um estágio anterior necessário para a seleção dos serviços candidatos à composição . Trabalhos da área de pesquisa em Serviços_Web  Semânticos exploram a utilização dos padrões da Web Semântica para enriquecer , com semântica explícita , a descrição dos Serviços_Web  . O problema da composição automática de Serviços_Web  é tratado neste trabalho por meio de três linhas de investigação : modelagem dos Serviços_Web  Semânticos ; descoberta automática de Serviços_Web  Semânticos ; e composição automática de Serviços_Web  Semânticos . As contribuições desta tese incluem : a plataforma RALOWS para modelagem de aplicações Web como Serviços_Web  Semânticos , tendo como estudo de caso aplicações para realização de experimentos remotos ; um algoritmo para descoberta automática de Serviços_Web  Semânticos ; uma proposta baseada em grafos e caminhos de custo mínimo para prover composição automática de Serviços_Web  Semânticos ; uma infra-estrutura e ferramentas de apoio à descrição , publicação , descoberta e composição de Serviços_Web  
 A popularização de aplicativos e dispositivos capazes de produzir , exibir e editar conteúdos multimídia fez surgir a necessidade de se adaptar , modificar e customizar diferentes_tipos  de mídia a diferentes necessidades do usuário . Nesse contexto , a área de Personalização e Adaptação de Conteúdo busca desenvolver soluções que atendam a tais necessidades . Sistemas de personalização , em geral , necessitam conhecer os dados presentes na mídia , surgindo , assim , a necessidade de uma indexação do conteúdo presente na mídia . No caso de vídeo digital , os esforços para a indexação automática utilizam como passo inicial a segmentação de vídeos em unidades de informação menores , como tomadas e cenas . A segmentação em cenas , em especial , é um desafio para pesquisadores graças a enorme variedade entre os vídeos e a própria ausência de um consenso na definição de cena . Diversas técnicas diferentes para a segmentação em cenas são reportadas na literatura . Uma técnica , em particular , destaca-se pelo baixo custo_computacional  : a técnica baseada em coerências visual . Utilizando-se operações de histogramas , a técnica objetiva-se a comparar tomadas adjacentes em busca de similaridades que poderiam indicar a presença de uma cena . Para melhorar os resultados obtidos , autores de trabalhos com tal enfoque utilizam-se de outras características , capazes de medir a `` quantidade de movimento '' das cenas , como os vetores de movimento . Assim , este trabalho apresenta uma técnica de segmentação de vídeo digital em tomadas e em cenas através da coerência visual e do fluxo óptico . Apresenta-se , ainda , uma série de avaliações de eficácia e de desempenho da técnica ao segmentar em tomadas e em cenas uma base de vídeo do domínio 
 A avaliação do estado nutricional das plantas de milho usualmente é feita através de análises químicas ou pela diagnose visual das folhas da planta , esta última , sujeita a erros de interpretação já que a ausência de algum nutriente na planta gera um padrão de mudança específico na superfície da folha que depende do nível de ausência do nutriente . As dificuldades que apresentam neste processo e sua importância na agricultura , criam a necessidade de pesquisar sistemas automáticos para a avaliação do estado nutricional de plantas . Desta forma , este mestrado teve como objetivo principal o desenvolvimento de um sistema de visão artificial para verificar a possibilidade de identificação de níveis dos macronutrientes Cálcio , Enxofre , Magnésio , Nitrogênio e Potássio em plantas de milho através da análise da superfície das folhas usando métodos de visão_computacional  . Este projeto realiza uma revisão bibliográfica do estado da arte dos métodos de extração de características de cor , textura em escala de cinza e textura colorida utilizadas em processamento de imagens . A alta similaridade entre os sintomas produzidos pelas deficiências e a pouca similaridade entre amostras de uma mesma deficiência motivou o desenvolvimento de novos métodos de extração de características que pudessem fornecer dados necessários para uma correta separação entre as classes . Os resultados obtidos demonstraram que o sistema desenvolvido possibilita a predição de deficiências nutricionais em estágios iniciais do crescimento da planta usando unicamente a textura da superfície da folha como fonte de 
 A Arquitetura orientada a serviço ( SOA ) é um estilo arquitetural para estruturar sistemas de software de modo que exista um baixo grau de acoplamento entre as aplicações e essas possam ser facilmente integradas de forma dinâmica . A incorporação de SOA e serviços_Web  em sistemas que modelam processos de negócios grandes e complexos contribui para a necessidade de testes mais formais e sistemáticos . Além_disso  , características próprias dessa nova classe de software fazem com que técnicas de teste tradicionais não possam ser diretamente aplicadas . O teste baseado em modelo ( TBM ) apresenta-se como uma abordagem promissora que busca a resolução desses problemas . Esta tese investiga como duas técnicas de modelagem , Máquina de Estados_Finitos  ( MEF ) e Grafo de Sequência de Eventos ( GSE ) , podem ser utilizadas para apoiar o TBM de aplicações orientadas a serviço . Essas técnicas modelam diferentes aspectos e podem ser aplicadas de forma complementar . Inicialmente , é definido um processo de TBM para aplicações orientadas a serviço que emprega MEFs . Com base na experiência adquirida , é proposta uma abordagem baseada em modelo para o teste de serviços compostos usando GSEs . Essa abordagem é holística uma vez que conjuntos de teste são gerados para cobrir tanto situações desejadas ( teste positivo ) quanto comportamentos inesperados ( teste negativo ) . Três estudos_experimentais  avaliam a abordagem proposta : ( i ) um estudo de caso , ( ii ) uma análise de custo e ( ii ) um estudo na indústria . Ferramentas de teste também são apresentadas para apoiar o uso prático da abordagem 
 O objetivo deste trabalho é apresentar uma técnica automática baseada em morfologia matemática para medida de sinal em imagens de cDNA desenvolvida no BIOINFO , em parceria com o Instituto Ludwig de Pesquisa contra o Câncer . A tecnologia de lâminas de cDNA é um processo baseado em hibridização que possibilita observar a concentração relativa de mRNA de amostras de tecidos analisando a luminosidade de sinais fluorescentes ou radioativos . Hibridização é o processo bioquímico onde duas fitas de ácido nucleico com seqüências complementares se combinam . A técnica apresentada permite o cálculo da expressão_gênica  com alto grau de automação , podendo o usuário corrigir com facilidade eventuais erros de segmentação . O usuário interage com o programa apenas para selecionar as imagens e inserir os dados de geometria da lâmina . A estratégia de solução usada tem três fases : gradeamento dos blocos , gradeamento dos spots e segmentação dos spots . Todas as fases utilizam filtros morfológicos e as fases de gradeamento possuem um passo final de correção baseado nos dados de geometria da lâmina o que aumenta a robustez do processo , que funciona bem mesmo em imagens ruidosas 
 Resultado da revolução tecnológica das últimas_décadas  , vários ecossistemas de startups de software surgiram ao redor do globo . Acelerados pela Internet , pela onipresença dos dispositivos_móveis  e pela abundância de serviços de nuvem , empresas de software com modelos de negócio escalável , conhecidas como startups , se tornaram o assunto da moda . Com empreendedores de tecnologia como seus principais agentes , alguns desses ecossistemas já existem há mais de 50 anos , enquanto outros são apenas recém-nascidos . Essa diferença no grau de evolução e maturidade torna a comparação de aglomerados de tecnologia um desafio . Mais ainda , se alguns ecossistemas querem evoluir para um estágio próspero e sustentável , ecossitemas nascentes precisam de uma visão clara de como desenvolver suas comunidades . Esta tese apresenta nossa pesquisa baseada em um estudo de caso múltiplo em três diferentes ecossistemas , e foi dividade em três fases . Durante a primeira fase , nós analisamos o ecossistema empreendedor de Israel e , utilizando teoria fundamentada em dados , criamos um arcabouço conceitual que provê uma versão generalizada para mapear ecossistemas . Desenvolvemos , também , uma metodologia e um protocolo sistemático para entrevistas a serem usadas na análise de ecossistemas específicos . A segunda fase da pesquisa foi realizada em São Paulo , com o objetivo de refinar e validar a metodologia e o arcabouço conceitual . Esta fase resultou na descoberta de como é importante analisar a dinâmica e o processo de evolução dos ecossistemas , nos levando a criar um modelo de maturidade para ecossistemas de startups de software . O modelo de maturidade foi baseado no modelo_conceitual  que criamos , mapeando os fatores mais importantes que definem as características de um ecossistema . Para validar e refinar o modelo de maturidade criado na segunda fase , realizamos um terceiro estudo de caso em Nova Iorque que contou com o feedback de mais de uma dezena de especialistas . Geramos um modelo de maturidade final , um guia prático para determinar o nível de maturidade de cada ecossistema . Com esse modelo , é possível não somente comparar diferentes ecossistemas , como também identificar lacunas e propor ações práticas e personalizadas que podem resultar em melhorias significativas e levar ecossistemas ao próximo nível de desenvolvimento 
 Este estudo tem como objetivo melhorar o desempenho computacional dos algoritmos de inferência em redes credais , aplicando técnicas de computação_paralela  e sistemas distribuídos em algoritmos de fatoração de matrizes esparsas . Grosso modo , técnicas de computação_paralela  são técnicas para transformar um sistema em um sistema com algoritmos que possam ser executados concorrentemente . E a fatoração de matrizes são técnicas da matemática para decompor uma matriz em um produto de duas ou mais matrizes . As matrizes esparsas são matrizes que possuem a maioria de seus valores iguais a zero . E as redes credais são semelhantes as redes bayesianas , que são grafos acíclicos que representam uma probabilidade conjunta através de probabilidades condicionais e suas relações de independência . As redes credais podem ser consideradas como uma extensão das redes bayesianas para lidar com incertezas ou a má qualidade dos dados . Para aplicar a técnica de paralelização de fatoração de matrizes esparsas na inferência de redes credais , a inferência utiliza-se da técnica de eliminação de variáveis onde o grafo acíclico da rede credal é associado a uma matriz esparsa e cada variável eliminada é análoga a eliminação de uma coluna 
 A representação e execução de processos de negócio têm gerado importantes desafios na área de Ciência da Computação . Um desses desafios é a escolha do melhor arcabouço formal para a especificação dos controles de fluxo padrões . Algumas linguagens defendem o uso de redes de Petri ou álgebras de processos como base formal . O uso de redes de Petri para especificar workflows clássicos é uma abordagem bastante conhecida . Entretanto , pesquisas recentes vêm difundindo o uso de novas extensões da álgebra de processos como uma alternativa para a especificação formal de workflows 
 A principal_contribuição  deste trabalho é a definição da Navigation Plan Definition Language ( NPDL ) . A NPDL foi implementada como uma extensão da linguagem SQL . Ela é uma alternativa para a representação de workflows que utiliza a álgebra de processos como arcabouço formal . A NPDL promove uma separação explícita entre o ambiente de especificação e o ambiente de execução de um workflow . Esta separação propicia o reaproveitamento de passos de negócio e o uso das propriedades da álgebra de processos não só na modelagem , mas também no controle da execução dos processos . Após a especificação de um workflow por meio da NPDL , a execução dos passos que o definem é controlada pela ferramenta NavigationPlanTool . Essa ferramenta é a segunda contribuição deste trabalho de pesquisa 

 Este trabalho foca o desenvolvimento de algoritmos de otimização para o problema de PSP puramente ab_initio  . Algoritmos que melhor exploram o espaço de potencial de soluções podem , em geral , encontrar melhores soluções . Esses algoritmos podem beneficiar ambas abordagens de PSP , tanto o modelo ab_initio  quanto os baseados em conhecimento a priori . Pesquisadores tem mostrado que Algoritmos_Evolutivos  Multiobjetivo podem contribuir significativamente no contexto do problema de PSP puramente ab_initio  . Neste contexto , esta pesquisa investiga o Algoritmo Evolutivo Multiobjetivo baseado em Tabelas aplicado ao PSP puramente ab_initio  , que apresenta interessantes resultados para proteínas relativamente simples . Por exemplo , um desafio para o PSP puramente ab_initio  é a predição de estruturas com folhas- . Para trabalhar com tais proteínas , foi desenvolvido procedimentos computacionalmente eficientes para estimar energias de ligação de hidrogênio e solvatação . Em geral , estas não são consideradas no PSP por abordagens que combinam métodos de otimização e conhecimento a priori . Considerando somente van der Waals e eletrostática , as duas energias de interação que mais contribuem para a definição da estrutura de uma proteína , com as energias de ligação de hidrogênio e solvatação , o problema de PSP tem quatro objetivos . Problemas combinatórios ( tais como o PSP ) , com mais de três objetivos , geralmente requerem métodos específicos capazes de lidar com muitos critérios . Para resolver essa limitação , este trabalho propõe um novo método para a otimização dos muitos objetivos , chamado Algoritmo Evolutivo Multiobjetivo com Muitas Tabelas ( AEMMT ) . Esse método executa uma amostragem mais adequada do espaço de funções objetivo e , portanto , pode mapear melhor as regiões promissoras deste espaço . A capacidade de lidar com muitos objetivos capacita o AEMMT a utilizar melhor a informação oriunda das energias de solvatação e de ligação de hidrogênio , e então predizer estruturas com folhas- e algumas proteínas relativamente mais complexas . Do ponto de vista computacional , o AEMMT é um novo método que lida com muitos objetivos ( mais de dez ) encontrando soluções 
 Talvez uma das maiores capacidades do ser humano seja a sua habilidade de aprender a partir de observações e transmitir o que aprendeu para outros humanos . Durante séculos , a humanidade vem tentado compreender o mundo em que vive e , a partir desse novo conhecimento adquirido , melhorar o mundo em que vive 

 O desenvolvimento da tecnologia colocou a descoberta de conhecimento em um momento ímpar na história da humanidade . Com os progressos da Ciência da Computação , e , em particular , da Inteligência_Artificial  - IA - e Aprendizado de Máquina -AM , hoje em dia é possível , a partir de métodos de inferência indutiva e utilizando um conjunto de exemplos , descobrir algum tipo de conhecimento implícito nesses exemplos 

 Entretanto , por ser uma área de pesquisa relativamente nova , e por envolver um processo tanto iterativo quanto interativo , atualmente existem poucas ferramentas que suportam eficientemente a descoberta de conhecimento a partir dos dados . Essa falta de ferramentas se agrava ainda mais no que se refere ao seu uso por pesquisadores em Aprendizado de Máquina e Aquisição de Conhecimento 


 Esses fatores , além do fato que algumas pesquisas em nosso Laboratório de Inteligência Computacional - LABIC - têm alguns componentes em comum , motivaram a elaboração do projeto Discover , que consiste em uma estratégia de trabalho em conjunto , envolvendo um conjunto de ferramentas que se integram e interajam , e que supram as necessidades de pesquisa dos integrantes do nosso laboratório . O Discover também pode ser utilizado como um campo de prova para desenvolver novas ferramentas e testar novas idéias 

 Como o Discover tem como principal finalidade o seu uso e extensão por pesquisadores , uma 
 principal é que a arquitetura do projeto seja flexível o suficiente para permitir que novas pesquisas sejam englobadas e , simultaneamente , deve impor determinados padrões que permitam a integração eficiente de seus componentes 

 Neste trabalho , é proposto um framework de integração de componentes que tem como principal objetivo possibilitar a criação de um sistema computacional a partir das ferramentas desenvolvidas para serem utilizadas no projeto Discover 

 Esse framework compreende um mecanismo de adaptação de interface que cria uma camada ( interface horizontal ) sobre essas ferramentas , um poderoso mecanismo de metadados , que é utilizado para descrever tanto os componentes que implementam as funcionalidades do sistema quanto as configurações de experimentos criadas pelos usuário , que serão executadas pelo framework , e um ambiente de execução para essas configurações de experimentos 
 O modelo de atores tem sido visto como uma abordagem alternativa à programação concorrente convencional , baseada em travas e variáveis de condição . Atores são agentes computacionais que se comunicam por troca de mensagens e que possuem uma caixa de correio e um comportamento . As mensagens destinadas a um ator são armazenadas na caixa de correio do ator e processadas de maneira assíncrona . Sistemas de middleware orientados a mensagens trabalham com troca assíncrona de mensagens e formam uma base que simplifica o desenvolvimento de aplicações distribuídas . Tais sistemas permitem interoperabilidade com baixo acoplamento e provêm suporte para tratamento robusto de erros em caso de falhas . Message brokers são frequentemente apresentados como uma tecnologia que pode mudar a maneira com que sistemas distribuídos são construídos . A especificação AMQP é uma proposta recente de padronização de um protocolo para message brokers . Neste trabalho exploramos a potencial sinergia entre um message broker e uma implementação do modelo de atores . Criamos uma versão modificada da implementação do modelo de atores do projeto Akka que utiliza um message broker AMQP como mecanismo de transporte de mensagens para atores remotos 
 Em Biomedicina , a detecção e a quanticação de anormalidades presentes num sinal são desejáveis . Uma estratégia de codicação baseada em extração de características , tais como picos ou frequências , pode não capturar todas as irregularidades . Assim , uma representação baseada em funções de base denidas com conhecimento a priori do sinal pode ser mais precisa para aplicações biomédicas . A escolha das funções base depende da natureza siológica do sinal e de suas peculiaridades . Sinais de eletrocardiograma ( ECG ) e eletroencefalograma ( EEG ) exibem características bem denidas . ECG , por exemplo , é um sinal elétrico composto de uma forma de onda especíca ( P , QRS e T ) . Se as características de um sinal a ser sintetizado são bem compreendidas , é possível derivar uma assinatura para o sinal . Uma codicação apropriada permite a extração de parâmetros relevantes para sua análise , tais como anormalidades num ciclo cardíaco representadas por uma alteração no sinal de ECG , ou então uma excitação das ondas cerebrais representada por uma modicação no sinal de EEG . O objetivo deste projeto é introduzir uma nova_técnica  de codicação de sinais , que representa um sinal pela soma de funções sigmoides para aproximar iterativamente o sinal medido , com foco em aplicações biomédicas . Funções sigmoides tendem a reproduzir bem as grandes variações presentes em sinais biomédicos , daí a escolha de usá-las na codicação deste tipo de sinal . Serão explorados o nível de compressão dos dados , bem como a taxa de convergência . A técnica desenvolvida será comparada com técnicas convencionais de codicação e sua robustez será avaliada . Uma estratégia de codicação ótima pode trazer benefícios não só para a compressão , mas também na criação de assinaturas de sinais representando tanto condições siológicas normais como patológicas 
 Esta dissertação apresenta a evolução do framework orientado a objetos GREN , realizada usando a abordagem de Programação Orientada a Aspectos . Nessa evolução foram considerados apenas requisitos não-funcionais que não haviam sido implementados nas versões originais do GREN , mais especificamente os requisitos de segurança ( autenticação , registro de acesso e controle de acesso ) . A implementação desses requisitos deu origem a um Subsistema de Segurança orientado a aspectos , que permite a autenticação , registro de acesso e controle de acesso de usuários sobre as aplicações instanciadas a partir do GREN . Também é apresentada a evolução da ferramenta para instanciação automática do GREN , o GREN-Wizard , para torná-la compatível com a nova versão do GREN . Para a evolução do GREN foi seguido um processo de evolução de frameworks proposto na literatura que sofreu influência deste trabalho em seu refinamento . Uma avaliação sucinta dos resultados obtidos é também apresentada 
 Uma grande quantidade e variedade de artefatos é gerada durante o processo de desenvolvimento de um software . A documentação por meio dos diversos artefatos de tal processo é importante para proporcionar um bom conhecimento do software , além de tornar menos complicada a sua manutenção e reuso . Tais documentos , em geral , não registram informações adicionais relativas às alternativas , escolhas e decisões - Design Rationale ( DR ) - feitas durante a elaboração de cada artefato . O '' armazenamento e recuperação de DR dos artefatos de software tornaria mais simples a sua manutenção e facilitaria seu reuso , já que possibilita que o software torne-se mais completo de informações . O presente_trabalho  apresenta a ferramenta DocRationale que captura , armazena e recupera DR de artefatos de software . A DocRationale enfatiza a colaboração entre os desenvolvedores e a anotação estruturada por meio de uma simplificação de um modelo de representação de DR 
 Segmentação de imagens é um dos problemas mais investigados na área de computação visual . A complexidade desse problema varia de acordo com o tipo de aplicação . Em geral , o objetivo é dividir a imagem em regiões que apresentem propriedades similares . No presente_trabalho  , métodos auto-organizáveis para classificação não supervisionada e agrupamento de padrões são utilizados em tarefas de segmentação . O primeiro modelo refere-se à rede_neural  Fuzzy ART e o segundo é o Modelo de Misturas ICA ( ICAMM ) que faz_uso  da técnica ICA ( Análise de Componentes Independentes ) para descrever os dados em cada classe . Além da avaliação de desempenho dos modelos auto-organizáveis utilizados , foram propostas melhorias nos resultados de segmentação por meio da incorporação de técnicas de préprocessamento de imagens , que sejam capazes de tratar questões referentes à presença de ruídos , suavização de imagens e realce de bordas , de modo que as imagens se tornem mais adequadas ao processo de segmentação , tornando-o mais eficiente . Com esse objetivo , foi proposta uma metodologia para pré-processamento de imagens , que combina o método Sparse Code Shrinkage , para redução de ruídos e suavização da imagem , e o detector de bordas de Sobel , que tem a função de restaurar as bordas que foram borradas pelo processo de suavização . Outra contribuição original deste trabalho refere-se ao desenvolvimento do método EICAMM , que surgiu por meio da proposta de melhorias incorporadas ao modelo ICAMM , levando em consideração algumas limitações do método original e análises de como este poderia se tornar mais eficiente . Finalmente , unificando as duas principais_contribuições  originais desta tese , o método EICAMM foi utilizado na segmentação de imagens nas suas versões originais e pré-processadas pela metodologia_proposta  neste trabalho , tendo apresentado resultados de segmentação satisfatórios 
 A Recuperação de Informação por Similaridade ( RIS ) é um processo complexo que normalmente envolve bancos de dados volumosos e objetos em altas dimensões . Dois grupos do técnicas são amplamente_utilizados  para esse fim , Mapas Auto-Organizáveis ( SOM Sdf- ( ) rganizin ( j Maps ) e Métodos de Acesso ( MA ) . Os dois grupos do técnicas apresentam limitações . A maioria dos SOM , especialmente os modelos derivados do mapa de Kohonen , utilizam quase que exclusivamente o processamento sequencial para encontrar a unidade vencedora . Por outro_lado  , tanto os Métodos de Acesso Espacial ( MAE ) quanto os Métodos de Acesso Métrico ( MAM ) não aproveitam o conhecimento gerado por consultas anteriores . ( 0111 o objetivo de resolver esses problemas , duas novas técnicas são propostas nesta tese . A primeira técnica está baseada em SOM e a segunda em MAE e MAM . Em primeiro lugar , para melhorar o desempenho de sistemas baseados em SOM , propoe-se a incorporação de MAE e MAM durante o treinamento , gerando-se as famílias denominadas SAM-SOM o MAM-SOM . Eni segundo lugar , os MAE e MAM foram aprimorados através da criação do módulo denominado PMAM , que é capaz de aproveitar o conhecimento gerado pelas consultas anteriores . A combinação do módulo PMAM com MAE e MAM deu origem às famílias MAE e MAM+ . respectivamente . Como resultado deste trabalho ressalta-se que , tanto a família SAM-SOM quanto a MAM-SOM proporcionam uma melhora considerável em relação aos modelos de SOM tradicionais , os quais normalmente precisam de muito tempo de treinamento . Por outro_lado  . as famílias MAE+ e MAM+ têm a capacidade de reduzir , gradualmente . o número de operações necessárias para realizar uma consulta . Isto é possível porque o módulo PMAM permite reaproveitar o conhecimento gerado pelas consultas anteriores 
 O processo de Mineração de Dados inicia-se com o entendimento do domínio da aplicação , considerando aspectos como os objetivos da aplicação e as fontes de dados . Em seguida , é realizado o pré-processamento dos dados e a extração de padrões . Após a etapa de extração de padrões , vem a de pós-processamento , na qual o conhecimento é avaliado quanto a sua qualidade e/ou utilidade a fim de ser utilizado para apoio a algum processo de tomada de decisão . Recentemente , as pesquisas têm se voltado para problemas de regressão , porém a regressão em Mineração de Dados preditiva é uma questão pouco explorada dentro do processo de extração de conhecimento de bases de dados , sendo de grande relevância o estudo de métodos para a exploração de tarefas desse tipo . Alguns trabalhos vêm_sendo  realizados no Laboratório de Inteligência Computacional ( LABIC ) em temas relacionados ao processo de Extração de Conhecimento de Bases de Dados e Textos e na construção de um ambiente computacional para extração de conhecimento de dados denominado DlSCOVER . Para apoiar a construção de um modelo de regressão simbólico e o pós-processamento de problemas de regressão foi proposto e desenvolvido o Ambiente 'RTJ^FL , Esse ambiente viabiliza a avaliação de regras de regressão , inclusive disponibilizando estratégias para o cálculo da matriz de contingência e consequente utilização de todas as medidas derivadas dessa matriz para avaliação de regras de regressão ; a combinação de regressores homogéneos e heterogéneos para melhorar a precisão dos regressores e a integração e poda de regras de regressão obtidas de diferentes amostras ou algoritmos . Essas funcionalidades do Ambiente íR $ í'I*PE incrementam a potencialidade do Ambiente DlSCOVER quanto ao tratamento de regressão 
 No início dos anos 90 , o pesquisador Mark Weiser vislumbrou uma forma de computação baseada na integração transparente entre tecnologia e atividades humanas a qual denominou computação ubíqua . Idealizada como uma mudança no paradigma de interação entre usuários e computadores , a computação ubíqua tem como temas de pesquisa interfaces naturais , aplicações cientes de contexto e aplicações de captura e acesso de atividades humanas . Desenvolvidos para apoiar serviços dedicados de captura , de armazenamento e de processamento de informações de contexto , esforços como Context Fabric , EventHeap e Gaia não exploram a plataforma de Web_Services  como alternativa para o tratamento da heterogeneidade entre aplicações ; além disso , as dimensões de contexto propostas na literatura não são apoiadas diretamente . Este trabalho teve como propósito investigar e formalizar um conjunto de operações para armazenamento e recuperação de informações de contexto com base nas dimensões propostas na literatura : who , when , where , what , why e how . Como resultado , desenvolveu-se um núcleo de operações , denominado Context Kernel , implementado como um Web Service . Neste trabalho sao apresentadas a modelagem do Context Kernel , as especificações dos serviços em WSDL , a linguagem padrão para descrição de Web_Services  , e das APIs implementadas para acesso ao Context Kernel por aplicações Java e PHP 
 Esta tese de doutorado aborda os tópicos de modelagem e de reconhecimento de objetos estruturados , ou sistemas estruturados de objetos , em imagens . Um objeto ou sistema estruturado é aquele que pode ser descrito através de elementos primitivos que o compõem e pelas relações existentes entre esses elementos . Por exemplo , uma aeronave pode ser descrita pelos seguintes elementos primitivos : asas direita e esquerda , fuselagem e cockpit . O aspecto relacional de um objeto estruturado direciona sua representação computacional e seu reconhecimento em imagens ao paradigma estrutural de reconhecimento de padrões . Contudo , a variabilidade das características dos seus elementos primitivos é melhor representada através do paradigma estatístico de reconhecimento de padrões . Devido à complementaridade dos paradigmas , a conjunção dessas abordagens é um tema de pesquisa de interesse atual . Para conjugar esses dois aspectos , esta tese propôs uma metodologia que combina o conhecimento a priori das relações que caracterizam um objeto estruturado com dados estatísticos coletados de amostras desse objeto , num modelo híbrido denominado grafo estatístico-relacional ( GER ) . Segundo essa representação , foi estudada uma abordagem probabilística para reconhecer um objeto estruturado em imagens . Nesse cenário , o GER modelo é considerado uma variável aleatória , enquanto uma rotulação de uma imagem de entrada é interpretada como uma potencial observação do modelo . A tarefa de reconhecimento foi então formulada como um problema de otimização , que busca maximizar a probabilidade da observação de acordo com o modelo . O método foi aplicado à modelagem de órgãos abdominais em imagens de ressonância_magnética  não-contrastadas . Esses órgãos apresentam um arranjo espacial consistente em imagens distintas , além de propriedades de aparência e anatômicas variáveis , o que vem ao encontro da proposta da representação por GER e da abordagem probabilística para o reconhecimento dos órgãos em novas imagens 
 A implantação de composições de serviços_web  de grande escala apresentam vários desafios , tais como falhas corriqueiras na infraestrutura , heterogeneidade tecnológica , distribuição do sistema por diferentes organizações e atualização frequente dos serviços em operação . Nesta dissertação , estudamos como uma implantação automatizada baseada em middleware pode auxiliar na superação de tais desafios . Para isso , desenvolvemos o CHOReOS Enactment Engine , um sistema de middleware que possibilita a implantação distribuída e automatizada de composições de serviços_web  em uma infraestrutura virtualizada , operando no modelo de computação em nuvem denominado Plataforma como um Serviço . O middleware desenvolvido é avaliado qualitativamente em comparação a abordagens de implantação ad-hoc e quantitativamente pela sua escalabilidade em relação ao tempo de implantação das composições de serviços 
 A busca por explicações de fatos ou fenômenos é algo que sempre permeou o raciocínio humano . Desde a antiguidade , o ser humano costuma observar fatos e , de acordo com eles e o conhecimento presente , criar hipóteses que possam explicá-los . Um exemplo clássico é quando temos consulta médica e o médico , após verificar todos os sintomas , descobre qual é a doença e os meios de tratá-la . Essa construção de explicações , dado um conjunto de evidências que o indiquem , chamamos de \textit { abdução } . A abdução tradicional para a lógica_clássica  estabelece que o dado meta não é derivado da base de conhecimento , ou seja , dada uma base de conhecimento $ \Gamma $ e um dado meta $ A $ temos $ \Gamma ot \vdash A $ . Métodos clássicos de abdução buscam gerar um novo dado $ H $ que , juntamente com uma base de conhecimento $ \Gamma $ , possamos inferir $ A $ ( $ \Gamma \cup H \vdash A $ ) . Alguns métodos tradicionais utilizam o tableaux ( como em \cite ) para a geração da fórmula $ H $ . Aqui , além de lidarmos com a abdução baseada em corte , através do KE-tableaux , que não necessita assumir que o dado meta não seja derivado da base de conhecimento , lidaremos também com a lógica probabilística , redescoberta por Nilsson , em \cite , onde temos a atribuição de probabilidades a fórmulas . Dizemos que uma instância em lógica probabilística é consistente se existe uma distribuição probabilística consistente sobre as valorações . Determinar essa distribuição probabilística é que o chamamos de problema PSAT . O objetivo de nosso trabalho é definir e estabelecer o que é uma abdução em Lógica Probabilística ( abdução em PSAT ) e , além disso , fornecer métodos de abdução para PSAT : dada uma instância PSAT $ \left\langle \Gamma , \Psi ightangle $ na forma normal atômica \cite e uma fórmula $ A $ tal que existe uma distribuição probabi bylística $ \pi $ que satisfaz $ \left\langle \Gamma , \Psi ightangle $ e $ \pi ( A ) = 0 $ , cada método é capaz de gerar uma fórmula $ H $ tal que $ \left\langle \Gamma \cup H , \Psi ightangle \ ! \ ! |\ ! \ ! \ ! \approx A $ onde $ \pi ( A ) > 0 $ para toda distribuição $ \pi $ que satisfaça $ \left\langle \Gamma \cup H , \Psi ightangle $ . Iremos também demonstrar que alguns dos métodos apresentados são corretos e completos na geração de fórmulas $ H $ que satisfaçam as condições de abdução 
 Nesta tese apresentamos duas estratégias para resolver o problema de job-shop flexível com o objetivo de minimizar o makespan . A primeira estratégia utiliza um algoritmo branch and cut ( B & C ) e a segunda abordagens matheuristics . O algoritmo B & C utiliza novas classes de inequações válidas , originalmente formulada para o problema de job-shop e estendida para o problema em questão . Para que as inequações válidas sejam eficientes , o modelo proposto por Birgin et_al  , ( 2014 ) ( A milp model for an extended version of the fexible job shop problem . Optimization Letters , Springer , v. 8 , n. 4 , 1417-1431 ) , é reformulado ( MILP-2 ) . A segunda estratégia utiliza as matheuristcs local branching e diversification , refining and tight-refining . Os experimentos computacionais mostraram que a inclusão dos planos de corte melhoram a relaxação do modelo MILP-2 e a qualidade das soluções . O algoritmo B & C reduziu o gap e o número de nós explorados para uma grande quantidade de instâncias . As abordagens matheuristics tiveram um excelente desempenho . Do total de 59 instâncias analisadas , somente em 3 problemas a resolução do modelo MILP-1 obteve melhores_resultados  do que as abordagens 
 A estimação da iluminação é essencial para aplicações de realidade mista que se propõem a integrar elementos virtuais a cenas reais de maneira harmoniosa e sem a perda do realismo . Um dos métodos mais utilizados para fazer essa estimação é conhecido como iluminação baseada em imagens ( Image Based Lighting - IBL ) , método que utiliza light probes para capturar a intensidade da iluminação incidente em uma cena . Porém , IBL estima a iluminação incidente apenas para um determinado instante e posição . Nesta dissertação , será avaliado um modelo de iluminação que utiliza séries_temporais  de imagens de light probes , obtidas de maneira esparsa em relação ao tempo , para renderizar cenas em instantes arbitrários . Novas cenas contendo objetos virtuais poderão ser renderizadas utilizando imagens de light probes artificiais , geradas a partir das amostras da iluminação originais . Diferentes funções de interpolação e aproximação são avaliadas para modelar o comportamento luminoso . As imagens finais produzidas pela metodologia também são verificadas por voluntários , de modo a determinar o impacto na qualidade de renderização em aplicações de realidade mista . Além da metodologia , foi desenvolvida uma ferramenta de software em forma de plugin para facilitar o uso de IBL temporalmente variável , permitindo assim a renderização realística de objetos virtuais para instantes 
 A análise de dados multidimensionais tem sido por muitos anos tópico de contínua investigação e uma das razões se deve ao fato desse tipo de dados ser encontrado em diversas_áreas  da ciência . Uma tarefa comum ao se analisar esse tipo de dados é a investigação de padrões pela interação em projeções_multidimensionais  dos dados para o espaço_visual  . O entendimento da relação entre as características do conjunto de dados ( dataset ) e a técnica utilizada para se obter uma representação visual desse dataset é de fundamental_importância  uma vez que esse entendimento pode fornecer uma melhor intuição a respeito do que se esperar da projeção . Por isso motivado , no presente_trabalho  investiga-se alguns aspectos de centralidade dos dados em dois cenários distintos : coleções de documentos com grafos de coautoria ; dados multidimensionais mais gerais . No primeiro cenário , o dado multidimensional que representa os documentos possui informações mais específicas , o que possibilita a combinação de diferentes aspectos para analisá-los de forma sumarizada , bem como a noção de centralidade e relevância dentro da coleção . Isso é levado em consideração para propor uma metáfora visual combinada que possibilite a exploração de toda a coleção , bem como de documentos individuais . No segundo cenário , de dados multidimensionais gerais , assume-se que tais informações não estão disponíveis . Ainda assim , utilizando um conceito de estatística não-paramétrica , deno- minado funções de profundidade de dados ( data-depth functions ) , é feita a avaliação da ação de técnicas de projeção multidimensionais sobre os dados , possibilitando entender como suas medidas de profundidade ( centralidade ) foram alteradas ao longo do processo , definindo uma também medida de qualidade para projeções 
 Anomalias ou outliers são exemplos ou grupo de exemplos que apresentam comportamento diferente do esperado . Na prática , esses exemplos podem representar doenças em um indivíduo ou em uma população , além de outros eventos como fraudes em operações bancárias e falhas em sistemas . Diversas técnicas existentes buscam identificar essas anomalias , incluindo adaptações de métodos de classificação e métodos estatísticos . Os principais desafios são o desbalanceamento do número de exemplos em cada uma das classes e a definição do comportamento normal associada à formalização de um modelo para esse comportamento . Nesta dissertação propõe-se a utilização de um novo espaço para realizar a detecção , esse espaço é chamado espaço de parâmetros . Um espaço de parâmetros é criado utilizando parâmetros estimados a partir da concatenação ( encadeamento ) de dois exemplos . Apresenta-se , então , um novo framework para realizar a detecção de anomalias através da fusão de detectores que utilizam fechos convexos em múltiplos espaços de parâmetros para realizar a detecção . O método é considerado um framework pois é possível escolher quais os espaços de parâmetros que serão utilizados pelo método de acordo como comportamento da base de dados alvo . Nesse trabalho utilizou-se , para experimentos , dois conjuntos de parâmetros ( média e desvio padrão ; média , variância , obliquidade e curtose ) e os resultados obtidos foram comparados com alguns métodos comumente utilizados para detecção de anomalias . Os resultados atingidos foram comparáveis ou melhores aos obtidos pelos demais métodos . Além_disso  , acredita-se que a utilização de espaços de parâmetros cria uma grande flexibilidade do método proposto , já que o usuário pode escolher um espaço de parâmetros que se adeque a sua aplicação . Tanto a flexibilidade quanto a extensibilidade disponibilizada pelo espaço de parâmetros , em conjunto como bom_desempenho  do método proposto nos experimentos_realizados  , tornam atrativa a utilização de espaços de parâmetros e , mais especificamente , dos métodos apresentados na solução de problemas de detecção de anomalias 
 Um emparelhamento em um grafo é um conjunto de arestas duas a duas não adjacentes . Dado um grafo G com pesos em suas arestas , o problema do emparelhamento de peso é máximo é encontrar um emparelhamento cuja soma dos pesos de suas arestas é máxima . Neste trabalho estudamos diferentes soluções para esse problema . Estudamos algoritmos combinatórios que resolvem o problema no caso em que G é bipartido e no caso geral . O algoritmo de Edmonds é um algoritmo polinomial cuja complexidade de tempo é O ( n^4 ) , onde n é o número de vértices do grafo G. Discutimos nesse trabalho nossa implementação desse algoritmo . Num trabalho de 1985 , Grötschel e Holland propuseram o uso de ferramentas de programação linear para resolver o mesmo problema . O método chamado de planos-de-corte baseia-se em um resultado de Padberg e Rao de que o problema da separação associado ao poliedro dos emparelhamentos pode ser resolvido em tempo polinomial . Neste trabalho fizemos implementações dos dois métodos e os utilizamos para resolver diversos tipos de instâncias do problema . Nossa conclusão é que o método poliédrico , apesar de utilizar ferramentas genéricas , é bastante eficiente na prática 
 Neste trabalho desenvolvemos um estudo sobre modelos auto-regressivos com heterocedasticidade ( ARCH ) e modelos auto-regressivos com erros ARCH ( AR-ARCH ) . Apresentamos os procedimentos para a estimação dos modelos e para a seleção da ordem dos mesmos . As estimativas dos parâmetros dos modelos são obtidas utilizando duas técnicas distintas : a inferência Clássica e a inferência Bayesiana . Na abordagem de Máxima Verossimilhança obtivemos intervalos de confiança usando a técnica Bootstrap e , na abordagem_Bayesiana  , adotamos uma distribuição a priori informativa e uma distribuição a priori não-informativa , considerando uma reparametrização dos modelos para mapear o espaço dos parâmetros no espaço real . Este procedimento nos permite adotar distribuição a priori normal para os parâmetros transformados . As distribuições a posteriori são obtidas_através  dos métodos de simulação de Monte_Carlo  em Cadeias de Markov ( MCMC ) . A metodologia é exemplificada considerando séries simuladas e séries do mercado financeiro 
 Veículos aéreos não tripulados ( UAVs - Unmanned Aerial Vehicles ) têm sido projetados para cumprir missões de reconhecimento e transporte . Podem ser controlados do solo ou operar de forma autônoma em missões pré-programadas . O projeto ARARA - Autonomous and Radiu-Assisted Reconnaissance Aircrafi ( Aeronaves de Reconhecimento Assistidas por Rádio e Autônomas ) , está centrado no desenvolvimento de UAVs para aplicação em agricultura de precisão e monitoramento ecológico , entre outras possibilidades . O projeto está dividido em quatro fases . Cada fase representa um marco na direção do objetivo final que consiste no cumprimento de missões completamente automáticas . Sistemas diferentes são definidos em cada fase com níveis crescentes de complexidade e aplicação . Este trabalho está inserido na fase III do projeto ARARA . Seu objetivo principal é o desenvolvimento de um sistema de comunicação entre uma aeronave autônoma e uma estação no solo . O sistema de comunicação desenvolvido é baseado em comunicação por satélite , provendo um canal confiável de comunicação de ampla cobertura geográfica para controle e monitoramento da missão realizada pela aeronave . Além de ser capaz de transmitir comandos para intervenção em missões previamente programadas , o sistema também permite o recebimento de imagens e dados dos sensores a bordo da aeronave 
 Os métodos de pontos_interiores  primal-dual c preditor-corretor são desenvolvidos para o problema , de fluxo de potência ótimo AC e a estrutura , matricial resultante é estudada . Foi adotada a , representação do problema , através de coordenadas cartesianas das tensões uma vez que neste modelo a Hessiana do problema é constante e a expansão em Taylor é exata para o termo de ordem dois . Além_disso  , o cálculo do termo de correção do método preditor-corretor pode ser feito de forma menos custosa computacionalmente . Por outro_lado  , a vantagem em se trabalhar com coordenadas polares , que modelam mais facilmente os limites de magnitude de tensão , perde importância devido ao tratamento de desigualdades eficiente proporcionado pelos métodos de pontos_interiores  , permitindo uma , revisão dos procedimentos geralmente adotados . Assim , a utilização de coordenadas cartesianas surge como uma abordagem natural , pois apresenta uma formulação mais simples que as coordenadas polares . A aplicação do método de Newton às condições de otimalidade leva a um método de pontos_interiores  primal-dual específico para , este modelo . As condições de otimalidade por sua , vez podem ser obtidas_através  da função lagrangiana , do problema onde ; as restrições de desigualdade são representadas por funções de barreira logarítmicas das variáveis de folga . Antes da aplicação do método , o número de variáveis do problema é reduzido através da , eliminação de variáveis duais livres , que serão calculadas no final . Esta redução não altera , a estrutura esparsa do problema . O sistema linear resultante pode então ser reduzido a duas vezes a quantidade do número de barras da rede de transmissão . Além_disso  , a matriz resultante é simétrica em estrutura . Esta característica pode ser explorada de forma eficiente reduzindo o esforço computacional por iteração 
 Neste trabalho são apresentados dois novos descritores de forma para tarefas de recuperação de imagens por conteúdo ( CBIR ) e análise de formas , que são construídos sobre uma extensão do conceito de tensor scale baseada na Transformada de Distância Euclidiana ( EDT ) . Primeiro , o algoritmo de tensor scale é utilizado para extrair_informações  da forma sobre suas estruturas locais ( espessura , orientação e anisotropia ) representadas pela maior elipse contida em uma região homogênea centrada em cada pixel da imagem . Nos novos descritores , o limite do intervalo das orientações das elipses do modelo de tensor scale é estendido de 180º para 360º , de forma a melhor discriminar a descrição das estruturas locais . Então , com base em diferentes abordagens de amostragem , visando resumir informações mais relevantes , os novos descritores são construídos . No primeiro descritor proposto , Tensor Scale Sector ( TSS ) , a distribuição das orientações relativas das estruturas locais em setores circulares é utilizada para compor um vetor de características de tamanho fixo , para uma caracterização de formas baseada em região . No segundo descritor , o Tensor Scale Band ( TSB ) , foram considerados histogramas das orientações relativas extraídos de bandas concêntricas , formando também um vetor de características de tamanho fixo , com uma função de distância de tempo linear . Resultados experimentais com diferentes bases de formas ( MPEG-7 e MNIST ) são apresentados para ilustrar e validar os métodos . TSS demonstra resultados comparáveis aos métodos estado da arte , que geralmente dependem de algoritmos custosos de otimização de correspondências . Já o TSB , com sua função de distância em tempo linear , se demonstra como uma solução adequada para grandes coleções de formas 
 Este trabalho apresenta um estudo , implementação e avaliação em ambiente real de um protótipo de servidor Web com diferenciação de serviços ( SWDS ) para provisão de QoS relativa em servidores_Web  . Para tanto foram considerados algoritmos de reserva de recursos e escalonamento baseado em prioridades para prover diferenciação entre as classes de serviço , além de mecanismos de controle de admissão a fim de controlar a carga no sistema . A meta é oferecer melhor tratamento para requisições de maior prioridade , sem prejudicar em excesso as requisições de prioridade menor . Foi observado que os algoritmos de reserva de recursos ( RSV e RSVadap ) são eficientes para prover diferenciação entre as classes consideradas , no entanto seus desempenhos não foram satisfatórios em alguns_casos  , tanto devido a arquitetura em que foram implementados , como por motivos inerentes à própria natureza do algoritmo . O algoritmo de escalonamento baseado em prioridades considerado ( PriProcess ) , mostrou-se eficiente tanto no sentido de prover diferenciação de serviços entre as classes , como na questão de desempenho , com a classe de maior prioridade sempre sendo melhor atendida em relação à classe de menor prioridade . Também foi alvo do estudo a criação de um mecanismo de controle de admissão com diferenciação de serviços . Os resultados alcançados sinalizam uma melhora em termos de tempos de respostas e número de requisições completadas para a classe de maior 
 Estudos comportamentais têm sido conduzidos , há séculos , por cientistas e filósofos , abordando assuntos tais como trajetórias de estrelas e planetas , organizações da sociedade , evolução dos seres vivos , comportamento e linguagem humana . Com o advento da computação , grandes quantidades de informação tornaram-se disponíveis , as quais geram novos desafios a fim de explorar e compreender variações comportamentais de interação com esses sistemas . Motivado por esses desafios e pela disponibilidade de informações , esta dissertação de mestrado_propõe  uma metodologia com objetivo de classificar , detectar e identificar padrões de comportamento . A fim de validar essa metodologia , modelou-se conhecimentos embutidos em informações relativas a interações de usuários durante a grafia digital de assinaturas ( tais informações foram obtidas de uma base de dados do campeonato SVC2004 -- First International Signature Verification Competition ) . Os modelos de conhecimento gerados foram , posteriormente , empregados em experimentos visando o reconhecimento de assinaturas . Resultados obtidos foram comparados a outras abordagens propostas na 
 No estudo da complexidade de problemas computacionais destacam-se duas classes conhecidas como P e NP . A questao P=NP e um dos maiores problemas nao resolvidos em Ciencia da Compu- tacao teorica e Matematica contemporanea . O problema SAT foi o primeiro problema reconhecido como NP-completo e consiste em verificar se uma determinada formula da logica proposicional clas- sica e ou nao satisfazivel . As implementacoes de algoritmos para resolver problemas SAT sao conhe- cidas como resolvedores SAT ( SAT Solvers ) . Existem diversas aplicacoes em Ciencia da Computacao que podem ser realizadas com SAT Solvers e com outros resolvedores de problemas NP-completos que podem ser reduzidos ao SAT como por exemplo problemas de coloracao de grafos , problemas de agendamento e problemas de planejamento . Dentre os mais eficientes algoritmos para resolvedores de SAT estao Sato , Grasp , Chaff , MiniSat e Berkmin . O Algoritmo Chaff e baseado no Algoritmo DPLL o qual existe a mais de 40 anos e e a estrategia mais utilizada para os Sat Solvers . Essa dissertacao apresenta um estudo aprofundado do comportamento do zChaff ( uma implementacao muito eficiente do Chaff ) para saber o que esperar de suas execucoes em geral 
 Em problemas de tomada de decisão sequencial modelados como Processos de Decisão Markovianos ( MDP ) pode não ser possível_obter  uma medida exata para as probabilidades de transição de estados . Visando resolver esta situação os Processos de Decisão Markovianos com Probabilidades Imprecisas ( Markov Decision Processes with Imprecise Transition Probabilities , MDP-IPs ) foram introduzidos . Porém , enquanto estes MDP-IPs se mostram como um arcabouço robusto para aplicações de planejamento no mundo_real  , suas soluções consomem muito tempo na prática . Em trabalhos anteriores , buscando melhorar estas soluções foram propostos algoritmos de programação_dinâmica  síncrona eficientes para resolver MDP-IPs com uma representação fatorada para as funções de transição probabilística e recompensa , chamados de MDP-IP fatorados . Entretanto quando o estado inicial de um problema do Caminho mais Curto Estocástico ( Stochastic Shortest Path MDP , SSP MDP ) é dado , estas soluções não utilizam esta informação . Neste trabalho será introduzido o problema do Caminho mais Curto Estocástico com Probabilidades Imprecisas ( Stochastic Shortest Path MDP-IP , SSP MDP-IP ) tanto em sua forma enumerativa , quanto na fatorada . Um algoritmo de programação_dinâmica  assíncrona para SSP MDP-IP enumerativos com probabilidades dadas por intervalos foi proposto por Buffet e Aberdeen ( 2005 ) . Entretanto , em geral um problema é dado de forma fatorada , i.e. , em termos de variáveis de estado e nesse caso , mesmo se for assumida a imprecisão dada por intervalos sobre as variáveis , ele não poderá ser mais aplicado , pois as probabilidades de transição conjuntas serão multilineares . Assim , será mostrado que os SSP MDP-IPs fatorados são mais expressivos que os enumerativos e que a mudança do SSP MDP-IP enumerativo para o caso geral de um SSP MDP-IPs fatorado leva a uma mudança de resolução da função objetivo do Bellman backup de uma função linear para uma não-linear . Também serão propostos algoritmos enumerativos , chamados de RTDP-IP ( Real-time Dynamic Programming with Imprecise Transition Probabilities ) , LRTDP-IP ( Labeled Real-time Dynamic Programming with Imprecise Transition Probabilities ) , SSiPP-IP ( Short-Sighted Probabilistic Planner with Imprecise Transition Probabilities ) e LSSiPP-IP ( Labeled Short-Sighted Probabilistic Planner with Imprecise Transition Probabilities ) e fatorados chamados factRTDP-IP ( factored RTDP-IP ) e factLRTDP-IP ( factored LRTDP-IP ) . Eles serão avaliados em relação aos algoritmos de programação_dinâmica  síncrona em termos de tempo de convergência da solução e de escalabilidade 
 Este trabalho descreve a Klein , uma biblioteca genérica para álgebra linear em C++ . A Klein facilita o uso de matrizes e vetores , permitindo que o usuário programe de modo similar ao Matlab . Com ela podemos , por exemplo , implementar um passo do método de Newton para a função f , através da expressão x = x - inv ( jac ( x ) ) * f ( x ) , onde x é o vetor , jac a Jacobiana e inv a inversa . Além_disso  , por se tratar de uma biblioteca genérica , os tipos envolvidos nestas expressões podem ser escolhidos pelo programador . O trabalho também discute como a biblioteca é testada , tanto do ponto de vista de corretude quanto de desempenho 
 A complexidade dos dados armazenados em grandes bases de dados aumenta sempre , criando a necessidade de novas formas de consulta . As consultas por similaridade vêm apresentando crescente_interesse  para tratar de dados complexos , sendo as mais representativas a consulta por abrangência ( 'R IND . q ' Range query ) e a consulta aos k-vizinhos mais próximos ( k-'NN IND . q ' k-Nearest Neighboor query ) . Até recentemente , essas consultas não estavam disponíveis nos Sistemas de Gerenciamento de Bases de Dados ( SGBD ) . Agora , com o início de sua disponibilidade , tem se tornado claro que os operadores de busca fundamentais usados para executá-las não são suficientes para atender às necessidades das aplicações que as demandam . Assim , estão sendo estudadas variações e extensões aos operadores fundamentais , em geral voltados às necessidades de domínios de aplicações específicas . Além_disso  , os seguintes problemas vêm impactando diretamente sua aceitação por parte dos usuários e , portanto , sua usabilidade : ( i ) os operadores fundamentais são pouco expressivos em situações reais ; ( ii ) a cardinalidade dos resultados tende a ser grande , obrigando o usuário analisar muitos elementos ; e ( iii ) os resultados nem sempre atendem ao interesse do usuário , implicando na reformulação e ajuste frequente das consultas . O objetivo desta dissertação é o desenvolvimento de uma técnica inédita para exibir um grau de variedade nas respostas às consultas aos k-vizinhos mais próximos em domínios de dados métricos , explorando aspectos de diversidade em extensões dos operadores fundamentais usando apenas as propriedades básicas do espaço métrico sem a solicitação de outra informação por parte do usuário . Neste sentido , são apresentados : a formalização de um modelo de variedade que possibilita inserir diversidade nas consultas por similaridade sem a definição de parâmetros por parte do usuário ; um algoritmo incremental para responder às consultas aos k-vizinhos mais próximos com variedade ; um método de avaliação de sobreposição de variedade para as consultas por similaridade . As propriedades desses resultados permitem usar as técnicas desenvolvidas para apoiar a propriedade de variedade nas consultas aos k-vizinhos mais próximos em Sistemas de Gerenciamento de Bases de 
 Na área de emparelhamento de ontologias , são utilizadas algumas métricas para avaliar os alinhamentos produzidos . As métricas baseadas em alinhamento têm como princípio básico confrontar um alinhamento proposto com um alinhamento de referência . Algumas destas métricas , entretanto , não têm alcançado êxito suficiente porque ( i ) não conseguem discriminar sempre entre um alinhamento totalmente errado e um quase correto ; e ( ii ) não conseguem estimar o esforço do usuário para refinar o alinhamento resultante . Este trabalho tem como objetivo apresentar uma nova abordagem para avaliar os alinhamentos de ontologias . A nossa abordagem apresenta uma métrica na qual utilizamos as próprias consultas normalmente já realizadas nas ontologias originais para julgar a qualidade do alinhamento proposto . Apresentamos também alguns resultados_satisfatórios  de nossa abordagem em relação às outras métricas já existentes e largamente utilizadas 
 Árvores de decisão são amplamente utilizadas como estratégia para extração de conhecimento de dados . Existem muitas estratégias diferentes para indução de árvores de decisão , cada qual com suas vantagens e desvantagens tendo em vista seu bias indutivo . Tais estratégias têm sido continuamente melhoradas por pesquisadores nos últimos 40 anos . Esta tese , em sintonia com recentes descobertas no campo de projeto automático de algoritmos de aprendizado de máquina , propõe a geração_automática  de algoritmos de indução de árvores de decisão . A abordagem proposta , chamada de HEAD-DT , é baseada no paradigma de algoritmos_evolutivos  . HEAD-DT evolui componentes de árvores de decisão que foram manualmente codificados e os combina da forma mais adequada ao problema em questão . HEAD-DT funciona conforme dois diferentes frameworks : i ) evolução de algoritmos customizados para uma única base de dados ( framework específico ) ; e ii ) evolução de algoritmos a partir de múltiplas bases ( framework geral ) . O framework específico tem por objetivo gerar um algoritmo por base de dados , de forma que o algoritmo projetado não necessite de poder de generalização que vá além da base alvo . O framework geral tem um objetivo mais ambicioso : gerar um único algoritmo capaz de ser efetivamente executado em várias bases de dados . O framework específico é testado em 20 bases públicas da UCI , e os resultados mostram que os algoritmos específicos gerados por HEAD-DT apresentam desempenho preditivo significativamente melhor do que algoritmos como CART e C4.5 . O framework geral é executado em dois cenários diferentes : i ) projeto de algoritmo específico a um domínio de aplicação ; e ii ) projeto de um algoritmo livre-de-domínio , robusto a bases distintas . O primeiro cenário é testado em 35 bases de expressão_gênica  , e os resultados mostram que o algoritmo gerado por HEAD-DT consistentemente supera CART e C4.5 em diferentes configurações experimentais . O segundo cenário é testado em 67 bases de dados da UCI , e os resultados mostram que o algoritmo gerado por HEAD-DT é competitivo com CART e C4.5 . No entanto , é mostrado que HEAD-DT é vulnerável a um caso particular de overfitting quando executado sobre o segundo cenário do framework geral , e indica-se assim possíveis soluções para tal problema . Por fim , é realizado uma análise detalhada para avaliação de diferentes funções de fitness de HEAD-DT , onde 5 medidas de desempenho são combinadas com três esquemas de agregação . As 15 versões são avaliadas em 67 bases da UCI e as melhores versões são utilizadas para geração de algoritmos customizados para bases balanceadas e desbalanceadas . Os resultados mostram que os algoritmos gerados por HEAD-DT apresentam desempenho preditivo significativamente melhor que CART e C4.5 , em uma clara indicação que HEAD-DT também é capaz de gerar algoritmos customizados para certo perfil estatístico dos dados de 
 Modelos de predição de defeitos auxiliam profissionais de teste na priorização de partes do software mais propensas a conter defeitos . A abordagem de predição de defeitos cruzada entre projetos ( CPDP ) refere-se à utilização de projetos externos já conhecidos para compor o conjunto de treinamento . Essa abordagem é útil quando a quantidade de dados históricos de defeitos é inapropriada ou insuficiente para compor o conjunto de treinamento . Embora o princípio seja atrativo , o desempenho de predição é um fator limitante nessa abordagem . Nos últimos_anos  , vários métodos foram propostos com o intuito de melhorar o desempenho de predição de modelos CPDP . Contudo , na literatura , existe uma carência de estudos comparativos que apontam quais métodos CPDP apresentam melhores desempenhos . Além_disso  , não há evidências sobre quais métodos CPDP apresentam melhor desempenho para um domínio de aplicação específico . De fato , não existe um algoritmo de aprendizado de máquina que seja apropriado para todos os domínios de aplicação . A tarefa de decisão sobre qual algoritmo é mais adequado a um determinado domínio de aplicação é investigado na literatura de meta-aprendizado . Um modelo de meta-aprendizado é caracterizado pela sua capacidade de aprender a partir de experiências anteriores e adaptar seu viés de indução dinamicamente de acordo com o domínio alvo . Neste trabalho , nós investigamos a viabilidade de usar meta-aprendizado para a recomendação de métodos CPDP . Nesta tese são almejados três principais objetivos . Primeiro , é conduzida uma análise experimental para investigar a viabilidade de usar métodos de seleção de atributos como procedimento interno de dois métodos CPDP , com o intuito de melhorar o desempenho de predição . Segundo , são investigados quais métodos CPDP apresentam um melhor desempenho em um contexto geral . Nesse contexto , também é investigado se os métodos com melhor desempenho geral apresentam melhor desempenho para os mesmos conjuntos de dados ( ou projetos de software ) . Os resultados revelam que os métodos CPDP mais adequados para um projeto podem variar de acordo com as características do projeto sendo predito . Essa constatação conduz à terceira investigação realizada neste trabalho . Foram investigadas as várias particularidades inerentes ao contexto CPDP a fim de propor uma solução de meta-aprendizado capaz de aprender com experiências anteriores e recomendar métodos CPDP adequados , de acordo com as características do software . Foram avaliados a capacidade de meta-aprendizado da solução proposta e a sua performance em relação aos métodos base que apresentaram melhor desempenho geral 
 A árvore de sufixos é uma estrutura dados , que representa em espaço linear todos os fatores de uma palavra , com diversos exemplos de aplicações práticas . Neste trabalho , definimos uma estrutura mais geral : a árvore de Ukkonen . Provamos para ela diversas propriedades combinatórias , dentre quais , a minimalidade em um sentido preciso . Acreditamos que a apresentação aqui oferecida , além de mais geral que as árvores de sufixo , tem a vantagem de oferecer uma descrição explícita da topologia da árvore , de seus vértices , arestas e rótulos , o que não vimos em nenhum outro trabalho . Como aplicações , apresentamos também a árvore esparsa de sufixos ( que armazena apenas um subconjunto dos sufixos ) e a árvore de k-fatores ( que armazena apenas os segmentos de comprimento k , ao invés dos sufixos ) definidas como casos particulares das árvores de Ukkonen . Propomos para as árvores esparsas um novo algoritmo de construção com tempo O ( n ) e espaço O ( m ) , onde n é tamanho da palavra e m é número de sufixos . Para as árvores de k-fatores , propomos um novo algoritmo online com tempo e espaço O ( n ) , onde n é o tamanho da palavra 
 O problema de planejamento da produção integrado ao problema de corte de estoque surge em várias indústrias de manufatura , tais como indústria de papel , móveis , aço entre outras , e consiste em um problema de otimização combinatória bastante complexo , devido ao fato de integrar dois problemas conhecidos na literatura de difícil resolução . As aplicações práticas deste problema vêm aumentando em muitas empresas que buscam tornar seus processos produtivos mais eficientes . Neste trabalho , estudamos o problema de otimização integrado que surge em pequenas indústrias de móveis , em que placas de MDF disponíveis em estoque devem ser cortadas em itens menores , de diversos tamanhos e quantidades para comporem os produtos demandados . O modelo matemático de otimização_linear  inteiro proposto permite que alguns produtos sejam antecipados e estocados . Essa antecipação da produção aumenta os custos de estoque , porém com o aumento da demanda de peças é possível gerar padrões de corte melhores e diminuir os custos com a perda de material . Consideramos no modelo dois tipos de variáveis de antecipação , uma de estoque convencional para atender uma demanda em carteira e outra para aproveitar a produção e atender uma demanda prevista , chamada variável oportunista . A função objetivo consiste em minimizar os custos dos processos de produção e de corte . Para resolver a relaxação linear deste problema , propomos um método lagrangiano e utilizamos a estratégia de horizonte rolante . Alguns testes computacionais são realizados e os resultados 
 A utilização de córpus tem crescido progressivamente em áreas como Lingüística e Processamento de Língua Natural . Como resultado , temos a compilação de novos e grandes córpus e a criação de sistemas processadores de córpus e de padrões para codificação e intercâmbio de textos eletrônicos . Entretanto , a metodologia para compilação de córpus históricos difere das metodologias usadas em córpus contemporâneos . Outro problema é o fato de a maior_parte  dos processadores de córpus proverem poucos recursos para o tratamento de córpus históricos , apesar de tais córpus serem numerosos . Da mesma forma , os sistemas para criação de dicionários não atendem satisfatoriamente necessidades de dicionários históricos . A motivação desta pesquisa é o projeto do Dicionário Histórico do Português do Brasil ( DHPB ) que tem como base a construção de um córpus de Português do Brasil dos séculos XVI a XVIII ( incluindo alguns textos do começo do século XIX ) . Neste trabalho são apresentados os desafios encontrados para o processamento do córpus do projeto do projeto DHPB e os requisitos para redação de verbetes do dicionário histórico . Um ambiente computacional para processamento de córpus , criação de glossários e redação de verbetes foi desenvolvido para o projeto DHPB sendo possível adaptá-lo para ser aplicado a outros projetos de criação de dicionários 
 WEb Services constituem uma tecnologia emergente utilizada para disponibilização de serviços na Web . Devido aos complexos processos de negócio existentes , esses serviços são combinados para que operem seguindo um fluxo de trabalho . Esse processo é chamado de composição de Web_Services  . Como no processo de desenvolvimento de um software tradicional , a atividade de teste é fundamental no processo de desenvolvimento da composição de Web_Services  . Neste trabalho é proposta uma estratégia de teste para a composição de Web_Services  , em que especificações de composições são transformadas para um modelo de teste de programas paralelos baseados em passagem de mensagens . É proposto o conceito de grupos de elementos requeridos para melhorar a cobertura dos critérios . Uma ferramenta de apoio à estratégia de teste foi desenvolvida . Alguns estudos_experimentais  para avaliar a aplicabilidade da estratégia proposta também são apresentados . Além_disso  , é proposto um método que utiliza redes de Petri para eliminar sincronizações não-executáveis no envio e recebimento de 
 Este trabalho faz parte do projeto de uma ferramenta denominada FIP ( Ferramenta Inteligente de Apoio à Pesquisa ) para recuperação , organização e mineração de grandes coleções de documentos . No contexto da ferramenta FIP , diversas técnicas de Recuperação de Informação , Mineração de Dados , Visualização de Informações e , em particular , técnicas de Extração de Informações , foco deste trabalho , são usadas . Sistemas de Extração de Informação atuam sobre um conjunto de dados não estruturados e objetivam localizar informações específicas em um documento ou coleção de documentos , extraí-las e estruturá-las com o intuito de facilitar o uso dessas informações . O objetivo específico desenvolvido nesta dissertação é induzir , de forma automática , um conjunto de regras para a extração de informações de artigos científicos . O sistema de extração proposto , inicialmente , analisa e extrai informações presentes no corpo dos artigos ( título , autores , a filiação , resumo , palavras chaves ) e , posteriormente , foca na extração das informações de suas referências bibliográficas . A proposta para extração automática das informações das referências é uma abordagem nova , baseada no mapeamento do problema de part-of-speech tagging ao problema de extração de informação . Como produto_final  do processo de extração , tem-se uma base de dados com as informações extraídas e estruturadas no formato XML , disponível à ferramenta FIP ou a qualquer outra aplicação . Os resultados obtidos foram avaliados em termos das métricas precisão , cobertura e F-measure , alcançando bons_resultados  comparados com sistemas 
 Technological development has significantly reduced the distance between the performance of systems designed using reconfigurable computing and dedicated hardware . The main sources of performance are the high density level of the FPGAs and the resources ? improvement offered by manufacturers , who make more its use more attractive in a variety of applications , emphatically in systems that demand a high degree of flexibility . In this context , the objective of this work consists on the exploration of the resources offered by FPGAs for the development of a multiprocessed platform with the purpose of parallel execution of tasks . In this way , the eCos operating system was modified , with the addition of new characteristics to support of the Symmetric Multiprocessing model , using three soft-Core Altera Nios_II  processors . On this operating system , all parallelism is directly related to execution of the threads . This platform was analyzed and validated through the execution of parallel algorithms , emphasizing aspects of performance and flexibility compared to other architectures . This work contributes for reaching better results in the execution of tasks in robotics area , which belongs to a domain that demand great competition of tasks , mainly in modules that involve interaction with the external 
 O volume crescente de software em funcionamento em todo tipo de organização vem despertando atenção para uma fase do ciclo de vida de software , até então considerada sempre de maneira secundária , a manutenção de software . O fato de geralmente não ser viável substituir os produtos de software de uma organização por outros baseados em tecnologias mais recentes , torna a manutenção daqueles sistemas legados um desafio adicional para a busca de técnicas e métodos para a manutenção de software . Os problemas oriundos dessa atividade precisam ser melhor compreendidos , e é justamente na definição e estudo dessas dificuldades que este trabalho se dedica . O confronto da teoria de engenharia de software com observações práticas conduz para a melhor definição de quais são os problemas típicos de manutenção de software e do que se dispõe para abordá-los . Finalmente , com base no entendimento formado sobre os problemas , neste trabalho são apresentdas diretrizes para guiar a elaboração de uma disciplina específica de manutenção de software para cursos de graduação na área de 
 A tendência de integração de diversos componentes em um único chip tem proporcionado um aumento da complexidade dos sistemas computacionais . Tanto as indústrias quanto o meio acadêmico estão em busca de técnicas que possibilitem diminuir o tempo e o esforço gastos com a verificação no processo de desenvolvimento de hardware , a fim de garantir qualidade , robustez e confiabilidade a esses dispositivos . De forma a contribuir para várias aplicações envolvendo a verificação de sistemas , tais como busca por erros de projeto , avaliação de desempenho , otimização de algoritmos e extração de dados do sistema , o presente_trabalho  propõe um sistema de monitoramento baseado em computação reconfigurável , capaz de observar de forma não intrusiva o comportamento de um SoC ( System-on-Chip ) em tempo de execução . Tal sistema é composto por um módulo de monitoramento responsável por captar informações de execução de software em um processador embarcado e uma ferramenta de análise , chamada ACAD , que interpreta esses dados . Por meio da realização de experimentos , verificou-se que o sistema desenvolvido foi capaz de fornecer dados fiéis sobre a quantidade de acessos a memória ou a outros periféricos , tempos de execução de porções ( ou a totalidade ) do código e número de vezes que cada instrução foi executada . Esses resultados permitem traçar , de maneira precisa , o comportamento de um software executado no processador softcore Nios_II  , contribuindo assim para facilitar o processo de verificação em sistemas baseados em computação 
 Ambientes e sistemas de apoio ao ensino e aprendizado estão sendo adotados como mecanismos facilitadores no processo de ensino e aprendizado . No entanto , uma limitação comum à maioria desses ambientes está relacionada ao fato de concentrarem-se apenas na criação da estrutura , armazenamento e controle de acesso ao material didático , sem oferecer qualquer tipo de suporte à atividade de modelagem do conteúdo . Nesse contexto , em trabalhos anteriores foi proposta a abordagem AIM-CID - uma abordagem integrada para modelagem de conteúdos educacionais . Dando continuidade às pesquisas já realizadas , este trabalho aborda o estudo de mecanismos de apoio à modelagem de conteúdos educacionais , propondo extensões às etapas de modelagem conceitual e instrucional da abordagem AIM-CID . A ideia é incorporar à abordagem aspectos_relacionados  ao reuso e compartilhamento de conteúdos educacionais . Além_disso  , também foi considerado no escopo deste trabalho o projeto e o desenvolvimento da AIM-Tool - uma ferramenta Web , de apoio à modelagem e geração_automática  de conteúdos educacionais , com ênfase na construção distribuída dos modelos estendidos da abordagem AIM-CID . A ferramenta fornece mecanismos para a geração_automática  dos conteúdos modelados em diferentes formatos , além de tratar aspectos_relacionados  ao compartilhamento do material gerado , por meio da adoção do padrão 
 O aprendizado semi-supervisionado é um paradigma do aprendizado de máquina no qual a hipótese é induzida aproveitando tanto os dados rotulados quantos os dados não rotulados . Este paradigma é particularmente útil quando a quantidade de exemplos_rotulados  é muito pequena e a rotulação manual dos exemplos é uma tarefa muito custosa . Nesse contexto , foi proposto o algoritmo Cotraining , que é um algoritmo muito utilizado no cenário semi-supervisionado , especialmente quando existe mais de uma visão dos dados . Esta característica do algoritmo Cotraining faz com que a sua aplicabilidade seja restrita a domínios multi-visão , o que diminui muito o potencial do algoritmo para resolver problemas reais . Nesta dissertação , é proposto o algoritmo Co2KNN , que é uma versão mono-visão do algoritmo Cotraining na qual , ao invés de combinar duas visões dos dados , combina duas estratégias diferentes de induzir classificadores utilizando a mesma visão dos dados . Tais estratégias são chamados de k-vizinhos mais próximos ( KNN ) Local e Global . No KNN Global , a vizinhança utilizada para predizer o rótulo de um exemplo não rotulado é conformada por aqueles exemplos que contém o novo exemplo entre os seus k vizinhos mais próximos . Entretanto , o KNN Local considera a estratégia tradicional do KNN para recuperar a vizinhança de um novo exemplo . A teoria do Aprendizado Semi-supervisionado Baseado em Desacordo foi utilizada para definir a base teórica do algoritmo Co2KNN , pois argumenta que para o sucesso do algoritmo Cotraining , é suficiente que os classificadores mantenham um grau de desacordo que permita o processo de aprendizado conjunto . Para avaliar o desempenho do Co2KNN , foram executados diversos experimentos que sugerem que o algoritmo Co2KNN tem melhor performance que diferentes algoritmos do estado da arte , especificamente , em domínios mono-visão . Adicionalmente , foi proposto um algoritmo otimizado para diminuir a complexidade computacional do KNN Global , permitindo o uso do Co2KNN em problemas reais de 
 Computação em nuvem refere-se a um modelo de disponibilização de recursos_computacionais  no qual a infraestrutura de software e hardware é ofertada como um serviço , e vem se estabelecendo como um paradigma de sucesso graças a versatilidade e ao custo-efetividade envolvidos nesse modelo de negócio , possibilitando o compartilhamento de um conjunto de recursos físicos entre diferentes usuários e aplicações . Com o advento da computação em nuvem e a possibilidade de elasticidade dos recursos_computacionais  virtualizados , a alocação dinâmica de recursos vem ganhando destaque , e com ela as questões referentes ao estabelecimento de contratos e de de qualidade de serviço . Historicamente , as pesquisas em QoS concentram-se na solução de problemas que envolvem duas entidades : usuários e servidores . Entretanto , em ambientes de nuvem , uma terceira entidade passa a fazer parte dessa interação , o consumidor de serviços em nuvem , que usa a infraestrutura para disponibilizar algum tipo de serviço aos usuários finais e que tem recebido pouca atenção das pesquisa até o momento , principalmente no que tange ao desenvolvimento de mecanismos automáticos para a alocação dinâmica de recursos sob variação de demanda . Este trabalho consiste na proposta de uma arquitetura de gerenciamento adaptativo de recursos sob a perspectiva do modelo de negócio envolvendo três entidades , focada na eficiência do consumidor . O trabalho inspira-se em técnicas de controle realimentado para encontrar soluções adaptativas aos problemas de alocação dinâmica de recursos , resultando em uma arquitetura de broker de consumidor , um respectivo protótipo e um método de projeto de controle para sistemas computacionais dessa 
 A velocidade das mudanças e a necessidade de informações disponíveis em vários meios têm feito com que os sistemas atuais se integrem cada vez mais . Neste cenário , arquiteturas orientadas a serviços e Web_Services  ganham evidência . Por utilizarem padrões já bem difundidos , por exemplo XML ( extended markup language ) , Web_Services  se destacam pela interoperabilidade e dinamismo , disponibilizados através da Internet e utilizado em ambientes heterogêneos . Contudo , isso afeta significativamente atributos não funcionais , por exemplo , confiabilidade , disponibilidade , entre outros . Para Web_Services  , além de todos os problemas existentes no paradigma de programação distribuída , temos ainda o problema de que erros em um determinado serviço não devem ser propagados para os demais . Dentre as diversas formas de se contornar o problema , existe a definição de padrões transacionais que visam delinear regras específicas para a comunicação entre os serviços . O objetivo do trabalho é identificar casos de testes que permitam validar a adequação , tanto do serviço quanto da aplicação de padrões definidos , de modo a garantir uma maior confiabilidade do sistema como um todo . Para isso , é avaliada a aplicabilidade de técnicas de perturbação e mutação de dados com relação ao padrão estabelecido . Como resultado , obtém-se um subconjunto das técnicas que possuem potencial de identificação de falhas nesse cenário 
 Teste de propriedades em grafos consiste no estudo de algoritmos aleatórios sublineares que determinam se um grafo $ G $ de entrada com $ n $ vértices satisfaz uma dada propriedade ou se é necessário adicionar ou remover mais do que $ \epsilon { n \choose 2 } $ arestas para fazer $ G $ satisfazê-la , para algum parâmetro $ \epsilon $ de erro fixo . Uma propriedade de grafos $ P $ é dita testável se , para todo $ \epsilon > 0 $ , existe um tal algoritmo para $ P $ cujo tempo de execução é independente de $ n $ . Um dos resultados de maior importância nesta área , provado por Alon e Shapira , afirma que toda propriedade hereditária de grafos é testável . Neste trabalho , apresentamos resultados análogos para torneios --_-  grafos completos nos quais são dadas orientações para cada aresta 
 O crescimento acelerado da Internet como meio de se obter informações dentro dos mais diversos_domínios  de aplicações torna necessária a incorporação de técnicas visuais que auxiliem o usuário a interagir com esse vasto universo de informações de forma eficaz e intuitiva . A Visualização de Informação é uma área de pesquisa que estuda o uso de representações_visuais  e interativas de informações com o objetivo de diminuir a sobrecarga cognitiva dos usuários ao interpretá-las . Os objetivos deste trabalho são investigar e implementar mecanismos que permitem ao usuário utilizar a Web para visualizar informações evolucionárias - informações que crescem ao longo do tempo e se relacionam . Diversas técnicas de visualização existentes foram pesquisadas . Foi investigada a utilização de padrões XML ( Extensible Markup Language ) para geração de representações gráficas através de documentos SVG ( Scalable Vector Graphics ) . Como resultado , foi definida a infra-estratura iVIEW - infra-estrutura de Visualização de Informação Evolucionária na Web - que provê um mecanismo para visualização de informação evolucionária através do processamento de documentos estruturados 
 Tecnologias recentes , como a de assistentes pessoais digitais ( PDAs ) , trazem uma série de questões no que tange novos recursos , limitações , possibilidades de utilização e , fatores técnicos relacionados o que frequentemente demanda amplo tempo de aprendizado . Além de lidar com essas questões , pessoas ligadas à produção de sistemas de software para PDAs precisam também conviver com a diversidade de possibilidades de desenvolvimento e , principalmente com a velocidade com que a tecnologia se modifica . Em tal conjuntura , pesquisas que auxiliem no processo de desenvolvimento , equacionando tempo gasto e resultados efetivos constituem contribuições importantes . Neste trabalho , propõe-se a ferramenta mCards , com mecanismos visuais de interação , que permite a criação de aplicações , baseadas em cards , para assistentes pessoais digitais 
 A computação ubíqua é uma área de pesquisa que envolve a integração de tecnologia de forma transparente às atividades diárias das pessoas . Dois dos principais temas da computação ubíqua são aplicações de captura e acesso e a computação ciente de contexto . O primeiro tema investiga o apoio à construção de aplicações que capturam informações relativas a uma experiência ao vivo com o objetivo de possibilitar posterior acesso à informação capturada , muitas_vezes  na forma de documentos Web . O segundo tema visa a facilitar a interação do usuário com o computador : ela descreve uma situação em que o dispositivo computacional está ciente das condições do usuário e adapta seu procedimento baseado em informações chamadas de informações de contexto . Uma lacuna identificada em relação a aplicações de captura e acesso é que , muitas_vezes  , tais aplicações são desenvolvidas sem atenção para um possível reuso , por parte de outras aplicações , do material capturado . Nesse sentido , um problema tratado nesta dissertação é ausência de propostas de reuso de informações armazenadas por aplicações de captura e acesso . Em outras_palavras  , há a demanda pela incorporação de metadados à informação capturada , os quais viabilizam o intercâmbio e reuso das informações . Nesse cenário , o trabalho tem por objetivo propor um mecanismo que permita incorporar metadados à informação capturada de modo que : quaisquer metadados manipulados sejam padronizados ; metadados possam ser associados de modo automático a permitir intercâmbio ; metadados possam ser adicionados sob_demanda  do usuário . Como resultado das investigações , foram realizadas implementações que , partindo de informações capturadas pelo sistema de captura e acesso iClass , permitem a criação de objetos de aprendizagem de acordo com o padrão de metadados para objetos de aprendizagem SCORM tanto de modo automático como manual , fazendo_uso  de informações de contexto armazenadas no Web Service Context Kernel . Tanto o iClass como o Context Kernel são ambientes desenvolvidos no Laboratório Intermídia , no qual este trabalho foi realizado 
 A computação ubíqua tem se estabelecido como um novo paradigma de interação usuário-computador ; sua essência está em auxiliar atividades humanas de modo minimamente intrusivo , permitindo a integração transparente de tecnologia no cotidiano dos indivíduos ( Weiser 1991 ) . Um tema frequentemente explorado dentro da computação ubíqua é a construção de aplicações de captura acesso ( Abowd and Mynatt 2000 ; Abowd et_al  . 2002 ) . Gerenciando um conjunto de dispositivos ubíquos que inclui lousas eletrônicas , câmeras de vídeo e microfones , essas aplicações realizam a captura das mais diferentes experiências humanas , desde reuniões de trabalho até atividades educacionais em sala de aula , com o objetivo de permitir o posterior acesso ao conteúdo capturado . O trabalho desenvolvido nesta tese tem como objetivo geral prover facilidades infra-estruturais e de serviços de modo a facilitar o desenvolvimento dessas aplicações . Trabalhos anteriores identificaram a necessidade de oferecer suporte de comunicação e funcionalidades às aplicações de captura e acesso . Esta tese tem como objetivos específicos tratar problemas associados à representação e ao gerenciamento da informação produzida pelas aplicações . Para tanto , foram definidas abstrações de alto nível capazes de facilitar o intercâmbio da informação capturada , seu armazenamento e sua apresentação na Web . O efetivo suporte à estruturação e ao gerenciamento dos dados manipulados pelas aplicações de captura e acesso demanda uma modelagem consistente das informações de contexto específicas de cada aplicação . No sentido de apoiar essa modelagem , desenvolveu-se o Coritextractor , metodologia que permite associar informações presentes no modelo conceituai das aplicações às dimensões de contexto comumente utilizadas por aplicações de captura e acesso . Entre seus resultados , o Contextractor produz esquemas XML usados como blocos arquiteturais na definição de uma infra-estrutura de software que oferece abstrações de alto nível para o desenvolvimento de aplicações de captura e acesso . Tais abstrações incluem : um modelo de desenvolvimento de aplicações baseado em componentes de software ; um serviço capaz de oferecer facilidades de armazenamento e apresentação de conteúdo capturado de maneira transparente ; e uma linguagem para consulta de conteúdo capturado baseada nas informações de contexto da aplicação . A metodologia de modelagem , o modelo de desenvolvimento e os serviços de gerenciamento de conteúdo , resultados desta tese , são suportados por um conjunto de softwares capazes de apoiar o desenvolvimento de aplicações voltadas a diferentes domínios de captura e acesso . As abstrações desenvolvidas possibilitaram a construção de diversas aplicações , algumas das quais vêm_sendo  utilizadas de maneira contínua há mais de três anos . A transformação de protótipos em aplicações robustas consiste em uma importante forma de validação do trabalho desenvolvido 
 A otimização_linear  tem sido objeto de intenso estudo desde a publicação do método simplex de Dantzig em 1947 , sendo revigorada a partir de 1984 com a publicação de um método de pontos_interiores  por Karmarkar , o qual demonstrou ser computacionalmente eficiente e com propriedade de convergência polinomial no estudo do pior caso . Embora muitas variantes do método simplex não tenham complexidade polinomial , elas apresentam um comportamento polinomial em termos do número de restrições do problema , para inúmeros problemas práticos , constituindo o chamado `` folclore '' ' simplex . Nos últimos_anos  , tem crescido o interesse pela pesquisa sobre eficiência dos métodos tipo simplex . Há uma pergunta subjacente , que talvez constitua o maior desafio da atualidade na teoria da otimização_linear  : `` E possível construir um algoritmo tipo simplex com complexidade polinomial ? Além_disso  , eficiente do ponto de vista prático ? '' ' A resposta a esta pergunta não deve ser trivial e talvez seja negativa , restando por enquanto a tarefa árdua da investigação da complexidade , caso a caso , dos métodos propostos . Neste trabalho aprofundamos a investigação sobre a versão mais utilizada dos métodos do tipo simplex : o método dual simplex , especializado para a forma geral ( restrições canalizadas ) , cujo problema dual é linear por partes . A importância da forma geral não somente porque as demais formas são facilmente representadas nela , mas porque muitos problemas práticos surgem naturalmente desta maneira e técnicas de pré-processamento que buscam apertar limitantes levam a ela . Foram investigadas buscas unidimensional lineares por partes , como regras anti-ciclagem influenciam positivamente sobre o eleito 'estagnação ' decorrente de soluções degeneradas , a regra de Dantzig normalizada e algumas técnicas de resolução de sistemas_lineares  , incluindo o método do gradiente bi-conjugado , que alimenta grande expectativa no aumento da eficiência computacional para resolução de problemas de grande porte 
 Processamento de imagens pode ser usado para resolver problemas em diversas_áreas  , como imagens médicas , processamento de documentos e segmentação de objetos . Operadores de imagens normalmente são construídos combinando diversos operadores elementares e ajustando seus parâmetros . Uma abordagem alternativa é a estimação de operadores de imagens a partir de pares de exemplos contendo uma imagem de entrada e o resultado esperado . Restringindo os operadores considerados para o que são invariantes à translação e localmente definidos ( $ W $ -operadores ) , podemos aplicar técnicas de Aprendizagem de Máquina para estimá-los . O formato que define quais vizinhos são usadas é chamado de janela . $ W $ -operadores treinados com janelas grandes frequentemente tem problemas de generalização , pois necessitam de grandes_conjuntos  de treinamento . Este problema é ainda mais grave ao treinar operadores em níveis de cinza . Apesar de técnicas como o projeto dois_níveis  , que combina a saída de diversos operadores treinados com janelas menores , mitigar em parte estes problemas , uma determinação de parâmetros complexa é necessária . Neste trabalho apresentamos duas técnicas que permitem o treinamento de operadores usando janelas grandes . A primeira , KA , é baseada em Máquinas de Suporte Vetorial ( SVM ) e utiliza técnicas de aproximação de kernels para realizar o treinamento de $ W $ -operadores . Uma escolha adequada de kernels permite o treinamento de operadores níveis de cinza e binários . A segunda técnica , NILC , permite a criação automática de combinações de operadores de imagens . Este método utiliza uma técnica de otimização específica para casos em que o número de características é muito grande . Ambos métodos obtiveram resultados competitivos com algoritmos da literatura em dois domínio de aplicação diferentes . O primeiro , Staff Removal , é um processamento de documentos binários frequente em sistemas de reconhecimento ótico de partituras . O segundo é um problema de segmentação de vasos sanguíneos em imagens em níveis de cinza 
 Redes complexas é um campo de pesquisa científica recente e bastante ativo que estuda redes de larga_escala  com estruturas topológicas não triviais , tais como redes de computadores , redes de telecomunicações , redes de transporte , redes_sociais  e redes biológicas . Muitas destas redes são naturalmente divididas em comunidades ou módulos e , portanto , descobrir a estrutura dessas comunidades é um dos principais problemas abordados no estudo de redes complexas . Tal problema está relacionado com o campo de aprendizado de máquina , que tem como interesse projetar e desenvolver algoritmos e técnicas que permitem aos computadores aprender , ou melhorar seu desempenho através da experiência . Alguns dos problemas identificados nas técnicas tradicionais de aprendizado incluem : dificuldades em identificar formas irregulares no espaço de atributos ; descobrir estruturas sobrepostas de grupos ou classes , que ocorre quando elementos pertencem a mais de um grupo ou classe ; e a alta complexidade computacional de alguns modelos , que impedem sua aplicação em bases de dados maiores . Neste trabalho tratamos tais problemas através do desenvolvimento de novos modelos de aprendizado de máquina utilizando redes complexas e dinâmica espaço-temporal , com capacidade para tratar grupos e classes sobrepostas , além de fornecer graus de pertinência para cada elemento da rede com relação a cada cluster ou classe . Os modelos desenvolvidos tem desempenho similar ao de algoritmos do estado da arte , ao mesmo tempo em que apresentam ordem de complexidade computacional menor do que a maioria 
 Avaliaçõs educacionais , de distúrbios psicológicos e da aceitação de um produto no mercado são exemplos de estudos que buscam quantificar um construto de interesse através de questionários compostos por itens de múltipla escolha . A Teoria de Resposta ao Item ( TRI ) é muito utilizada na análise de dados provenientes da aplicação desses questionários . Há vários modelos da TRI já muito utilizados na prática com tal finalidade , tanto para respostas dicotômicas aos itens ( certo/errado , presente/ausente , sim/não ) , quanto para itens com mais de duas categorias de resposta ( nominais ou ordinais ) . No entanto , a grande maioria supôe que apenas um traço latente é necessário para explicar a probabilidade de resposta ao item ( modelos unidimensionais ) . Como as situações práticas são usualmente caracterizadas por várias aptidões ( traços latentes ) influenciando a probabilidade de um indivíduo apresentar certa resposta ao item , os modelos multidimensionais são de grande_importância  . Neste trabalho , após um levantamento bibliográfico dos principais modelos multidimensionais da TRI existentes na literatura , realizou-se um estudo detalhado de um deles : o modelo logístico multidimensional de dois parâmetros . O método de estimação dos parâmetros dos itens por máxima_verossimilhança  marginal e dos traços latentes por máxima_verossimilhança  são explicitados assim como a estimação por métodos bayesianos . Todos os métodos foram implementados em R , comparados e aplicados a um conjunto de dados reais para avaliação do Inventário de Depressão de Beck ( BDI ) e do Exame Nacional do Ensino Médio ( ENEM 
 Neste trabalho apresentamos uma nova abordagem para a exploração de fibras neurais a partir de imagens de tensores de difusão . A estratégia combina técnicas de visualização de informação e visualização científica para obter uma rápida e precisa interpretação das fibras . Para isto fazemos uma transformação das fibras para vetores , e utilizamos uma nova_técnica  de projeção multi-dimensional ( P-LSP ) , para trabalhar com conjuntos grandes de dados . A exploração do espaço das fibras é feita através desta projeção . Além_disso  , é apresentada a extensão das técnicas LSP e P-LSP para criar projeções em 3D , assim como estratégias que permitem interagir com pontos em 3D . Outra contribuição deste trabalho é a modificação de um método apresentado para criar superfícies de densidade fechadas sobre pontos esparsos . Esta modificação torna possível criar superfícies sobre conjuntos de pontos maiores com uma qualidade aceitável , o que é utilizado para representar conjunto de fibras como uma superfície . Esta nova abordagem é comparada com trabalhos similares mostrando nossas vantagens em termos de tempo de processamento , qualidade e funcionalidades para analisar esta categoria de 
 O cada vez maior número de tecnologias que fornecem serviços de geolocalização tem possibilitado gerar uma grande quantidade de dados de geolocalização . Estes dados , são armazenados principalmente como pontos de localização com informação temporal . Uma trajetória é definida como uma sequência discreta e finita destes pontos de localização . Nos últimos_anos  , a recente área de mineração de trajetórias visa aproveitar esta abundância de dados . Nesta área , existem várias técnicas de mineração desenvolvidas , mas todas elas dependem diretamente da qualidade das trajetórias . Assim , o preprocessamento tem um papel primordial na mineração de trajetórias . Entre as tarefas de preprocessamento , um problema relevante é a reconstrução ou inferência de trajetórias . Devido ao alto consumo de energia de dispositivos de localização como o GPS e ao crescente uso de geo-marcações nas redes_sociais  , que possibilita a construção de trajetórias ordenando temporalmente estas marcações , muitas das trajetórias existentes apresentam taxas de amostragem muito baixas . A maioria das pesquisas nesse problema utilizam , no caso de áreas urbanizadas , informações do grafo formado por ruas e cruzamentos . Porém , elas levam em conta apenas trajetórias de veículos principalmente pelo fato que muitos dos percursos dos pedestres ficam fora das ruas . Atualmente , graças às plataformas livres de mapas colaborativos , é possível incluir estes trajetos como parte das informações de ruas . Assim , este projeto tem o objetivo de investigar o uso das informações das ruas na reconstrução de trajetórias , principalmente de pedestres . O escopo da proposta compreende o desenvolvimento de uma rede social geo-localizada com o intuito de capturar dados de localização . Posteriormente , estes dados serão anonimizados , utilizados na reconstrução de trajetórias de pedestres e disponibilizados para uso em pesquisas futuras 
 Dentro da classe de problemas de corte e empacotamento , existem os problemas de corte de itens_irregulares  ( não-circulares e não-retangulares ) , os quais visam determinar um arranjo ótimo de objetos irregulares menores ( itens ) , sem sobreposição , dentro de objetos maiores ( recipientes ) a fim de atender a uma demanda . Possuem grande_importância  prática , uma vez que surgem em vários_tipos  de indústrias , como a têxtil , a de móveis e a de calçados , por exemplo . Entre estes problemas , ainda temos o chamado problema de corte de itens_irregulares  em recipientes , no qual estes últimos são fechados , isto é , possuem dimensões fixas , podendo ser retangulares ou irregulares . Neste caso , o objetivo é arranjar todos os itens de modo a utilizar o menor número possível de recipientes . A estes problemas , uma outra restrição ainda pode ser adicionada : os recipientes podem ter defeitos , isto é , áreas onde não pode ser posicionado qualquer item , e regiões com diferentes níveis de qualidade , chamadas de zonas de qualidades , em que apenas determinados itens podem ser alocados . Neste trabalho , portanto , introduzimos um conjunto de heurísticas construtivas para a resolução do problema de corte de itens_irregulares  em recipientes irregulares com defeitos e zonas de qualidades . Os experimentos computacionais foram realizados utilizando um conjunto com 15 instâncias adaptadas de outro problema de corte de itens_irregulares  , uma vez que não encontramos instâncias disponíveis na literatura para o problema abordado neste trabalho . Os resultados mostraram que todos os métodos são capazes de resolver o problema em um tempo computacional considerado baixo , sendo que alguns deles apresentam melhor desempenho que outros 
 Imagética motora é um processo mental que produz modulações na amplitude dos sinas de eletroencefalogramas em progresso . Os padrões presentes nestas modulações podem ser usados para classificar este processo mental , mas a identificação destes padrões não é uma tarefa trivial , porque eles estão presentes em bandas de frequências que são específicas para cada pessoa . Neste trabalho , apresenta-se um novo método para selecionar as bandas de frequência específicas para cada pessoa baseado na arquitetura do método Filter Bank Common Spatial Pattern . Para selecionar as bandas de frequência mais relevantes para cada pessoa , o método proposto aplica uma busca exaustiva para encontrar o melhor subconjunto de bandas de frequência contendo os padrões mais discriminativos dentro de um espaço de busca restrito a um tamanho fixo para este subconjunto . Esse tamanho é determinado usando validação cruzada e o método Sequential Forward Floating Selection . O método proposto foi avaliado usando a base de dados pública 2b da BCI Competition IV , mostrando melhores_resultados  do que todos os métodos também avaliados nessa base de dados 
 Várias atividades humanas dependem da previsão do tempo . Algumas delas são transporte , saúde , trabalho , segurança e agricultura . Tais atividades exigem solucões computacionais para previsão do tempo através de modelos numéricos . Estes modelos numéricos devem ser precisos e ágeis para serem processados no computador.Este projeto visa portar uma pequena parte do software do modelo de previsão de tempo do Brasil , o BRAMSBrazilian developments on the Regional Atmospheric Modelling Systempara uma arquitetura heterogênea composta por processadores Xeon ( Intel ) acoplados a um circuito reprogramável em FPGA via barramento PCIe . De acordo com os estudos , o termo da química da equação de continuidade da massa é o termo mais caro computacionalmente . Este termo calcula várias equações lineares do tipo Ax = b. Deste modo , este trabalho implementou estas equações em hardware , provendo um ´codigo portável e paralelo na linguagem OpenCL . O framework OpenCL também nos permitiu acoplar o código legado do BRAMS em Fortran90 junto com o hardware desenvolvido . Embora as ferramentas de desenvolvimento tenham apresentado vários problemas , a solução implementada mostrou-se viável com a exploração de técnicas de paralelismo . Entretando sua perfomance ficou muito aquém do desejado 
 Transformada de distância euclidiana ( TDE ) é a operação que converte uma imagem binária composta de pontos de objeto e de fundo em outra , chamada mapa de distâncias euclidianas , onde o valor armazenado em cada ponto corresponde à menor distância euclidiana entre este ponto e o fundo da imagem . A TDE é muito utilizada em visão_computacional  , análise de imagens e robótica , mas é uma transformação muito demorada , principalmente em imagens 3-D. Neste trabalho são utilizados dois tipos de computadores paralelos , ( i ) multiprocessadores simétricos ( SMPs ) e ( ii ) agregados de computadores , para reduzir o tempo de execução da TDE . Dois algoritmos de TDE são paralelizados . O primeiro , um algoritmo de TDE por varredura independente , é paralelizado em um SMP e em um agregado . O segundo , um algoritmo de TDE por propagação ordenada , é paralelizado no agregado 
 À medida que a ciência avança , muitas aplicações em diferentes áreas precisam de grande poder_computacional  . A computação em grade é uma importante alternativa para a obtenção de alto poder de processamento , no entanto , esse alto poder_computacional  deve ser bem aproveitado . Mediante o uso de técnicas de escalonamento especializadas , os recursos podem ser utilizados adequadamente . Atualmente existem vários algoritmos propostos para computação em grade , portanto , é necessário seguir uma boa metodologia para escolher o algoritmo que ofereça melhor desempenho , dadas determinadas características . No presente_trabalho  comparamos os algoritmos de escalonamento : Heterogeneous Earliest Finish Time ( HEFT ) , ( b ) Critical Path on a Processor ( CPOP ) e ( c ) Path Clustering Heuristic ( PCH ) ; cada algoritmo é avaliado com diferentes aplicações e sobre diferentes arquiteturas usando técnicas de simulação , seguindo quatro critérios : ( i ) desempenho , ( ii ) escalabilidade , ( iii ) adaptabilidade e ( iv ) distribuição da carga do trabalho . Diferenciamos as aplicações para grade em dois tipos : ( i ) aplicações regulares e ( ii ) aplicações irregulares ; dado que em aplicações irregulares não é facil comparar o critério de escalabilidade . Seguindo esse conjunto de critérios o algoritmo HEFT possui o melhor desempenho e escalabilidade ; enquanto que os três algoritmos possuem o mesmo nível de adaptabilidade . Na distribuição de carga de trabalho o algoritmo HEFT aproveita melhor os recursos do que os outros . Por outro_lado  os algoritmos CPOP e PCH usam a técnica de escalonar o caminho crítico no processador que ofereça o melhor tempo de término , mas essa abordagem nem sempre é a mais adequada 
 Os softwares de Geometria Interativa ( GI ) foram desenvolvidos com o objetivo de possibilitar aos estudantes explorarem a geometria por meio do computador , permitindo a construção e manipulação de objetos geométricos , como retas , pontos e circunferências . Por meio da interface destes softwares , os estudantes são capazes de compreender e manipular os objetos geométricos , podendo influenciar na aprendizagem da geometria . Para Laborde ( 2007 ) , os desenvolvedores de softwares de GI deveriam se preocupar com os aspectos pedagógicos e com o design da interface , pois , as interfaces indevidamente projetadas podem causar frustrações aos alunos , fazendo com que sintam dificuldade em utilizar o software e terminando por não direcionar o aprendizado da geometria . Apesar desta importância , há poucos_estudos  que investigam metodologias que relacionam os conceitos pedagógicos e os recursos_computacionais  . Além disto , há uma carência de pesquisas sobre diferentes_tipos  de interações disponíveis para a criação de interfaces para softwares de GI . A fim de preencher esta lacuna , este trabalho de mestrado tem como objetivo investigar metodologias para o desenvolvimento de gestos para interfaces gestuais em softwares de GI . É proposto um novo tipo de interação para estes softwares , com o objetivo de apoiar a aprendizagem da geometria por meio das interfaces gestuais 
 O custo do acesso exato a dados complexos tende a ser muito alto , do ponto de vista da carga de processamento computacional . Além_disso  , a operação de busca em dados multimídia não é efetuada realmente sobre os dados originais , mas sobre características_extraídas  desses dados , as quais os descrevem . Por exemplo , na busca por imagens similares utilizando-se histogramas de cor , realizando uma consulta exata , o que se obtém são as imagens cujos histogramas são exatamente os mais similares aos da imagem referenciada 11a consulta , mas isso não implica necessariamente que se obtenha as imagens que atendam exatamente a consulta efetuada , pois as imagens recuperadas podem ser muito diferentes quanto a forma , por exemplo . Portanto , em muitas aplicações que acessam dados complexos , a recuperação exata deixa de um requisito fundamental , podendo a exatidão das respostas ser trocada por um melhor desempenho Neste trabalho foram desenvolvidos algoritmos para recuperação aproximada do conjunto-resposta de consultas por similaridade em domínios métricos utilizando algoritmos genéticos . Neste trabalho , com a utilização de algoritmos genéticos , foram desenvolvidas técnicas de recuperação aproximada de dados cm domínio métrico , cujo refinamento das respostas obtidas é dependente do tempo de processamento disponível , definido pelo usuário . Os algoritmos desenvolvidos foram implementados utilizando a Slim-tree , mas outros métodos de acesso podem ser extendidos para utilizá-los também . Os algoritmos contemplam consultas por abrangência e vizinhos mais próximos , além de algumas variações . Os algoritmos desenvolvidos foram testados e validados com conjuntos de dados sintéticos e reais , e mostraram-se capazes de obter respostas aproximadas com boa precisão utilizando apenas uma fração do tempo exigido pela consulta exata . Os resultados obtidos nos experimentos mostram que é possível_obter  respostas com precisão superior a 90 % utilizando apenas metade do tempo da consulta exata com o algoritmo convencional , e até 65 % de precisão com apenas um quinto do tempo da consulta exata 
 Teste de software desempenha um papel fundamental no processo de produção de um produto de software de qualidade . Com o passar dos anos , diversas técnicas e critérios de teste de software foram desenvolvidos a fim de estabelecer meios e métricas para guiar a criação de casos de teste efetivos , capazes de evidenciar defeitos no produto avaliado . Dentre os principais critérios para teste de software está o Teste de Mutação , que foi amplamente difundido e é tido como uma das abordagens mais eficazes para guiar a criação de conjuntos de casos de teste capazes de revelar defeitos em software . Entretanto , à medida que esse critério possui uma grande efetividade para revelar defeitos , ele peca pelo baixo poder de escalabilidade , o que acaba comprometendo diretamente a sua capacidade de aplicação . Neste sentido , diversos estudos foram desenvolvidos nesta área dedicando-se a aprimorar o seu desempenho e torná-lo uma alternativa viável para aplicação durante a fase de teste de software . Este trabalho apresenta indícios de que a utilização de estruturas complexas de processamento pode apoiar a aplicação do Teste de Mutação . Para tal , foi concebida uma arquitetura que possibilite a aplicação do Teste de Mutação em paralelo . Após a implementação da arquitetura foram avaliados cinco algoritmos de balanceamento de carga responsáveis por controlar a distribuição e execução do Teste de Mutação . Durante a avaliação_experimental  da arquitetura e dos algoritmos , observou-se que nos piores cenários avaliados foi possível atingir um ganho de desempenho acima de 70 % em relação à aplicação sequencial convencional do Teste de Mutação enquanto nos melhores cenários o ganho de desempenho foi acima 95 % , contudo , necessitando utilizar-se de uma infraestrutura mais robusta para a execução da arquitetura 
 Um dos maiores desafios em Mineração de Dados é a integração da informação temporal ao seu processo . Esse fato tem desafiado profissionais de diferentes domínios de aplicação e recebido investimentos consideráveis da comunidade_científica  e empresarial . No contexto de predição de Séries Temporais , os investimentos se concentram no subsídio de pesquisas destinadas à adaptação dos métodos convencionais de Aprendizado de Máquina para a análise de dados na qual o tempo constitui um fator importante . À vista disso , neste trabalho é proposta uma nova extensão do algoritmo de Aprendizado de Máquina k-Nearest Neighbors ( kNN ) para predição de Séries Temporais , intitulado de kNN - Time Series Prediction with Invariances ( kNN-TSPI ) . O algoritmo concebido difere da versão convencional pela incorporação de três técnicas para obtenção de invariância à amplitude e deslocamento , invariância à complexidade e tratamento de casamentos triviais . Como demonstrado ao longo desta dissertação de mestrado , o uso simultâneo dessas técnicas proporciona ao kNN-TSPI uma melhor correspondência entre as subsequências de dados e a consulta de referência . Os resultados de uma das avaliações empíricas mais extensas , imparciais e compreensíveis já conduzidas no tema de predição de Séries Temporais evidenciaram , a partir do confronto de dez métodos de projeção , que o algoritmo kNN-TSPI , além de ser conveniente para a predição automática de dados a curto prazo , é competitivo com os métodos estatísticos estado-da-arte ARIMA e SARIMA . Por mais que o modelo SARIMA tenha atingido uma precisão relativamente superior a do método baseado em similaridade , o kNN-TSPI é consideravelmente mais simples de ajustar . A comparação objetiva e subjetiva entre algoritmos estatísticos e de Aprendizado de Máquina para a projeção de dados temporais vem a suprir uma importante lacuna na literatura , a qual foi identificada por meio de uma revisão_sistemática  seguida de uma meta-análise das publicações selecionadas . Os 95 conjuntos de dados empregados nos experimentos computacionais juntamente com todas as projeções analisadas em termos de Erro Quadrático Médio , coeficiente U de Theil e taxa de acerto Prediction Of Change In Direction encontram-se disponíveis no portal Web ICMC-USP Time Series Prediction Repository . A presente pesquisa abrange também contribuições e resultados significativos em relação às propriedades inerentes à predição baseada em similaridade , sobretudo do ponto de vista prático . Os protocolos experimentais delineados e as diversas conclusões obtidas poderão ser usados como referência para guiar o processo de escolha de modelos , configuração de parâmetros e aplicação dos algoritmos de Inteligência_Artificial  para predição de Séries Temporais 
 Sistemas de Recomendação ( SR ) vêm se apresentando como poderosas ferramentas para portais web tais como sítios de comércio_eletrônico  . Para fazer suas recomendações , os SR se utilizam de fontes de dados variadas , as quais capturam as características dos usuários , dos itens e suas transações , bem como de modelos de predição . Dada a grande quantidade de dados envolvidos , é improvável que todas as recomendações possam ser bem representadas por um único modelo global de predição . Um outro importante aspecto a ser observado é o problema conhecido por cold-start , que apesar dos avanços na área de SR , é ainda uma questão relevante que merece uma maior atenção . O problema está relacionado com a falta de informação prévia sobre novos usuários ou novos itens do sistema . Esta tese apresenta uma abordagem híbrida de recomendação capaz de lidar com situações extremas de cold-start . A abordagem foi desenvolvida com base no algoritmo SCOAL ( Simultaneous Co-Clustering and Learning ) . Na sua versão original , baseada em múltiplos modelos lineares de predição , o algoritmo SCOAL mostrou-se eficiente e versátil , podendo ser utilizado numa ampla gama de problemas de classificação e/ou regressão . Para melhorar o algoritmo SCOAL no sentido de deixá-lo mais versátil por meio do uso de modelos não lineares , esta tese apresenta uma variante do algoritmo SCOAL que utiliza modelos de predição baseados em Máquinas de Aprendizado Extremo . Além da capacidade de predição , um outro fator que deve ser levado em consideração no desenvolvimento de SR é a escalabilidade do sistema . Neste sentido , foi desenvolvida uma versão paralela do algoritmo SCOAL baseada em OpenMP , que minimiza o tempo envolvido no cálculo dos modelos de predição . Experimentos computacionais controlados , por meio de bases de dados amplamente usadas na prática , comprovam que todos os desenvolvimentos propostos tornam o SCOAL ainda mais atraente para aplicações práticas variadas 
 A biometria é a ciência que estuda a mensuração dos seres vivos . Muitos trabalhos exploram as características dos seres_humanos  tais como , impressão digital , íris e face , a fim de desenvolver sistemas biométricos , utilizados em diversas aplicações ( monitoramento de segurança , computação ubíqua , robótica ) . O reconhecimento de faces é uma das técnicas biométricas mais investigadas , por ser bastante intuitiva e menos invasiva que as demais . Alguns trabalhos envolvendo essa técnica se preocupam apenas em localizar a face de um indivíduo ( fazer a contagem de pessoas ) , enquanto outros tentam identificá-lo a partir de uma imagem . Este trabalho propõe uma abordagem capaz de identificar faces a partir de quadros de vídeo e , posteriormente , reconhecê-las por meio de técnicas de análise de imagens . Pode-se dividir o trabalho em dois módulos principais : ( 1 ) - Localização e rastreamento de faces em uma seqüência de imagens ( frames ) , além de separar a região rastreada da imagem ; ( 2 ) - Reconhecimento de faces , identificando a qual pessoa pertence . Para a primeira etapa foi implementado um sistema de análise de movimento ( baseado em subtração de quadros ) que possibilitou localizar , rastrear e captar imagens da face de um indivíduo usando uma câmera de vídeo . Para a segunda etapa foram implementados os módulos de redução de informações ( técnica Principal Component Analysis - PCA ) , de extração de características ( transformada wavelet de Gabor ) , e o de classificação e identificação de face ( distância Euclidiana e Support Vector Machine - SVM ) . Utilizando-se duas bases de dados de faces ( FERET e uma própria - Própria ) , foram realizados testes para avaliar o sistema de reconhecimento implementado . Os resultados encontrados foram satisfatórios , atingindo 91,92 % e 100,00 % de taxa de acertos para as bases FERET e Própria , respectivamente 
 A Visualização Computacional trata de técnicas para representar e interagir graficamente com dados complexos , em geral de alta dimensionalidade . Dados de alta dimensionalidade são caracterizados por pontos representados em espaços vetoriais de alta dimensão , cada coordenada representando um atributo do vetor . Num grande número de aplicações da visualização multidimensional uma medida de similaridade existe entre esses vetores . Técnicas de projeção_multidimensional  podem ser utilizadas para posicionamento desses dados num plano de forma a facilitar a interpretação das relações de similaridade . Entretanto alguns problemas dessas técnicas comprometem a interpretação dos resultados obtidos . Este trabalho identifica esses problemas e propõe , uma técnica para posicionar os pontos no plano , através da formação de árvores filogenéticas a partir de relações de similaridade . Em geral árvores filogenéticas são utilizadas para codificação de relações de ancestralidade . Um algoritmo de geração e um algoritmo de traçado dessas árvores foram implementados no contexto do sistema PEx ( Projection Explorer ) e a solução é comparada com a funcionalidade das projeções na interpretação de dados multidimensionais em geral e , em particular , na representação de coleções de documentos , uma aplicação bastante estratégica da visualização computacional e da mineração visual de 
 Sabe-se biologicamente que o nível de expressão dos genes está entre os fatores podem indicar o quanto estes estão em atividade em determinado momento . Avanços na tecnologia de microarray têm possibilitado medir os níveis de expressão de milhares de genes ao mesmo tempo . Esses dados podem ser medidos de maneira a formarem uma série_temporal  , que pode ser tratada estatisticamente para serem obtidas informações sobre as relações entre os genes . Já foram propostos vários modelos para tratar redes gênicas matematicamente . Esses modelos têm evoluído de forma a agregarem cada vez mais características das redes reais . Neste trabalho , será feita uma revisão de modelos discretos para redes de regulação gênica , primeiramente com as redes Booleanas , modelo determinístico , e depois as redes Booleanas probabilísticas e as redes genéticas probabilísticas , modelos que tratam o problema estocasticamente . Usando o último modelo citado , serão mostrados dois métodos para estimar o nível de predição entre os genes , coeficiente de determinação e informação mútua . Além de se estimar essas relações , foram desenvolvidas algumas técnicas para construir redes a partir de genes específicos , que são chamados sementes . Também serão apresentados dois desses métodos de crescimento de redes e , baseado neles , um terceiro método que foi desenvolvido neste trabalho . Foi criado um algoritmo que realiza o crescimento da rede mudando as sementes a cada iteração , agrupando estes genes em grupos com diferentes níveis de confiança , chamados camadas . O algoritmo também usa outros critérios para agregar novos genes à rede . Após a explanação desses métodos , será mostrado um software que , a partir de dados temporais de expressão_gênica  , estima as dependências entre os genes e executa o crescimento da rede em torno de genes que se deseje estudar . Também serão mostradas as melhorias feitas no programa . Ao final , serão apresentados alguns testes feitos com dados do Plasmodium falciparum , parasita causador da malária 
 Em diversas análises estatísticas , nos deparamos com dados multinomiais , dos quais precisamos analisar o comportamento ao longo do tempo e sua relação com fatores determinantes . Os métodos clássicos para modelos de regressão multinomiais consistem em utilizar a estrutura de modelos lineares generalizados para desenvolver tais modelos McCullagh & Nelder ( 1989 ) . No entanto , este enfoque apresenta algumas desvantagens como não admiter a incidência de zeros em nenhuma categoria , a hipótese da proporcionalidade da razão de chances e o fato de não serem modelos adequados para análise de dados censurados . Com o objetivo de analisar dados multinomiais com essas características propomos um modelo que é uma extensão do modelo de intensidade multiplicativo desenvolvido por Aalen ( 1978 ) e apresentado em Fleming & Harrington ( 2005 ) , para variáveis aleatórias multinomiais . Com isso , ao invés de modelarmos as probabilidades associadas às categorias , como nos métodos clássicos , modelamos a função intensidade associada à variável aleatória multinomial . Através do critério martingale , estimamos os parâmetros do modelo ajustado e propomos testes de hipóteses para estes parâmetros para uma e duas populações . O teste para comparação de duas populações é baseado na estatística de 
 O Problema do Multicorte Dirigido Mínimo é um problema clássico em otimização combinatória . Ele é NP-difícil mesmo para instâncias muito simples . Este trabalho faz uma análise dos algoritmos exatos e de aproximação para resolver o problema . Também implementa alguns desses algoritmos e compara seus desempenhos 
 Detecção de objetos é um problema clássico em visão_computacional  , presente em aplicações como vigilância automatizada , análise de imagens médicas e recuperação de informação . Dentre as abordagens existentes na literatura para resolver esse problema , destacam-se métodos baseados em reconhecimento de pontos-chave que podem ser interpretados como diferentes implementações de um mesmo arcabouço . O objetivo desta pesquisa de doutorado é desenvolver e avaliar uma versão generalizada desse arcabouço , na qual reconhecimento de pontos-chave é substituído por reconhecimento de grafos-chave . O potencial da pesquisa reside na riqueza de informação que um grafo pode apresentar antes e depois de ser reconhecido . A dificuldade da pesquisa reside nos problemas que podem ser causados por essa riqueza , como maldição da dimensionalidade e complexidade computacional . Três contribuições serão incluídas na tese : a descrição detalhada de um arcabouço para detecção de objetos baseado em grafos-chave , implementações fiéis que demonstram sua viabilidade e resultados experimentais que demonstram seu desempenho 
 A implementação das regras de negócio em sistemas de informação não é uma atividade trivial . Isso acontece porque os modelos utilizados para representá-las não determinam como elas podem ser efetivamente implementadas em situações reais . Neste trabalho é proposta um método para a transformação das regras de negócio descritas em SBVR para um modelo de implementação . Essa tradução é feita mapeando todos os elementos relevantes do modelo em SBVR em um modelo em $ \pi $ -calculus baseado em eventos . Neste modelo , existe uma representação explícita de todos os eventos que precisam ser monitorados a fim de identificar quando a regra pode ter sido violada . O objetivo é facilitar o gerenciamento das regras de negócio ao permitir que elas permaneçam separadas dos outros elementos da arquitetura e com isso possam ser gerenciadas de forma independente 
 Nesse trabalho é descrito um ambiente interativo para visualização de dados escalares e vetoriais definidos em malhas de dados estruturadas . O ambiente desenvolvido oferece uma interface de acesso para diversas classes de algoritmos de visualização implementadas na biblioteca VTK 3.1 ( The Visualization Toolkit ) . O objetivo foi desenvolver um sistema voltado para um grupo particular de usuários , pesquisadores do Grupo de Visão Cibernética do IFSC , que geram dados relativos à forma e comportamento de neurônios artificiais . Esses usuários desejam manipular diretamente os modelos gerados a partir desses dados , necessitando de recursos adicionais aos oferecidos pela interface de manipulação do VTK . O ambiente permite a aplicação de técnicas de extração de superfícies e rendering volumétrico direto aos dados escalares , bem como de técnicas de visualização vetorial . Em nível de interface , foram implementadas diversas técnicas que visam facilitar a manipulação e a exploração dos modelos de visualização pelos usuários . Essas técnicas incluem : uma função para `` recortar '' regiões de interesse em modelos de superfície e a utilização de uma sonda sonora pela qual um bloco de tamanho variável é inserido no modelo com o objetivo de colher informações e apresentá-las na forma de som . Também é possível gerar um arquivos VRML diretamente a partir dos modelos e criar animações dos mesmos . Apesar de ter sido concebido para atender a um grupo particular de usuários , o ambiente é genérico e pode ser utilizado por usuários em outros domínios de aplicação 
 A partir da análise de diversos trabalhos publicados sobre requisitos para sistemas de autoria hipermídia educacional , foi proposto um novo conjunto de requisitos que tem por objetivo auxiliar as etapas de engenharia de requisitos e de avaliação no processo de desenvolvimento de um sistema pertencente a este domínio . De forma geral , foi proposto um conjunto de requisitos que privilegia tanto as necessidades do contexto educacional quanto as necessidades relacionadas aos ambientes de autoria hipermídia . Esses requisitos foram utilizados na avaliação do sistema SASHE , tendo sido obtidos resultados que indicaram a eficácia desta proposta e , simultaneamente , a qualidade da implementação do sistema 
 A proposta deste trabalho consiste no desenvolvimento de um sistema leitor de e-maus com conversão de texto para voz , em português , tendo em seu resultado , uma voz próxima àquela falada por nós humanos e não uma voz simplesmente sintetizada . Para isso , foi criada uma tecnologia de Conversão TextoNoz , que utiliza Redes_Neurais  e opera com um banco de fonemas da língua_portuguesa  . A adaptação desta tecnologia , a um sistema leitor de e-malls , propõe um software específico à leitura de e-malls , com Conversão TextoNoz para a língua_portuguesa  . Esta tecnologia de conversão pode auxiliar os deficientes visuais , e também aqueles que não possuem tal deficiência , como um usuário comum , que tem como lucro a leitura automática de sua caixa postal , enquanto o mesmo realiza tarefas paralelas , sem que estas sejam interrompidas , conseguindo assim um ganho de tempo considerável , o que nos dias de hoje é fundamental . É válido ressaltar que o desenvolvimento deste trabalho proporciona um enriquecimento aos sistemas de correio eletrônico atuais , através do acréscimo de novas funções , tais como , leitura automática de e-malls , leitura de e-maus novos , entre outras que poderão vir a existir , seguindo a técnica proposta . Este trabalho também tem outras vantagens , devido ao fato de abrir as portas para outras interfaces , possibilitando efetuar a migração desta técnica de Conversão TextoNoz criada , para outras aplicações nos mais variados níveis de interação com o usuário ; tornando possível , por exemplo , a leitura agradável em editores de textos como o Microsoft Word ; o agendamento de tarefas com lembretes sonoros ; a leitura de programas educacionais para jovens , crianças e adultos , facilitando o entendimento da língua ; a utilização de sistemas de telefonia com integração de voz ; e ainda outras atividades alternativas 
 Com avanço da World_Wide  Web ( WWW ) ocorreu um aumento significativo em desenvolvimento de hiperdocumentos Web ( websites ) . Esse desenvolvimento , no entanto , tem se mostrado sujeito a muitos `` erros '' , que em geral são atribuídos à manipulação de um grande número de nós e links . Tentando diminuir esses erros em potencial , muitos trabalhos têm proposto diferentes soluções para auxiliar a avaliação de hiperdocumentos e a aplicação de métricas se apresenta como uma possível abordagem , seguindo os preceitos de Engenharia de Software . Por sua vez , as métricas , em geral , são de difícil interpretação por parte dos webmasters . Assim , neste trabalho investigamos a abordagem de aplicação de métricas a websites contando com auxílio de técnicas de Aprendizado de Máquina ( AM ) . Estas técnicas têm sido utilizadas em uma variedade de tarefas no contexto Web , tais como auxiliar os usuários em suas pesquisas , aprendizado relacionado aos interesses do usuário , extração de informações , entre outras . Este trabalho visa , através do suporte de técnicas de AM , auxiliar a abordagem de análise de métricas de websítes 
 Não 
 A televisão é um meio de comunicação extremamente popular e que possui linguagem e funcionalidades próprias . A televisão está passando por um período de revolução ; a primeira grande mudança está sendo a digitalização do sinal e a segunda é a possibilidade de interação . Essas duas mudanças terão um grande impacto no modo como as pessoas assistem a um programa de TV . A TV Interativa é uma aplicação multimf dia que insere novas_funcionalidades  em um ambiente aiteriormente não interativo . Um grande número de tecnologias pode ser usado em sua implementação e , devido ao crescente_interesse  da indústria na área , diversos esforços têm sido feitos para sua padronização . Este trabalho apresenta um novo conceito para criar e distribuir programas interativos que provê um avanço na adaptação dos programas às necessidades ou expectativas dos usuários . Este trabalho utiliza os padrões para a TV Interativa e estende-os para o uso em ambientes conscientes de contexto 
 Este projeto de mestrado visa dar continuidade ao desenvolvimento do gerador de malhas não estruturadas tridimensionais que será utilizado pelo grupo de Mecânica de Fluídos Computacional do ICMC na simulação de escoamento de fluídos . Sistemas integrados de modelagem geométrica com alto grau de interatividade exigem estruturas de dados eficazes na representação dos objetos . Na modelagem interativa , as estruturas devem ser eficientes no tempo de acesso às relações de incidências e adjacências . Com esse propósito é que apresentamos uma estrutura de dados para representar objetos volumétricos , tal estrutura deve ser o mais eficiente possível ao mesmo tempo que armazena o mínimo possível de informações . Esta também poderá representar singularidades em vértices e em arestas , não ficando portanto restrita à representação somente de variedades 
 À medida que os computadores se tomaram mais baratos , os fatores de usabilidade se tornaram cada vez i- -terminantes nas decisões de aceitabilidade pelos usuários de software . sabilidad- é definida diferentemente por vários autores , porém nesse trabalho utiliza-se uma definição mais ampla que inclui os atributos : facilidade de aprendizado , eficiência de uso , retenção , mínimo de erros e satisfação . Como a usabilidade é um dos fatores que afetam aualiddaade de software ) muitos trabalhos têm desenvolvido métodos de avaliação de usabilidade para interfaces de sistemas computacionais interativos . Vários desses métodos têm se mostrado úteis , mas todos eles apresentam problemas relevantes . A partir de uma análise de custo e benefício , baseada em vários critérios ( a importância de precisão , custo e tempo ) ) escolheu-se o Método USINE , que utiliza modelos de tarefa e Iogs gerados de um teste com o usuário de uma aplicação , para ser adaptado e dar suporte a um processo de avaliação de usabilidade elaborado nesse trabalho de mestrado . O processo elaborado utiliza o método USINE e a abordagem de melhoria contínua proposta por Nielsen , que aplica três etapas de avaliação , sendo que à cada etapa melhorias na interface são realizadas . O processo de avaliação foi aplicado à Ferramenta de Crítica do ambiente de suporte e auxílio à escrita AMADEUS e modificações foram realizadas para melhorar a interface da ferramenta 
 Em qualquer sistema de cache real , o espaço de armazenamento é finito . Quando o cache estiver cheio e , for solicitado um objeto que não esteja presente no cache , será necessário definir o ( s ) objeto ( s ) que será ( o ) retirado ( s ) do cache para ceder lugar ao objeto que está sendo inserido . Essa tarefa é cumprida pelas políticas de substituição de objetos . Um passo fundamental para melhorar o desempenho de caches na Web é identificar características comuns em suas cargas de trabalho para delinear diretrizes que buscam uma melhor parametrização das políticas de substituição e a organização do espaço de armazenamento de maneira eficaz . Este trabalho tem como propósito ponderar sobre o impacto provocado pelas políticas de substituição de objetos em caches na Web , por meio da investigação , avaliação de desempenho e comparação do uso destas políticas , considerando diversas configurações no espaço de armazenamento de caches . Foram implementadas e analisadas oito políticas , sendo duas delas tradicionais ( LRU e LFU ) ; uma política que implementa uma técnica de envelhecimento à política LFU ( LFU-Aging ) ; uma política que considera apenas o tamanho dos objetos para substituição ( SIZE ) ; duas que tratam de maneira direta os objetos com um único acesso ( LFU* e LFU*Aging ) ; a política FBR , que divide o cache em três segmentos e ; uma nova política proposta neste trabalho , denominada PESO . A avaliação é feita através de um simulador de caches para Web 
 A distribuição Binomial é freqüentemente usada quando estamos interessados em ajustar dados de contagens de y sucessos em n ensaios de um mesmo experimento aleatório , onde cada ensaio admite duas respostas : sucesso ou fracasso . Contudo , em muitas aplicações , podemos ter uma variabilidade observada dos dados maior ou menor do que a variabilidade esperada a partir de uma suposição Binomial com parâmetros n e p. Essa variabilidade superior ou inferior dos dados observados em relação a variabilidade do modelo Binomial é chamada variabilidade extra-Binomial e ela pode ser provocada por várias fontes . Alguns modelos tem sido_propostos  na literatura para ajustar a variabilidade extra- Binomial . Entre eles se destacam os modelos Beta-Binomial , Binomial Correlacionado e Mistura de duas distribuições Binomiais . Nesta dissertação , analisamos esses modelos sob o enfoque Bayesiano utilizando os métodos de Monte Cano em Cadeia de Markov ( MCMC ) . Em particular utilizamos os algoritmos Gibbs Sampling e Metropolis-Hastings para obter estimadores de Monte_Carlo  das quantidades a posteriori de interesse dos parâmetros . Outro importante objetivo do trabalho é o estudo da variabilidade extra-Binomial na presença de covariáveis . Apresentamos exemplos com dados reais assumindo os diferentes modelos propostos e uma discriminação dos modelos via Fator de Bayes 
 A Ethernet se tornou um padrão aberto de conectividade e através do protocolo HSE ( High peed Ethernet ) pode se tornar o padrão mais popular dos fieldbuses . A Fieldbus foundation mapeou neste protocolo toda a tecnologia do padrão H1 confirmando a tendência de que a Ethernet dominará também o chão de fábrica . Grande parte das aplicações da rede HSE da Fieldbus Foundation necessitam de uma disponibilidade maior do que um sistema não tolerante a falhas pode fornecer . Usualmente é incorporada uma redundância protetora através da adição de componentes que não seriam necessários num sistema livre de falhas . O alicerce fundamental da redundância é a transparência operacional . Computação reconfigurável é uma solução que combina processadores dedicados com memória , lógica e programação ( hardware/software ) , permitindo a integração de sistemas em um único FPGA ( Field Programmmable Gate Array ) . Estes conceitos possibilitarão a implementação do algoritmo de redundância previsto pela norma HSE de forma transparente para o processador , e consequentemente para a aplicação do usuário . O propósito deste trabalho é fornecer um hardware que tenha duas portas Ethernet de 100Mbp implementados os algoritmos de redundância especificados pelo protocolo HSE . A proposta é a definição de um hardware com dois controladores Ethernet de 100Mbps através do estudo de algumas tecnologias de fabricação de componentes e de alguns paradigmas de projeto de hardware . A forma de manipulação dos dados é um grande desafio para este trabalho , que irá identificar a melhor alternativa para controle do fluxo de dados entre as portas Ethernet , uma vez que as velocidades envolvidas na transmissão são da ordem de 100Mbps . A melhor alternativa deve levar em conta o custo , complexidades e desempenho da solução . A incorporação de tecnologias como HSE e Web aos equipamentos para automação e controle industrial fará com que as soluções para o chão de fábrica acompanhem e desfrutem das tendências tecnológicas da Internet e ainda será uma porta aberta para a tão desejada interoperabilidade 
 O desenvolvimento da tecnologia e a propagação de sistemas computacionais nos mais variados domínios do conhecimento têm contribuído para a geração e o armazenamento de uma quantidade constantemente crescente de dados , em uma velocidade maior da que somos capazes de processar . De um modo geral , a principal razão para o armazenamento dessa enorme_quantidade  de dados é a utilização deles em benefício da humanidade . Diversas áreas têm se dedicado à pesquisa e a proposta de métodos e processos para tratar esses dados . Um desses processos é a Descoberta de Conhecimento em Bases de Dados , a qual tem como objetivo extrair conhecimento a partir das informações contidas nesses dados . Para alcançar esse objetivo , usualmente são construídos modelos ( hipóteses ) , os quais podem ser gerados com o apoio de diferentes áreas tal como a de Aprendizado de Máquina . A Seleção de Atributos desempenha uma tarefa essencial dentro desse processo , pois representa um problema de fundamental_importância  em aprendizado de máquina , sendo freqüentemente realizada como uma etapa de pré-processamento . Seu objetivo é selecionar os atributos mais importantes , pois atributos não relevantes e/ou redundantes podem reduzir a precisão e a compreensibilidade das hipóteses induzidas por algoritmos de aprendizado_supervisionado  . Vários algoritmos para a seleção de atributos relevantes têm sido propostosna literatura . Entretanto , trabalhos recentes têm mostrado que também deve-se levar em conta a redundância para selecionar os atributos importantes , pois os atributos redundantes também afetam a qualidade das hipóteses induzidas . Para selecionar alguns e descartar outros , é preciso determinar a importância dos atributos segundo algum critério . Entre os vários critérios de importância de atributos propostos , alguns estão baseados em medidas de distância , consistência ou informação , enquanto outros são fundamentados em medidas de dependência . Outra questão essencial são as avaliações experimentais , as quais representam um importante instrumento de estimativa de performance de algoritmos de seleção de atributos , visto que não existe análise matemática que permita predizer que algoritmo de seleção de atributos será melhor que outro . Essas comparações entre performance de algoritmos são geralmente realizadas por meio da análise do erro do modelo construído a partir dos subconjuntos de atributos selecionados por esses algoritmos . Contudo , somente a consideração desse parâmetro não é suficiente ; outras questões devem ser consideradas , tal como a percentagem de redução da quantidade de atributos desses subconjuntos de atributos selecionados . Neste trabalho é proposto um algoritmo que separa as análises de relevância e de redundância de atributos e introduz a utilização da Dimensão Fractal para tratar atributos redundantes em aprendizado_supervisionado  . É também proposto um modelo de avaliação de performance de algoritmos de seleção de atributos baseado no erro da hipótese construída e na percentagem de redução da quantidade de atributos selecionados . Resultados experimentais utilizando vários conjuntos de dados e diversos algoritmos consolidados na literatura , que selecionam atributos importantes , mostram que nossa proposta é competitiva com esses algoritmos . Outra questão importante relacionada à extração de conhecimento a partir de bases de dados é o formato no qual os dados estão representados . Usualmente , é necessário que os exemplos estejam descritos no formato_atributo-valor  . Neste trabalho também propomos um metodologia para dar suporte , por meio de um processo semi-automático , à construção de conjuntos de dados nesse formato , originados de informações de pacientes contidas em laudos médicos que estão descritos em linguagem_natural  . Esse processo foi aplicado com sucesso a um caso real 
 Neste trabalho foi realizado um estudo sobre o uso e influências do efeito small world , ou seis graus de separação , no roteamento de redes de sensores sem fio ( RSSFs ) . Para esse objetivo , foram analisadas as características das RSSFs que influenciam no roteamento e os diferentes_tipos  de protocolos . Além_disso  , foram estudadas as características do efeito small world e suas propriedades , de um modo geral , em redes de larga_escala  e com alta densidade de nós , incluindo o modelo de small world para o estudo de redes ad hoc . Realizou-se um breve estudo sobre redes overlay , redes lógicas criadas sobre a rede física com o propósito de melhorar suas qualidades e seu desempenho . A conclusão neste trabalho é que small worlds pode ser empregado para melhorar o funcionamento de protocolos de roteamento em RSSFs 
 O problema de reconhecimento de expressões matemáticas manuscritas envolve três subproblemas importantes : segmentação de símbolos , reconhecimento de símbolos e análise estrutural de expressões . Para avaliar métodos e técnicas de reconhecimento , eles precisam ser testados sobre conjuntos de amostras representativos do domínio de aplicação . Uma das preocupações que tem sido apontada ultimamente é a quase inexistência de base de dados pública de expressões matemáticas , o que dificulta o desenvolvimento e comparação de diferentes abordagens . Em geral , os resultados de reconhecimento apresentados na literatura restringem-se a conjuntos de dados pequenos , não disponíveis publicamente , e muitas_vezes  formados por dados que visam avaliar apenas alguns aspectos específicos do reconhecimento . No caso de expressões online , para treinar e testar reconhecedores de símbolos , as amostras são em geral obtidas solicitando-se que as pessoas escrevam uma série de símbolos individualmente e repetidas vezes . Tal tarefa é monótona e cansativa . Uma abordagem alternativa para obter amostras de símbolos seria solicitar aos usuários a transcrição de expressões modelo previamente definidas . Dessa forma , a escrita dos símbolos seria realizada de forma natural , menos monótona , e várias amostras de símbolos poderiam ser obtidas de uma única expressão . Para evitar o trabalho de anotar manualmente cada símbolo das expressões transcritas , este trabalho propõe um método para casamento de expressões matemáticas manuscritas , no qual símbolos de uma expressão transcrita por um usuário são associados aos correspondentes símbolos ( previamente identificados ) da expressão modelo . O método proposto é baseado em uma formulação que reduz o problema a um problema de associação simples , no qual os custos são definidos em termos de características dos símbolos e estrutura da expressão . Resultados experimentais utilizando o método proposto mostram taxas médias de associação correta superiores a 99 % 
 A Técnica de Geração de Colunas é uiri procedimento para resolver problemas de otimização_linear  de grande escala , onde o número de variáveis é muito grande . Esta técnica tem a vantagem de trabalhar com as colunas implicitamente . Colunas adicionais serão geradas , quando necessário , resolvendo um subproblema . A Técnica de Geração de Colunas também pode ser usada em combinação com o método Branch and Bound para resolver problemas de Otimização Inteira . Neste trabalho serão apresentadas as aplicações da Técnica de Geração de Colunas em vários problemas de otimização e alguns testes computacionais 
 A linguagem XML ( Exlensibk ' Markup Language ) vem_sendo  muito utilizada em diversas aplicações e já é reconhecida como uma linguagem padrão para representação de documentos tanto no meio acadêmico quanto no comercial . Porém , a disponibilidade de uma grande quantidade de dados apresenta vários problemas práticos que o padrão XML , por si só , não pode resolver . O objetivo deste trabalho é desenvolver um servidor XML capaz de realizar diversas operações ( como adicionar , consultar , apagar e transformar ) sobre um repositório de documentos XML e XSL ( Exiensihk ' Stylesheei Language ) . Esses documentos são representados na forma de objetos Java que obedecem ao modelo de interfaces DOM ( Documeni Ob/ect Model ) , um modelo padrão usado em várias aplicações da Internet . Para o armazenamento é utilizado o serviço JavaSpaces , que pode ser usado como um repositório persistente para esses objetos DOM-Java . O servidor XML utiliza a linguagem XQL ( XML Query Language ) na realização de consultas . Além_disso  , ele permite , através da linguagem XSL , a conversão de documentos XML para outras linguagens 
 Seja H ' uma orientação do grafo H. Alon e Yuster [ The number of orientations having no fixed tounament , Combinatória , 26 ( 2006 ) , no . 1 , 1-6 ] propuseram o problema de determinar ou estimar D ( n , m , H ' ) , a quantidade máxima de orientações livres de H ' de um grafo com n vértices e m arestas . Se substituirmos o máximo pelo 'máximo essencial ' , ou seja , consideramos o máximo sobre quase todos os grafos de n vértices e com m arestas , em oposição à todos deles , o problema é mais acessível . Mostramos que esse máximo essencial é 2^o ( m ) se H ' é o circuito direcionado cíclico C'_l de tamanho l ( l > = 3 ) , se m > > n^ { 1+1/ ( l-1 ) } . Por outro_lado  , o mínimo essencial é 2^ ( 1-o ( 1 ) ) m , se m < < n^ { 1+1/ ( l-1 ) } . O método de prova nos dá resultado da mesma natureza para grafos orientados bipartidos H ' que contém circuito direcionado cícliclo 
 Neste trabalho , apresentamos um estudo sobre o problema de dimensionamento de lotes com múltiplas plantas , múltiplos itens e múltiplos períodos . As plantas têm capacidade de produção limitada e a fabricação de cada produto incorre em tempo e custo de preparação de máquina . Nosso objetivo é encontrar um plano de produção que satisfaça a demanda de todos os clientes , considerando que a soma dos custos de produção , de estoque , de transporte e de preparação de máquina seja a menor possível . Este trabalho tem duas contribuições centrais . Primeiramente , propomos a modelagem do problema de dimensionamento de lotes com múltiplas plantas utilizando o conceito de localização de facilidades . Para instâncias de pequena dimensão , os testes computacionais mostraram que a resolução do problema remodelado apresenta , como esperado , resultados melhores que o modelo original . No entanto , seu elevado número de restrições e de variáveis faz com que as instâncias de maiores magnitudes não consigam ser resolvidas . Para trabalhar com instâncias maiores , propomos um método híbrido ( math-heurística ) , que combina o método relax-and-fix , com a restrição de local branching . Testes computacionais mostram que o método proposto apresenta soluções factíveis de boa_qualidade  para estas 
 Um aspecto importante da interação humana é a atenção compartilhada . Ela é um processo de comunicação onde uma pessoa redireciona a sua atenção para um objeto ou evento e a outra pessoa ou pessoas seguem o seu olhar para o mesmo lugar . O processo é finalizado com a pessoa que segue a atenção realizando um apontamento sobre o objeto e um comentário sobre a situação . Esta habilidade importante é aprendida por nós durante o período da infância e hoje , alguns pesquisadores em robótica estão tentando desenvolver arquiteturas robóticas para aprender essa habilidade em robôs . Deste modo , o laboratório de aprendizado de robôs está trabalhando em uma arquitetura robótica para esse fim . Ela é composta por três_módulos  , percepção de estímulo , controle de consequência e emissão de resposta . Esta arquitetura robótica foi avaliada no controle de uma cabeça robótica e foi capaz de aprender a seguir o olhar e identificar alguns objetos . No entanto , todos esses módulos têm algumas limitações . A fim de ter uma melhor interação entre um robô e um humano e reduzir os efeitos das limitações , algumas melhorias foram desenvolvidas . Entre elas incluem um novo algoritmo de classificação das posições da cabeça através do histograma de gradiente orientado , inserir novas_funcionalidades  ( definidas como reflexos ) ao módulo de controle de consequência e novos algoritmos de aprendizado para selecionar a melhor ação . Todas as modificações realizadas reduziram as limitações e pode melhorar as interações entre um robô e um ser 
 O objetivo deste trabalho consiste no desenvolvimento , estudo e implementação de métodos de pontos_interiores  específicos para o problema de planejamento do tratamento de câncer por radioterapia . Este é um problema de grande porte que contém uma estrutura matricial particular . A exploração desta estrutura de forma eficiente obtém bom_desempenho  computacional , através da redução da dimensão dos sistemas_lineares  que devem ser resolvidos a cada iteração , agilizando a definição de um tratamento adequado , uma vez que tipicamente várias simulações são realizadas antes da definição de um plano definitivo . Resultados_numéricos  em Matlab ilustram a eficiência desta abordagem em problemas reais e mostram a superioridade do método preditor corretor em comparação ao método primal-dual 
 Um mosaico é o conjunto de fotos de uma determinada área , recortadas e montadas técnica e artísticamente , de forma a dar a impressão de que todo o conjunto é uma única fotografia . No caso de fotografias aéreas , sua utilização soluciona o problema da necessidade de se retratar uma área de interesse mais extensa do que o campo de cobertura das lentes da câmera utilizada . O foco deste trabalho é a criação automática de mosaicos buscando encontrar a posição real de um conjunto de imagens imagens adquiridas em baixa altitude , de baixa escala , em relação à um Mapa de Base , de escala maior , realizando , assim , uma correlação entre imagens de escalas diferentes . Este problema é abordado por técnicas de análise multi-escala , mais precisamente , pela utilização de filtros de Gabor . A metodologia desenvolvida utiliza um banco de filtros de Gabor aplicado sobre uma imagem de referência de modo que , a partir da aplicação destes filtros sobre a mesma , seja possível gerar um processo automático de geração do mosaico para o restante do conjunto de imagens . Experimentos realizados utilizando o método proposto demonstram a eficácia do mesmo para imagens com texturas de orientação marcante , como o caso de imagens aéreas de plantação de 
 Este trabalho descreve a prova de conceito de uma abordagem que utiliza o modelo de computação a fluxo de dados , inerentemente paralelo , associado ao modelo de computação reconfigurável parcial e dinamicamente , visando à obtenção de sistemas computacionais de alto_desempenho  . Mais especificamente , trata da obtenção de um modelo para o particionamento dos grafos a fluxo de dados dinâmicos e de um protocolo de comunicação entre suas partes , a fim de permitir a sua implementação em arquiteturas dinamicamente reconfiguráveis , em especial em FGPAs Virtex da Xilinx . Enquadra-se no contexto do projeto ChipCFlow , de escopo mais amplo , que pretende obter uma ferramenta para geração_automática  de descrição de hardware sintetizável , a partir de código em alto nível , escrito em linguagem C , fazendo_uso  da abordagem a fluxo de dados para extrair o paralelismo implícito nas aplicações originais . O modelo proposto é aplicado em um grafo a fluxo de dados dinâmico , e através de simulações sua viabilidade é 
 Um método lagrangeano-euleriano arbitrário para a resolução de escoamentos dominados por tensão superficial é apresentado neste trabalho . Tais escoamentos são importantes em muitas aplicações , especialmente em canais capilares que frequentemente aparecem em escoamentos em microescala . A resolução deste tipo de escoamento apresenta vários desafios que são abordados neste trabalho . O escoamento é resolvido somente para a fase líquida , com condições de contorno apropriadas para a superfície_livre  que delimita o líquido e o gás , que é representada por arestas e vértices da malha computacional . Esta se move e se deforma , sendo que sua qualidade é mantida sob controle para não degradar a solução_numérica  . As equações de Navier-Stokes são discretizadas pelo método de elementos finitos em um referencial arbitrário . O método de incorporação dos efeitos de tensão superficial e linha de contato é explicado em detalhes . Validações comprovam a precisão do método proposto , com comparações através de soluções pseudo-analíticas para casos simples . Finalmente alguns resultados sobre escoamentos em capilares são 
 Os avanços tecnológicos e científicos ocorridos nas últimas_décadas  têm proporcionado o desenvolvimento de métodos cada vez mais eficientes para o armazenamento e processamento de dados . Através da análise e interpretação dos dados , é possível_obter  o conhecimento . Devido o conhecimento poder auxiliar a tomada de decisão , ele se tornou um elemento de fundamental_importância  para diversas organizações . Uma grande parte dos dados disponíveis hoje se encontra na forma textual , exemplo disso é o crescimento vertiginoso no que se refere à internet . Como os textos são dados não estruturados , é necessário realizar uma série de passos para transformá-los em dados estruturados para uma possível análise . O processo denominado de Text Mining é uma tecnologia emergente e visa analisar grandes coleções de documentos . Esta dissertação de mestrado aborda a utilização de diferentes técnicas e ferramentas para Text Mining . Em conjunto com o módulo de Pré-processamento de textos , projetado e implementado por Imamura ( 2001 ) , essas técnicas e ferramentas podem ser utilizadas para textos em português . São explorados alguns algoritmos utilizados para extração de conhecimento de dados , `` como : Vizinho mais Próximo , Naive Bayes , Árvore de Decisão , Regras de Decisão , Tabelas de Decisão e Support Vector Machines . Para verificar o comportamento desses algoritmos para textos em português , foram realizados alguns experimentos 
 Aplicações que demandam uma sincronização entre usuários móveis requerem um mecanismo para a difusão ( multicast ) de mensagens entre dispositivos_móveis  . Em alguns_casos  , o multicast deve ser atômico , isto é , ou todos os elementos do grupo processam a mensagem difundida , ou nenhum deles . O AM2C é um protocolo para multicast atômico em Computação Móvel . Entretanto , o principal problema do AM2C é sua falta de escalabilidade , uma vez que este protocolo faz uma difusão para todas as estações-base na rede fixa 

 Esta dissertação descreve o projeto , implementação , simulação e avaliação dos protocolos AM2C e iAM2C , onde o segundo protocolo é uma variante escalável do primeiro . As implementações e simulações foram feitas usando o ambiente MobiCS - Mobile Computing Simulator 
 que é uma ferramenta para o teste e a análise de desempenho de protocolos distribuídos em redes móveis . A dissertação apresenta uma comparação detalhada dos protocolos para diferentes configurações de rede e diversos padrões de migração dos hosts móveis 
 A análise de dados em grande escala é um dos grandes desafios computacionais atuais e está presente não somente em áreas da ciência moderna mas também nos setores público e industrial . Nesses cenários , o processamento dos dados geralmente é modelado como um conjunto de atividades interligadas por meio de fluxos de dados os workflows . Devido ao alto custo_computacional  , diversas estratégias já foram propostas para melhorar a eficiência da execução de workflows intensivos em dados , tais como o agrupamento de atividades para minimizar as transferências de dados e a paralelização do processamento , de modo que duas ou mais atividades sejam executadas ao mesmo tempo em diferentes recursos_computacionais  . O paralelismo nesse caso é definido pela estrutura descrita em seu modelo de composição de atividades . Em geral , os Sistemas de Gerenciamento de Workflows , responsáveis pela coordenação e execução dessas atividades em um ambiente distribuído , desconhecem o tipo de processamento a ser realizado e por isso não são capazes de explorar automaticamente estratégias para execução paralela . As atividades paralelizáveis são definidas pelo usuário em tempo de projeto e criar uma estrutura que faça uso eficiente de um ambiente distribuído não é uma tarefa trivial . Este trabalho tem como objetivo prover execuções mais eficientes de workflows intensivos em dados e propõe para isso um método para a paralelização automática dessas aplicações , voltado para usuários não-especialistas em computação de alto_desempenho  . Este método define nove anotações semânticas para caracterizar a forma como os dados são acessados e consumidos pelas atividades e , assim , levando em conta os recursos_computacionais  disponíveis para a execução , criar automaticamente estratégias que explorem o paralelismo de dados . O método proposto gera réplicas das atividades anotadas e define também um esquema de indexação e distribuição dos dados do workflow que possibilita maior acesso paralelo . Avaliou-se sua eficiência em dois modelos de workflows com dados reais , executados na plataforma de nuvem da Amazon . Usou-se um SGBD relacional ( PostgreSQL ) e um NoSQL ( MongoDB ) para o gerenciamento de até 20,5 milhões de objetos de dados em 21 cenários com diferentes configurações de particionamento e replicação de dados . Os resultados obtidos mostraram que a paralelização da execução das atividades promovida pelo método reduziu o tempo de execução do workflow em até 66,6 % sem aumentar o seu custo monetário 
 Esta dissertação de mestrado apresenta a prototipação de uma arquitetura denominada ServidorWeb com Diferenciação de Serviços ( SWDS ) . O trabalho também apresenta a proposta , implementação e a avaliação de desempenho de dois algoritmos de controle de admissão denominados Algoritmo de Negociação e Algoritmo de Reserva de Conexões . O objetivo principal deste trabalho é a avaliação de desempenho dos algoritmos de controle de admissão no protótipo do SWDS . Como resultados , verificou-se que a Classe de maior prioridade consegue atender mais requisições que a Classe de menor prioridade em todos as configurações de carga . Também pode-se observar que o algoritmo de reserva de conexões consegue se adaptar a todas as variações de 
 Esta dissertação de mestrado fez parte do projeto Qualipso ( Quality Platform for Open Source Software ) que teve como principal objetivo melhorar a confiabilidade de sistemas de software_livre  . Nesse contexto , o enfoque desta pesquisa é um dos atributos de qualidade de software : usabilidade . As práticas de usabilidade no desenvolvimento de software_livre  , são aplicadas na maioria das vezes , em projetos patrocinados por grandes empresas ou que possuam especialistas em usabilidade como membros da equipe . Mas , em projetos menores da comunidade , compostos geralmente por desenvolvedores , raramente ela é considerada . Porém , a usabilidade é um atributo fundamental para a qualidade durante o uso de um sistema . Com base em valores compartilhados entre as comunidades de métodos ágeis e de software_livre  , esta dissertação_propõe  a adaptação de práticas de usabilidade no contexto de métodos ágeis para o contexto de comunidades de software_livre  . Por meio de pesquisa bibliográfica , levantamos as principais práticas de usabilidade tanto no âmbito de métodos ágeis , quanto no âmbito de software_livre  , e as classificamos de acordo com as fases do Design Centrado em Usuário , descrevendo cada uma com o formato nome-contexto-problema-solução-exemplos . As práticas foram exploradas em projetos de software_livre  , o que possibilitou maior entendimento de problemas enfrentados em contextos reais . Essa experiência resultou na proposta de adaptação de práticas de usabilidade ágil no contexto de comunidades de software_livre  . Dessa forma , descrevemos a realização de uma pesquisa-ação no projeto Arquigrafia-Brasil , um estudo de caso no projeto Mezuro e a aplicação de práticas de usabilidade em quatro projetos do Centro de Competência em Software Livre do IME-USP 
 Em algumas disciplinas dos cursos de computação , a quantidade excessiva de conteúdo torna mais difícil o ensino dos conceitos . A disciplina de Sistemas Operacionais é um exemplo deste tipo de disciplina e objetivando facilitar o aprendizado , podese fazer uso de diversas técnicas , ferramentas e metodologias , como por exemplo , a AIM-CID ( Abordagem Integrada de Modelos Conceitual , Instrucional e Didático ) . Este projeto de mestrado tem como objetivo principal a aplicação da metodologia AIM-CID sobre os tópicos abordados no ensino de Sistemas Operacionais . Este trabalho foi desenvolvido para ajudar os professores dessa disciplina no ordenamento dos conceitos a serem ensinados . Um objetivo secundário é utilizar os resultados do uso da metodologia para criar a base para o desenvolvimento de materiais educativos visando o ensino do sistemas operacionais . Desse modo , a contribuição deste trabalho permite a criação de elementos que auxiliem os alunos na fixação dos conteúdos da disciplina . Uma vez aplicada a metodologia AIM-CID sobre o conteúdo da disciplina de Sistemas Operacionais , foi utilizada uma metodologia que avalia a usabilidade de softwares pedagógicos , e obteve-se um resultado que valida o modelo desenvolvido neste 
 A crescente demanda por cursos baseados na World_Wide  Web ( Web ) tem imposto uma série de desafios para a área da Tecnologia da Informação e da Comunicação ( TIC ) , em particular quando se considera a Educação apoiada pela Web . Uma das necessidades emergentes da Educação via Web está relacionada com o uso de Objetos de Aprendizagem ( OA ) dentro de Sistemas Gerenciadores de Curso ( SGC ) . Além de considerar a produção de OA com `` qualidade '' , deve-se providenciar seu armazenamento e disseminação de modo eficaz . Entretanto , algumas dificuldades tem sido reportadas por professores envolvidos com cursos pela Web , tais como : o `` custo elevado '' para a autoria do OA ; a falta de ferramentas para o compartilhamento desses materiais ; e experiências práticas negativas no reúso desses conteúdos em um SGC . Uma primeira proposta para mitigar esses problemas tem sido o desenvolvimento de ferramentas de Repositório de Objetos de Aprendizagem ( ROA ) , visando principalmente facilitar a disseminação dos OA . Apesar do progresso já alcançado , ainda existem várias barreiras para a sua adoção pelos professores , como falta de integração simples entre o SGC e o ROA . Em geral as integrações atuais mostram-se difíceis para um usuário típico . Neste cenário , o objetivo deste projeto é apresentar um ROA inovador , o Repositório Interativo de Conteúdo Digital - iRepositório , cujo modelo prevê uma integração simplificada com um particular SGC e ainda oferece vários recursos para melhorar os processos de ensino e de aprendizagem , como o registro da eficácia educacional do OA . Como contribuição prática os fundamentos do iRepositório foram implementados para um SGC específico , o Moodle . Além de compartilhar OA entre cursos e usuários , ele provê recursos a outro componente do Moodle , o Tarefa Interativa - iTarefa , para armazenamento , classificação e busca de OA ou ainda facilidades para a manipulação dos Módulos de Aprendizagem Interativa ( iMA ) 
 Localização e Mapeamento são problemas fundamentais da robótica que vêm_sendo  estudados exaustivamente pela comunidade_científica  para a navegação de robôs_móveis  . A maior_parte  das pesquisas estão concentradas em implementações para computadores_pessoais  , mas pouco se tem feito na área de computação embarcada . Este trabalho mostra a análise e implementação em FPGA de um algoritmo de localização para ambientes dinâmicos composto por um filtro de partículas , e também de um algoritmo de mapeamento baseado na técnica de scan matching . Os algoritmos originais desenvolvidos em linguagem de programação C foram analisados e modificados para uma abordagem embarcada ( embedded ) em robôs reconfiguráveis utilizando-se o processador Nios_II  da Altera . Os algoritmos são comparados quanto ao desempenho , no intuito de servir como referência no futuro desenvolvimento da ferramenta de codesign autom´atico 
 A capacidade das células para responder corretamente a sinais externos e perceber mudanças no seu microambiente é a base do desenvolvimento , reparação de tecidos e de imunidade , bem como a homeostase do tecido normal . Transdução de sinal é o principal meio pelo qual as células respondem a sinais externos de seu ambiente e coordenam alterações celulares complexas . O estudo das vias de sinalização molecular permite-nos tentar compreender o funcionamento dessas transduções de sinais e , consequentemente , as respostas celulares a estímulos externos . Uma abordagem adequada para tais estudos é o uso de modelos matemáticos para simular a cinética das reações químicas que descrevem uma dada via de sinalização , o que nos permite gerar predições testáveis de processos celulares . Construir modelos cinéticos preditivos de vias de sinalização molecular através de dados de alto rendimento produzidos utilizando técnicas ômicas ( i.e. , genômica , transcriptômica , ( fosfo- ) proteômica ) constitui um dos atuais desafios enfrentados pelos pesquisadores na área de Biologia Molecular . Recentemente , para lidar com este desafio , o arcabouço de e-Science SigNetSim foi introduzido pelo Grupo de Biologia Computacional e de Bioinformática do Instituto Butantan . Esse arcabouço permite fazer a descrição de vias de sinalização molecular através da descrição da estrutura de um modelo através de um conjunto de reações químicas , que por sua vez é mapeado para um sistema de Equações Diferencias Ordinárias ( EDOs ) , numericamente simuladas e avaliadas . Todavia , modificações na estrutura das vias precisam ser feitas manualmente , o qual restringe severamente o número de estruturas da via que precisam ser testadas , especialmente no caso de modelos grandes . Portanto , diante desse panorama , este trabalho propõe o desenvolvimento de um método para modificar vias de sinalização molecular . Esse método se baseia no uso de bancos de dados de interatomas para fornecer um conjunto de espécies químicas candidatas para serem incluídas na via de sinalização . Um componente integrado ao arcabouço SigNetSim capaz de testar diferentes hipóteses de modificação de vias foi desenvolvido neste projeto utilizando a metodologia de heurística incremental . Para avaliar a eficiência do componente implementado , utilizamos como estudo de caso um modelo de vias sinalização de MAPKs e PI3K/Akt para realizar testes experimentais e analisar os resultados obtidos 
 Estruturas de indexação para domínios métricos são úteis para agilizar consultas por similaridade sobre dados complexos , tais como imagens , onde o custo_computacional  da comparação de dois itens de dados geralmente é alto . O estado da arte para executar consultas por similaridade está centrado na utilização dos chamados `` Métodos de Acesso Métrico '' ( MAM ) . Tais métodos consideram os dados como elementos de um espaço métrico , onde apenas valem as propriedades fundamentais para que um espaço seja considerado métrico , onde a única informação que os MAMs utilizam é a medida de similaridade entre pares de elementos do domínio . No campo teórico , espaços_métricos  são extensamente estudados e servem de base para diversas_áreas  da Matemática . No entanto , a maioria dos trabalhos que têm sido desenvolvidos em Computação se restringem a utilizar as definições básicas desses espaços , e não foram encontrados estudos que explorem em mais profundidade os muitos conceitos teóricos existentes . Assim , este trabalho aplica conceitos teóricos importantes da Teoria de Espaços Métricos para desenvolver técnicas que auxiliem o tratamento e a manipulação dos diversos dados complexos , visando principalmente o desenvolvimento de métodos de indexação mais eficientes . É desenvolvida uma técnica para realizar um mapeamento de espaços_métricos  que leva à atenuação do efeito da maldição da dimensionalidade , a partir de uma aplicação lipschitziana real baseada em uma função de deformação do espaço das distâncias entre os elementos do conjunto . Foi mostrado que uma função do tipo exponecial deforma as distâncias de modo a diminuir os efeitos da maldição da dimensionalidade , melhorando assim o desempenho nas consultas . Uma segunda contribuição é o desenvolvimento de uma técnica para a imersão de espaços_métricos  , realizada de maneira a preservar a ordem das distâncias , possibilitando a utilização de propriedades no espaço de imersão . A imersão de espaços_métricos  no ' R POT . n ' possibilita a utilização da lei dos cossenos e assim viabiliza o cálculo de distâncias entre elementos e um hiperplano métrico , permitindo aumentar a agilidade à consultas por similaridade . O uso do hiperplano métrico foi exemplificado construindo uma árvore binária métrica , e também foi aplicado em um método de acesso métrico , a família MMH de métodos de acesso métrico , melhorando o particionamento do espaço dos 
 A grande capacidade de disponibilização de informações que a Web possibilita se traduz em múltiplas possibilidades e oportunidades para seus usuários . Essas pessoas são capazes de acessar conteúdos provenientes de todas as partes do planeta , independentemente de onde elas estejam . Mas essas possibilidades não são estendidas a todos , sendo necessário mais que o acesso a um computador e a Internet para que sejam realizadas . Indivíduos que apresentem necessidades especiais ( deficiência visual , cognitiva , dificuldade de locomoção , entre outras ) são privados do acesso a sites e aplicações web que façam mal emprego de tecnologias web ou possuam o conteúdo sem os devidos cuidados para com a acessibilidade . Um dos grupos que é privado do acesso a esse ambiente é o de pessoas com dificuldade de leitura ( analfabetos funcionais ) . A ampla utilização de recursos textuais nas aplicações pode tornar difícil ou mesmo impedir as interações desses indivíduos com os sistemas computacionais . Nesse contexto , este trabalho tem por finalidade o desenvolvimento de tecnologias assistivas que atuem como facilitadoras de leitura e compreensão de sites e aplicações web a esses indivíduos ( analfabetos funcionais ) . Essas tecnologias assistivas utilizam recursos de processamento de língua natural visando maximizar a compreensão do conteúdo pelos usuários . Dentre as técnicas utilizadas são destacadas : simplificação sintática , sumarização_automática  , elaboração léxica e reconhecimento das entidades nomeadas . Essas técnicas são utilizadas com a finalidade de promover a adaptação automática de conteúdos disponíveis na Web para usuários com baixo_nível  de alfabetização . São descritas características referentes à acessibilidade de aplicações web e princípios de design para usuários com baixo_nível  de alfabetização , para garantir a identificação e entendimento das funcionalidades que são implementadas nas duas tecnologias assistivas resultado deste trabalho ( Facilita e Facilita Educacional ) . Este trabalho contribuiu com a identificação de requisitos de acessibilidade para usuários com baixo_nível  de alfabetização , modelo de acessibilidade para automatizar a conformidade com a WCAG e desenvolvimento de soluções de acessibilidade na camada de agentes de 
 Este trabalho apresenta um sistema de navegação_autônoma  para veículos terrestres com foco em ambientes não estruturados , tendo como principal meta aplicações em campos abertos com vegetação esparsa e em cenário agrícola . É aplicada visão_computacional  como sistema de percepção principal utilizando uma câmera estéreo em um veículo com modelo cinemático de Ackermann . A navegação é executada de forma deliberativa por um planejador baseado em malha de estados sobre um mapa de custos e localização por odometria e GPS . O mapa de custos é obtido através de um modelo de ocupação probabilístico desenvolvido fazendo_uso  de uma OctoMap . É descrito um modelo sensorial para atualizar esta OctoMap a partir da informação espacial proveniente de nuvens de pontos obtidas a partir do método de visão estéreo . Os pontos são segmentados e filtrados levando em consideração os ruídos inerentes da aquisição de imagens e do processo de cálculo de disparidade para obter a distância dos pontos . Os testes foram executados em ambiente de simulação , permitindo a replicação e repetição dos experimentos . A modelagem do veículo foi descrita para o simulador físico Gazebo de acordo com a plataforma real CaRINA I ( veículo elétrico automatizado do LRM-ICMC/USP ) , levando-se em consideração o modelo cinemático e as limitações deste veículo . O desenvolvimento foi baseado no ROS ( Robot Operating System ) sendo utilizada a arquitetura básica de navegação deste framework a partir da customização dos seus componentes . Foi executada a validação do sistema no ambiente real em cenários com terreno irregular e obstáculos diversos . O sistema apresentou um desempenho satisfatório tendo em vista a utilização de uma abordagem baseada em apenas uma câmera estéreo . Nesta dissertação são apresentados os principais componentes de um sistema de navegação_autônoma  e as etapas necessárias para a sua concepção , assim como resultados de experimentos simulados e com o uso de um veículo autônomo 
 O consumo de energia é uma das questões mais importantes em sistemas embarcados . Estudos demonstram que neste tipo de sistema a cache é responsável por consumir a maior_parte  da energia fornecida ao processador . Na maioria dos processadores embarcados , os parâmetros de configuração da cache são fixos e não permitem mudanças após sua fabricação/síntese . Entretanto , este não é o cenário ideal , pois a configuração da cache pode não ser adequada para uma determinada aplicação , tendo como consequência menor desempenho na execução e consumo excessivo de energia . Neste contexto , este trabalho apresenta uma implementação em hardware , utilizando computação reconfigurável , capaz de reconfigurar automática , dinâmica e transparentemente a quantidade de ways e por consequência o tamanho da cache de dados do processador embarcado LEON3 , de forma que a cache se adeque à aplicação em tempo de execução . Com esta técnica , espera-se melhorar o desempenho das aplicações e reduzir o consumo de energia do sistema . Os resultados dos experimentos demonstram que é possível reduzir em até 5 % o consumo de energia das aplicações com degradação de apenas 0.1 % de 
 Contexto : Epilepsia não é uma única doença , mas uma família de síndromes que compartilham a recorrência de crises . Estima-se que 3 % da população em geral terá epilepsia em algum momento em suas vidas . A detecção de crises epiléticas é frequentemente feita através da análise de exames de eletroencefalografia . Há várias dificuldades na detecção de crises , variabilidade entre pessoas , localização do conteúdo espectral , interferências , dentre outras . Motivação : Há um crescente uso com bons_resultados  de redes complexas para análise de séries_temporais  , mas poucos destes são voltados à análise de sinais de epilepsia . Os trabalhos que analisam epilepsia , em geral , negligenciam uma análise estatística rigorosa . Ainda há dúvida quanto à utilização de algoritmos prospectivos para predição de crises . Métodos : As séries_temporais  são analisadas utilizando 7 tamanhos diferentes de janelas , 256 , 303 , 512 , 910 , 1.024 , 2.048 , e 2.730 pontos . São utilizados 6 algoritmos de conversão de série_temporal  em rede complexa , redes de k vizinhos mais próximos , redes de k vizinhos mais próximos adaptativos , redes de epsilon vizinhança , redes cíclicas , redes de transição , e grafos de visibilidade . Cada um desses algoritmos têm seus parâmetros , e no total são realizadas 75 conversões . Para cada rede complexa gerada , são extraídas 21 medidas que as caracterizam . Com a extração dessas medidas , um novo conjunto de dados é formado e utilizado para treinar 37 classificadores diferentes , divididos em 4 classes , análise de discriminante linear , árvore de decisão , k vizinhos mais próximos , e máquina de vetores de suporte . É utilizada uma validação cruzada com 10-folds numa parte do conjunto de dados separada para o treino dos classificadores , e apenas o melhor classificador dentre os 37 foi selecionado em cada conversão realizada . No conjunto de teste , é feita a estimativa de desempenho do melhor classificador , que é então comparado à um preditor aleatório e ao estado da arte . Resultados : A rede de epsilon vizinhança obteve o melhor resultado , com 100 % de acurácia no conjunto de teste em quase todos os cenários , com janelas de tamanho pequeno e com a análise de discriminante linear . As outras redes também tiveram bons_resultados  , comparáveis ao estado da arte , exceto a rede de transição cujo desempenho foi ruim . Conclusão : Foi possível desenvolver um algoritmo prospectivo com classificador linear utilizando a rede de epsilon vizinhança , com desempenho comparável ao estado da arte e com rigorosa avaliação estatística , e não apenas utilizando a acurácia como medida de desempenho 
 Nesta tese é analisado situações onde eventos de interesse podem ocorrer mais que uma vez para o mesmo indivíduo . Embora os estudos nessa área tenham recebido considerável atenção nos últimos_anos  , as técnicas que podem ser aplicadas a esses casos especiais ainda são pouco exploradas . Além_disso  , em problemas desse tipo , é razoável supor que existe dependência entre as observações . Uma das formas de incorporá-la é introduzir um efeito aleatório na modelagem da função de risco , dando origem aos modelos de fragilidade . Esses modelos , em análise de sobrevivência , visam descrever a heterogeneidade não observada entre as unidades em estudo . Os modelos estatísticos apresentados neste texto são fundamentalmente modelos de sobrevivência baseados em processos de contagem , onde é representado o problema como um processo de Poisson homogêneo e não-homogêneo com um termo de fragilidade , para o qual um indivíduo com um dado vetor de covariável x é acometido pela ocorrência de eventos repetidos . Esses modelos estão divididos em duas classes : modelos de fragilidade multiplicativos e aditivos ; ambos visam responder às diferentes formas de avaliar a influência da heterogeneidade entre as unidades na função de intensidade dos processos de contagem . Até agora , a maioria dos estudos tem usado a distribuição gama para o termo de fragilidade , a qual é matematicamente conveniente . Este trabalho mostra que a distribuição gaussiana inversa tem propriedade igualmente simples à distribuição gama . Consequências das diferentes distribuições são examinadas , visando mostrar que a escolha da distribuição de fragilidade é importante . O objetivo deste trabalho é propor alguns métodos estatísticos para a análise de eventos recorrentes e verificar o efeito da introdução do termo aleatório no modelo por meio do estudo do custo , da estimação dos outros parâmetros de interesse . Também um estudo de simulação bootstrap é apresentado para fazer inferências dos parâmetros de interesse . Além_disso  , uma abordagem_Bayesiana  é proposta para os modelos de fragilidade multiplicativos e aditivos . Métodos de simulações são utilizados para avaliar as quantidades de interesse a posteriori . Por fim para ilustrar a metodologia , considera-se um conjunto de dados reais sobre um estudo dos resultados experimentais de animais cancerígenos 
 A dissertação apresenta o problema de corte de estoque , que é um problema de otimização inteiro , difícil de ser resolvido computacionalmente . Resolvemos o problema relaxando a condição de integralidade pelo método simplex com geração de colunas , mas esta solução não é viável na prática . Estudamos várias heurísticas para a obtenção da solução inteira do problema 
 Estruturas do tipo linha/curva ( line-like , curve-like ) , alongadas e ramificadas são comumente encontradas nos ecossistemas que conhecemos . Na biomedicina e na biociências , por exemplo , diversas aplicações podem ser observadas . Justamente por este motivo , extrair este tipo de estrutura em imagens é um constante desafio em problemas de análise de imagens . Porém , diversas dificuldades estão envolvidas neste processo . Normalmente as características espectrais e espaciais destas estruturas podem ser muito complexas e variáveis . Especificamente as mais `` finas '' são muito frágeis a qualquer tipo de processamento realizado na imagem e torna-se muito fácil a perda de informações importantes . Outro problema bastante comum é a ausência de parte das estruturas , seja por motivo de pouca resolução , ou por problemas de aquisição , ou por casos de oclusão . Este trabalho tem por objetivo explorar , descrever e desenvolver técnicas de detecção/segmentação de estruturas finas e ramificadas . Diferentes métodos são utilizados de forma combinada , buscando uma melhor representação topológica e perceptual das estruturas e , assim , melhores_resultados  . Grafos são usados para a representação das estruturas . Esta estrutura de dados vem_sendo  utilizada com sucesso na literatura na resolução de diversos problemas em processamento e análise de imagens . Devido à fragilidade do tipo de estrutura explorado , além das técnicas de processamento de imagens , princípios de visão_computacional  são usados . Busca-se , desta forma , obter um melhor `` entendimento perceptual '' destas estruturas na imagem . Esta informação perceptual e informações contextuais das estruturas são utilizadas em um modelo de campos aleatórios de Markov , buscando o resultado final da detecção através de um processo de otimização . Finalmente , também propomos o uso combinado de diferentes modalidades de imagens simultaneamente . Um software é resultado da implementação do arcabouço desenvolvido e o mesmo é utilizado em duas aplicações para avaliar a abordagem proposta : extração de estradas em imagens de satélite e extração de raízes em imagens de perfis de solo . Resultados do uso da abordagem proposta na extração de estradas em imagens de satélite mostram um melhor desempenho em comparação com método existente na literatura . Além_disso  , a técnica de fusão proposta apresenta melhora significativa de acordo com os resultados apresentados . Resultados inéditos e promissores são apresentados na extração de raízes de plantas 
 O crescimento em quantidade e complexidade dos dados armazenados nas organizações torna a extração de conhecimento utilizando técnicas de mineração uma tarefa ao mesmo tempo fundamental para aproveitar bem esses dados na tomada de decisões estratégicas e de alto custo_computacional  . O custo vem da necessidade de se explorar uma grande quantidade de casos de estudo , em diferentes combinações , para se obter o conhecimento desejado . Tradicionalmente , os dados a explorar são representados como atributos numéricos ou categóricos em uma tabela , que descreve em cada tupla um caso de teste do conjunto sob análise . Embora as mesmas tarefas desenvolvidas para dados tradicionais sejam também necessárias para dados mais complexos , como imagens , grafos , áudio e textos longos , a complexidade das análises e o custo_computacional  envolvidos aumentam significativamente , inviabilizando a maioria das técnicas de análise atuais quando aplicadas a grandes quantidades desses dados complexos . Assim , técnicas de mineração especiais devem ser desenvolvidas . Este Trabalho de Doutorado visa a criação de novas técnicas de mineração para grandes bases de dados complexos . Especificamente , foram desenvolvidas duas novas técnicas de agrupamento e uma nova_técnica  de rotulação e sumarização que são rápidas , escaláveis e bem adequadas à análise de grandes bases de dados complexos . As técnicas propostas foram avaliadas para a análise de bases de dados reais , em escala de Terabytes de dados , contendo até bilhões de objetos complexos , e elas sempre apresentaram resultados de alta qualidade , sendo em quase todos os casos pelo menos uma ordem de magnitude mais rápidas do que os trabalhos_relacionados  mais eficientes . Os dados reais utilizados vêm das seguintes aplicações : diagnóstico automático de câncer de mama , análise de imagens de satélites , e mineração de grafos aplicada a um grande grafo da web coletado pelo Yahoo ! e também a um grafo com todos os usuários da rede social Twitter e suas conexões . Tais resultados indicam que nossos algoritmos permitem a criação de aplicações em tempo real que , potencialmente , não poderiam ser desenvolvidas sem a existência deste Trabalho de Doutorado , como por exemplo , um sistema em escala global para o auxílio ao diagnóstico médico em tempo real , ou um sistema para a busca por áreas de desmatamento na Floresta Amazônica em tempo 
 Neste projeto , é apresentado um método_numérico  com uma abordagem do tipoMAC para a simulação de escoamentos viscoelásticos incompressíveis tridimensionais com superfície_livre  governados pelo modelo de fluido SXPP . A formulação apresentada nesse trabalho é uma extensão dos resultados obtidos por Oishi et_al  . ( 2011 ) , sobre o estudo de métodos_numéricos  para a simulação de escoamentos_incompressíveis  viscoelásticos com superfície_livre  a baixos números de Reynolds , para o caso bidimensional . No contexto de problemas transientes , metodologias explícitas para solução_numérica  das equações governantes apresentam restrições de estabilidade muito severas para a definição do passo temporal , acarretando em um custo_computacional  relativamente alto . Sendo_assim  , utilizamos um método implícito para resolver a equação de conservação da quantidade de movimento , eliminando assim , a restrição de estabilidade parabólica e diminuindo significativamente o custo_computacional  . Mas tal estratégia acopla os campos de velocidade e pressão . Dessa forma , para desacoplar esses campos , foi utilizado uma abordagem que combina método de projeção com uma técnica implícita para o tratamento da pressão na superfície_livre  . A equação_constitutiva  foi resolvida pelo método de Runge-Kutta de segunda-ordem . A validação do método_numérico  foi realizada utilizando refinamento da malha no escoamento em um canal . Como aplicação , apresentamos resultados numéricos sobre o problema do jato oscilante e do inchamento do 
 A crescente disponibilidade de dados em diferentes domínios tem motivado o desenvolvimento de técnicas para descoberta de conhecimento em grandes volumes de dados complexos . Trabalhos recentes mostram que a busca em dados complexos é um campo de pesquisa importante , já que muitas tarefas de mineração de dados , como classificação , detecção de agrupamentos e descoberta de motifs , dependem de algoritmos de busca ao vizinho mais próximo . Para resolver o problema da busca dos vizinhos mais próximos em domínios complexos muitas abordagens determinísticas têm sido propostas com o objetivo de reduzir os efeitos da maldição da alta dimensionalidade . Por outro_lado  , algoritmos probabilísticos têm sido pouco explorados . Técnicas recentes relaxam a precisão dos resultados a fim de reduzir o custo_computacional  da busca . Além_disso  , em problemas de grande escala , uma solução aproximada com uma análise teórica sólida mostra-se mais adequada que uma solução exata com um modelo teórico fraco . Por outro_lado  , apesar de muitas soluções exatas e aproximadas de busca e mineração terem sido propostas , o modelo de programação em CPU impõe restrições de desempenho para esses tipos de solução . Uma abordagem para melhorar o tempo de execução de técnicas de recuperação e mineração de dados em várias ordens de magnitude é empregar arquiteturas emergentes de programação paralela , como a arquitetura CUDA . Neste contexto , este trabalho apresenta uma proposta para buscas kNN de alto_desempenho  baseada numa técnica de hashing e implementações paralelas em CUDA . A técnica proposta é baseada no esquema LSH , ou seja , usa-se projeções em subespac¸os . O LSH é uma solução aproximada e tem a vantagem de permitir consultas de custo sublinear para dados em altas dimensões . Usando implementações massivamente paralelas melhora-se tarefas de mineração de dados . Especificamente , foram desenvolvidos soluções de alto_desempenho  para algoritmos de descoberta de motifs baseados em implementações paralelas de consultas kNN . As implementações massivamente paralelas em CUDA permitem executar estudos_experimentais  sobre grandes_conjuntos  de dados reais e sintéticos . A avaliação de desempenho realizada neste trabalho usando GeForce GTX470 GPU resultou em um aumento de desempenho de até 7 vezes , em média sobre o estado da arte em buscas por similaridade e descoberta de 
 Ferramentas e técnicas de visualização promovem uma análise de dados mais efetiva pelo fato de explorar a capacidade humana na percepção de padrões , principalmente em representações gráficas . Muitos fenômenos são associados a algum tipo de referência , temporal ou geográfica , que pode oferecer informação importante quando são submetidos a processos de análise . Este trabalho aborda representações_visuais  de dados geradas por técnicas de projeção_multidimensional  , e propõe uma estratégia para o tratamento diferenciado das referências temporais ou geográficas presentes em conjuntos de dados , no processo de gerar uma projeção_multidimensional  . Foi proposta e implementada uma variação da técnica Least Square Projection ( LSP ) que evidencia a informação das referências e permite ao usuário interagir com os mapas visuais gerados , bem como diversas funcionalidades que auxiliam no processo de análise exploratória . A nova abordagem é ilustrada por meio de estudos de caso envolvendo bases de dados temporais e com referências geográficas , em que foi possível observar o comportamento global dos elementos , bem como comportamentos de elementos ou grupos de elementos de interesse . Limitações da estratégia proposta também são 
 A presente tese de doutorado propõe uma arquitetura de cloud broker para ambientes de computação em nuvem híbrida . Um cloud broker tem o objetivo de executar a mediação entre clientes e provedores , recebendo requisições dos clientes e encaminhando-as ao serviço do provedor que melhor se adaptar aos requisitos de qualidade de serviço ( QoS ) solicitados . A arquitetura de broker de serviços com QoS proposta denomina-se QBroker , características de implementação de seu modo de operação bem como sua interação com os recursos virtuais de um ambiente de nuvem são apresentadas . O modelo de nuvem considerado foi o de nuvem híbrida com uma caracterização de arquitetura orientada a serviços ( SOA ) na qual serviços remotos são disponibilizados aos clientes . A política de escalonamento de tarefas desenvolvida para o QBroker foi a de intermediação de serviços , considerando tratativas de QoS , diferenciação das instâncias de serviços ( SOA ) e alocação dinâmica de serviços . Além_disso  , toda a caracterização do modo de operação do QBroker foi baseada no conceito de intermediação do modelo de referência de nuvem do NIST . O componente QBroker foi introduzido numa arquitetura de computação em nuvem BEQoS ( Bursty Energy and Quality of Service ) , desenvolvida no Laboratório de Sistemas_Distribuídos  e Programação Concorrente do ICMC-USP de São Carlos . Avaliações de desempenho para a implementação da arquitetura QBroker foram conduzidas por meio de programas de simulação com uso da API do simulador CloudSim e da arquitetura CloudSim-BEQoS . Três cenários experimentais foram avaliados e , segundo a análise de resultados efetuada , foi possível validar que as características arquiteturais implementadas no QBroker resultaram em significativo impacto nas variáveis de resposta consideradas . Assim , foi possível comprovar que o uso do QBroker como mecanismo de mediação em ambientes de nuvem híbrida com SOA promoveu ganhos em desempenho para o sistema de nuvem e permitiu melhoria na qualidade dos serviços oferecidos 
 Linhas de Produtos de Software ( LPS ) abstraem a semelhança entre produtos e envolvem o conceito de reúso de software para desenvolver software em larga_escala  com mais rapidez e qualidade . O reúso dos artefatos em uma LPS é planejado e executado desde sua concepção , sendo assim possível armazenar esses artefatos em um repositório ( núcleo de ativos reusáveis ) para utilização posterior . Porém , ao se construir um repositório para uma certa LPS , este atende apenas às especificações desta arquitetura e não promove o reúso desses ativos em outras LPS , causando uma certa perda no potencial de reusabilidade dos ativos produzidos e armazenados . Além_disso  , a definição e os dados da execução do processo de desenvolvimento dessas LPS também não são armazenados prevendo o reúso , o que gera retrabalho para definir e instanciar um processo de software sempre que uma nova LPS é criada . Neste contexto , este trabalho tem por objetivo apresentar uma abordagem baseada em um conjunto de serviços para promover o amplo reúso de ativos produzidos e de elementos de processo utilizados , além de facilitar a criação e o desenvolvimento de linha de produtos de software , permitindo o gerenciamento do núcleo de ativos reusáveis . Por possuir uma interface baseada em serviços , esta abordagem poderá ser incorporada também a outros ambientes . Para validar a abordagem , verificar vantagens e sugerir melhorias , foi conduzida uma prova de conceito com sua aplicação . Os resultados dessa prova comprovaram a viabilidade da abordagem em cumprir os objetivos propostos 
 Engenharia de Linha de Produtos de Software ( ELPS ) contribui para a redução dos custos de desenvolvimento e de manutenção , a melhoria do time-to-market , e o aumento da qualidade de produtos desenvolvidos a partir de uma família de produtos por meio do reuso sistemático dos ativos principais da linha de produtos . A ELPS vem_sendo  utilizada com sucesso no desenvolvimento de sistemas embarcados_críticos  , especificamente nos domínios de sistemas automotivos e aeroespaciais . Sistemas embarcados_críticos  devem ser desenvolvidos de acordo com os requisitos definidos em padrões de segurança , que demandam a produção de artefatos de análise de segurança , árvores de falhas e casos de segurança . Entretanto , a realização de atividades de análise de segurança , análise de árvores de falhas e construção de casos de segurança de forma_manual  para cada produto de uma linha de produtos é uma tarefa demorada e propensa a erros . O gerenciamento de variabilidade em artefatos de análise de segurança pode ser automatizado com o apoio de técnicas de gerenciamento de variabilidades . Em virtude de safety ser uma propriedade dependente de contexto , a variabilidade no projeto e contexto inerente uma linha de produtos software impacta na definição de propriedades de segurança do sistema , modificando as ameaças à segurança do sistema , suas causas e riscos , medidas de mitigação aplicáveis , e resultados de análise de árvore de falhas . Dessa forma , gerenciar variabilidades em artefatos relacionados à safety em diferentes níveis de abstração aumenta a complexidade do modelo de variabilidade mesmo com o apoio de técnicas de gerenciamento de variabilidades . Para alcançar o equilíbrio eficaz entre os benefícios e a complexidade da adoção de uma abordagem de ELPS para o desenvolvimento de sistemas embarcados_críticos  é necessário fazer a distinção entre artefatos de safety reusáveis , em que a variabilidade deve ser gerenciada , e artefatos de safety que devem ser gerados a partir de artefatos reusáveis . Por outro_lado  , tanto a indústria quanto os padrões de segurança têm reconhecido o uso de técnicas dirigidas a modelos para apoiar a análise segurança e a construção de casos de segurança . Técnicas de análise de segurança composicional e otimização de projeto , e de construção de casos de segurança dirigido a modelos vêm_sendo  utilizadas para apoiar a geração de artefatos de safety requeridos para certificação . O objetivo desta tese é a proposta de uma abordagem dirigida a modelos que integra técnicas de desenvolvimento dirigido a modelos , análise de segurança composicional e otimização de projeto , e construção de casos de segurança dirigido a modelos para apoiar o reuso sistemático e a geração de artefatos de safety em engenharia de linhas de produtos de sistemas embarcados_críticos  . A abordagem proposta reduz o esforço e os custos de análise e avaliação de segurança para produtos de uma linha de produtos , uma vez que tal análise é realizada a partir de artefatos de safety reusados . Assim , artefatos como análises de árvores de falhas e de modos de falha e efeitos , e casos de segurança requeridos para certificação podem ser gerados automaticamente com o apoio de técnicas dirigidas a modelos 
 Sistemas de acompanhamento musical automatizado são de grande utilidade como ferramentas didáticas . Com eles , um aluno de música consegue ensaiar uma peça musical sendo acompanhado pelo computador , com a liberdade de tocar na velocidade em que desejar e , além disso , de cometer erros acidentais ou desvios intencionais em relação ao esperado ( definido em partitura ) . Um sistema de acompanhamento musical deve ter as habilidades de analisar a execução ao vivo de um músico ( pré-processador da entrada ) , comparar os eventos recebidos com os de uma partitura pré-carregada ( rastreador ) , gerar o acompanhamento conforme o andamento inferido ( acompanhador ) e sintetizar os sons relativos a esse acompanhamento ( sintetizador ) . O trabalho apresentado neste texto consistiu em estudar diversas técnicas relativas ao problema de acompanhamento musical , implementar alguns algoritmos baseados na literatura abordada , validar a qualidade obtida com tais algoritmos e disponibilizar um sistema de código_aberto  modular com mais de uma alternativa de algoritmo para cada módulo . Além_disso  , foi desenvolvido um elemento adicional para o sistema - o MetaRastreador - que combina todas as técnicas de rastreamento implementadas , executando-as em paralelo e obtendo uma maior confiabilidade nas informações extraídas da entrada , considerando diversas opiniões diferentes sobre a posição do músico na partitura 
 Com o advento da tecnologia de microarray , uma grande quantidade de dados de expressão_gênica  encontra-se disponível . Após a extração das taxas de expressão dos genes , técnicas de formação de clusters são utilizadas para a análise dos dados . Diante da diversidade do conhecimento que pode ser extraído dos dados de expressão_gênica  , existe a necessidade de diferentes técnicas de formação de clusters . O modelo dinâmico desenvolvido em ( Zhao et . al . 2003a ) apresenta diversas características interessantes para o problema de formação de clusters , entre as quais podemos citar : a não necessidade de fornecer o número de cluster , a propriedade de multi-escala , serem altamente paralelos e , principalmente , permitirem a inserção de regras e mecanismos mais complexos para a formação dos clusters . Todavia , este modelo apresenta dificuldades em determinar clusters de formato e tamanho arbitrários , além de não realizar a clusterização hierárquica , sendo estas duas características desejáveis para uma técnica de clusterização . Neste trabalho , foram desenvolvidas três técnicas para superar as limitações do modelo dinâmico proposto em ( Zhao et . al . 2003a ) . O Modelo1 , o qual é uma simplificação do modelo dinâmico original , porém mais eficiente . O Modelo2 , que a partir da inserção de um novo conjunto de elementos no modelo dinâmico , permite a formação de clusters de formato e tamanho arbitrário . E um algoritmo para a clusterização hierárquica que utiliza o Modelo1 como bloco de construção . Os modelos desenvolvidos foram aplicados em dados biológicos , segmentando imagens de microarray e auxiliando na análise do conjunto expressão de genes de St. Jude Leukemia 
 A Engenharia de Software evolui gradativamente para uma disciplina científica baseada em observação , formulação teórica e experimentação . Nesse contexto , estudos_experimentais  têm sido conduzidos para proporcionar evidências sobre a qualidade e a produtividade de técnicas , ferramentas e métodos de desenvolvimento de software . Porém , resultados obtidos a partir de pesquisas experimentais são úteis somente se os pesquisadores puderem consolidá-las em um contexto significativo de conhecimento . O projeto Readers : A Collaborative Research to Develop , Validate and Package Reading Techniques for Software Defect Detection aborda diversos aspectos envolvidos na construção de um corpo significativo de resultados a partir de experimentos controlados . Nesse contexto , sustenta-se que a aplicação de meios alternativos de análise e exploração de dados dos experimentos conduzidos pode colaborar para a evolução do Pacote de Laboratório , e como efeito colateral , contribuir com a formação do corpo de conhecimento almejado . Este projeto propõe mecanismos para integrar técnicas de visualização exploratória à análise de dados de experimentos controlados . Para isso , foram propostas adequações ao processo de experimentação , bem como um processo de visualização para Engenharia de Software Experimental ? V iDAESE . Para ilustrar o V iDAESE , ciclos de análises são apresentados , juntamente com as conclusões obtidas em cada um . As análises visuais apóiam o processo de consolidação de conhecimento a partir dos dados e a melhoria do Pacote de Laboratório . Adicionalmente , uma abordagem para simulação de dados experimentais foi proposta , permitindo explorar cenários que apóiem o planejamento de experimento 
 Técnicas de Visualização Exploratória acopladas a estratégias de interação podem ser muito úteis para apoiar processos de descoberta de conhecimento a partir de dados . Múltiplas visualizações de um mesmo conjunto de dados permitem observá-lo sob várias perspectivas , bem como explorar os pontos fortes e minimizar o efeito dos pontos fracos de técnicas específicas . Nesse contexto , é interessante que as múltiplas visualizações estejam coordenadas de forma que ações de interação executadas pelo usuário sobre uma delas possam ser propagadas para as demais durante as etapas exploratórias . Um aspecto complexo do projeto de ferramentas de visualização é justamente como acoplar representações_visuais  e controles interativos de maneira flexível . Esse projeto de mestrado abordou a concepção e implementação de um modelo de coordenação de ações capaz de tratar a coordenação entre múltiplas visualizações de forma genérica e flexível . Para isso , foram estudados alguns modelos de coordenação descritos na literatura e utilizados em Sistemas de Visualização de uso geral . O modelo proposto foi implementado no InfoVis , uma plataforma de software extensível que incorpora múltiplas técnicas de visualização exploratória , em desenvolvimento no 
 A criação e atualização de bases de conhecimentos com dados que comprovem , ou não , a eficácia de métodos , técnicas ou produtos de software podem ser facilitadas por meio de estudos_experimentais  e suas replicações em diferentes contextos , para que a base de conhecimento possa ser ampliada e consolidada . Porém , a condução dessas replicações gera uma grande quantidade de dados que devem ser coletados e armazenados , e posteriormente analisados , sendo que a gestão dessas informações de forma_manual  é propensa a erros . Nesse contexto , o Projeto Readers - um projeto de pesquisa colaborativa formado por pesquisadores brasileiros e norte-americanos - no qual este trabalho está inserido , teve como objetivos principais desenvolver , validar , empacotar e consolidar uma base de conhecimento sobre algumas técnicas , como por exemplo a PBR ( Perspective Based Reading ) , que é uma técnica de leitura para detecção de defeitos em artefatos de software . Com as várias replicações de estudos_experimentais  que foram conduzidas no âmbito desse projeto , em particular replicações do Experimento PBR ( Basili et al. , 1996 ) , ficou evidenciada a necessidade de apoio computacional em diversas etapas do processo de experimentação . Sendo_assim  , neste trabalho foi definida , modelada e implementada a ferramenta ExpVVT com a finalidade de automatizar as atividades de Planejamento e Operação do Experimento PBR . A ferramenta foi implementada com base em uma estrutura de meta-tabelas , o que facilita possíveis alterações no âmbito desse experimento . Salienta-se que o Pacote de Laboratório , o qual contém todos os artefatos necessários para que um estudo_experimental  possa ser replicado , também fica armazenado eletronicamente , com o apoio da ferramenta , o que facilita o registro de sua evolução . O uso da ferramenta foi explorado simulando-a com os dados de uma das replicações do Experimento PBR que foi conduzida no contexto do Projeto Readers . Embora tenha sido apenas uma simulação , pode-se perceber o auxílio que essa ferramenta pode trazer para futuras replicações desse experimento , reduzindo os custos associados com todo material necessário para uma replicação e também guiando os participantes do experimento , para que as atividades sejam feitas de forma mais controlada 
 Este trabalho insere-se no contexto de métodos de multiplicadores para a resolução de problemas de programação convexa semidefinida e a análise de suas propriedades através do método proximal aplicado sobre o problema dual . Nosso foco será uma subclasse de problemas de programação convexa semidefinida com restrições afins , para a qual estudaremos relações de dualidade e condições para a existência de soluções dos problemas primal e dual . Em seguida , analisaremos dois métodos de multiplicadores para resolver essa classe de problemas e que são extensões de métodos conhecidos para programação não-linear . O primeiro , proposto por Doljansky e Teboulle , aborda um método de ponto proximal interior entrópico e sua conexão com um método de multiplicadores exponenciais . O segundo , apresentado por Mosheyev e Zibulevsky , estende para a classe de problemas de nosso interesse um método de lagrangianos aumentados suaves proposto por Ben-Tal e Zibulevsky . Por fim , apresentamos os resultados de testes numéricos feitos com o algoritmo proposto por Mosheyev e Zibulevsky , analisando diferentes escolhas de parâmetros , o aproveitamento do padrão de esparsidade das matrizes do problema e critérios para a resolução aproximada dos subproblemas irrestritos que devem ser resolvidos a cada iteração desse algoritmo de lagrangianos aumentados 
 O foco central desta tese é o desenvolvimento de algoritmos para o problema de recoloração convexa de caminhos . Neste problema , é dado um caminho cujos vértices estão coloridos arbitrariamente , e o objetivo é recolorir o menor número possível de vértices de modo a obter uma coloração convexa . Dizemos que uma coloração de um grafo é convexa se , para cada cor , o subgrafo induzido pelos vértices dessa cor é conexo . Sabe-se que este problema é NP-difícil . Associamos a este problema um poliedro , e estudamos sua estrutura facial , com vistas ao desenvolvimento de um algoritmo . Mostramos várias inequações válidas para este poliedro , e provamos que várias delas definem facetas . Apresentamos um algoritmo de programação_dinâmica  que resolve em tempo polinomial o problema da separação para uma classe grande de inequações que definem facetas . Implementamos um algoritmo branch-and-cut baseado nesses resultados , e realizamos testes computacionais com instâncias geradas aleatoriamente . Apresentamos adicionalmente uma heurística baseada numa formulação linear que obtivemos . Estudamos também um caso especial deste problema , no qual as instâncias consistem em caminhos coloridos , onde cada cor ocorre no máximo duas vezes . Apresentamos um algoritmo de 3/2-aproximação para este caso , que é também NP-difícil . Para o caso geral , é conhecido na literatura um algoritmo de 2-aproximação 
 Neste projeto apresentamos três modelos para publicidade na Internet , e mecanismos associados a cada modelo , analisando suas propriedades como estabilidade , otimalidade , prova de estratégia , e consumo de tempo . Primeiramente apresentamos um dos primeiros artigos publicados sobre o problema de leilões para publicidade na Internet , que descreve essencialmente o funcionamento dos atuais leilões para publidade na Internet . Em seguida , apresentamos outros dois modelos . O primeiro deles permite publicidade com exclusividade . O segundo modelo permite mais opções ao leiloeiro e aos anunciantes , ao permitir preços mínimos de venda e preços máximos de compra 
 A segmentação de imagens e , em visão_computacional  , uma tarefa de grande_importância  , para a qual existem várias abordagem . A complexidade de tais abordagens está relacionada à natureza da imagem e também ao grau de precisão da segmentação , que e um conceito bastante subjetivo , normalmente associado a semelhança que apresenta a segmentaçã produzida pela visão humana . Na segmentação de imagens baseada em algoritmos de agrupamento em grafos , geralmente os pixels da imagem compôem os nós do grafo e as arestas representam a similaridade entre estes nós . Assim , a segmentação pode ser obtida por meio do agrupamento dos nós do grafo . É importante salientar , no entanto , que as técnicas de agrupamento em grafos surgiram no contexto de reconhecimento de padrões , cujo_objetivo  primario era o tratamento de dados diversos que não envolviam imagens . O uso de tais tecnicas para a segmentação de imagens e relativamente recente e revela alguns problemas desaadores . O primeiro deles é a deficiente escalabilidade de alguns métodos , o que impede o seu uso efetivo em imagens de altas dimensões . Outra questão é a falta de estudos que avaliam as medidas de similaridade na montagem do grafo e critérios que aferem a qualidade do agrupamento para a área específica de segmentação de imagens . Em outras_palavras  , faltam na literatura análises mais específicas que indiquem quais algoritmos de agrupamento em grafos são mais efetivos para a segmentação de imagens e que procurem associar ( ou correlacionar ) as várias medidas de similaridade e métricas de qualidade de agrupamento que produzam segmentações mais precisas . Neste trabalho é apresentada a avaliação de 6 algoritmos de agrupamento em grafos formulados em base a 3 categorias identificadas ( agrupamento espectral , algoritmos de particionamento multinível e algoritmos para detectar comunidades ) e aplicadas na segmentação automática de imagens de cenas naturais com grandes dimensões . Esta avaliação objetiva aferir , sobretudo , a qualidade da segmentação , a escalabilidade , o desempenho de 7 funções de similaridade formuladas , e também visa corroborar a existência da correlação entre a qualidade do agrupamento e a qualidade da segmentação . Para reduzir o esforço computacional e contribuir com a escalabilidade dos algoritmos formulados é utilizado um algoritmo de pré-processamento ( SLIC ) que agrupa váarios pixels da imagem em uma unica região ( superpixels ) , o que contribui para reduzir o tamanho do grafo e , consequentemente , reduzindo o custo_computacional  do agrupamento . Os resultados demostram que os algoritmos formulados LP ( Label Propagation ) e FG ( Fast Greedy ) apresentam boa escalabilidade e boa_qualidade  de segmentação . Seis das sete funções de similaridade avaliadas apresentam um bom_desempenho  , independentemente do algoritmo de agrupamento empregado . É mostrado também que exites correlação entre a medida de qualidade de agrupamento conhecido como índice de silhueta e a qualidade de segmentação , ou seja , quanto maior o valor de silhueta , melhor a segmentação . A qualidade de segmentação foi avaliada quantitativamente , utilizando-se um conjunto de imagens segmentadas 
 ALGORITMOS de Estimação de Distribuição ( EDAs ) compõem uma frente de pesquisa em Computação Evolutiva que tem apresentado resultados promissores para lidar com problemas complexos de larga_escala  . Nesse contexto , destaca-se o Algoritmo de Otimização Bayesiano ( BOA ) que usa um modelo probabilístico multivariado ( representado por uma rede Bayesiana ) para gerar novas soluções a cada iteração . Baseado no BOA e na investigação de algoritmos de detecção de estrutura de comunidades ( para melhorar os modelos multivariados construídos ) , propõe-se dois novos algoritmos denominados CD-BOA e StrOp . Mostra-se que ambos apresentam vantagens significativas em relação ao BOA . O CD-BOA mostra-se mais flexível que o BOA , ao apresentar uma maior robustez a variações dos valores de parâmetros de entrada , facilitando o tratamento de uma maior diversidade de problemas do mundo_real  . Diferentemente do CD-BOA e BOA , o StrOp mostra que a detecção de comunidades a partir de uma rede Bayesiana pode modelar mais adequadamente problemas decomponíveis , reestruturando-os em subproblemas mais simples , que podem ser resolvidos por uma busca gulosa , resultando em uma solução para o problema original que pode ser ótima no caso de problemas perfeitamente decomponíveis , ou uma aproximação , caso contrário . Também é proposta uma nova_técnica  de reamostragens para EDAs ( denominada REDA ) . Essa técnica possibilita a obtenção de modelos_probabilísticos  mais representativos , aumentando significativamente o desempenho do CD-BOA e StrOp . De uma forma geral , é demonstrado que , para os casos testados , CD-BOA e StrOp necessitam de um menor tempo de execução do que o BOA . Tal comprovação é feita tanto experimentalmente quanto por análise das complexidades dos algoritmos . As características principais desses algoritmos são avaliadas para a resolução de diferentes problemas , mapeando assim suas contribuições para a área de Computação 
 Neste trabalho foi feita uma investigação sobre a realização de processamento de áudio digital em tempo real utilizando três dispositivos com características computacionais fundamentalmente distintas porém bastante acessíveis em termos de custo e disponibilidade de tecnologia : Arduino , GPU e Android . Arduino é um dispositivo com licenças de hardware e software abertas , baseado em um microcontrolador com baixo poder de processamento , muito utilizado como plataforma educativa e artística para computações de controle e interface com outros dispositivos . GPU é uma arquitetura de placas de vídeo com foco no processamento paralelo , que tem motivado o estudo de modelos de programação específicos para sua utilização como dispositivo de processamento de propósito_geral  . Android é um sistema_operacional  para dispositivos_móveis  baseado no kernel do Linux , que permite o desenvolvimento de aplicativos utilizando linguagem de alto nível e possibilita o uso da infraestrutura de sensores , conectividade e mobilidade disponível nos aparelhos . Buscamos sistematizar as limitações e possibilidades de cada plataforma através da implementação de técnicas de processamento de áudio digital em tempo real e da análise da intensidade computacional em cada ambiente 
 Este trabalho tem como fulcro o desenvolvimento de um gerador de analisadores sintáticos do tipo top-down para gramáticas LL ( 1 ) com entrada gráfica da gramática , bem como uma comparação do mesmo com outros geradores em uso no mercado . Como resultado foi obtido um gerador totalmente funcional , e foi mostrado como ele é superior aos outros analisadores . São descritos detalhes da implementação e foi elaborado um manual de uso do sistema implementado em Java independente de ambientes de programação 
 A expansão da modalidade de Educação a Distância ( EAD ) e a presença cada vez maior das Tecnologias de Informação e Comunicação ( TIC ) na educação , tornam a utilização de ferramentas computacionais imprescindíveis no auxílio aos professores , tanto para produção de conteúdos digitais , quanto na tarefa de avaliação do aprendizado . A avaliação do aprendizado acarreta muito trabalho ao professor , principalmente aquelas de caráter formativo , que favorecem o desenvolvimento continuado do aprendizado dos alunos . Uma forma de aliviar essa carga de trabalho tem sido a utilização dos Sistemas Gerenciadores de Curso ( SGC ) . Dentre esses sistemas encontra-se o Moodle , que disponibiliza o módulo Quiz , que dispõe de recursos para avaliações automáticas . Contudo seu uso não é simples , sendo frequentes as reclamações de professores quanto a sua usabilidade , o que pode desmotivar seu uso . Nesse contexto , o objetivo do presente_trabalho  é ajudar a compreender e reduzir as dificuldades enfrentadas por professores no emprego do módulo Quiz do Moodle . Para isso foi realizado um estudo para identificar os principais elementos que interferem na usabilidade de ferramentas para autoria de questionários online . Os resultados obtidos foram utilizados para modelar e produzir um protótipo alternativo ao do módulo Quiz , denominado de iQuiz , que resultou na seguinte contribuição tecnológica : uma interface para aplicação sobre o núcleo do Quiz e um modelo para integração dos componentes envolvidos na composição do novo ambiente . Para a criação do protótipo foi aplicado a abordagem de desenvolvimento centrado no usuário , utilizando testes de usabilidade em conjunto com outros métodos , tais como avaliação heurística . Neste trabalho também apresentamos os resultados de uma avaliação do modelo implementado , indicando uma efetiva melhoria de usabilidade . Tanto especialistas em TIC e professores participantes das avaliações mostraram-se mais confortáveis com o uso do iQuiz , alcançando melhores_resultados  quanto à produção de questionários 
 O objetivo deste trabalho é reconhecer gestos em tempo real apenas com o uso de câmeras , sem marcadores , roupas ou qualquer outro tipo de sensor . A montagem do ambiente de captura é simples , com apenas duas câmeras e um computador . O fundo deve ser estático , e contrastar com o usuário . A ausência de marcadores ou roupas especiais dificulta a tarefa de localizar os membros . A motivação desta tese é criar um ambiente de realidade virtual para treino de goleiros , que possibilite corrigir erros de movimentação , posicionamento e de escolha do método de defesa . A técnica desenvolvida pode ser aplicada para qualquer atividade que envolva gestos ou movimentos do corpo . O reconhecimento de gestos começa com a detecção da região da imagem onde se encontra o usuário . Nessa região , localizamos as regiões mais salientes como candidatas a extremidades do corpo , ou seja , mãos , pés e cabeça . As extremidades encontradas recebem um rótulo que indica a parte do corpo que deve representar . Um vetor com as coordenadas das extremidades é gerado . Para descobrir qual a pose do usuário , o vetor com as coordenadas das suas extremidades é classificado . O passo final é a classificação temporal , ou seja , o reconhecimento do gesto . A técnica desenvolvida é robusta , funcionando bem mesmo quando o sistema foi treinado com um usuário e aplicado a dados de outro 
 A implementação de uma aplicação cliente/servidor de vídeo apresenta aspectos complexos e que exigem tratamento diferenciado , como trabalhar com arquivos que ocupam grande espaço de armazenamento e fluxos de dados que necessitam de alta largura de banda para serem transmitidos . Quando o fluxo de dados é compactado e a transmissão é feita com técnicas de multicasting , a transmissão se torna ainda mais complexa . Um exemplo típico é a utilização de pacotes UDP , que podem ser perdidos e/ou chegarem ao destino desordenados , durante uma sessão de transmissão e recepção . Nesta dissertação são discutidas as implementações de duas aplicações cliente/servidor que exploram as técnicas de multicasting ( uma delas possuindo suporte para IPv+ e RTP , e a outra suporte para IPv4 ou IPv6 ) . A problemática do envio e recebimento de pacotes para posterior exibição do vídeo pelo cliente foi estudada e alguns testes foram feitos com os padrões de compressão MJPEG e um método proposto , implementado com transformadas warelets e codificação LZW . Medidas e comparações de desempenho foram realizadas , utilizando-se os sistemas operacionais Linux e Windows . As conclusões obtidas com a metodologia aplicada a este trabalho podem contribuir para a solução da problemática da transmissão de vídeo em ambientes multicasting , especialmente para o caso de extensões e refinamentos nas implementações realizadas e no desenvolvimento de aplicações que incluam outros componentes de hardware e software 
 A complexidade transacional envolvida na atualização de um Data Warehouse ( DW ) é considerável . Tal complexidade pode inviabilizar o projeto do DW . Em alguns_casos  o ambiente operacional não consegue atender as requisições de consultas do DW . Isto acontece quando não existe agilidade para responder as consultas no ambiente operacional ou quando a sobrecarga para atualização do DW é grande . Este trabalho apresenta uma nova estrutura para armazenamento de dados denominada de Data Warehouse Temporal ( D ) . Esta nova estrutura objetiva diminuir a complexidade da comunicação entre o ambiente operacional e analítico . Tal estrutura facilita as atualizações dos dados históricos dos ambientes operacionais para o Data Warehouse 
 O projeto ARARA - Autonomous and Radio-Assisted Reconnaissance Aircraft ( aeronaves de reconhecimento assistidas por rádio e autônomas ) , está centrado no uso de aeronaves no tripuladas para a aquisição de dados . O seu objetivo principal está na substituição das aeronaves convencionais usadas na obtenção de fotografias aéreas no monitoramento de plantações e áreas sob controle ambiental . Neste trabalho 6 descrito o sistema SiNaCoM , Sistema de Navegação e Controle de Missão , para o projeto ARARA , que , juntamente com um sistema de aumento de estabilidade , permitirá que aeronaves do projeto ARARA possam realizar missões de forma completamente autônoma . O sistema SiNaCoM está dividido em dois módulos : o Planejador de Missão e o Sistema de Navegação . O primeiro módulo permite que o usuário defina uma rota e um conjunto de tarefas associado a cada waypoint da rota que será seguida pela aeronave . O segundo módulo controla a navegação da aeronave , ao longo da rota traçada pelo usuário , incluindo as correções de curso necessárias ocasionadas por rajadas de vento . A navegação 6 baseada no sistema de posicionamento GPS e em um conjunto de sensores de navegação instalados a bordo da aeronave . O Planejador de Missão foi implementado e produz na saída um plano de vôo para ser carregado na aeronave . O sistema de navegação foi completamente definido , projetado e o algoritmo foi especificado em sua quase totalidade . A implementação depende do hardware que será utilizado a bordo das aeronaves e foi deixada como trabalho futuro 
 A competição pelo mercado de comunicações e o crescimento do número de aplicações e de máquinas conectadas à infra-estrutura de redes estão tomando dimensões imprevisíveis . Redes de Alto Desempenho , desenvolvimento de novos protocolos e de técnicas para transmissão de vídeo são extremamente necessários nesta nova configuração das redes . O presente projeto tem como foco o uso do protocolo IPv6 e a técnica de multicasting para o desenvolvimento de aplicações multimídia proporcionando economia de largura de banda e de recursos nos servidores . Apresenta uma avaliação da implementação em JAVA do IPv6 multicasting ( Java IP version Six readY - JIPSY ) através do desenvolvimento de uma aplicação de vídeo e sua comparação com o protocolos IPv4 . Este trabalho foi dividido em quatro partes : revisão da literatura relacionada ao protocolo IPv6 , multicasting e ATM ; configuração de um ambiente de testes ; desenvolvimento de uma aplicação usando JIPSY e testes . Os resultados obtidos_mostram  uma comparação do uso da rede e do tempo de transmissão do vídeo usando os protocolos IPv4 e IPv6 e as formas de endereçamento multicast e unicast 
 Dados importantes para o acompanhamento de uma área agrícola podem ser avaliados através de imagens aéreas . Dentre estes , destaca-se como um dos mais significativos , a identificação e a classificação da cobertura do solo . A grande dificuldade reside na não disponibilidade de metodologias apropriadas para a análise e a classificação dos padrões de cobertura , principalmente para monitoramento de pequenas propriedades . Imagens de cobertura são imagens complexas , com padrões dificeis de serem definidos . Os padrões variam para cada tipo de solo , dependem dascondições a & condições de iluminação ambiente , da resolução da imagem , do tipo de planta e resíduos orgânicos sobre o solo , dentre outros fatores . A extração de atributos de cada pixel é de extrema importância na diferenciação das regiões . Neste trabalho , apresenta-se uma revisão das principais técnicas de segmentação de imagens_digitais  que serviram de base para a escolha dos métodos utilizados . A cor foi a característica discriminante utilizada com o objetivo de segmentar de forma automática diferentes padrões de cobertura do solo . Foram testados métodos clássicos de análise como a transformada de Hotelling e o discriminante linear de Mahalanobis . Também foram estudadas técnicas não convencionais , como as Redes_Neurais  , principalmente pela possibilidade de implementação em hardware específico de alto_desempenho  . Foram selecionados modelos de redes supervisionadas e não supervisionadas . Os resultados obtidos indicam a viabilidade de utilização das técnicas avaliadas neste trabalho na segmentação de imagens aéreas e mostram suas limitações e vantagens principais 
 O propósito principal desta dissertação de Mestrado é a apresentação de um método para análise e projeto de sistemas de workflow administrativo com interface na Web . O processo de desenvolvimento proposto baseia-se na integração de métodos e técnicas preexistentes , específicas para modelar sistemas de workflow e aplicações_hipermídia  em geral . As técnicas integradas são : a UML , que é a notação padrão para a modelagem de sistemas orientados a objetos , amplamente aceita pela comunidade de Engenharia de Software , o BPI , que consiste em um processo direcionado especificamente para o desenvolvimento de sistemas de workflow ; o RMM , que consiste na primeira metodologia estendida com o intuito de modelar aplicações_hipermídia  para a Web , e o W2000 , que consiste em um framework para o projeto de aplicações para a Web baseado no HDM - o antecessor de uma família formada por vários métodos que apóiam o projeto de aplicações_hipermídia  . Para testar o método proposto , foi desenvolvido um sistema para administração de cursos pela Web . Esse sistema , denominado Atena , tem características de administração de workflows e foi usado inicialmente como base para o desenvolvimento do método e , em um momento posterior , como base para testar a abrangência e adequação do método ao domínio proposto 
 No mundo moderno as pessoas estão cada vez mais atarefadas e este acúmulo de tarefas ocasiona uma alta demanda por sistemas e dispositivos que facilitem seu dia-a-dia . Setores empresariais diversos voltaram sua atenção a esse mercado emergente . O interesse se estende desde empresas fabricantes de computadores e equipamentos eletro-eletrônicos até companhias fornecedoras de energia elétrica , gás e água . Atualmente , o grande desafio consiste em integrar as diversas tecnologias utilizadas por uma grande diversidade de fabricantes de sistemas e dispositivos . O principal objetivo deste trabalho é o projeto de um sistema de gerenciamento de residências com interface simplificada , o SmartHome , de fácil utilização por pessoas idosas e/ou inválidas . A análise de requisitos do sistema é baseada em um estudo de caso de uma residência futurista . É introduzida uma linguagem gráfica para a representação das sentenças de comando aceitas pelo sistema . O desenvolvimento do sistema foi feito utilizando-se uma metodologia baseada na notação UML . Um protótipo da interface do sistema Smartl-Iome foi implementado , permitindo que usuários possam realizar requisições por meio do mouse ou reconhecimento de voz . O mouse no protótipo substitui a tela sensível a toques prevista no projeto da interface , para facilidade de desenvolvimento . Os testes iniciais mostraram que a interface Smartl-Iome uniformiza o modo de uso dos dispositivos conectados na rede doméstica . Adicionalmente , a interface apresenta um elevado grau de intuição com a utilização de uma linguagem de comandos gráfica e comandos por voz , o que reduz o período de aprendizado de novos usuários 
 A avaliação de desempenho de sistemas computacionais tem constituído uma área de grande investigação ao longo das últimas_décadas  . Dentre as diversas ferramentas e técnicas disponíveis para esse propósito , a simulação se destaca por ser flexível , podendo ser utilizada em várias situações e por oferecer soluções a um custo relativamente atrativo . O objetivo deste trabalho é avaliar o desempenho da ferramenta WARPED para simulação distribuída , através de alguns modelos de Redes de Filas . O WARPED é um simulador distribuído otimista que implementa o paradigma de Virtual Time , tendo sido escolhido por permitir a execução de simulações tanto em um sistema distribuído , como em uma máquina_paralela  , e por ser um software_livre  ( código_aberto  ) . O projeto WARPED implementa um núcleo para o simulador Time Warp , livremente disponível , de fácil portabilidade , simples de se modificar e de ser estendido . O sistema foi escrito em C++ ( GNU C++ ) , orientado a objeto e utiliza o MPI ( Message Passing Interface ) para troca de mensagens entre os processos distribuídos . Nos experimentos_realizados  neste trabalho , foi observado que aumentando-se a granulosidade dos modelos , speedup pode ser alcançado com a simulação distribuída . A comunicação entre processos fisicamente separados pode sobrecarregar a execução da simulação distribuída , podendo mesmo inviabilizar sua utilização . Por isso , a partição do modelo de simulação deve ser feita considerando-se as características do modelo e da arquitetura de hardware que será usada . Em geral , deve-se tentar minimizar a necessidade de comunicação e submeter aos processadores uma carga de trabalho de tamanho adequado , explorando granulosidades de média a grossa . No caso estudado , o próprio sistema de passagem de mensagens ( MPICH ) introduz sobrecargas difíceis de serem contornadas . Modelos simples dificilmente levarão a ganho de desempenho , sendo preferido o uso da simulação seqüencial . Modelos de complexidade média podem obter ganhos de desempenho , desde que haja um balanceamento cauteloso entre a granulosidade dos processos lógicos e a necessidade de comunicação entre processos alocados em máquinas distintas . Apesar de nem sempre ser fácil obter-se speedup com as simulações distribuídas , adotando-se a granulosidade adequada e minimizando-se a comunicação , bons_resultados  podem ser sempre esperados 
 O acesso a dados baseado em ontologia ( OBDA , de Ontology-Based Data Access ) propõe facilitar ao usuário acesso a dados sem o conhecimento específico de como eles estão armazenados em suas fontes . Para isso , faz-se uso de uma ontologia como camada conceitual de alto nível , explorando sua capacidade de descrever o domínio e lidar com a incompletude dos dados . Atualmente , os sistemas NoSQL ( Not Only SQL ) estão se tornando populares , oferecendo recursos que os sistemas de bancos de dados relacionais não suportam . Desta forma , surgiu a necessidade dos sistemas OBDA se moldarem a estes novos tipos de bancos de dados . O objetivo desta pesquisa é propor uma arquitetura nova para sistemas OBDA possibilitando o acesso a dados em bancos de dados relacionais e bancos de dados NoSQL . Para tal , foi proposta a utilização de um mapeamento mais simples responsável pela comunicação entre ontologia e bancos de dados . Foram construídos dois protótipos de sistemas OBDA para sistemas NoSQL e sistemas de bancos de dados relacional para uma validação empírica da arquitetura proposta neste trabalho 
 Um serviço de ligações oferece funcionalidades hipermídia a aplicações que devem suportar ligações entre seus documentos , mantendo-os em seus formatos nativos . Este trabalho apresenta a definição da infra-estrutura do Serviço Aberto de Ligações Hipermídia para Web , chamado WLS ( Web Linking Service ) . A infra-estrutura do WLS compreende uma base externa de ligações , um protocolo de comunicação entre o WLS e aplicações que o utilizam , e um conjunto de funções , sob a forma de uma API que têm acesso a sua base de ligações . Ao utilizar padrões baseados na tecnologia XML , o WLS fornece funcionalidades hipermídia para que aplicações XML se tornem hipermídia habilitadas , segundo uma abordagem de sistemas hipermídia abertos 
 Este trabalho apresenta o projeto e implementação de uma interface de comunicação entre o ambiente de escalonamento AMIGO e TAO , que é uma implementação da especificação CORBA . Essa interface permite que aplicações desenvolvidas com TAO utilizem os serviços de escalonamento providos pelo AMIGO . O ambiente AMIGO oferece uma maneira flexível e dinâmica de escalonamento , possibilitando um melhor desempenho para aplicações do usuário . A ferramenta TAO possibilita o desenvolvimento de aplicações distribuídas em conformidade com a especificação CORBA e , também , aplicações paralelas distribuídas através do modelo callback de mensagens assíncronas . Em trabalhos anteriores realizados no grupo de Sistemas_Distribuídos  e Programação Concorrente do ICMC-USP , foram implementadas com sucesso interfaces para os softwares PVM e LAM-MPI . Os resultados alcançados por esses trabalhos motivaram a implementação de uma nova interface , entre o AMIGO e uma ferramenta CORBA , com o objetivo de explorar o desenvolvimento de aplicações paralelas distribuídas na arquitetura CORBA e , conseqüentemente , oferecer serviços aprimorados de escalonamento para essas aplicações . Após a implementação da interface CORBA-AMIGO neste trabalho , foi possível verificar os ganhos que o uso de técnicas aprimoradas de escalonamento podem trazer sobre o desempenho final da aplicação . Além_disso  , os resultados obtidos mostraram que a ferramenta TAO é uma solução viável para o desenvolvimento de algumas classes d2 aplicações paralelas distribuídas podendo , inclusive , alcançar desempenho comparável ao desempenho observado com ferramentas construídas especificamente para essas classes de aplicações 
 Este trabalho descreve a criação e implementação de uma ferramenta para simulação do sistema visual humano . Tal ferramenta se baseia na modelagem tridimensional dos principais componentes que são responsáveis pela visão humana . A fim de modelar o caminho dos raios de luz dentro do olho utiliza-se unica técnica de computação gráfica chamada ray-traciny . O modelo proposto neste trabalho é capaz de simular a visão a partir de dados reais de olhos humanos , os quais são obtidos de medições oftalmológicas . Como consequência disso , alguns distúrbios visuais como miopia e hipermetropia podem ser simulados e seus efeitos sobre o sistema visual analisados . Os resultados obtidos dessas simulações são apresentados e o uso da ferramenta em oftalmologia é discutido 
 Um importante problema de programação da produção surge em indústrias de papel integrando o problema de planejamento em múltiplas máquinas_paralelas  com o problema de corte . O problema de dimensionamento de lotes deve determinar a quantidade de jumbos ( bobinas grandes de papel ) de diferentes_tipos  de papel a serem produzidos em cada máquina . Estes jumbos são então cortados para atender a demanda de itens ( bobinas menores de papel ) . O planejamento , que minimiza custos de produção e preparação , deve produzir jumbos ( cada máquina produz jumbos de larguras diferentes ) que diminuam a perda no processo de corte . Por outro_lado  , o melhor número de jumbos do ponto de vista de minimizar a perda no processo de corte pode acarretar em altos custos de preparação . Ambos são problemas de otimização combinatória não trivial , o que tem motivado extensas pesquisas nas últimas_décadas  , entretanto , essa combinação não é bem explorada na literatura . Neste trabalho , são propostos um modelo de otimização integrado e métodos heurísticos de solução . Foram realizados_experimentos  computacionais com o intuito de analisar o desempenho dos métodos propostos e os resultados apresentaram- se bastante satisfatórios , significando que tais métodos são apropriados para tratar o problema integrado 
 Atualmente , uma proliferação de ferramentas e ambientes de Engenharia de Software tem sido observada , impactando positivamente na produção de software . Contudo , apesar do reúso ser foco de muitas das pesquisas em Engenharia de Software , a grande maioria dessas ferramentas e ambientes é construída de forma individual , sem a preocupação com o reúso dos esforços de desenvolvimento . Além_disso  , a literatura carece de trabalhos que estabeleçam arquiteturas de referência adequadas para esses ambientes , o que pode estar influenciando nas dificuldades de integração e evolução que esses ambientes têm sofrido . Não sendo diferente , o domínio de teste de software tem contribuído com uma diversidade de ferramentas de teste , viabilizando a condução de estudos comparativos de diversos critérios e técnicas de teste que vêm_sendo  propostos . Vale destacar também que não se encontram na literatura trabalhos que abordem o desenvolvimento de ferramentas de teste de forma efetiva e com base em reúso ; adicionalmente , observa-se a carência de arquiteturas de referência adequadas , recentes e eficazes para a construção de ferramentas para esse domínio . Apesar das vantagens da Web como plataforma de disponibilização de sistemas de software , ainda são poucas as iniciativas de ferramentas e ambientes de Engenharia de Software para essa plataforma . Nesse contexto , o objetivo deste trabalho consiste em investigar mecanismos que facilitem o desenvolvimento de ambientes e ferramentas de Engenharia de Software , visando à diminuição dos custos de desenvolvimento e manutenção . Mais especificamente , será estabelecida uma arquitetura de referência de ambientes de Engenharia de Software , buscando atender , em especial , à evolução contínua e à facilidade de uso e integração que esses ambientes requerem . Para isso , serão também explorados diversos mecanismos de forma a estabelecer uma arquitetura adequada e , ao mesmo tempo , eficaz , baseado no desenvolvimento orientado a aspectos , no uso de frameworks e de ontologias , bem como na norma internacional ISO/IEC 12207 . Um estudo de caso para refinamento da arquitetura de referência proposta para o domínio de teste de software é apresentado . Por fim , com base na experiência do estudo de caso , é estabelecido um processo preliminar de especialização e instanciação arquitetural para a construção de arquiteturas de referência para domínios específicos de Engenharia de 
 O empreendedorismo e a cultura de inovação ganham cada vez mais lugar na realidade econômica mundial . Grandes centros inovadores , como o Vale do Silício , nos Estados Unidos , e Tel-Aviv , em Israel , surgem como modelos para outras nações que pretendem introduzir definitivamente o empreendedorismo como uma cultura local . Nesta dissertação , uma nova metodologia de coleta e análise de dados proposta pelo Grupo de Pesquisa em Empreendedorismo do IME-USP foi refinada e utilizada para desenvolver um modelo do ecossistema de startups de software da cidade de São Paulo . Para tanto , os dados que compõem o modelo foram reunidos através de pesquisa qualitativa com membros e fundadores de startups , aceleradoras , investidores de risco e outros agentes importantes para o ecossistema . Os resultados do estudo constatam que a cidade possui todos os elementos fundamentais para o ecossistema empreendedor , tais como instituições , clientes interessados em experimentar novos produtos , a presença de importantes agentes financeiros , e um ambiente promissor ; no entanto , ainda é necessária uma melhor conexão entre esses componentes , e certos problemas críticos de grandes metrópoles , tais como a mobilidade urbana e o alto custo de vida , precisam ser melhorados para que o ecossistema da cidade possa continuar progredindo . A dissertação conclui com sugestões de ações para a melhoria do ecossistema , baseadas nos comentários dos entrevistados e em recomendações de outros estudos e relatórios com foco similar 
 Técnicas de agrupamento desempenham um papel fundamental na análise exploratória de dados . Seu objetivo é a organização de objetos em um conjunto finito de categorias , i.e. , grupos ( clusters ) , na expectativa de que relações significativas entre objetos resultem do processo . Nem todos resultados de agrupamento são relevantes , entretanto . De fato , a vasta maioria dos algoritmos de agrupamento existentes produzirá um resultado ( partição ) , mesmo em casos para os quais não existe uma estrutura real de grupos nos dados . Se grupos de fato existem , a determinação do melhor conjunto de parâmetros para estes algoritmos ainda é necessária , a fim de evitar a utilização de resultados espúrios . Tal determinação é usualmente feita por meio de critérios de validação , os quais avaliam os resultados de agrupamento de forma quantitativa . A avaliação/validação de resultados de agrupamentos é o foco desta tese . Em um contexto geral , critérios de validação relativos e a combinação dos mesmos ( ensembles ) são propostas . No que tange critérios , propõe-se o uso da área sob a curva ( AUC Area Under the Curve ) proveniente de avaliações ROC ( Receiver Operating Characteristics ) como um critério de validação relativo no contexto de agrupamento . Além de uma avaliação empírica da AUC , são exploradas algumas de suas propriedades teóricas , bem como a sua relação com outro critério relativo existente , conhecido como Gamma . Ainda com relação à critérios , um índice relativo para a validação de resultados de agrupamentos baseados em densidade , proposto com a participação do autor desta tese , é revisado . No que diz_respeito  à combinação de critérios , mostra-se que : ( i ) combinações baseadas em uma seleção arbitrária de índices possuem aplicação prática limitada ; e ( ii ) com o uso de heurísticas para seleção de membros da combinação , melhores_resultados  podem ser obtidos . Finalmente , considera-se a avaliação/validação no contexto de dados de expressão_gênica  . Neste caso particular estuda-se o uso de informação da Gene Ontology , na forma de similaridades semânticas , na avaliação de medidas de dissimilaridade e resultados de agrupamentos de genes 
 Técnicas de análise de dados podem ser úteis em processos de tomada de decisão , quando padrões de interesse indicam tendências em domínios específicos . Tais tendências podem auxiliar a avaliação , a definição de alternativas ou a predição de eventos . Atualmente , os conjuntos de dados têm aumentado em tamanho e complexidade , impondo desafios para recursos modernos de hardware . No caso de grandes_conjuntos  de dados que podem ser representados como grafos , aspectos de visualização e processamento escalável têm despertado interesse . Arcabouços distribuídos são comumente usados para lidar com esses dados , mas a implantação e o gerenciamento de clusters computacionais podem ser complexos , exigindo recursos técnicos e financeiros que podem ser proibitivos em vários cenários . Portanto é desejável conceber técnicas eficazes para o processamento e visualização de grafos em larga_escala  que otimizam recursos de hardware em um único nó computacional . Desse modo , este trabalho apresenta uma técnica de visualização chamada StructMatrix para identificar relacionamentos estruturais em grafos reais . Adicionalmente , foi proposta uma estratégia de processamento bimodal em blocos , denominada Bimodal Block Processing ( BBP ) , que minimiza o custo de I/O para melhorar o desempenho do processamento . Essa estratégia foi incorporada a um arcabouço de processamento de grafos denominado M-Flash e desenvolvido durante a realização deste trabalho.Foram conduzidos experimentos a fim de avaliar as técnicas propostas . Os resultados mostraram que a técnica de visualização StructMatrix permitiu uma exploração eficiente e interativa de grandes grafos . Além_disso  , a avaliação do arcabouço M-Flash apresentou ganhos significativos sobre todas as abordagens baseadas em memória secundária do estado da arte . Ambas as contribuições foram validadas em eventos de revisão por pares , demonstrando o potencial analítico deste trabalho em domínios associados a grafos em larga_escala  
 O ensino de computação envolve muitos assuntos que formam a base para uma aprendizagem eficaz . A falta de recursos apropriados torna difícil a apresentação de tais assuntos de forma clara , devido à dinâmica e à complexidade dos mesmos . O ensino de assuntos referentes ao conteúdo de hierarquia de memória e memória virtual é desafiador , porque ambos consideram diferentes aspectos estruturais , funcionais e de desempenho . Muitas abordagens vêm_sendo  estudadas para tornar o ensino desses assuntos mais atrativo . Uma dessas abordagens considera o desenvolvimento e aplicação de Recursos Educacionais Abertos ( REA ) . Os REA têm sido aplicados com sucesso para ajudar o ensino e a aprendizagem de assuntos desafiadores . Mesmo com conhecimento da possibilidade do uso de REA , muito conteúdos ainda são desprovidos desses recursos . Tendo isso em mente , este trabalho apresenta a transformação do simulador Amnesia em um REA para facilitar o ensino e o aprendizado de memória virtual , simulando aspectos estruturais , funcionais e de desempenho . O foco deste trabalho é melhorar o ensino de Memória Virtual com o auxílio do REA Amnesia . Foram desenvolvidos materiais didáticos para auxiliar os professores e alunos com planos de aula , tutorial de utilização e texto com conteúdo teórico , os quais são disponibilizados juntamente com o REA Amnesia . Para verificar a melhora no ensino e aprendizado foram realizados três experimentos com alunos de graduação e pós-graduação , aplicando avaliações quantitativas e qualitativas . Os experimentos seguiram uma base experimental similar , mas cada experimento teve sua particularidade na aplicação e na análise dos resultados . Os experimentos_realizados  mostram uma considerável evolução no aprendizado do assunto memória virtual . No primeiro experimento foi possível observar uma melhora 28,8 % na quantidade de acertos . No segundo experimento foi possível comparar resultados de alunos que utilizaram o REA Amnesia com alunos que não tiveram nenhuma aula entre as avaliações . Os resultados do terceiro experimento mostram melhorias de até 180 % na quantidade de acertos com a utilização do REA Amnesia , para alunos com dificuldades ( demonstradas em suas notas ) , onde o uso do Amnesia se mostrou mais importante 
 A literatura apresenta muitos sistemas que foram desenvolvidos a fim de automatizar o processo de geração de hiperdocumentos multimídia a partir de experiências ao vivo ; ambientes educacionais e de reunião são os domínios de aplicação mais comuns . Conforme reportados , os sistemas não contemplam características importantes relativas à evolução de hiperdocumentos Web , tais como a possibilidade de atualizações constantes de conteúdo e de layout , e a independência de plataforma . Este trabalho visa a implementação do sistema eMeet/SERVE , que realiza a geração_automática  de hiperdocumentos multimídia a partir de informações capturadas em ambientes de reuniões distribuídas na Web . O eMeet/SERVE foi implementado com a utilização da arquitetura LAIAc  proposta neste trabalho  habilitada a suportar a evolução dos hiperdocumentos gerados . As interfaces dos hiperdocumentos gerados pelo eMeet/SERVE foram desenvolvidas a partir de padrões e regras propostas na literatura . Além_disso  , um mecanismo de sincronização foi implementado nos hiperdocumentos a fim de permitir uma reprodução adequada dos elementos multimídia . A arquitetura LAIAc pode ser utilizada por outros sistemas associados à geração_automática  de hiperdocumentos . Com a realização deste trabalho , o eMeet/SERVE está preparado para integração com outros serviços implementados no contexto do projeto maior em que se insere , o Projeto InCA-SERVE , visando a evolução contínua dos hiperdocumentos gerados automaticamente 
 Veículos Aéreos não tripulados ( VANTs ) vêm_sendo  cada vez mais utilizados em diversos países , tanto na área militar como na civil . O cenário considerado nesse estudo é o de um VANT realizando captura de vídeo em tempo real , transmitindo-o a uma base terrestre por meio de rede sem fio . O problema consiste no fato de não ser possível garantir uma taxa de transmissão contínua , com banda estável . Isso ocorre devido a fatores como a velocidade da aeronave ( da ordem centenas de km/h ) , irregularidades de terreno ( impedindo a linha de visada do enlace de transmissão ) , ou do clima , como tempestades que podem interferir na transmissão da RF . Por fim , os movimentos que o VANT pode realizar no vôo ( Rolagem , Arfagem ou Guinada ) podem prejudicar a disponibilidade do link . Dessa forma , é necessário que seja realizada adaptação de vídeo de acordo com a banda disponível . Assim , quando a qualidade do enlace for degradada , deverá ser realizada uma redução no tamanho do vídeo , evitando a interrupção na transmissão . Por outro_lado  , a adaptação também deverá fazer com que a banda disponível seja utilizada , evitando o envio de vídeos com qualidade inferior à que seria possível para determinado valor de largura de banda . Nesse trabalho será considerada a faixa de valores de largura de banda de 8 Mbps até zero . Para realizar a adaptação será utilizado o padrão H.264/AVC com codificação 
 Sistemas web de larga_escala  são distribuídos em milhares de servidores em múltiplos centros de processamento de dados em diferentes localizações geográficas , operando sobre redes de longa distância ( WANs ) . Várias técnicas são usadas para atingir os altos níveis de escalabilidade requeridos por esses sistemas . Replicação de dados está entre as principais delas , e tem por objetivo diminuir a latência , aumentar a vazão e/ou aumentar a disponibilidade do sistema . O principal problema do uso de replicação em sistemas georeplicados é a dificuldade de garantir consistência entre as réplicas sem prejudicar consideravelmente o desempenho e a disponibilidade do sistema . O desempenho do sistema é afetado pelas latências da ordem de centenas de milissegundos da WAN , enquanto a disponibilidade é afetada por falhas que impedem a comunicação entre as réplicas . Quanto mais rígido o modelo de consistência de um sistema de armazenamento , mais simples é o desenvolvimento do sistema que o usa , mas menores são seu desempenho e disponibilidade . Entre os modelos de consistência mais relaxados e mais difundidos em sistemas web georeplicados está a consistência em momento indeterminado ( eventual consistency ) . Esse modelo de consistência garante que em algum momento as réplicas convergem após as escritas terem cessado . Um modelo mais rígido e menos difundido é a consistência na linha do tempo . Esse modelo de consistência usa uma réplica mestre para garantir que não ocorram conflitos na escrita . Nas leituras , os clientes podem ler os valores mais recentes a partir da cópia mestre , ou optar explicitamente por ler valores possivelmente desatualizados para obter maior desempenho ou disponibilidade . A consistência na linha do tempo apresenta disponibilidade menor que a consistência em momento indeterminado em determinadas situações , mas não há dados comparando o desempenho de ambas . O objetivo principal deste trabalho foi a comparação do desempenho de sistemas de armazenamento georeplicados usando esses dois modelos de consistência . Para cada modelo de consistência , foram realizados_experimentos  que mediram o tempo de resposta do sistema sob diferentes cargas de trabalho e diferentes condições de rede entre centros de processamento de dados . O estudo mostra que um sistema usando consistência na linha do tempo apresenta desempenho semelhante ao mesmo sistema usando consistência em momento indeterminado em uma WAN quando a localidade dos acessos é alta . Esse comparativo pode auxiliar desenvolvedores e administradores de sistemas no planejamento de capacidade e de desenvolvimento de sistemas georeplicados 
 Trabalhos anteriores sobre a síntese de imagens de difusão por ressonância_magnética  se limitaram a estudos sobre estruturas microscópicas , menores que as dimensões típicas de um voxel ( e.g. , [ BF08 ] [ BF13 ] [ LFS + 10 ] e [ BA94 ] ) . Isto decorre em parte devido às metodologias utilizadas , que têm como ponto em comum serem simulações de tipo Monte_Carlo  , nas quais os elementos mínimos da simulação são as partículas de água . Portanto o custo_computacional  destas simulações é proporcional ao número de partículas a simular , e isto limita os volumes que podem ser simulados a tamanhos microscópicos . Propomos uma metodologia alternativa , que utiliza a imagem T 2 de uma amostra para sintetizar imagens de difusão por ressonância_magnética  . Os elementos mínimos desta nova metodologia são os pontos da imagem T 2 , e portanto seu custo_computacional  é proporcional à resolução da imagem T 2 utilizada , o que permite a síntese a partir de amostras de qualquer tamanho físico . Estas sínteses são realizadas por meio da integração numérica da equação do artigo seminal de Stejskal e Tanner [ ST65 ] que relaciona a atenuação do sinal de ressonância_magnética  devida à difusão com os parâmetros da sequência de pulsos PGSE . Usamos os parâmetros típicos dessa sequência ( b , gamma , tau ' , g 0 , g , delta e Delta ) , que podem ser configurados explicitamente em máquinas de ressonância_magnética  , para calcular valores do coeficiente de difusão aparente D em direções arbitrárias . Desenvolvemos software , disponibilizado [ Bor ] por licença GPL [ Fou07 ] , para realizar estas simulações , e para especificar uma máscara de direções , útil para modelar a difusão de uma amostra . Estas ferramentas permitem o estudo sistemático das variações dos parâmetros na síntese de imagens de difusão por ressonância_magnética  . Apresentamos um estudo de um fantoma de capilares imersos em água , exemplificando como utilizar as ferramentas para investigar a influência destes parâmetros na difusão da água da amostra 
 Devido ao contínuo crescimento das redes e a utilização em tais ambientes de recursos em geral heterogêneos , a tarefa de administrá-las torna-se cada vez mais árdua . Assim , aumenta a importância da utilização de ferramentas capazes de auxiliar o administrador nas suas funções . Algumas delas , utilizando recursos de voz , podem possibilitar características de maior interação entre o administrador e os usuários do sistema gerenciado . Dentro destes termos , esta dissertação trata de um sistema de processamento de voz , baseado na língua_portuguesa  , como ferramenta auxiliar no gerenciamento de redes de computadores . O fato de tal interação ser baseada na língua_portuguesa  , torna o trabalho de maior relevância , pois a quase totalidade das pesquisas nesta área são calcadas em línguas estrangeiras , o que inviabiliza a incorporação pura e simples dos seus resultados no Brasil . Este trabalho , fiel a sua natureza multidisciplinar , trata do assunto envolvendo as áreas de fonética , fonologia , processamento de sinais , redes e seu gerenciamento . Convém observar que este projeto não se destina a implementar o desenvolvimento de um sistema de voz completo , mas sim a realização da tarefa de reunir , analisar e expor de modo sistemático as informações relativas ao tema proposto . Um protótipo simples é descrito 
 Nos últimos_anos  , a evolução dos processadores e redes para computadores de baixo custo foi muito maior se comparada com o aumento do desempenho dos discos de armazenamento de dados . Com isso , muitas aplicações estão encontrando dificuldades em atingir o pleno uso dos processadores , pois estes têm de esperar até que os dados cheguem para serem utilizados 

 Uma forma popular para resolver esse tipo de empecílio é a adoção de sistemas de arquivos paralelos , que utilizam a velocidade da rede local , além dos recursos de cada máquina , para suprir a deficiência de desempenho no uso isolado de cada disco 

 Neste estudo , analisamos alguns sistemas de arquivos paralelos e distribuídos , detalhando aqueles mais interessantes e importantes . Por fim , mostramos que o uso de um sistema de arquivos paralelo pode ser mais eficiente e vantajoso que o uso de um sistema de arquivos usual , para apenas um cliente 
 Redução de dimensionalidade é um problema muito importante da área de reconhecimento de padrões com aplicação em diversos campos do conhecimento . Dentre as técnicas de redução de dimensionalidade , a de seleção de características foi o principal foco desta pesquisa . De uma forma geral , a maioria dos métodos de redução de dimensionalidade presentes na literatura costumam privilegiar casos nos quais os dados sejam linearmente separáveis e só existam duas classes distintas . No intuito de tratar casos mais genéricos , este trabalho propõe uma função critério , baseada em sólidos princípios de teoria estatística como entropia e informação mútua , a ser embutida nos algoritmos de seleção de características existentes . A proposta dessa abordagem é tornar possível classificar os dados , linearmente separáveis ou não , em duas ou mais classes levando em conta um pequeno subespaço de características . Alguns resultados com dados sintéticos e dados reais foram obtidos confirmando a utilidade dessa técnica 
 Este trabalho tratou dois problemas de bioinformática . O primeiro trata de distinguir dois fenômenos biológicos através de seleção de 
 subconjunto apropriado de genes . Foi estudada uma técnica de seleção de genes fortes utilizando máquinas de suporte vetorial ( MSV ) que já vinha sendo aplicada para este fim em dados de SAGE do genoma humano . Grande parte dos genes fortes encontrados por esta técnica para distinguir tumores de cérebro ( glioblastoma e astrocytoma ) , foram validados pela metodologia apresentada neste trabalho . O segundo problema que foi tratado neste trabalho é o de identificação de redes de regulação gênica , utilizando a metodologia_proposta  , em dados produzidos pelo trabalho de DeRisi et_al  sobre microarray do genoma do Plasmodium falciparum , agente causador da malária , durante as 48 horas de seu ciclo de vida . O presente texto apresenta evidências de que a utilização da entropia condicional média para estimar redes genéticas probabilísticas ( PGN ) pode ser uma abordagem bastante promissora nesse tipo de aplicação 
 No contexto de processamento de imagens , tal técnica pôde ser aplicada com sucesso em obter W-operadores minimais para realização de filtragem de imagens e reconhecimento de texturas 
 O Reconhecimento de Entidades Mencionadas ( REM ) é uma subtarefa da extração de informações e tem como objetivo localizar e classificar elementos do texto em categorias pré-definidas tais como nome de pessoas , organizações , lugares , datas e outras classes de interesse . Esse conhecimento obtido possibilita a execução de outras tarefas mais avançadas . O REM pode ser considerado um dos primeiros passos para a análise semântica de textos , além de ser uma subtarefa crucial para sistemas de gerenciamento de documentos , mineração de textos , extração da informação , entre outros . Neste trabalho , estudamos alguns métodos de Aprendizado de Máquina aplicados na tarefa de REM que estão relacionados ao atual estado da arte , dentre eles , dois métodos aplicados na tarefa de REM para a língua_portuguesa  . Apresentamos três diferentes formas de avaliação destes tipos de sistemas presentes na literatura da área . Além_disso  , desenvolvemos um sistema de REM para língua_portuguesa  utilizando Aprendizado de Máquina , mais especificamente , o arcabouço de máxima entropia . Os resultados obtidos com o nosso sistema alcançaram resultados equiparáveis aos melhores sistemas de REM para a língua_portuguesa  desenvolvidos utilizando outras abordagens de aprendizado de máquina 
 A área de revisão de crenças estuda a forma como agentes racionais operam sobre seus conhecimentos , permitindo que estes evoluam à vista de novas informações . Muitas das formas de manipulação do conhecimento hoje utilizadas foram desenvolvidas tendo como base o paradigma estabelecido por Alchourrón , Gärdenfors e Makinson em 1985 , que evoluiu nas últimas_décadas  sob influência de diversas correntes de pensamento , dando origem a múltiplas abordagens para a estruturação de operadores sobre crenças . Esta dissertação limita-se à análise das duas principais construções de operadores sobre bases de crenças , bem como suas propriedades e relações . A partir desta análise , foi construído um framework computacional que permitiu a análise empírica do comportamento destas construções em diversos casos gerados , de modo a averiguar a eficiência real dos algoritmos envolvidos . Também são expostos detalhes do framework , construído de forma flexível para permitir sua reutilização em trabalhos futuros 
 É muito comum modelar problemas em finanças com processos estocásticos , dada a incerteza de suas variáveis de análise . Além_disso  , problemas reais nesse domínio são , em geral , de grande custo_computacional  , o que sugere a utilização de plataformas de alto_desempenho  ( HPC ) em sua implementação . As novas gerações de arquitetura de hardware gráfico ( GPU ) possibilitam a programação de propósito_geral  enquanto mantêm alta banda de memória e grande poder_computacional  . Assim , esse tipo de arquitetura vem se mostrando como uma excelente alternativa em HPC . Com isso , a proposta principal desse trabalho é estudar o ferramental matemático e computacional necessário para modelagem estocástica em finanças com a utilização de GPUs como plataforma de aceleração . Para isso , apresentamos a GPU como uma plataforma de computação de propósito_geral  . Em seguida , analisamos uma variedade de geradores de números aleatórios , tanto em arquitetura sequencial quanto paralela . Além_disso  , apresentamos os conceitos fundamentais de Cálculo Estocástico e de método de Monte_Carlo  para simulação estocástica em finanças . Ao final , apresentamos dois estudos de casos de problemas em finanças : `` Stops Ótimos '' e `` Cálculo de Risco de Mercado '' . No primeiro caso , resolvemos o problema de otimização de obtenção do ganho ótimo em uma estratégia de negociação de ações de `` Stop Gain '' . A solução proposta é escalável e de paralelização inerente em GPU . Para o segundo caso , propomos um algoritmo paralelo para cálculo de risco de mercado , bem como técnicas para melhorar a solução obtida . Nos nossos experimentos , houve uma melhora de 4 vezes na qualidade da simulação estocástica e uma aceleração de mais de 50 vezes 
 Nos últimos_anos  , tem-se observado um aumento significativo no volume de transações financeiras realizadas pela Internet . Esse crescimento no volume financeiro , associado à fragilidade inerente à ausência de verificações básicas , possíveis somente em transações do mundo físico , tem atraído a atenção de pessoas com o objetivo de obter vantagens financeiras de forma ilícita . Devido aos prejuízos causados pelas fraudes , surgiram empresas de pagamento online com o objetivo de tornar as transações de compra e venda na Internet mais seguras . Essas empresas atuam como um intermediário das transações e assumem os riscos associados , mostrando-se ser esse um negócio de alto risco . Dado o alto volume de transações com as quais essas empresas precisam lidar , torna-se clara a necessidade de métodos computacionais para detecção de transações fraudulentas , visto que a utilização estrita de verificações manuais é inviável para lidar com tal volume de transações . Essa tarefa de análise e identificação de transações fraudulentas pode ser vista como um problema computacional de classificação , sendo então aplicáveis técnicas de classificação , aprendizado computacional e mineração de dados . Porém , dada a complexidade do problema , a aplicação de técnicas computacionais só é possível após um profundo entendimento do problema e a definição de uma modelagem eficiente associada a um processo consistente e abrangente , capaz de lidar com todas as etapas necessárias para a análise eficiente de uma transação . Face a isso , o presente_trabalho  propõe uma abordagem abrangente para tratar o problema da fraude nesse novo mercado de intermediação de pagamentos online utilizando como base um processo já muito bem estabelecido na indústria . Abordaremos mais especificamente uma das fases desse processo , que se refere justamente a utilização de ferramentas computacionais para a detecção das fraudes , e apresentaremos um sub-processo que envolve a utilização de várias ferramentas para o tratamento do ponto de vista computacional do problema de detecção de fraudes . Para a validação dos resultados da proposta , utilizaremos uma enorme_quantidade  de dados reais disponibilizados por uma grande empresa do setor de intermediação de pagamentos online que colaborou com nossa pesquisa 
 O problema de coloração de grafos é um problema clássico em teoria dos grafos cujo_objetivo  é particionar o conjunto de vértices em um número mínimo de conjuntos estáveis . Nesta tese apresentamos nossas contribuições sobre três problemas de coloração de grafos e um problema relacionado a uma antiga conjectura sobre subdivisão de digrafos . Primeiramente , abordamos o problema de recoloração convexa no qual é dado um grafo arbitrariamente colorido G e deseja-se encontrar uma recoloração de peso mínimo tal que cada classe de cor induza um subgrafo conexo de G. Mostramos resultados sobre inaproximabilidade , introduzimos uma formulação linear_inteira  que modela esse problema , e apresentamos alguns resultados computacionais usando uma abordagem de geração de colunas . O problema de k-upla coloração é uma generalização do problema clássico de coloração de vértices e consiste em cobrir o conjunto de vértices de um grafo com uma quantidade mínima de conjuntos estáveis de tal forma que cada vértice seja coberto por pelo menos k conjuntos estáveis ( possivelmente idênticos ) . Apresentamos uma formulação linear_inteira  para esse problema e fazemos um estudo detalhado do politopo associado a essa formulação . O último problema de coloração estudado nesta tese é o problema de orientação própria . Ele consiste em orientar o conjunto de arestas de um dado grafo de tal forma que vértices adjacentes possuam graus de entrada distintos e o maior grau de entrada seja minimizado . Claramente , os graus de entrada induzem uma partição do conjunto de vértices em conjuntos estáveis , ou seja , induzem uma coloração ( no sentido convencional ) dos vértices . Nossas contribuições nesse problema são em complexidade computacional e limitantes superiores para grafos bipartidos . Finalmente , estudamos um problema relacionado a uma conjectura de Mader , dos anos oitenta , sobre subdivisão de digrafos . Esta conjectura afirma que , para cada digrafo acíclico H , existe um inteiro f ( H ) tal que todo digrafo com grau mínimo de saída pelo menos f ( H ) contém uma subdivisão de H como subdigrafo . Damos evidências para essa conjectura mostrando que ela é válida para classes particulares de digrafos acíclicos 
 Métodos ágeis são técnicas adequadas para o desenvolvimento de software sujeito a mudanças constantes . Essas mudanças não devem afetar o cronograma , orçamento do projeto e devem assegurar o atendimento às necessidades do cliente . Diversos valores , princípios e boas práticas de desenvolvimento e de condução de projeto são aplicados em projetos ágeis com esse objetivo . Algumas dessas práticas são relacionadas a atividade de teste de software . Este trabalho teve como objetivo caracterizar a atividade de teste de software aplicada dentro de métodos de desenvolvimento ágil , buscando eliminar aspectos de teste não produtivos , identificando boas práticas e , principalmente , criando formas de acompanhar e melhorar continuamente a condução da atividade de teste . A partir da caracterização da atividade foi proposta a adoção de um conjunto de métricas para facilitar o seu acompanhamento e melhoria constante da mesma . Algumas dessas métricas de acompanhamento de testes foram implementadas na ferramenta Agile Testing Metrics Management ( ATMM ) . O objetivo principal da ferramenta é gerenciar as iterações de desenvolvimento do projeto ágil e , também , exibir a evolução das métricas relacionadas ao código que está sendo testado e aos casos de teste desenvolvidos utilizando a ferramenta JUnit . Para validar a ferramenta e as métricas foram conduzidos estudos de casos com dois projetos de software de domínios diferentes que utilizaram métodos ágeis e testes de 
 O reconhecimento de padrões tem provocado um grande interesse nas últimas_décadas  . Como consequência , numerosas aplicações foram desenvolvidas . Entre as mais importantes podem-se citar as seguintes : ajuda ao diagnóstico médico , análise automática de sinais , inspeção automática de produtos industriais , sistemas de vigilância automática , busca automática de informação digitalizada , etc . A complexidade de um sistema de reconhecimento de padrões é alta , devido ao fato de que padrões reais se apresentam com grande variedade , sofrendo transformações e deformações não-lineares . Este trabalho tem como objetivo desenvolver um sistema celular evolutivo , para reconhecimento de padrões invariantes à rotação , baseado em mecanismos fundamentais de Autómatos Celulares , os quais foram usados com sucesso para modelagem e simulação de problemas complexos . O modelo proposto neste trabalho extrai eficientemente características globais invariantes à rotação de padrões , a partir das interações locais das células 
 O objetivo desse trabalho é extender o ambiente de simulação FreeFlow-3D para escoamentos_incompressíveis  com superfície_livre  não-isotérmicos . As equações de Navier-Stokes com as condições de fronteira associadas são discretizadas usando o método de diferenças_finitas  sobre malhas deslocadas . Os efeitos da temperatura são incluídos no código usando a aproximação de Boussinesq e a viscosidade é calculada como uma função da temperatura ou é colocada constante dependendo da escolha do usuário do FreeFlow-3D . Resultados_numéricos  mostrando a aplicabilidade do ambiente FreeFlow-3D para escoamentos não-isotérmicos são apresentados e discutidos 
 Num curto período de tempo , a Web tornou-se o aspecto central de muitas aplicações em diferentes áreas . Rapidamente , cresceu e diversificou seu uso , e diversos setores de negócio realizam suas operações no ambiente da Web . Entretanto , à medida que cresce a extensão de uso de aplicações Web , que se tornam mais complexas , aumenta a preocupação com a maneira como as aplicações são desenvolvidas . Freqüentemente , a abordagem de desenvolvimento é ad hoc , desprovida de técnicas sistemáticas e de metodologias sólidas , resultando em aplicações de baixa qualidade . A fim de alcançar aplicações Web bem sucedidas , há a necessidade de melhores princípios de esenvolvimento que devem considerar as características especiais dessas aplicações . Dessa forma , o objetivo do trabalho é propor Processos de Apoio ao desenvolvimento das aplicações Web , baseando-se nas características dessas aplicações e nos Processos de Apoio da ISO/IEC 12207 . Os processos de Apoio visam auxiliar outros processos do ciclo de vida da aplicação a alcançar produtos de qualidade e o sucesso do 
 Esta dissertação de mestrado_propõe  uma nova política de distibuição de requisições em cluster de web services , denominada Política Performance . Essa política provê uma distribuição transparente , flexível e dinâmica das requisições na plataforma em que é executada . Um estudo sistemático também é realizado para analisar a qualidade dos índices de carga empregados no contexto de web services e propõe um novo índice capaz de representar fielmente o desempenho dos web services e encapsular a complexidade estrutural da plataforma . Também é proposto em Módulo Gerenciador de Energia capaz de prover sustentabilidade à plataforma , reduzindo o consumo de energia elétrica sem prejudicar a alta confiabilidade na distribuição das requisições e com baixo impacto no tempo_médio  de resposta . Os estudos_experimentais  realizados neste trabalho mostraram que a Política Performance permitiu um melhor desempenho no atendimento das requisições realizadas à plataforma . EStes resultados referem-se a um desempenho_superior  a 70 % no tempo_médio  de resposta , quando comparado ao desempenho demosntrado pela política padrão do Mod_cluster . O Módulo Gerenciador de Energia proporcionou uma redução de aproximadamente 30 % no consumo de energia da plataforma mantendo a alta confiabilidade na distribuição das 
 A quantidade de dados complexos ( imagens , vídeos , séries_temporais  e outros ) tem crescido rapidamente . Dados complexos são adequados para serem recuperados por similaridade , o que significa definir consultas de acordo com um dado critério de similaridade . Além_disso  , dados complexos usualmente são associados com outras informações , geralmente de tipos de dados convencionais , que devem ser utilizadas em conjunto com operações por similaridade para responder a consultas complexas . Vários trabalhos propuseram técnicas para busca por similaridade , entretanto , a maioria das abordagens não foi concebida para ser integrada com um SGBD , tratando consultas por similaridade como operações isoladas , disassociadas do processador de consultas . O objetivo principal desta tese é propor alternativas algébricas , estruturas de dados e algoritmos para permitir um uso abrangente de consultas por similaridade associadas às demais operações de busca disponibilizadas pelos SGBDs relacionais e executar essas consultas compostas eficientemente . Para alcançar este objetivo , este trabalho apresenta duas contribuições principais . A primeira contribuição é a proposta de uma nova operação por similaridade , chamada consulta aos k-vizinhos mais próximos estendida com condições ( ck-NNq ) , que estende a consulta aos k-vizinhos mais próximos ( k-'NN SUB . q ' ) de maneira a fornecer uma condição adicional , modificando a semântica da operação . A operação proposta permite representar consultas demandadas por várias aplicações , que não eram capazes de ser representadas anteriormente , e permite homogeneamente integrar condições de filtragem complementares à k-'NN IND.q ' . A segunda contribuição é o desenvolvimento do FMI-SiR ( user-defined Features , Metrics and Indexes for Similarity Retrieval ) , que é um módulo de banco de dados que permite executar consultas por similaridade integradas às demais operações do SGBD . O módulo permite incluir métodos de extração de características e funções de distância definidos pelo usuário no núcleo do gerenciador de banco de dados , fornecendo grande exibilidade , e também possui um tratamento especial para imagens médicas . Além_disso  , foi verificado através de experimentos sobre bancos de dados reais que a implementação do FMI-SiR sobre o SGBD Oracle é capaz de consultar eficientemente grandes bancos de dados 
 A manutenção de software é reconhecida como uma atividade bastante importante , senão indispensável , da Engenharia de Software . As alterações realizadas no software durante a sua manutenção , se não forem realizadas de forma sistematizada , acabam por descaracterizar e prejudicar a manutenibilidade desse software . Nesse contexto , a customização de sistemas de software , ou seja , a manutenção adaptativa de sistemas de software para domínios e ambientes específicos , vem_sendo  largamente adotada pela indústria . Em outra perspectiva , a Programação Orientada a Aspectos ( POA ) foi proposta como uma abordagem para melhorar a separação de interesses em sistemas de software . Em paralelo , tem-se Componentes de Software que enfatizam o projeto e a construção de sistemas de software visando o aumento do reúso e da produtividade . Ainda nessa linha , algumas propostas têm sido feitas com o objetivo de combinar os conceitos de componentes de software com o paradigma de orientação a aspectos , ou seja , a criação de componentes que utilizem aspectos como interface para entrecortar outros sistemas . Vale destacar que na indústria , a manutenção de software , incluindo a customização , não tem sido realizada de maneira sistemática o que contribui para a degradação da qualidade dos sistemas . Portanto , a principal_contribuição  deste trabalho é propor um processo para customização de sistemas de software utilizando componentes orientados a aspectos visando a diminuição do acoplamento na adaptação de sistemas de software e , consequentemente , melhorando sua manutenibilidade . Além_disso  , um estudo de caso foi conduzido no qual a abordagem proposta foi utilizada na customização de um sistema de software comercial de grande porte . Os resultados desse estudo de caso foram analisados qualitativamente e contribuíram para avaliar a aplicabilidade da abordagem proposta neste trabalho como um processo sistematizado de customização de sistemas de software utilizando componentes orientados a 
 O mecanismo de tratamento de exceções disponível em linguagens orientadas a objetos está sendo cada vez mais utilizado e representa cerca de 8 % das linhas de código em programas escritos com linguagens de terceira geração , de acordo com algumas pesquisas . No entanto , este mecanismo está entre os menos compreendido pelos desenvolvedores e menos testado . A dificuldade de testar exceções aumenta na fase de teste de integração , porque exceções lançadas em um determinado nível e não tratadas podem subir na hierarquia de chamada . O uso de aspectos também pode introduzir novos tipos de defeitos , como por exemplo um adendo pode inserir lançamentos de exceções no programa sem que exista algum tratamento previsto para elas . Este trabalho apresenta uma proposta para testes de integração estrutural de fluxos de exceções de programas Java e AspectJ . Isto é feito propondo-se alterações na representação do fluxo de exceção no grafo de fluxo de controle integrado e propondo-se novos critérios baseados no fluxo de exceção . Além_disso  , essas propostas foram implementadas e geraram uma extensão da ferramenta chamada Ja-BUTi/AJ . Para avaliar a proposta deste trabalho , um estudo de caso e um experimento formal foram conduzidos . O estudo de caso teve como objetivo validar a extensão da JaBUTi/AJ desenvolvida e fazer uma avaliação inicial do custo de aplicação dos critérios propostos . O experimento formal realizou uma comparação entre as ferramentas JaBUTi/AJ e VITTAE no contexto do teste do fluxo de exceções . Ambos os estudos são apresentados e discutidos nesta 
 O estudo da evolução da concentração de elementos de uma reação química , conhecida como Cinética Química , é de extrema importância para a compreensão das complexas interações em sistemas biológicos . Uma maneira de descrever a cinética de uma reação química é utilizando um sistema de equações_diferenciais  ordinárias ( EDOs ) . Uma vez que para resolver um sistema de equações_diferenciais  ordinárias pode ser uma tarefa_difícil  ( ou mesmo inviável ) , métodos_numéricos  são utilizados para realizar simulações , ou seja , para obter concentrações aproximadas das espécies químicas envolvidas durante um determinado período de tempo . No entanto , quanto maior for o sistema simulado de EDOs , mais os métodos_numéricos  estão sujeitos a erros . Além_disso  , o aumento do tamanho do sistema muitas_vezes  resulta em simulações que são mais exigentes do ponto de vista computacional . Assim , o objetivo deste projeto de mestrado é o desenvolvimento de regras para simplificar os sistemas de equações_diferenciais  ordinárias que modelam a cinética de reações químicas e , portanto , a obtenção de um algoritmo para executar simulações numéricas de um modo mais rápido e menos propenso a erros . Mais do que diminuir o erro e o tempo de execução , esta simplificação possibilita o biólogo escolher a solução mais factível do ponto de vista de medida . Isso porque , a identificação dos sistemas ( i.e. , inferência dos parâmetros ) requer que a concentração de todas as espécies químicas seja conhecida , ao menos em um certo intervalo de tempo . Contudo , em muitos_casos  , não é possível medir a concentração de todas as espécies químicas consideradas . Esta simplificação gera sistemas equivalentes ao original , mas que dispensa a utilização de certas concentrações de espécies químicas . Um sistema de equações_diferenciais  ordinárias pode ser simplificado considerando as relações de conservação de massa , que são equações algébricas . Além_disso  , no caso de reações enzimáticas , o sistema de equações_diferenciais  ordinárias pode ser simplificado pelo pressuposto de que a concentração do complexo enzima-substrato mantém-se constante , o que permite a utilização da equação de Michaelis-Menten . De todas as combinações possíveis das equações algébricas com as equações_diferenciais  , uma família de sistemas simplificados de EDOs foi construída , permitindo a escolha do sistema mais simples . Esta escolha segue um critério guloso que favorece a minimização do número de equações_diferenciais  e do número total de termos . As regras em desenvolvimento de simplificação dos sistemas de equações_diferenciais  ordinárias foram utilizados para projetar um algoritmo , que foi implementado usando a linguagem de programação Python . O algoritmo concebido foi testado utilizando instâncias artificiais 
 Este trabalho apresenta um estudo sobre problemas de multifluxo em redes , juntamente com três abordagens de solução : o Particionamento Primal , a Decomposição Recurso-Diretor e a Decomposição Dantzig-Wolfe . O produto num problema de multifluxo pode ser definido de três maneiras . Cada uma dessas definições para produto produz uma formulação distinta para o problema de multifluxo : o Problema Produto Específico ( PPE ) , o Problema do Destino Específico ( PDE ) e o Problema Origem e Destino Específico ( POD ) . Neste trabalho foi feita uma investigação do impacto das três formulações sobre a decomposição de Dantzig-Wolfe 
 Os sistemas de aplicações científicas e comerciais geram , cada vez mais , imensas quantidades de dados os quais dificilmente podem ser analisados sem que sejam usados técnicas e ferramentas adequadas de análise . Além_disso  , muitas destas aplicações são voltadas para Internet , ou seja , possuem seus dados distribuídos , o que dificulta ainda mais a realização de tarefas como a coleta de dados . A área de Extração de Conhecimento de Base de Dados diz_respeito  às técnicas e ferramentas usadas para descobrir automaticamente conhecimento embutido nos dados . Num ambiente de rede de computadores , é mais complicado realizar algumas das etapas do processo de KDD , como a coleta e processamento de dados . Dessa forma , pode ser feita a utilização de novas tecnologias na tentativa de auxiliar a execução do processo de descoberta de conhecimento . Os agentes de software são programas de computadores com propriedades , como , autonomia , reatividade e mobilidade , que podem ser utilizados para esta finalidade . Neste sentido , o objetivo deste trabalho é apresentar a proposta de um sistema multi-agente , chamado Minador , para auxiliar na execução e gerenciamento do processo de Extração de Conhecimento de Base de Dados 
 O trabalho apresentado nesta tese trata do desenvolvimento de técnicas com suporte à ciência de contexto , baseadas nos padrões MPEG-4 e MPEG-7 , para personalizar e adaptar conteúdo em TV Interativa . Um dos desafios dessa área é desenvolvimento de programas personalizados com rico conteúdo multimídia , com alta interatividade e que , além disso , sejam acessíveis a partir de uma variedade de dispositivos ( fixos ou móveis ) , atendendo às expectativas de interação e de acesso dos usuários . Grande parte do problema está no fato de que os modos encontrados na literatura para representar , descrever e compor programas de TV Interativa não oferecem suporte a contexto , não permitem a separação entre descrições de programas e descrições de objetos e possuem baixa granulosidade de segmentação . Essas características dificultam e , em alguns_casos  , impedem o desenvolvimento de aplicações avançadas em TV Interativa . As técnicas desenvolvidas neste trabalho são baseadas em esquemas de descrição , compatíveis com o padrão MPEG-7 , e na segmentação de programas em objetos MPEG-4 . Os esquemas são utilizados para descrever a estrutura , a composição e a semântica de programas e de seus objetos componentes . Também foi definida e implantada uma infra-estrutura para produção , distribuição e consumo de programas . A utilização conjunta da infra-estrutura e das técnicas permite o desenvolvimento de aplicações avançadas em TV Interativa . Como um exemplo dessas aplicações , foi desenvolvido um serviço automático para personalizar e adaptar programas de TV Interativa , permitindo que um usuário possa acessar , sob_demanda  , programas especialmente produzidos para ele , contendo apenas assuntos de seu interesse e permitindo que o acesso possa ser realizado por dispositivos fixos ou móveis 
 Aplicações científicas atuais têm produzido volumes de dados cada vez maiores . O processamento , a manipulação e a análise desses dados requerem infraestruturas computacionais de larga_escala  tais como aglomerados e grades de computadores . Nesse contexto , várias pesquisas visam o aumento de desempenho dessas aplicações por meio da otimização de acesso a dados . Para alcançar tal objetivo , pesquisadores têm utilizado técnicas de replicação , migração , distribuição e paralelismo de dados . No entanto , uma das principais lacunas dessas pesquisas está na falta de emprego de conhecimento sobre aplicações com objetivo de realizar essa otimização . Essa lacuna motivou esta tese que visa empregar comportamento histórico e preditivo de aplicações a fim de otimizar suas operações de leitura e escrita sobre dados distribuídos . Os estudos foram iniciados empregando-se informações previamente monitoradas de aplicações a fim de tomar decisões relativas à replicação , migração e manutenção de consistência . Observou-se , por meio de uma nova heurística , que um conjunto histórico de eventos auxilia a estimar o comportamento futuro de uma aplicação e otimizar seus acessos . Essa primeira abordagem requer ao menos uma execução prévia da aplicação para composição de histórico . Esse requisito pode limitar aplicações reais que apresentam mudanças comportamentais ou que necessitam de longos períodos de execução para completar seu processamento . Para superar essa limitação , uma segunda abordagem foi proposta baseada na predição on-line de eventos comportamentais de aplicações . Essa abordagem não requer a execução prévia da aplicação e permite adaptar estimativas de comportamento futuro em função de alterações adjacentes . A abordagem preditiva analisa propriedades de séries_temporais  com objetivo de classificar seus processos geradores . Essa classificação aponta modelos que melhor se ajustam ao comportamento das aplicações e que , portanto , permitem predições com maior acurácia . As duas_abordagens  propostas foram implementadas e avaliadas utilizando o simulador OptorSim , vinculado ao projeto LHC/CERN , amplamente adotado pela comunidade_científica  . Experimentos constataram que as duas_abordagens  propostas reduzem o tempo de resposta ( ou execução ) de aplicações que manipulam grandes volumes de dados distribuídos em aproximadamente 50 
 Escoamentos sobre superfícies côncovas estão sujeitos à instabilidade centrífuga , dando origem a vórtices longitudinais , conhecidos como vórtices de Görtler . Esses vórtices são responsáveis por gerar distorções fortes nos perfis de velocidade . Como os vórtices são contra-rotativos , duas regiões surgem entre os mesmos : uma região de upwash e uma região de downwash . Na região de upwash o fluido próximo à parede é jogado para longe da mesma . Na região de downwash acontece o contrário , o fluido que se desloca a uma velocidade maior é jogado em direção à parede . Os vórtices se amplificam inicialmente de forma linear . À jusante na região não linear de desenvolvimento dos vórtices , a amplitude dos mesmos já é elevada , e há a formação de uma estrutura do tipo cogumelo com a distribuição da componente de velocidade na direção principal do escoamento . Essa nova distribuição de velocidade é tridimensional e difere em muito da camada limite obtida com a solução das equações de Blasius . Levando-se em consideração a camada limite térmica , já foi observado que , na média , há um aumento de transferência de calor na direção da parede . No presente_trabalho  , é verificado numericamente a transferência de calor na presença de vórtices de Görtler . Para tal , foi desenvolvido e implementado um código de simulação_numérica  direta espacial ( DNS - do inglês Direct Numerical Simulation ) . Os resultados deste trabalho mostram a intensificação da transferência de calor através dos vórtices de Görtler , tanto no regime não-linear como na instabilidade 
 Atualmente , podemos perceber que uma grande dependência dos sistemas desenvolvidos sob a seara da criptografia foi instaurada em todos nós . Principalmente no tocante dos sistemas criptográficos de chave_pública  , que são vastamente utilizados na Internet . No entanto , a criptografia de chave_pública  viu-se ameaçada e começou a investigar novas fontes de problemas para seus sistemas quando Shor em 1997 desenvolveu um algoritmo de tempo polinomial para fatorar inteiros e para calcular o logaritmo discreto em um computador quântico . Neste contexto , Patarin propõe a função alçapão HFE ( Hidden Field Equations ) , uma trapdoor baseada nos Problemas MQ ( Multivariate Quadratic ) e IP ( Isomorfismo de Polinômios ) . Tais problemas não são afetados pelo algoritmo de Shor , além disto o Problema MQ foi demonstrado por Patarin e Goubin como sendo NP-completo . Apesar do HFE ter sua versão básica quebrada , ele apresenta variações -- obtidas_através  de modificadores genéricos -- resistentes aos principais ataques da atualidade . O Quartz -- esquema de assinatura digital baseado no HFEv- , com escolha especial de parâmetros -- é um bom exemplo desta resistência a ataques algébricos que visem a recuperação da chave privada , pois até hoje permanece seguro . Além de também se destacar por gerar assinaturas curtas . Todavia , Joux e Martinet -- baseados em axiomas do Ataque pelo Paradoxo de Aniversário -- provaram que o Quartz é maleável , demonstrando que caso o adversário possua um par ( mensagem , assinatura ) válido , ele conseguirá obter uma segunda assinatura com 2^ ( 50 ) computações e 2^ ( 50 ) chamadas ao oráculo de assinatura , logo muito abaixo dos padrões de segurança atuais que são de , no mínimo , 2^ ( 112 ) . Desta forma , baseado no Quartz , apresentamos um novo esquema de assinatura digital resistente a ataques adaptativos de mensagem escolhida que realizem chamadas ao oráculo aleatório , com um nível de segurança estimado em 2^ ( 112 ) . Nosso criptossistema proporciona , ainda , um ganho de eficiência no algoritmo de verificação de assinatura e na inicialização dos vetores que serão utilizados pelos algoritmos de assinatura e verificação . Além de , também , disponibilizarmos uma implementação do Quartz Original e do Quartz Aprimorado , na linguagem de programação Java 
 O reconhecimento de faces é uma área de pesquisa desafiadora que abre portas para a implementação de aplicações muito promissoras . Embora muitos algoritmos eficientes e robustos já tenham sido_propostos  , ainda restam vários desafios 
 Dentre os principais obstáculos a serem uperados , está a obtenção de uma representação robusta e compacta de faces que possibilite distinguir os indivíduos rapidamente 
 Visando abordar esse problema , foi realizado um estudo de técnicas de reconhecimento estatístico de padrões , principalmente na área de redução de dimensionalidade dos dados , além de uma revisão de métodos de reconhecimento de faces . Foi proposto ( em colaboração com a pesquisadora Isabelle Bloch ) um método de seleção de características que une um algoritmo de busca eficiente ( métodos de busca seqüencial flutuante ) com uma medida de distância entre conjuntos nebulosos ( distância nebulosa baseada em tolerância ) . Essa medida de distância possui diversas vantagens , sendo possível considerar as diferentes tipicalidades de cada padrão dos conjuntos de modo a permitir a obtenção de bons_resultados  mesmo com conjuntos com sobreposição 
 Os resultados preliminares com dados sintéticos mostraram o caráter promissor dessa abordagem 
 Com o objetivo de verificar a eficiência de tal técnica com dados reais , foram efetuados testes com reconhecimento de pessoas usando imagens da região dos olhos . Nesse caso , em se tratando de um problema com mais de duas classes , nós propusemos uma nova função critério inspirada na distância supracitada . Além_disso  foi proposto ( juntamente com o estudante de mestrado Rogério S. Feris ) um esquema de reconhecimento a partir de seqüências de vídeo . Esse esquema inclui a utilização de um método eficiente de rastreamento de características faciais ( Gabor Wavelet Networks ) e o método proposto anteriormente para seleção de características . Dentro desse contexto , o trabalho desenvolvido nesta dissertação implementa uma parte dos módulos desse esquema 
 Este trabalho aborda o problema de planejamento de caminhos em robótica_móvel  autônoma utilizando campos potenciais . Dentre as várias técnicas de campos potenciais para controlar robôs , encontram-se as técnica de Campos Potenciais de Khatib1 ( CP ) , Campo Potencial Harmônico ( CPH ) , Campo Potencial Orientado ( CPO ) e Campo Potencial Localmente Orientado ( CPLO ) . As técnicas CPH , CPO e CPLO são chamadas de técnicas baseadas em Problema de Valor de Contorno ( PVC ) , pois são obtidas a partir de soluções de Equações Diferenciais Parciais ( EDP ) Elípticas em uma determinada condição de contorno , é obtido um sistema planejador de caminhos . Tais técnicas necessitam de uma etapa de solução de sistemas_lineares  , na qual se utiliza métodos iterativos , decorrentes da aplicação do método de diferenças_finitas  como solucionador das EDP . No presente_trabalho  , as técnicas de Campos Potenciais baseados em PVC foram estudadas e implementadas ( usando processamento sequencial e paralelo ) , de modo a obter resultados de forma mais rápida e confiável . Foram utilizadas arquiteturas paralelas do tipo manycore . Finalmente , são feitas análises comparativas entre os vários métodos implementados . Todos os métodos estão prontos para serem incorporados tanto no simulador quanto nos times de robôs em desenvolvimento pelo grupo Warthog 
 As plantações de eucaliptos representam grande potencial econômico para a indústria de papel , celulose , entre outras , além de apresentar uma série de características positivas como alta produtividade , grande potencial de adaptação e ampla diversidade de espécies . Em consequência a tais vantagens , há décadas diversas pesquisas vem_sendo  realizadas com o intuito de monitorar e detectar diversas doenças que aferem este tipo de cultura . O monitoramento rápido das doenças em eucaliptos torna-se um requisito para evitar grandes perdas econômicas . Neste projeto de pesquisa utilizou-se imagens aéreas obtidas por VANTs ( Veículos Aéreos Não-Tripulados ) para detectar um tipo específico de estresse que afeta as plantações de eucaliptos : a Murcha de Ceratocyst is . Após rotular eucaliptos doentes e saudáveis e outras estruturas em imagens aéreas , técnicas de Aprendizado de Máquina Supervisionado foram desenvolvidas para generalizar o conhecimento e possibilitar uma rápida detecção através das imagens RGB e multiespectrais . Dentre as técnicas utilizadas , destacou-se a arquitetura de Redes_Neurais  Convolucional chamada de Custom- CNN , inspirada no modelo da tradicional arquitetura Lenet -5 agregando-se melhorias do estado-da-arte , como a camada convolucional 1x1 . Na classificação do conjunto RGB , a Custom-CNN obteve o maior F-score , de 0,81 , sendo que a técnica SVM-rbf obteve 0,67 . No conjunto de dados com imagens multiespectrais , a Lenet -5 e a Custom-CNN at ingiram , respectivamente , 0,63 e 0,66 de F-score , enquanto o SVM-rbf obteve 0,46 . Esta dissertação apresenta a metodologia utilizada para a classificação , elencando as principais características dos algoritmos utilizados , bem como os resultados experimentais obtidos . Há ainda uma aplicação do classificador Regressão Logística para o planejamento de trajetória com VANTs 
 Modelagem geométrica envolvendo objetos implícitos é um tema de intensa pesquisa em Computação_Gráfica  . Portanto , obter técnicas eficientes para representar esses objetos é de extrema importância . Dois grupos de objetos implícitos relevantes para Computação_Gráfica  são as curvas implícitas e superfícies implícitas . As técnicas tradicionais para se aproximar curvas e superfícies implícitas envolvem dividir o domínio e buscar em suas partições partes da curva ou da superfície . Neste projeto propomos um novo métodos de poligonização robusta de curvas implícitas usando uma ferramenta numérica auto-validada chamada de Aritmética Afim . O método consiste na poligonização adaptativa de curvas implícitas em malhas triangulares tridimensionais 
 Com o aumento da capacidade de armazenamento de informação em bancos de dados e em computadores_pessoais  , surge a necessidade de algoritmos computacionais capazes de realizar o processamento automático para recuperação desses dados . Esse fato não é diferente para objetos tridimensionais armazenados em formato de arquivos . Nesta Dissertação de Mestrado foram estudadas novas técnicas para processamento desses objetos utilizando uma abordagem não comum à área : técnicas para análise de séries_temporais  , tais como scattering wavelets e gráficos de recorrência . No caso de recuperação total de objetos , ou seja , dado uma malha tridimensional encontrar outras malhas que são visualmente semelhantes , uma única característica é extraída curvatura gaussiana e variação de superfície , por exemplo e ordenada como uma série com a informação provida pelo vetor de Fiedler . Então processa-se essa série utilizando a técnica scattering wavelets , que são capazes de analisar o comportamento temporal de conjunto de dados seriais . Para esse problema , os resultados obtidos são comparáveis com outras abordagens apresentadas na literatura que utilizam várias características para se chegar ao resultado . Já no caso de recuperação parcial de objetos , em que apenas uma parte do objeto é dado como parâmetro de busca , é necessário realizar uma segmentação das malhas para se encontrar outras partes que são visualmente semelhantes . Ao utilizarmos um gráfico de recorrência para analisar os objetos , é possível encontrar não apenas a região mais semelhante dentro da mesma ( ou de outra ) malha , mas também se obtém todas as regiões que são similares ao parâmetro de busca 
 A popularização dos dispositivos computacionais portáteis ao longo dos últimos_anos  , principalmente os smartphones e tablets , tem motivado o desenvolvimento de aplicações , que possuem um grande potencial para facilitar o acesso a informação e auxiliar os usuários na realização de tarefas do cotidiano . Entretanto , notamos que essa tecnologia não é acolhida por todos os nichos de usuários de modo semelhante . Apesar do envelhecimento natural das populações possibilitar um crescimento considerável de pessoas idosas nos últimos_anos  , observa-se que a taxa de utilização dos dispositivos táteis pelos idosos , atualmente , pode ser considerada baixa em relação às demais faixas etárias . Esta menor participação dos usuários idosos do mercado de aplicação para dispositivos_móveis  ocorre devido ao desenvolvimento tecnológico não considerar as especificidades para permitir seu uso , limitando a satisfação dos usuários por meio da concepção de componentes de interface e funcionalidades que dificultam a utilização pelo público idoso , contribuindo até para uma rejeição a essa tecnologia . Assim , neste projeto busca-se propor recomendações de usabilidade específicas para o desenvolvimento de aplicações em dispositivos_móveis  voltadas para os usuários idosos , a partir do desenvolvimento centrado no usuário , de uma aplicação de interesse para esse público alvo , que foi denominada Eldernote . Em direção a esse objetivo realizamos estudos para identificarmos na literatura as principais recomendações e diretrizes de usabilidade para a implementação de aplicações móveis voltadas para os usuários idosos . A partir desses estudos , iniciamos um levantamento sobre esse perfil de usuários e dos requisitos da aplicação . Com base nas informações coletadas , foi desenvolvida a aplicação Eldernote . Todo o processo de desenvolvimento é descrito neste trabalho , desde a concepção das interfaces até a realização de testes preliminares com usuários reais , com o objetivo de investigar as principais características de projetar-se uma aplicação com usabilidade especialmente para usuários idosos . Finalmente , foram realizados os testes de usabilidade com os usuários em um ambiente real de utilização , que propiciou inferirmos as preferências dos usuários , componentes de interface prejudiciais a utilização e a análise da navegação dos usuários durante o uso da aplicação . Ao final , não só foi possível identificar recomendações de usabilidade mas também resultou na disponibilização da aplicação Eldernote à comunidade de usuários no intuito de contribuir para a melhoria da qualidade de vida dos usuários idosos 
 Com a presença cada vez maior de sistemas computacionais e novas tecnologias no cotidiano das pessoas , garantir que eles não falhem e funcionem corretamente tornou-se algo de extrema importância . Além de indicar a qualidade do sistema , assegurar seu bom funcionamento é essencial para se evitar perdas , desde financeiras até de vidas . Uma das técnicas utilizadas para esta finalidade é a chamada verificação formal de programas . A partir da especificação do sistema , descrita numa linguagem formal , são definidas propriedades a serem satisfeitas e que certificariam a qualidade do software . Estas propriedades devem então ser implementadas para uso num verificador , que é a ferramenta responsável por executar a verificação e informar quais propriedades foram satisfeitas e quais não foram ; no caso das propriedades terem sido violadas , o verificador deve indicar aos desenvolvedores os possíveis locais com código incorreto no sistema . A desvantagem do uso da verificação formal é , além do seu alto custo , a necessidade de haver pessoas com experiência em métodos formais para definir propriedades a partir da especificação formal do sistema , e convertê-las numa representação que possa ser entendida pelo verificador . Este processo de definição de propriedades é particularmente complexo , demorado e suscetível a erros , por ser feito em sua maior_parte  de forma_manual  . Para auxiliar os desenvolvedores na utilização da verificação formal em programas escritos em Java , propomos neste trabalho a geração de representação de propriedades para uso direto num verificador . As propriedades a serem geradas são objetivos de teste derivados da especificação formal do sistema . Estes objetivos de teste descrevem o comportamento esperado do sistema que deve ser observado durante sua execução . Ao estabelecer que o universo de propriedades corresponde ao universo de objetivos de teste do programa , garantimos que as propriedades geradas em nosso trabalho descrevem o comportamento esperado do programa por meio de caminhos de execução que levam a um estado de aceitação da propriedade , ou a um estado de violação . Assim , quando o verificador checa o objetivo de teste , ele consegue dar como resultado o veredicto de sucesso ou falha para a propriedade verificada , além de dados da cobertura dos caminhos de execução do programa que podem ser usados para análise do comportamento do programa que levou ao sucesso ou falha da propriedade verificada 
 Em computação ubíqua , existe a ideia de tornar o computador onipresente , `` invisível '' , de modo a aproximar computadores e humanos . Com o avanço das tecnologias de hardware e de software , torna-se interessante investigar possibilidades inovadoras de interação com os computadores . Neste trabalho , exploramos novas formas de interação inspiradas nos atos de desenhar , agarrar e gesticular . Para testá-las , desenvolvemos novos algoritmos baseados em câmeras RGBD para detecção , classificação e rastreamento de objetos , o que permite a concepção de uma instalação interativa que utilize equipamentos portáteis e de baixo custo . Para avaliar as formas de interação propostas , desenvolvemos a Superfície Mágica , um sistema que transforma uma superfície comum ( como uma parede ou uma mesa ) num espaço interativo multi-toque . A Superfície Mágica identifica toques de dedos de mãos , de canetas coloridas e de um apagador , oferecendo também suporte a uma varinha mágica para interação 3D . A Superfície Mágica suporta a execução de aplicativos , permitindo que uma superfície comum se transforme numa área interativa para desenho , num explorador de mapas , num simulador 3D para navegação em ambientes virtuais , entre outras possibilidades . As áreas de aplicação do sistema vão desde a educação até a arte interativa e o entretenimento . A instalação do protótipo envolve : um sensor Microsoft Kinect , um projetor de vídeo e um computador pessoal 
 Esta dissertação de Mestrado apresenta uma extensão do ASiA ( Ambiente de Simulação Automático ) , para simulação de arquiteturas de computadores , denominada Módulo Arquitetura . Este módulo possibilita que o usuário utilize arquiteturas já definidas ( alterando ou não os seus parâmetros ) ou desenvolva o modelo de uma nova arquitetura utilizando ferramentas específicas para simulação de arquitetura de computadores . Dois exemplos ilustram a utilização do Módulo Arquitetura , destacando as vantagens de sua aplicação tanto em ensino como em pesquisa 
 Este trabalho apresenta ainda algumas alterações efetuadas no ASiA para torná-lo mais amigável e flexível . Uma revisão bibliográfica dos assuntos relacionados ao tema é também apresentada 
 A execução da engenharia_reversa  orientada a objetos de um sistema legado desenvolvido com orientação procedimental é usada como base para sua reengenharia , seguindo duas_abordagens  diferentes . Na primeira , o sistema passa por reengenharia com mudança de orientação , por meio de segmentação e , posteriormente , é transformado para uma linguagem orientada a objetos de forma semi-automática . Na segunda , é feito o reconhecimento de padrões recorrentes de software no modelo de objetos produzido pela engenharia_reversa  , para depois efetuar a reengenharia utilizando esses padrões . Os resultados obtidos por intermédio dessas duas_abordagens  podem ser comparados quanto à manutenibilidade , legibilidade e reuso . A versão original do sistema legado escolhido para a realização da experiência está implementado na linguagem Clipper e possui cerca de vinte mil linhas de código . Trata-se de uma oficina auto-elétrica e mecânica de veículos . Para a engenharia_reversa  foi escolhido o método Fusion/RE , sendo feita uma proposta para sua evolução , adicionando um maior detalhamento da etapa de abstração do modelo de análise do sistema . Para mudança de orientação do paradigma de desenvolviemnto , de procedimental para orientado a objetos , são propostas duas etapas adicionais a serem executadas após a aplicação do Fusion/RE : o projeto avante do sistema e a segmentação do programa legado . Indicações sobre como fazer a segmentação são fornecidas . A transformação do código segmentado em Clipper para Java é feita com auxílio da máquina Draco-Puc . Uma estratégia é proposta para o reconhecimento de padrões a partir do modelo de objetos do sistema obtido pela engenharia_reversa  . Por meio dela , instâncias dos padrões Type-Object , Association-Object , State Across a Collection e Behaviour Across a Collection podem ser reconhecidas . Experiências de implementação de alguns desses padrões , em Delphi , são feitas 
 Os atuais gerenciadores de banco de dados não são adequados para manipulação de dados complexos ; e entre eles destacamos os dados multimídia que , para agilizar as consultas usam a operação de igualdade sobre as estruturas de indexação.Operações de igualdade são pouco utilizadas em operações que envolvem dados complexos , uma vez que , a existência de dois elementos extremamente iguais é rara . Uma classe de operadores que se adequa melhor para manipulação desses dados são os operadores por similaridade . Exemplo de operadores de seleção por similaridade são a consulta por abrangência ( range queries ) e consulta aos vizinhos mais próximos . Exemplificando , o operador de seleção aos vizinhos mais próximos responde a consultas como , ? selecione as cinco proteínas mais parecidas pelo alinhamento da proteína Sparc ( responsável pelo câncer de pele ) ? . Existem muitos trabalhos desenvolvidos no sentido de prover operadores de seleção por similaridade envolvendo estruturas baseadas em árvores . Entretanto , poucos_estudos  têm sido realizados envolvendo a utilização de operadores diferentes da seleção , por exemplo , a junção . Um operador de junção compara pares de objetos de elementos pertencentes ao domínio dos dados , ao passo que um operador de seleção recebe uma constante para a comparação dos elementos . Podemos ter assim , três operadores de junção por similaridade : operadores de junção por abrangência , por vizinhos mais próximos e sobre os pares de vizinhos mais próximos . Exemplificando , uma consulta utilizando junção por abrangência responde a consultas do tipo : ? Selecione as proteínas contidas no vírus da Hepatite B que diferem em até duas unidades de alinhamento das contidas no vírus da Hepatite C ? . Este trabalho apresenta um novo método de acesso métrico em extrema quantidade de dados bem como , formas de implementação das formas de junção em estruturas 
 O setor de fundições é importante para a economia , pois produz componentes básicos para muitos outros setores , de modo que seu bom_desempenho  tem repercussão nos demais . Um modelo de programação inteira_mista  para uma fundição de mercado de pequeno_porte  , que visa principalmente minimizar atrasos na entrega dos pedidos , foi proposto na literatura . Neste trabalho é feito um estudo do modelo e é proposta uma nova abordagem , independente de qualquer software comercial , baseada na decomposição do problema em dois subproblemas : o planejamento da produção das ligas e o planejamento da produção dos itens . Ambos foram resolvidos por uma heurística lagrangiana baseada em transferêrencias . Testes computacionais mostraram que a abordagem proposta é capaz de gerar soluções de boa_qualidade  , em tempo computacional 
 A elevada quantidade e variedade de informações adquirida e armazenada em meio eletrônico e a incapacidade humana de analizá-las , têm motivado o desenvolvimento da área de Mineracão de Dados - MD - que busca , de maneira semi-automática , extrair conhecimento novo e útil de grandes bases de dados . Uma das fases do processo de MD é o pré-processamento dessas bases de dados . O pré-processamento de dados tem como alguns de seus principais objetivos possibilitar que o usuário do processo obtenha maior compreensão dos dados utilizados , bem como tornar os dados mais adequados para as próximas fases do processo de MD . Uma técnica que busca auxiliar no primeiro objetivo citado é a geracão de gráficos de dados , a qual consiste na representação gráfica dos registros ( exemplos ) de uma base de dados . Existem diversos métodos de geracão de gráficos , cada qual com suas características e objetivos . Ainda na fase de pré-processamento , de modo a tornar os dados brutos mais adequados para as demais fases do processo de MD , diversas técnicas podem ser aplicadas , promovendo transformações nesses dados . Uma delas é a discretização de dados , que transforma um atributo contínuo da base de dados em um atributo discreto . Neste trabalho são abordados alguns métodos de geração de gráficos e de discretização de dados freqüentemente utilizados pela comunidade . Com relação aos métodos de geração de gráficos , foi projetado e implementado o sistema DISCOVERGRAPHICS que provê interfaces para a geração de gráficos de dados . As diferentes interfaces criadas permitem a utilização do sistema por usuários avançados , leigos e por outros sistemas computacionais . Com relação ao segundo assunto abordado neste trabalho , discretização de dados , foram considerados diversos métodos de discretização supervisionados e não-supervisionados , freqüentemente utilizados pela comunidade , e foi proposto um novo método não-supervisionado denominado K-MeansR . Esses métodos foram comparados entre sí por meio da realização de experimentos e analise estatística dos resultados , considerando-se diversas medidas para realizar a avaliação . Os resultados obtidos indicam que o método proposto supera vários dos métodos de discretização 
 A complexidade dos dados armazenados em grandes bases de dados tem aumentado sempre , criando a necessidade de novas operações de consulta . Uma classe de operações de crescente_interesse  são as consultas por similaridade , das quais as mais conhecidas são as consultas por abrangência ( 'R IND . q ' ) e por k-vizinhos mais próximos ( 'kNN IND . q ' ) . Qualquer consulta e agilizada pelas estruturas de indexação dos Sistemas de Gerenciamento de Bases de Dados ( SGBDs ) . Outro modo de agilizar as operações de busca e a manutenção de métricas sobre os dados , que são utilizadas para ajustar parâmetros dos algoritmos de busca em cada consulta , num processo conhecido como otimização de consultas . Como as buscas por similaridade começaram a ser estudadas seriamente para integração em SGBDs muito mais recentemente do que as buscas tradicionais , a otimização de consultas , por enquanto , e um recurso que tem sido utilizado para responder apenas a consultas tradicionais . Mesmo utilizando as melhores estruturas existentes , a execução de consultas por similaridade tende a ser mais custosa do que as operações tradicionais . Assim , duas estratégias podem ser utilizadas para agilizar a execução de qualquer consulta e , assim , podem ser empregadas também para responder às consultas por similaridade . A primeira estratégia e a reescrita de consultas baseada em propriedades algébricas e em funções de custo . A segunda técnica faz_uso  de fatores externos à consulta , tais como a semântica esperada pelo usuário , para restringir o espaço das respostas . Esta tese pretende contribuir para o desenvolvimento de técnicas que melhorem o processo de otimização de consultas por similaridade , explorando propriedades algebricas e restrições semânticas como refinamento de 
 Como avanço dos paradigmas de desenvolvimento de sistemas eletrônicos , novos conceitos , modelos e técnicas resultaram dessa evolução , gerando ferramentas mais eficientes e objetivas . Entre estas , as de automação de projetos eletrônicos ( EDA - Electronic Design Automation ) em nível de sistema ( ESL - Electronic System Level ) trouxeram um incremento considerável de produtividade à confecção de sistemas eletrônicos , inclusive de sistemas embarcados . Já no que se refere ao desempenho do sistema elaborado , monitorar sua execução e determinar seu perfil de funcionamento são tarefas essenciais para avaliar , a partir do seu comportamento , quais os pontos que representam gargalos ou pontos críticos , afetando sua eficiência geral . Dessa forma , faz-se necessário pesquisar princípios de verificação e otimização dos sistemas elaborados que estejam mais bem adaptados aos novos paradigmas de desenvolvimento de projetos . O presente_trabalho  tem por objetivo implementar um módulo de coleta e processamento de dados para análise de perfil de programas escritos na linguagem C e que sejam executados em processadores soft core , como o NiosII , da Altera . Entretanto , diferentemente das estatísticas oferecidas pela ferramenta GProf ( GNU Profiling ) com relação à análise de desempenho , em que cada amostra obtida implica no incremento de um contador para a função flagrada , o presente_trabalho  volta seu interesse à análise do perfil de uso de memória heap , que encontra-se mormente no volume alocado constatado em cada amostragem . Dessa forma , para diferentes amostragens de uma mesma função interessa saber qual a maior quantidade de memória utilizada pela função entre todas as amostras coletadas . Isso significa que , ao invés de incremento por amostragem , adotar-se-á o princípio do registro do maior valor , em número de bytes , de uso de memória constatado em cada função . Os principais recursos do módulo proposto são : a ) o armazenamento das informações de uso de memória heap obtidas no processo de Profiling em formato apropriado para uso posterior por aplicações de co-projeto de hardware e software ; e b ) a geração de relatórios de Profiling que apresentem o volume de memória dinâmica alocada durante o processamento dos programas analisados para que se possa identificar os locais onde esse uso é mais crítico , permitindo ao projetista tomar decisões quanto à reformulação do código fonte , ou quanto ao incremento no tamanho da memória a ser instalada no sistema , ou quanto à reformulação da arquitetura de um modo 
 O teste de programas concorrentes e uma atividade desaadora , devido a fatores que não estão presentes em programas sequenciais , como comunicação , sincronização e não determinismo . Algumas técnicas de teste têm sido propostas para o contexto de programação concorrente , mas raramente sua aplicabilidade e avaliada por estudos teóricos ou experimentais . Este trabalho contribui nesse sentido , propondo e conduzindo um estudo_experimental  para avaliar o custo , eficácia e aspecto complementar dos critérios de teste estruturais para programas concorrentes no contexto de memória_compartilhada  , implementados usando o padrão PThreads ( Posix Threads ) . A ferramenta de teste ValiPThread e usada para auxiliar a condução do experimento . Os programas usados no experimento foram selecionados de benchmarks , como o Inspect , Helgrind e Rungta . Esses benchmarks são comumente usados no estudo de técnicas de teste para programas concorrentes . Programas que resolvem problemas clássicos da programação concorrente também foram incluídos no estudo . Com base nos resultados obtidos foi definida uma estratégia de aplicação , considerando aspectos de custo e eficácia dos critérios de teste . Além_disso  , todo o material utilizado e gerado durante o experimento foi reunido em um pacote de laboratório , a fim de contribuir com a comunidade de pesquisa , possibilitando replicações e comparações desses critérios com outras técnicas de teste no contexto de programas 
 Técnicas de visualização de informação , tais como as que utilizam posicionamento de pontos baseado na similaridade do conteúdo , são utilizadas para criar representações_visuais  de dados que evidenciem certos padrões . Essas técnicas são sensíveis à qualidade dos dados , a qual , por sua vez , depende de uma etapa de pré-processamento muito influente . Esta etapa envolve a limpeza do texto e , em alguns_casos  , a detecção de termos e seus pesos , bem como a definição de uma função de ( dis ) similaridade . Poucos são os estudos realizados sobre como esses cálculos de ( dis ) similaridade afetam a qualidade das representações_visuais  geradas para dados textuais . Este trabalho apresenta um estudo sobre o papel das diferentes medidas de ( dis ) similaridade entre pares de textos na geração de mapas visuais . Nos concentramos principalmente em dois tipos de funções de distância , aquelas computadas a partir da representação vetorial do texto ( Vector Space Model ( VSM ) ) e em medidas de comparação direta de strings textuais . Comparamos o efeito na geração de mapas visuais com técnicas de posicionamento de pontos , utilizando as duas_abordagens  . Para isso , foram utilizadas medidas objetivas para comparar a qualidade visual dos mapas , tais como Neighborhood Hit ( NH ) e Coeficiente de Silhueta ( CS ) . Descobrimos que ambas as abordagens têm pontos a favor , mas de forma geral , o VSM apresentou melhores_resultados  quanto à discriminação de classes . Porém , a VSM convencional não é incremental , ou seja , novas adições à coleção forçam o recálculo do espaço de dados e das dissimilaridades anteriormente computadas . Nesse sentido , um novo modelo incremental baseado no VSM ( Incremental Vector Space Model ( iVSM ) ) foi considerado em nossos estudos comparativos . O iVSM apresentou os melhores_resultados  quantitativos e qualitativos em diversas configurações testadas . Os resultados da avaliação são apresentados e recomendações sobre a aplicação de diferentes medidas de similaridade de texto em tarefas de análise visual , são 
 Este trabalho apresenta um estudo teórico da Técnica de Geração de Colunas ( GC ) aplicada em alguns Problemas de Roteamento de Veículo ( PRV ) . Essa técnica foi inicialmente utilizada para tratar problemas de otimização de grande porte com estruturas especiais [ Dantzig & Wolfe , 1960 ] . Dentre as diversas classes de problemas de roteamento ; revisamos a aplicação dessa técnica a dois casos particulares : O problema de roteamento de helicópteros em plataformas marítimas , cujo_objetivo  minimizar o custo total do transporte ; O problema de roteamento com janela de tempo , onde a função objetivo é descrita pelo tamanho da frota e o custo do percurso . Revisamos e implementamos um algoritmo de caminho mínimo com janela de tempo ( CMJT ) . Esse algoritmo surge como um sub-problema do algoritmo Primai Simplex para resolver o problema de partição de conjunto , utilizado para modelar o problema de roteamento com janela de tempo 
 Nos últimos_anos  , tem-se dado grande enfoque à resolução de problemas utilizando-se redes neurais_artificiais  , principalmente devido às suas características de aprendizado e adaptação , generalização , processamento paralelo e distribuído , etc ... Pesquisadores também têm proposto o desenvolvimento de sistemas híbridos , que consiste na união de mais de um paradigma , por exemplo , redes neurais , sistemas especialistas e sistemas fuzzy , num mesmo sistema , provendo uma solução melhor para um determinado problema . Neste trabalho , uma técnica para a modelagem de sistemas de produção através dos paradigmas conexionista e fuzzj está sendo proposta . Um sistema de produção neuro . fuzzy , que utiliza esta técnica para a detecção e previsão de falhas na Hidrelétrica de Itaipu , foi desenvolvido . Este sistema , chamado de Sistema de Produção NEUFI ( NETJro Fuzzy para a Itaipu ) , permite a rápida tomada de decisões em emergências e melhor monitoração do sistema Itaipu em situações normais . Para este propósito , uma base de conhecimento baseada em regras , já em uso na operação da Usina pelo sistema R-TESE ( Real Time Expert System Environment ) , foi utilizada como referência inicial . Além_disso  um simulador , denominado Simulador Neuro-Fuzzj , foi também desenvolvido para facilitar a modelagem das regras . A abordagem proposta , quando comparada com o sistema R-TESE , tem as seguintes vantagens . Primeiro , a incorporação de técnicas de redes neurais dá ao sistema NEUFI maior flexibilidade e fácil manutenção . Segundo , possibilidade de visualizar às regras através de estruturas neurais . Outra vantagem é a redução do número de regras devido a similaridade de algumas delas , que têm seu conhecimento implicitamente representado durante o treinamento da rede 
 O Supremo Tribunal Federal ( STF ) mantém uma base de documentos que relatam suas decisões tomadas em todos os julgamentos passados . Esses documentos são chamados de acórdãos e compõem a jurisprudência do STF , pois abordam assuntos que dizem respeito a constituição . Eles estão disponíveis a todos , porém encontrar uma informação relevante é uma tarefa árdua , que muitas_vezes  exige um nível de conhecimento da área jurídica . O STF oferece um mecanismo de busca para esses acórdãos , porém o mecanismo atual utiliza uma forma tradicional de busca baseado em formulários com inúmeros campos a serem preenchidos e selecionados , se assemelhando a um questionário , no qual cada pergunta está relacionada a filtragem de certas informações em toda a base persistida em bancos de dados relacional . Esta abordagem do ponto de vista do usuário é pouco intuitiva e em alguns_casos  imprecisa . Com base nesta dificuldade , neste trabalho é apresentada uma abordagem de um mecanismo de pesquisa que utiliza uma ontologia para a criação de uma representação do conhecimento contido nos acórdãos do STF . Sua construção é feita com o auxílio da tecnologia OBDA ( Ontology Based Data Access ) , que permite a criação de uma camada semântica sobre uma base de dados relacional , o que possibilita a realização de consultas em SPARQL 
 Este trabalho apresenta a especificação e implementação do ASTRAL - Ambiente de Simulação e Teste de pRogramas parALelos . Neste ambiente os programas paralelos são representados através de grafos , que indicam o fluxo de execução da aplicação e as comunicações entre as tarefas que a compõem . A simulação desses grafos permite ao usuário observar e controlar o comportamento do programa , possibilitando a identificação de possíveis erros de comunicação e de problemas relacionados ao desempenho . A especificação do ASTRAL é independente de plataforma , permitindo sua utilização para simulação de qualquer linguagem e ou biblioteca paralela . Uma instanciação deste ambiente para a plataforma PVM é apresentada . O ASTRAL faz parte de um ambiente maior que visa a depuração de programas paralelos , responsabilizando-se pela implementação dos grafos necessários e sua simulação 
 Neste trabalho é descrito uma seqüência de procedimentos para estimar parâmetros e selecionar ordem de modelos Auto-Regressivos com heterocedasticidade , ARCH ( p ) , e Auto- Regressivos generalizados , GARCH ( p , q ) . As estimativas são obtidas utilizando duas técnicas : a inferência clássica e a bayesiana em conjunto com simulação de Monte_Carlo  em Cadeia de Markov ( MCMC ) . Na análise_bayesiana  utilizamos densidades a priori normais para os parâmetros do modelo . Os métodos desenvolvidos foram aplicados em duas séries geradas e em três séries do mercado financeiro : Índice Bovespa , Telebrás e Cotação em Dólar Americano da moeda Iene Japonês . Em geral , as estimativas de máxima_verossimilhança  e bayesiana apresentaram resultados próximos . Porém , em algumas séries , o intervalo com 95 % de confiança para certos parâmetros do modelo apresentou valores negativos , o que viola as restrições impostas aos parâmetros dos modelos ARCH ( p ) , destacando a vantagem da abordagem_bayesiana  
 A utilização da Internet cresceu amplamente nos últimos_anos  e tem propiciado o desenvolvimento de diversas ferramentas de comunicação via web . Têm se destacado , de maneira especial , as ferramentas que possibilitam a disponibilização online de conteúdos diversos , criados pelos próprios usuários , como as wikis . O sucesso obtido pelas wikis deve-se , em grande parte , à pequena quantidade de esforço necessário para a edição das páginas , indicando que esta característica é muito apreciada pelos usuários . Visando tornar o processo de edição de wikis ainda mais ágil , este trabalho apresenta uma proposta de como os mensageiros instantâneos podem auxiliar nesta tarefa . Assim , uma nova nova forma de interação no processo de edição de wikis , por meio de Mensageiro Instantâneo , foi projetada e implementada . Essa forma de interação proposta altera a forma de interação com wikis convencional , no sentido de possibilitar que o autor do conteúdo a ser editado na wiki não necessite mudar de seu ambiente de comunicação , que atualmente tem sido muito utilizado , o de troca de mensagens por meio do mensageiro instantâneo . Além_disso  , esta pesquisa possibilitou a identificação de diversas vantagens e desvantagens da utilização de bots de mensageiros instantâneos , encontradas na literatura , bem como durante os experimentos e estudos de caso 
 Com o intuito de tornarem-se mais eficientes , muitas organizações -- empresas , órgãos governamentais , centros de pesquisa , etc . -- optam pela utilização de ferramentas de software para apoiar a realização de seus processos . Uma opção que vem se tornando cada vez mais popular é a utilização de sistemas de Gestão de Processos de Negócio ( GPN ) , que são ferramentas genéricas , ou seja , não são específicas a nenhuma organização , altamente configuráveis e ajustáveis às necessidades dos objetos de atuação de cada organização . Uma das principais responsabilidades de um sistema de GPN é prover mecanismos de tratamento de exceções à execução de instâncias de processos de negócio . Exceções , se forem ignoradas ou se não forem corretamente tratadas , podem causar o aborto da execução de instâncias e , dependendo da gravidade da situação , podem causar falhas em sistemas de GPN ou até mesmo em sistemas subjacentes ( sistema_operacional  , sistema gerenciador de banco de dados , etc. ) . Sendo_assim  , mecanismos de tratamento de exceções têm por objetivo resolver a situação excepcional ou conter seus efeitos colaterais garantindo , ao menos , uma degradação controlada ( graceful degradation ) do sistema . Neste trabalho , estudamos algumas das principais deficiências de modelos atuais de tratamento de exceções , no contexto de sistemas de GPN , e apresentamos soluções baseadas em Modelos Transacionais Avançados para contorná-las . Isso é feito por meio do aprimoramento dos mecanismos de tratamento de exceções da abordagem de modelagem e gerenciamento de execução de processos de negócio WED-flow . Por fim , estendemos a ferramenta WED-tool , uma implementação da abordagem WED-flow , através do desenvolvimento de seu gerenciador de recuperação de falhas 
 Este trabalho apresenta unia nova abordagem para a avaliação de desempenho , inserindo-se nesse contexto a especificação Statecharts . Essa inserção está devidamente associada a uma solução viável para o modelo . São formalizadas duas extensões aos Statecharts : ( 1 ) os Statecharts Estocásticos , os quais utilizam a notação original dos Statecharts , modificando-se somente a semântica formal , e ( 2 ) os Queuing Statecharts , que não possuem a notação Statecharts pura , mas sim uma aglutinação entre a representação Statecharts e a de redes de filas . Na formalização proposta , são redefinidos alguns , elementos básicos dos Statecharts , tais como eventos e condições , além de alguns conceitos da dinâmica do sistema , por exemplo , passos e configurações . As especificações propostas descrevem o funcionamento básico de um sistema de filas genérico , através de tempiates e eventos-padrão . A esses eventos estão associadas tanto uma solução analítica quanto uma solução por simulação sm p1 ( iMulation Erogramming Language ) . Com o intuito de demonstrar a aplicabilidade das especificações propostas , é desenvolvido um estudo de caso referente à programação distribuída baseada em PVM ( Para/lei Virtual Machire ) . Com os parâmetros obtidos empiricamente , os modelos em Statecharts Estocásticos , Queuing Statecharts e redes de Petri estocásticas generalizadas ( GSPN ) são alimentados e resolvidos tanto através da solução da análise do valor médio ( AVM ) quanto por simulação smpl . Inicialmente , a análise é feita conforme o esquema realizado pelo PVM e , posteriormente , é .acrescentado um mecanismo de escalonamento que leva em consideração o tempo_médio  de serviço das máquinas . Há uma discussão_sobre  os resultados obtidos a partir dos mecanismos adotados para realizar o escalonamento . Por fim , são expostas algumas ponderações sobre as idéias apresentadas nos capítulos componentes desta tese , além de serem apresentadas as contribuições provenientes desta pesquisa e alguns trabalhos futuros propostos a partir desta tese 
 As Lógicas de Descrição são usadas como base para a linguagem OWL , padrão para representação de ontologias na web . No entanto , conhecimento não é estático e , com tal dinamismo , o estudo de revisão de crenças e sua correta e adequada aplicação tornam-se muito importantes . Neste trabalho , pretendemos desenvolver uma ferramenta para revisão de ontologias como um plug-in para o Protégé , o editor de ontologias mais utilizado atualmente 
 O problema de empacotamento de elipsoides consiste em arranjar uma dada coleção de elipsoides dentro de um determinado conjunto . Os elipsoides podem ser rotacionados e transladados e não podem se sobrepor . Um caso particular desse problema surge quando os elipsoides são bolas . O problema de empacotamento de bolas tem sido alvo de intensa pesquisa teórica e experimental . Em particular , muitos trabalhos têm abordado esse problema com ferramentas de otimização . O problema de empacotamento de elipsoides , por outro_lado  , começou a receber mais atenção apenas recentemente . Esse problema aparece em um grande número de aplicações práticas , como o projeto de materiais cerâmicos de alta densidade , na formação e crescimento de cristais , na estrutura de líquidos , cristais e vidros , no fluxo e compressão de materiais granulares e vidros , na termodinâmica e cinética da transição de líquido para cristal e em ciências biológicas , na organização de cromossomos no núcleo de células humanas . Neste trabalho , tratamos do problema de empacotamento de elipsoides dentro de conjuntos compactos do ponto de vista de otimização . Introduzimos modelos de programação não-linear contínuos e diferenciáveis e algoritmos para o empacotamento de elipsoides no espaço n-dimensional . Apresentamos dois modelos diferentes para a não-sobreposição de elipsoides . Como esses modelos têm números quadráticos de variáveis e restrições em função do número de elipsoides a serem empacotados , também propomos um modelo com variáveis implícitas que possui uma quantidade linear de variáveis e restrições . Também apresentamos modelos para a inclusão de elipsoides em semi-espaços e dentro de elipsoides . Através da aplicação de uma estratégia multi-start simples combinada com uma escolha inteligente de pontos iniciais e um resolvedor para otimização local de programas não-lineares , apresentamos experimentos numéricos que mostram as capacidades dos modelos propostos 
 Diferentes organizações dentro de um mesmo domínio de aplicação costumam ter requisitos de dados bastante semelhantes . Apesar disso , cada organização também tem necessidades específicas , que precisam ser consideradas no projeto e desenvolvimento dos sistemas de bancos de dados para o domínio em questão . Dessas necessidades específicas , resultam variações estruturais nos dados das organizações de um mesmo domínio . As técnicas tradicionais de modelagem conceitual de banco de dados ( como o Modelo Entidade-Relacionamento - MER - e a Linguagem Unificada de Modelagem - UML ) não nos permitem expressar em um único esquema de dados essa variabilidade . Para abordar esse problema , este trabalho de mestrado propôs um novo método de modelagem conceitual baseado no uso de Diagramas de Características de Banco de Dados ( DBFDs , do inglês Database Feature Diagrams ) . Esse método foi projetado para apoiar a criação de famílias de esquemas conceituais de banco de dados . Uma família de esquemas conceituais de banco de dados compreende todas as possíveis variações de esquemas conceituais de banco de dados para um determinado domínio de aplicação . Os DBFDs são uma extensão do conceito de Diagrama de Características , usado na Engenharia de Linhas de Produtos de Software . Por meio dos DBFDs , é possível gerar esquemas conceituais de banco de dados personalizados para atender às necessidades específicas de usuários ou organizações , ao mesmo tempo que se garante uma padronização no tratamento dos requisitos de dados de um domínio de aplicação . No trabalho , também foi desenvolvida uma ferramenta Web chamada DBFD Creator , para facilitar o uso do novo método de modelagem e a criação dos DBFDs . Para avaliar o método proposto neste trabalho , foi desenvolvido um estudo de caso no domínio de dados experimentais de neurociência . Por meio do estudo de caso , foi possível concluir que o método proposto é viável para modelar a variabilidade de dados de um domínio de aplicação real . Além_disso  , foi realizado um estudo exploratório com um grupo de pessoas que receberam treinamentos , executaram tarefas e preencheram questionários de avaliação sobre o método de modelagem e a sua ferramenta de software de apoio . Os resultados desse estudo exploratório mostraram que o método proposto é reprodutível e que a ferramenta de software tem boa usabilidade , amparando de forma apropriada a execução do passo-a-passo do método 
 Linhas de produtos de software ( LPS ) são coleções de sistemas que compartilham características comuns , desenvolvidas de forma sistemática a partir de um conjunto comum de ativos centrais . Dentre as técnicas propostas por vários autores para o desenvolvimento de LPS , dois padrões podem ser observados : o processo de desenvolvimento geralmente é sequencial e o foco do projeto ( design ) costuma estar em interesses técnicos , como alocação de componentes e separação em subsistemas , e não em um modelo de domínio . Embora essas práticas sejam reportadas como bem-sucedidas , um outro paradigma de desenvolvimento , baseado em métodos ágeis e em um conjunto de princípios de projeto conhecido como domain-driven design , é apresentado neste trabalho e pode produzir resultados mais satisfatórios , comparados aos métodos tradicionais . Essa hipótese é sustentada por comparações entre padrões de modelagem e por um estudo de caso feito neste 
 Considerando-se o contexto de gerenciamento energético em grades móveis , neste trabalho foram propostos dois algoritmos de escalonamento ( Maximum Regret e Greedy ) que , além de minimizar o consumo de energia , visam assegurar o cumprimento dos requisitos de qualidade de serviço das aplicações submetidas pelos usuários . Tais algoritmos foram projetados a partir de soluções heurísticas para o problema de escalonamento ciente de consumo de energia em grades móveis , que foi modelado como um problema de otimização envolvendo variáveis binárias . Por meio de experimentos , que consideraram tanto cenários estáticos quanto dinâmicos , foi demonstrada a viabilidade dos algoritmos de escalonamento propostos em relação à redução do consumo de energia . Em seu pior caso , o algoritmo Maximum Regret foi 12,18 % pior que o referencial determinado pela melhor solução do solver Gurobi ; já no pior caso do algoritmo Greedy , tal diferença foi de apenas 8,14 
 O protocolo Token Ring sem o ( WTRP ) é um protocolo distribuído de controle de acesso ao meio que provê qualidade de serviço em termos de uso de largura de banda e latência limitada . WTRP consiste de nós ( estações ) que formam topologicamente um anel . Contudo , quando o número de nós em um anel aumenta a latência aumenta e o tempo de reuso de token por parte de um nó em anel também aumenta . Neste trabalho , apresentamos uma versão extendida de WTRP com foco em reduzir a latência , tempo de reuso de token e permitir encaminhamento de dados entre anéis sem aumentar signicativamente o consumo de energia . Para provar o conceito que propomos , implementamos e testamos a nossa versão de WTRP usando simulador de rede - NS 
 Em problemas convencionais de classificação , cada exemplo de um conjunto de dados é associado a apenas uma dentre duas ou mais classes . No entanto , existem problemas de classificação mais complexos , nos quais as classes envolvidas no problema são estruturadas hierarquicamente , possuindo subclasses e superclasses . Nesses problemas , exemplos podem ser atribuídos simultaneamente a classes pertencentes a dois ou mais caminhos de uma hierarquia , ou seja , exemplos podem ser classificados em várias classes localizadas em um mesmo nível hierárquico . Tal hierarquia pode ser estruturada como uma árvore ou como um grafo acíclico direcionado . Esses problemas são chamados de problemas de classificação_hierárquica  multirrótulo , sendo mais difíceis devido à alta complexidade , diversidade de soluções , difícil modelagem e desbalanceamento dos dados . Duas abordagens são utilizadas para tratar esses problemas , chamadas global e local . Na abordagem global , um único classificador é induzido para lidar com todas as classes do problema simultaneamente , e a classificação de novos exemplos é realizada em apenas um passo . Já na abordagem local , um conjunto de classificadores é induzido , sendo cada classificador responsável pela predição de uma classe ou de um conjunto de classes , e a classificação de novos exemplos é realizada em vários passos , considerando as predições dos vários classificadores . Nesta Tese de Doutorado , são propostos e investigados dois métodos para classificação_hierárquica  multirrótulo . O primeiro deles é baseado na abordagem local , e associa uma rede_neural  Multi-Layer Perceptron ( MLP ) a cada nível da hierarquia , sendo cada MLP responsável pelas predições no seu nível associado . O método é chamado Hierarchical Multi- Label Classification with Local Multi-Layer Perceptrons ( HMC-LMLP ) . O segundo método é baseado na abordagem global , e induz regras de classificação hierárquicas multirrótulo utilizando um Algoritmo Genético . O método é chamado Hierarchical Multi-Label Classification with a Genetic Algorithm ( HMC-GA ) . Experimentos utilizando hierarquias estruturadas como árvores mostraram que o método HMC-LMLP obteve desempenhos de classificação superiores ao método estado-da-arte na literatura , e desempenhos superiores ou competitivos quando utilizando hierarquias estruturadas como grafos . O método HMC-GA obteve resultados competitivos com outros métodos da literatura em hierarquias estruturadas como árvores e grafos , sendo capaz de induzir , em muitos_casos  , regras menores e em menor 
 Modelos para aprendizado não supervisionado podem fornecer restrições complementares úteis para melhorar a capacidade de generalização de classificadores . Baseando-se nessa premissa , um algoritmo existente , denominado de C3E ( Consensus between Classification and Clustering Ensembles ) , recebe como entradas estimativas de distribuições de probabilidades de classes para objetos de um conjunto alvo , bem como uma matriz de similaridades entre esses objetos . Tal matriz é tipicamente construída por agregadores de agrupadores de dados , enquanto que as distribuições de probabilidades de classes são obtidas por um agregador de classificadores induzidos por um conjunto de treinamento . Como resultado , o C3E fornece estimativas refinadas das distribuições de probabilidades de classes como uma forma de consenso entre classificadores e agrupadores . A ideia subjacente é de que objetos similares são mais propensos a compartilharem o mesmo rótulo de classe . Nesta tese , uma versão mais simples do algoritmo C3E , baseada em uma função de perda quadrática ( C3E-SL ) , foi investigada em uma abordagem que permitiu a estimação automática ( a partir dos dados ) de seus parâmetros críticos . Tal abordagem faz_uso  de um nova estratégia evolutiva concebida especialmente para tornar o C3E-SL mais prático e flexível , abrindo caminho para que variantes do algoritmo pudessem ser desenvolvidas . Em particular , para lidar com a escassez de dados rotulados , um novo algoritmo que realiza aprendizado semissupervisionado foi proposto . Seu mecanismo explora estruturas intrínsecas dos dados a partir do C3E-SL em um procedimento de autotreinamento ( self-training ) . Esta noção também inspirou a concepção de um outro algoritmo baseado em aprendizado ativo ( active learning ) , o qual é capaz de se autoadaptar para aprender novas classes que possam surgir durante a predição de novos dados . Uma extensa análise experimental , focada em problemas do mundo_real  , mostrou que os algoritmos propostos são bastante úteis e promissores . A combinação de classificadores e agrupadores resultou em modelos de classificação com grande potencial prático e que são menos dependentes do usuário ou do especialista de domínio . Os resultados alcançados foram tipicamente melhores em comparação com os obtidos por classificadores tradicionalmente usados 
 A área de Análise de Sentimentos ou Mineração de Opiniões tem como um dos objetivos principais analisar computacionalmente opiniões , sentimentos e subjetividade presentes em textos . Por conta da crescente quantidade de textos opinativos nas mídias_sociais  da web , e também pelo interesse de empresas e governos em insumos que auxiliem a tomada de decisões , esse tópico de pesquisa tem sido amplamente estudado . Classificar opiniões postadas na web , usualmente expressas em textos do tipo conteúdo gerado por usuários , ou UGC ( user-generated content ) , é uma tarefa bastante desafiadora , já que envolve o tratamento de subjetividade . Além_disso  , a linguagem utilizada em textos do tipo UGC diverge , de várias maneiras , da norma culta da língua , o que impõe ainda mais dificuldade ao seu processamento . Este trabalho relata o desenvolvimento de métodos e sistemas que visam ( a ) a normalização de textos UGC , isto é , o tratamento do texto com correção ortográfica , substituição de internetês , e normalização de caixa e de pontuação , e ( b ) a classificação de opiniões , particularmente de avaliações de produtos , em nível de texto , para o português brasileiro . O método proposto para a normalização é predominantemente simbólico , uma vez que usa de forma explícita conhecimentos linguísticos . Já para a classificação de opiniões , que nesse trabalho consiste em atribuir ao texto um valor de polaridade , positivo ou negativo , foram utilizadas abordagens baseadas em léxico e em aprendizado de máquina , bem como a combinação de ambas na construção de um método híbrido original . Constatamos que a normalização melhorou o resultado da classificação de opiniões , pelo menos para métodos baseados em léxico . Também verificamos extrinsecamente a qualidade de léxicos de sentimentos para o português . Fizemos , ainda , experimentos avaliando a confiabilidade das notas dadas pelos autores das opiniões , já que as mesmas são utilizadas para a rotulação de exemplos , e verificamos que , de fato , elas impactam significativamente o desempenho dos classificadores de opiniões . Por fim , obtivemos classificadores de opiniões para o português brasileiro com valores de medida F1 que chegam a 0,84 ( abordagem baseada em léxico ) e a 0,95 ( abordagem baseada em AM ) , e que são similares aos sistemas para outras línguas , que representam o estado da arte no domínio de avaliação de produtos 
 A crescente disponibilidade de recursos_computacionais  para o cálculo , simulação e aquisição de dados permite que cientistas e engenheiros produzam enormes conjuntos de dados , bi ou tridimensionais , em geral multivariados . A aplicação de técnicas de Computação_Gráfica  com o objetivo de ganhar compreensão desses dados compreende o objeto de estudo da área conhecida por Visualização Científica 
 Texturização é uma forma de variar as propriedades de uma superfície ponto a ponto de forma que esta simule detalhes que não estão de fato presentes na sua geometria . A texturização pode ser aplicada usando as técnicas de mapeamento de textura , e a textura procedimental 
 Conjuntos de dados vetoriais tridimensionais necessitam de técnicas complexas e computacionalmente caras para que sejam visualizadas com sucesso . No caso de dados vetoriais densos , a visualização torna-se mais difícil , uma vez que as técnicas convencionais não produzem uma visualização adequada desse dados 
 A técnica conhecida por Line Integral Convolution ( LIC ) produz bons_resultados  para conjunto de dados vetoriais densos a um custo_computacional  aceitável . Esta técnica utiliza uma textura de ruído branco e o cálculo de streamlines como base para a geração da imagem LIC . Esta técnica produz assim uma imagem bidimensional de uma fatia selecionada desse conjunto de dados , interpolando a textura ruído segundo o campo vetorial a ser visualizado 
 Este trabalho propõe-se a invertigar a técnica LIC e sua relevância no contexto de visualização científica , através da implementação da classe vtkImageLIC . Esta classe segue os princípios de orientação a objetos podendo , assim , ser integrada à biblioteca de visualização VTK , conferindo-lhe portabilidade e capacidade de extensão 
 Este texto trata de um dos temas fundamentais no planejamento de produção , o problema de dimensionamento de lotes de um único item . Uma descrição sucinta e informal do problema segue abaixo . Considere um intervalo de tempo dividido em períodos e que a cada_período  de tempo está associada a demanda de um item . Dados os custos e as eventuais restrições na produção e no armazenamento , determine os períodos em que se produzirá e em que quantidade para que as demandas sejam atendidas com o menor_custo  possível , respeitando as restrições impostas . Apresentamos aqui resultados sobre a estrutura ótima do problema , sobre complexidade e algoritmos para os casos básicos do 
 O fenômeno de molhamento , estudo de como um líquido se deposita em um sólido , apresenta problemas ainda em aberto , dos pontos de vista da modelagem física e da simulação_numérica  . O maior interesse acadêmico neste tipo de escoamento é a linha tríplice ( ou linha de contato ) formada da interação sólido-líquido-gás . A condição de contorno clássica de não escorregamento na interface líquido-sólido leva a uma singularidade no tensor de tensões nesta linha . Além_disso  , ainda não está estabelecido qual o melhor modelo para descrever o ângulo de contato formado entre a superfície_livre  e o substrato ( o sólido ) . Neste trabalho , são discutidos métodos_numéricos  para a simulação de linhas de contato dinâmicas . Os efeitos da tensão superficial são estudados com a abordagem do princípio do trabalho virtual , o qual leva o problema à equações na formulação variacional , linguagem_natural  para o tratamento numérico com o método dos elementos finitos ( FEM ) . O domínio é discretizado por uma malha não-estruturada de forma que as interfaces separadoras são explicitamente representadas pela malha . As derivadas temporais são tratadas em uma abordagem Lagrangeana-Euleriana arbitrária ( ALE ) . Finalmente , são apresentados os resultados numéricos obtidos com o método ALE-FEM , discutindo alguns aspectos da sua convergência temporal e espacial 
 Microorganismos planctônicos constituem a base da cadeia alimentar marinha e desempenham um grande papel na redução do dióxido de carbono na atmosfera . Além_disso  , são muito sensíveis a alterações ambientais e permitem perceber ( e potencialmente neutralizar ) as mesmas mais rapidamente do que em qualquer outro meio . Como tal , não só influenciam a indústria da pesca , mas também são frequentemente utilizados para analisar as mudanças nas zonas costeiras exploradas e a influência destas interferências no ambiente e clima locais . Como consequência , existe uma forte necessidade de desenvolver sistemas altamente eficientes , que permitam observar comunidades planctônicas em grandes escalas de tempo e volume . Isso nos fornece uma melhor compreensão do papel do plâncton no clima global , bem como ajuda a manter o equilíbrio do frágil meio ambiente . Os sensores utilizados normalmente fornecem grandes quantidades de dados que devem ser processados de forma eficiente sem a necessidade do trabalho manual intensivo de especialistas . Um novo sistema de monitoramento de plâncton em grandes volumes é apresentado . Foi desenvolvido e otimizado para o monitoramento contínuo de plâncton ; no entanto , pode ser aplicado como uma ferramenta versátil para a análise de fluídos em movimento ou em qualquer aplicação que visa detectar e identificar movimento em fluxo unidirecional . O sistema proposto é composto de três estágios : aquisição de dados , detecção de alvos e suas identificações . O equipamento óptico é utilizado para gravar imagens de pequenas particulas imersas no fluxo de água . A detecção de alvos é realizada pelo método baseado no Ritmo Visual , que acelera significativamente o tempo de processamento e permite um maior fluxo de volume . O método proposto detecta , conta e mede organismos presentes na passagem do fluxo de água em frente ao sensor da câmera . Além_disso  , o software desenvolvido permite salvar imagens segmentadas de plâncton , que não só reduz consideravelmente o espaço de armazenamento necessário , mas também constitui a entrada para a sua identificação automática . Para garantir o desempenho máximo de até 720 MB/s , o algoritmo foi implementado utilizando CUDA para GPGPU . O método foi testado em um grande conjunto de dados e comparado com a abordagem alternativa de quadro-a-quadro . As imagens obtidas foram utilizadas para construir um classificador que é aplicado na identificação automática de organismos em experimentos de análise de plâncton . Por este motivo desenvolveu-se um software para extração de características . Diversos subconjuntos das 55 características foram testados através de modelos de aprendizagem disponíveis . A melhor exatidão de aproximadamente 92 % foi obtida através da máquina de vetores de suporte . Este resultado é comparável à identificação manual média realizada por especialistas . Este trabalho foi desenvolvido sob a co-orientacao do Professor Rubens Lopes ( IO-USP ) 
 Este projeto propõe a utilização de sistemas OLTP ( Processamento Transacional On-Line 2 no gerenciamento do fluxo de informações entre os segmentos de um ambiente de manufatura integrada voltado à produção discreta . Para tanto , são analisados os aspectos_relacionados  com o desenvolvimento de aplicações cliente-servidor para sistemas transacionais utilizando-se monitores OLTP . Gomo plataforma base para o sistema escolheu-se o ambiente Novell NetWare , por este oferecer uma rede de processamento distribuído 'multithreading ' aberta e flexível . O núcleo do sistema ( monitor OLTP ) reside , juntamente com as aplicações servidoras , no servidor Novell , sendo acessado por todas as estações clientes da rede atra.vés de uma arquitetura cliente-servidor . Será mostrado que a utilização de sistemas OLTP no gerencianzento de ambientes de manufatura integrada voltados à produção discreta , disponibiliza mecanismos para alta performance e disponibilidade do sistema em situações críticas , mantendo a segurança e integridade dos dados , através do compartilhamento e utilização eficiente dos recursos_computacionais  oferecidos 
 Neste trabalho é apresentado um algoritmo para a reconstrução tridimensional de imagens_digitais  , o qual foi baseado na combinação de transformada rápida de Fourier com janela de Hamming e o uso de função trilinear para interpolação . O algoritmo possibilita não somente a geração de mapas tridimensionais da distribuição espacial de spins para tomografia por ressonância_magnética  ( MRT ) , como também mapas de coeficientes de atenuação para tomógrafos de raio-X ou raio-Gamma . A técnica de tons de cinza foi utilizada para a visualização das imagens , bem como foi utilizado ferramentas desenvolvidas no ICMSC , as quais possibilitam análises de fatias coronais ou sagitais das imagens reconstruídas . Resultados mostraram a grande utilidade e eficiência do algoritmo , o qual primeiramente reconstrói as imagens bidimensionalmente para em seguida ser aplicada a interpolação trilinear . Em particular , dados obtidos dos tomógrafos de ressonância_magnética  do IFSC-USP e do de raio-X e Gamma do CNPDIA-Embrapa , ambos de São Carlos-SP , foram utilizados para avaliação do algoritmo . Adicionalmente , uma comparação com o método de retroprojeção é também apresentado . O algoritmo foi desenvolvido em linguagem C++ e em ambientes DOS e UNIX 
 Dada uma função holomorfa f : Cn+1 → C com f ( 0 ) = 0 e , O é uma singularidade isolada , a hipersuperfície de nível f-1 ( 0 ) na vizinhança de O , Dε ∩ f-1 ( 0 ) é homeomorfo ao cone com base K = Sε ∩ f-1 ( 0 ) . Logo o estudo de K é essencial para o entendimento de hipersuperfície de nível na vizinhança de zero , sob um ponto de vista topológica . A aplicação Φ = f / & Iota ; f Ι : Sε - K → S1 , é a projeção de um fibrado localmente trivial denominado de Fibração de Milnor e a sua fibra F0= Φ-1 ( 1 ) tem o tipo de homotopia de um bouquet de esferas SnvSnv ... vSn . Para um difeomorfismo específico h : F0 → F0 , o polinômio característico Δ ( t ) de h* : Hn ( F0 ) → Hn ( F0 é um invariante de K , e se n ≠ 2 então K é homeomorfo a esfera de dimensão 2n -1 se , e somente se Δ ( 1 ) = ±1 . Nesta dissertação , estudaremos K nos casos em que n=1 e nos casos em que f é da forma f ( z1 , z2 , ... , zn+1 ) = za11 + za22 + ... + zan+1n+1 onde ai ' s são inteiros maiores que 1 ( polinômio de Brieskorn ) . Também analizaremos Δ ( t ) e Δ ( 1 ) para o caso em que f seja polinômio de Brieskorn ou um polinômio f para a qual existam racionais positivos { w1 , w2 , ... , wn+1 } tal que f ( ec/w1z1 , ec/w2 , ... ec/wn+1 zn+1 ) = ec/f ( z1 , z2 , ... , zn+1 ) , para todo c ∈ C ( polinômio quase-homogêneo ) 
 Nesta tese estudamos dois problemas em digrafos : um problema de empacotamento e um problema de contagem . Estudamos o problema de empacotamento máximo de arborescências no digrafo aleatório D ( n , p ) , onde cada possvel arco é inserido aleatoriamente ao acaso com probabilidade p = p ( n ) . Denote por ( D ( n , p ) ) o maior inteiro possvel 0 tal que , para todo 0 l , temos ^ ( l-1 ) _i=0 ( l-i ) | { v in d^in ( v ) = i } | Provamos que a quantidade máxima de arborescências em D ( n , p ) é ( D ( n , p ) ) assintoticamente quase certamente . Nós também mostramos estimativas justas para ( D ( n , p ) ) para todo p [ 0 , 1 ] . As principais ferramentas que utilizamos são relacionadas a propriedades de expansão do D ( n , p ) , o comportamento do grau de entrada do digrafo aleatório e um resultado clássico de Frank que serve como ligação entre subpartições em digrafos e a quantidade de arborescências . Para o problema de contagem , estudamos a densidade de subtorneios fortemente conexos com 5 vértices em torneios grandes . Determinamos a densidade assintótica máxima para 5 torneios bem como as famlias assintóticas extremais de cada torneios . Como subproduto deste trabalho caracterizamos torneios que são blow-ups recursivos de um circuito orientado com 3 vértices como torneios que probem torneios especficos de tamanho 5 . Como principal ferramenta para esse problema utilizados a teoria de álgebra de flags e configurações combinatórias obtidas_através  do método semidefinido 
 Várias pesquisas têm sido conduzidas no contexto de ensino e treinamento , sendo o desenvolvimento de módulos educacionais uma das atividades relevantes investigada . Neste trabalho são discutidos e estabelecidos mecanismos de apoio à atividade de modelagem de conteúdos e ao processo de desenvolvimento de tais módulos . Requisitos e perspectivas para a modelagem conceitual , instrucional e didática são identificadas . Uma abordagem integrada ( AIM-CID ) envolvendo diferentes aspectos associados à modelagem de conteúdos também é proposta . No nível conceitual , mapas conceituais estendidos são utilizados . No nível instrucional , é estabelecido o modelo HMBS/Instrucional . No nível didático , propõe-se o modelo HMBS/Didático . Quanto ao processo de desenvolvimento , atividades e tarefas sistemáticas são estabelecidas no contexto de um processo padrão para a elaboração de módulos educacionais . Investigam-se ainda aspectos associados às atividades de especialização e instanciação do processo padrão definido . Um modelo de maturidade de processos - CMM/Educacional - é proposto . Além_disso  , uma instância do processo padrão definido é aplicada no desenvolvimento de um módulo educacional no domínio de Teste de 
 As técnicas de renderização baseadas em imagens permitem que novas visualizações de uma cena sejam geradas a partir de um conjunto de imagens , obtidas a partir de pontos de vista distintos . Pela extensão dessas técnicas para o tratamento de vídeos , podemos permitir a navegação no tempo e no espaço de uma cena obtida a partir de múltiplas câmeras . Nesse trabalho , abordamos o problema de gerar novas visualizações fotorealistas de cenas dinâmicas , com objetos móveis independentes , a partir de vídeos obtidos de múltiplas câmeras com pontos de vista distintos . Os desafios para a solução do problema envolvem a fusão das imagens das múltiplas câmeras minimizando as diferenças de brilho e cor entre elas , a detecção e extração dos objetos móveis da cena e a renderização de novas visualizações combinando um modelo estático da cena com os modelos aproximados dos objetos móveis . Além_disso  , é importante que novas visualizações possam ser geradas em taxas de quadro interativas de maneira a permitir que um usuário navegue com naturalidade pela cena renderizada . As aplicações dessas técnicas são diversas e incluem aplicações na área de entretenimento , como nas televisões digitais interativas que permitem que o usuário escolha o ponto de vista de filmes ou eventos esportivos , e em simulações para treinamento usando realidade virtual , onde é importante que se haja cenas realistas e reconstruídas a partir de cenas reais . Apresentamos um algoritmo para a calibração das cores capaz de minimizar a diferença de cor e brilho entre as imagens obtidas a partir de câmeras que não tiveram as cores calibradas . Além_disso  , descrevemos um método para a renderização interativa de novas visualizações de cenas dinâmicas capaz de gerar visualizações com qualidade semelhante à dos vídeos da cena 
 O Processamento de Línguas_Naturais  é uma área interdisciplinar cujas pesquisas podem ser divididas em duas grandes linhas : análise e síntese da língua . Esta pesquisa de doutorado traz contribuições para ambas . Para a análise da língua , um modelo integrativo capaz de unir diferentes níveis linguísticos é apresentado e avaliado em relação aos níveis morfológico , ( incluindo subníveis léxico e morfossintático ) , sintático e semântico . Enquanto análises tradicionais são feitas dos níveis mais baixos da língua para os mais altos , em uma estratégia em cascata , na qual erros dos níveis mais baixos são propagados para os níveis mais altos , o modelo de análise proposto é capaz de unificar a análise de diferentes níveis a partir de uma abordagem bidirecional . O modelo é baseado em uma grande rede_neural  , treinada em córpus , cujos padrões de treinamento são extraídos de tokens presentes nas orações . Um tipo de recorrência denominado coativação é aplicado no modelo para permitir que a análise de um padrão modifique e seja modificada pela análise de outros padrões em um mesmo contexto . O modelo de análise permite investigações para as quais não foi originalmente planejado , além de apresentar resultados considerados satisfatórios em lematização e análise morfossintática , porém ainda demandando aprimoramento para a tarefa de análise sintática . A ferramenta associada a esse modelo permitiu investigar a recorrência proposta e a interação bidirecional entre níveis da língua , incluindo seus subníveis . Experimentos para coativação e bidirecionalidade foram realizados e considerados satisfatórios . Para a área de síntese da língua , um modelo de simplificação sintática , tarefa considerada como adaptação de texto para texto , baseado em regras manuais é aplicado em textos analisados sintaticamente , tendo como objetivo tornar os textos sintaticamente mais simples para leitores com letramento rudimentar ou básico . A ferramenta associada a esse modelo permitiu realizar simplificação sintática com medida-f de 77,2 % , simplificando aproximadamente 16 % de orações em textos do gênero 
 Pesquisas em Interfaces Naturais , sub-área da Computação Ubíqua , investigam o uso de dispositivos não-tradicionais para possibilitar a interação entre usuários e aplicaçõs de maneiras menos intrusivas ( gestos , voz e escrita baseada em tinta eletrônica , por exemplo ) . Com o aumento da popularidade de dispositivos equipados com sensores de aceleração , os desenvolvedores agora dispõem de um novo dispositivo que pode ser utilizado para prover interação entre usuários e diferentes aplicações , como por exemplo as que se encontram presentes em ambientes de TV interativos . Assim , aplicações que fazem_uso  de acelerômetros vêm_sendo  desenvolvidas para situações específicas , e suas implementações e formatos de dados manipulados são dependentes do domínio para o qual foram projetados . Este trabalho apresenta um modelo para a formalização do modo como esses dados podem ser manipulados , por meio de uma abordagem genérica e extensível . Além_disso  , o modelo permite a descrição de regras para agregação de valor a estes dados por meio da adição de significados . Isto e obtido com a proposta de uma arquitetura em camadas que possibilita a estruturação e compartilhamento desses dados de modo flexível . Três protótipos foram implementados na linguagem de programação Java , fazendo-se uso dessa arquitetura e de uma API desenvolvida para facilitar o uso do modelo . Essas implementações demonstram a viabilidade do modelo proposto como solução para a questão da interoperabilidade nos cenários ilustrados , e para a extensibilidade dos dados , nos casos em que uma mudança de requisitos faz-se 
 Segmentar uma image é visto nos dias de hoje como uma prerrogativa para melhorar a capacidade de sistemas de computador para realizar tarefas complexas de natureza cognitiva tais como detecção de objetos , reconhecimento de padrões e monitoramento de alvos . Esta pesquisa de doutorado visa estudar dois temas de fundamental_importância  no contexto de segmentação de imagens : clusterização espectral e segmentação interativa de imagens . Foram propostos dois novos algoritmos de segmentação dentro das linhas supracitadas , os quais se baseiam em operadores do Laplaciano , teoria espectral de grafos e na minimização de funcionais de energia . A eficácia de ambos os algoritmos pode ser constatada através de avaliações visuais das segmentações originadas , como também através de medidas quantitativas computadas com base nos resultados obtidos por técnicas do estado-da-arte em segmentação de imagens . Nosso primeiro algoritmo de segmentação , o qual ´e baseado na teoria espectral de grafos , combina técnicas de decomposição de imagens e medidas de similaridade em grafos em uma única e robusta ferramenta computacional . Primeiramente , um método de decomposição de imagens é aplicado para dividir a imagem alvo em duas componentes : textura e cartoon . Em seguida , um grafo de afinidade é gerado e pesos são atribuídos às suas arestas de acordo com uma função escalar proveniente de um operador de produto interno . Com base no grafo de afinidade , a imagem é então subdividida por meio do processo de corte espectral . Além_disso  , o resultado da segmentação pode ser refinado de forma interativa , mudando-se , desta forma , os pesos do grafo base . Experimentos visuais e numéricos foram conduzidos tomando-se por base métodos representativos do estado-da-arte e a clássica base de dados BSDS a fim de averiguar a eficiência da metodologia_proposta  . Ao contrário de grande parte dos métodos existentes de segmentação interativa , os quais são modelados por formulações_matemáticas  complexas que normalmente não garantem solução única para o problema de segmentação , nossa segunda metodologia aqui proposta é matematicamente simples de ser interpretada , fácil de implementar e ainda garante unicidade de solução . Além_disso  , o método proposto possui um comportamento anisotrópico , ou seja , pixels semelhantes são preservados mais próximos uns dos outros enquanto descontinuidades bruscas são impostas entre regiões da imagem onde as bordas são mais salientes . Como no caso anterior , foram realizadas diversas avaliações qualitativas e quantitativas envolvendo nossa técnica e métodos do estado-da-arte , tomando-se como referência a base de dados GrabCut da Microsoft . Enquanto a maior_parte  desta pesquisa de doutorado concentra-se no problema específico de segmentar imagens , como conteúdo complementar de pesquisa foram propostas duas novas técnicas para tratar o problema de retoque digital e colorização de imagens 
 Nesta tese , são estudados algoritmos para agrupamento de dados , com particular ênfase em Agrupamento de Dados com Restrições , no qual , além dos objetos a serem agrupados , são fornecidos pelo usuário algumas informações sobre o agrupamento desejado . Como fundamentação para o agrupamento , são considerados os modelos de mistura finitos , em especial , com componentes gaussianos , usualmente chamados de modelos de mistura de gaussianas . Dentre os principais problemas que os algoritmos desenvolvidos nesta tese de doutorado buscam tratar destacam-se : ( i ) estimar parâmetros de modelo de mistura de gaussianas ; ( ii ) como incorporar , de forma eficiente , restrições no processo de aprendizado de forma que tanto os dados quanto as restrições possam ser adicionadas de forma online ; ( iii ) estimar , via restrições derivadas de conceitos pré-determinados sobre os objetos ( usualmente chamados de classes ) , o número de grupos destes conceitos . Como ferramenta para auxiliar no desenvolvimento de soluções para tais problemas , foram utilizados algoritmos_evolutivos  que operam com mais de uma solução simultaneamente , além de utilizarem informações de soluções anteriores para guiar o processo de busca . Especificamente , foi desenvolvido um algoritmo_evolutivo  baseado na divisão e união de componentes para a estimação dos parâmetros de um modelo de mistura de gaussianas . Este algoritmo foi comparado com o algoritmo do mesmo gênero considerado estado-da-arte na literatura , apresentando resultados competitivos e necessitando de menos parâmetros e um menor_custo  computacional . Nesta tese , foram desenvolvidos dois algoritmos que incorporam as restrições no processo de agrupamento de forma online . Ambos os algoritmos são baseados em algoritmos bem-conhecidos na literatura e apresentaram , em comparações empíricas , resultados melhores que seus antecessores . Finalmente , foram propostos dois algoritmos para se estimar o número de grupos por classe . Ambos os algoritmos foram comparados com algoritmos reconhecidos na literatura de agrupamento de dados com restrições , e apresentaram resultados competitivos ou melhores que estes . A estimação bem sucedida do número de grupos por classe pode auxiliar em diversas tarefas de mineração de dados , desde a sumarização dos dados até a decomposição de problemas de classificação em sub-problemas potencialmente mais simples 
 Devido à sua constante evolução , associada ao surgimento contínuo de novas tecnologias , a computação tem se tornado cada vez mais presente no cotidiano das pessoas , sendo utilizada não só no meio acadêmico e corporativo , mas também no entretenimento e no auxílio às atividades pessoais , entre outros . Nesse contexto , vem ocorrendo um grande avanço em termos de redução de custo e de aumento da variedade de dispositivos do tipo Personal Digital Assistants ( PDAs ) . Esse avanço aumenta a demanda por aplicações que explorem esses equipamentos , mas são poucas as ferramentas que auxiliam no desenvolvimento dessas aplicações . O objetivo da presente pesquisa foi investigar o desenvolvimento de aplicações para dispositivos_móveis  , especificamente para PDAs , com independência de plataforma . A abordagem adotada foi a extensão da ferramenta MCards , um construtor de aplicações para PDAs , com o propósito de permitir a geração de aplicações para dispositivos_móveis  com a utilização da tecnologia J2ME ( Java 2 Micro Edition ) . Para tanto , foi necessário , inicialmente , um entendimento do protótipo disponível da ferramenta MCards realizado através da aplicação de um processo de Engenharia Reversa na ferramenta MCards , baseando-se no modelo FUSION-RE/I . Foram executadas , também , manutenções na ferramenta com a finalidade de corrigir imperfeições e problemas de execução e de melhorar suas funcionalidades . Após o processo de Engenharia Reversa e manutenção da ferramenta , foi implementado e incorporado à ferramenta MCards o módulo de geração de aplicações J2ME . O módulo de geração de aplicações J2ME transforma a especificação abstrata , gerada pela manipulação do usuário no editor visual da ferramenta MCards , em código para a tecnologia J2ME com a utilização do perfil CDC / Personal Profile , compilando o código gerado para dar origem aos arquivos da aplicação J2ME que serão utilizados para instalação no dispositivo móvel 
 Esta dissertação apresenta o desenvolvimento e avaliação de um método para a segmentação de vasos sangüíneos em imagens de retina , em que se usa a transformada wavelet contínua bidimensional combinada com classificação supervisionada . A segmentação dos vasos é a etapa inicial para a análise automática das imagens , cujo_objetivo  é auxiliar a comunidade médica na detecção de doenças . Entre outras doenças , as imagens podem revelar sinais da retinopatia diabética , uma das principais causas de cegueira em adultos , que pode ser prevenida se detectada em um diagnóstico precoce . A abordagem apresentada consiste na geração de segmentações pela classificação supervisionada de pixels nas classes `` vaso '' e `` não vaso '' . As características usadas para classificação são obtidas_através  da transformada wavelet contínua bidimensional usando a wavelet de Gabor . Resultados são avaliados nos bancos públicos DRIVE e STARE de imagens coloridas através da análise ROC ( `` receiver operating characteristic '' , ou característica de operação do receptor ) . O método atinge áreas sob curvas ROC de 0.9614 e 0.9671 nos bancos DRIVE e STARE , respectivamente , ligeiramente superiores àquelas apresentadas por outros métodos do estado da arte . Apesar de bons_resultados  ROC , a análise visual revela algumas dificuldades do método , como falsos positivos ao redor do disco óptico e de patologias . A wavelet de Gabor mostra-se eficiente na detecção dos vasos , superando outros filtros lineares . Bons resultados e uma classificação rápida são obtidos usando o classificador bayesiano em que as funções de densidade de probabilidade condicionais às classes são descritas por misturas de gaussianas . A implementação do método está disponível na forma de `` scripts '' código_aberto  em MATLAB para pesquisadores interessados em detalhes de implementação , avaliação ou desenvolvimento de métodos 
 Este trabalho apresenta uma avaliação do algoritmo EBS , uma política de escalonamento proposta para sistemas de tempo real flexíveis com qualidade de serviço baseado em limites superiores para tempos médios de resposta . Experimentos têm demonstrado propriedades vantajosas da política EBS em servidores_Web  com diferenciação de serviço . O objetivo do presente estudo é compreender o comportamento da EBS em relação à diferentes parâmetros que descrevem a carga de trabalho . Esse conhecimento é útil para obtenção de um melhor aproveitamento computacional . São apresentados experimentos e resultados que analisam a influência de cada um dos fatores considerados na qualidade do serviço oferecido . A partir desses resultados são tecidas conclusões acerca de abordagens para o dimensionamento de carga e de capacidade do 
 A Engenharia de Software tem desenvolvido técnicas e métodos para apoiar o desenvolvimento de software confiável , flexível , com baixo custo de desenvolvimento e fácil manutenção . A avaliação da manutenibilidade contribui para fornecer meios para produzir software com alta qualidade . Este trabalho apresenta um experimento controlado para avaliar a manutenibilidade entre as abordagens de web services : RESTful e SOAP-WSDL . Esta avaliação foi conduzida usando 3 programas implementados na linguagem de programação Java e com a mesma arquitetura . Com base na arquitetura projetada , os web services desenvolvidos foram utilizados como objetos em estudos de caso , possibilitando avaliar e comparar a sua manutenibilidade . Os resultados obtidos demonstraram relações entre as informações sobre o custo e a qualidade dos serviços_web  , que contribuíram para esclarecer os critérios para a obtenção de uma boa relação entre o custo da manutenção e a evolução dos serviços_web  . Para concluir , os resultados indica que os web services RESTful são mais manuteníveis do lado do servidor , e os web services SOAP-WSDL são mais manuteníveis do lado do cliente . Os estudos realizados no experimento controlado são promissores e podem auxiliar na redução de custo na manutenção dos serviços_web  , melhorando dessa forma a qualidade do software no 
 Um sensor ou biossensor é um dispositivo analítico que tem por objetivo principal a identificação de substâncias e suas concentrações . O emprego efetivo dos sensores permite resolver diversos problemas em várias áreas como química , biologia , medicina , farmacologia , ciências ambientais , entre outras . Dentre desses problemas podemos ressaltar dois : o diagnóstico de doenças e a substituição de seres_humanos  por línguas eletrônicas para evitar as análises subjetivas . O primeiro problema é resolvido pela identificação de molécula única mas apresenta problemas relacionados a grande quantidade de dados gerados pelos sensores Surface Enhanced Raman Spectroscopy ( SERS ) . Já o segundo problema envolve a recalibração de sensores de espectroscopia de impedância para que possam ser substituídos em línguas eletrônicas quando apresentem deficiências . Em ambos casos foram desenvolvidas abordagens que empregaram técnicas de visualização de informação e permitem uma exploração eficiente da grande quantidade de dados produzidos . A primeira abordagem conseguiu melhorar o tempo de execução do processo de detecção de molécula única em contrapartida a análise manual atualmente empregada além de reduzir o número de falsos positivos . A segunda abordagem , a troca de sensores em línguas eletrônicas , mostrou resultados_satisfatórios  . O uso da definição do circuito eletrônico equivalente de Taylor em combinação com Generalized Procrustes Analysis ( GPA ) permitiu de forma adequada a troca de sensores por conseguir transformar a resposta de um sensor na resposta do outro , característica que no futuro poderá ser usada para conseguir produzir sistemas mais reprodutíveis e 
 O aumento da complexidade dos sistemas embarcados e a necessidade de um desenvolvimento cada vez mais acelerado têm motivado o uso de modelos abstratos que possibilitem maior flexibilidade e reusabilidade . Para isso , faz-se necessária a aceitação das linguagens e perfis mais abstratos , como o MARTE . Neste trabalho , foi desenvolvida uma ferramenta para conversão de sistemas embarcados independente de plataforma ( PIM ) em sistemas de uma plataforma específica ( PSM ) , denominada I2S ( Independente to Specific ) . O I2S é totalmente acoplável a novos desenvolvimentos e necessidades do projetista , capaz de modelar representações gráficas de sistemas embarcados , usando componentes do MARTE e permitindo uma implementação final em tecnologia reconfigurável . A partir de um modelo independente de plataforma faz-se a conversão para o padrão de projeto SOPC-Builder da Altera e XPS da Xilinx , possibilitando a exploração do espaço de projeto nessas duas tecnologias de modo automático . O trabalho faz análise de sistemas convertidos em diversas configurações e traz resultados relevantes para a área que validam o uso da proposta , atendendo aos requisitos de 
 A negociação algorítmica oferece algoritmos que tomam decisões de compra e/ou venda com base em parâmetros pré-determinados , oscilações de preços no mercado , dados históricos etc . Uma vantagem oferecida por ela é a possibilidade de atuação rápida no mercado , possivelmente aproveitando as melhores ofertas disponíveis . A Bovespa disponibiliza dados referentes à troca de mensagens entre as partes que constituem o mercado nanceiro . A partir dessas mensagens , geralmente é possível fazer a construção do livro de ofertas , que contém informações referentes às ofertas de compra e venda disponíveis em dado instante e também sobre negociações que foram concretizadas . Esses dados são disponibilizados em diferentes formatos . Os dados de futuros utilizados neste trabalho , por exemplo , seguem o formato padrão do protocolo FIX , que dene cada mensagem como uma coleção de pares de chave/valor . Um outro formato de dados próprio da Bovespa é utilizado para a disponibilização de dados de ações . Neste trabalho faz-se a construção do livro de ofertas a partir dos dados de futuros , com a proposta de uma estrutura de dados eciente para a manipulação de mensagens no formato do protocolo FIX . Também discute-se sobre a possibilidade de construção do livro de ofertas a partir dos dados de ações . Finalmente , um algoritmo de predição de valores baseado em técnicas de mineração de dados como agrupamento é proposto e analisado quanto à sua aplicabilidade 
 A adoção de Software como Serviço ( do inglês , Software as a Service ou simplesmente SaaS ) está em expansão em todo o mundo , alavancada pelas muitas vantagens que esse modelo de distribuição de software oferece tanto para os provedores desses serviços quanto para seus clientes . Em SaaS , o provedor do serviço também é responsável pelo seu desenvolvimento e execução , o que permite a esses provedores fazerem escolhas sobre a arquitetura de seus sistemas visando diminuir a complexidade e os custos relacionados ao seu desenvolvimento e operação . Nesse contexto , um padrão de arquitetura freqüentemente utilizado é o Multitenant , que torna uma mesma instância do software capaz de servir a múltiplos clientes ( tenants ) simultaneamente . No entanto , sistemas que utilizam Multitenancy enfrentam uma série de desafios , principalmente no que se refere à flexibilidade em atender os requisitos específicos de cada cliente na customização de processos , de fluxos e regras de negócio , e de interfaces com o usuário . Especificamente no problema de customização de interfaces , embora existam trabalhos na literatura relacionados à customização de interfaces Web , e existam implementações de mercado de mecanismos para possibilitar essas customizações , esses trabalhos e mecanismos ou não são projetados especificamente para sistemas Multitenant ou , quando são , não têm sua arquitetura publicada e seu código fonte disponíveis para serem reutilizados . Assim , esta pesquisa investigou uma alternativa para a customização de interfaces Web no contexto de Software como Serviço Multitenant . Como resultado , foi inicialmente definido e implementado um mecanismo não intrusivo para o desenvolvimento de aplicações Multitenant chamado DORMT ( Domain-Based Shared-Database Multitenancy ) , sobre o qual foi possível construir um mecanismo para a customização de interfaces Web , chamado MHT ( Multitenant Hierarchical Themes ) , baseado no padrão MVC ( Model-View-Controller ) e nos conceito de temas e de 
 No desenvolvimento de software , a qualidade do produto está diretamente relacionada à qualidade do processo de desenvolvimento . Diante disso , atividades de Verificação , Validação & Teste ( VV & T ) realizadas por meio de métodos , técnicas e ferramentas são de extrema necessidade para o aumento da produtividade , qualidade e diminuição de custos no desenvolvimento de software . Do mesmo modo , técnicas e critérios contribuem para a produtividade das atividades de teste . Um ponto crucial para o teste de software é sua automatização , tornando as atividades mais confiáveis e diminuindo significativamente os custos de desenvolvimento . Na automatização dos testes , os oráculos são essenciais , representando um mecanismo ( programa , processo ou dados ) que indica se a saída obtida para um caso de teste está correta . Este trabalho de mestrado utiliza a ideia de mutação para criar implementações alternativas de oráculos de teste e , assim , avaliar a sua qualidade . O teste de mutação se refere à criação de versões do sistema em desenvolvimento com pequenas alterações sintáticas de código . A mutação possui alta eficácia na detecção de defeitos e é bastante flexível na sua aplicação , podendo ser utilizada em diversos tipos de artefatos . Adicionalmente , este trabalho propõe operadores de mutação específicos para oráculos , implementa uma ferramenta de apoio à utilização desses operadores para oráculos e também descreve um estudo empírico dos operadores , destacando benefícios e desafios associados ao seu uso 
 A gerência de configuração vem_sendo  estudada desde os anos sessenta . Inicialmente , era aplicada da mesma forma para software e hardware , sendo que no final dos anos setenta já havia padrões de gerência de configuração específicos para software . Com a crescente demanda por qualidade de software , as práticas de gerência de configuração vem_sendo  cada vez mais utilizadas no desenvolvimento de software , uma vez que a gerência de configuração de software é um dos processos fundamentais para se ter qualidade no desenvolvimento e manutenção de software . Embora cada vez mais amplamente utilizado , não existe um consenso de práticas e nomenclaturas sobre as práticas de gerência de configuração . Dessa forma , o objetivo deste trabalho é elaborar um modelo de avaliação do processo de Gerência de Configuração , que permite avaliar as práticas e processos fundamentais para a implantação da gerência de configuração , levando em consideração as práticas e processos descritos nas principais normas internacionais de desenvolvimento de software 
 Esse modelo de avaliação também permite que as ferramentas disponíveis para gerência de configuração sejam avaliadas , identificando quais práticas e processos elas auxiliam na execução 

 Comunidades online proporcionam um ambiente fértil para análise do comportamento de indivíduos e processos sociais . Por exemplo , ao modelarmos interações sociais online , é importante compreendemos quando indivíduos estão reagindo a outros indivíduos . Além_disso  , pessoas e comunidades mudam com o passar do tempo , e levar em consideração sua evolução_temporal  nos leva a resultados mais precisos . Entretanto , em muitos_casos  , o comportamento dos usuários pode ser perdido : suas reações ao conteúdo ao qual são expostos não são capturadas por indicadores explícitos ( likes no Facebook , replies no Twitter ) . Agregações temporais de dados pouco criteriosas podem ocultar , enviesar ou até levar a conclusões equivocadas sobre como usuários evoluem . Apresentamos uma nova abordagem para o problema de detectar respostas não-explicitas que utiliza similaridade tf-idf entre tweets de um usuário e tweets recentes que este usuário recebeu de quem segue . Com base em dados de postagens de um mês para 449 redes egocêntricas do Twitter , este método evidencia que temos um volume de ao menos 11 % de reações não capturadas pelos mecanismos explicitos de reply e retweet . Além_disso  , essas reações não capturadas não estão uniformemente distribuídas entre os usuários : alguns usuários que criam replies e retweets sem utilizar os mecanismos formais da interface são muito mais responsivos a quem eles seguem do que aparentam . Isso sugere que detectar respostas não-explicitas é importante para mitigar viéses e construir modelos mais precisos a fim de estudar interações sociais e difusão de informação . Abordamos o problema de evolução de usuários no Reddit com base em dados entre o período de 2007 a 2014 . Utilizando métodos simples de diferenciação temporal dos usuários -- cohorts anuais -- encontramos amplas diferenças entre o comportamento , que incluem criação de comentários , métricas de esforço e sobrevivência . Desconsiderar a evolução_temporal  pode_levar  a equívocos a respeito de fenômenos importantes . Por exemplo , o tamanho médio dos comentários na rede decresce ao longo de qualquer intervalo de tempo , mas este tamanho é crescente em cada uma das cohorts de usuários no mesmo período , salvo de uma queda inicial . Esta é uma observação do Paradoxo de Simpson . Dividir as cohorts de usuários em sub-cohorts baseadas em anos de sobrevivência na rede nos fornece uma perspectiva melhor ; usuários que sobrevivem por mais tempo apresentam um maior nível de atividade inicial , com comentários mais curtos do que aqueles que sobrevivem menos . Com isto , compreendemos melhor como usuários evoluem no Reddit e levantamos uma série de questões a respeito de futuros desdobramentos do estudo de comportamento online 
 O planejamento de experimentos em sistemas computacionais não é uma tarefa trivial , pois envolve diversas etapas tais como , o planejamento propriamente dito , a execução dos experimentos e a análise dos resultados . A definição e a utilização de metodologias adequadas para cada uma destas etapas facilita a obtenção dos resultados de um experimento em um sistema computacional . Neste trabalho são apresentados mecanismos para auxiliar o planejamento e execução de experimentos em sistemas orientados a serviços . O planejamento de experimento é realizado a partir de um modelo baseado nos conjuntos de entradas comuns a arquiteturas orientadas a serviço . A execução deste planejamento é feita em um ambiente colaborativo real , a qual auxilia a identificação de gargalos que não estão presentes em simulações ou modelos analíticos . Um estudo de caso aplicado na arquitetura WSARCH , possibilitou avaliar seu desempenho e identificar problemas de 
 Neste trabalho foi estudado um problema de controle de sistemas_lineares  com saltos Markovianos sem observação da variável de salto , que pode ser escrito como um problema de otimização de considerável complexidade . As contribuições para a área estão divididas em três aspectos . Um dos avanços foi a elaboração de um contraexemplo para a conjectura de que há somente um mínimo local isolado para o problema . Além_disso  , foi estudado o problema de otimização intermediário , que consiste em fixar todas as variáveis do problema exceto duas matrizes de ganhos , e os resultados indicam que , com uma pequena alteração na formulação , este é um problema biquadrático . Por fim , novos algoritmos foram elaborados a partir de um método disponível na literatura , chamado de método Variacional , adaptando-o para atualizar os ganhos aos pares , levando a problemas intermediários biquadráticos . Três métodos foram implementados para a resolução destes problemas : dois métodos clássicos de descida , Newton e Gradiente , e uma adaptação do próprio método Variacional . Para a análise dos resultados foram utilizados exemplos gerados aleatoriamente a partir do Gerador de SLSM , que pode ser encontrado na literatura , e o método Variacional como referência para comparação com os métodos 
 A representação de imagem baseada em superpixels tem se tornado indispensável na melhoria da eficiência em sistemas de Visão Computacional . Reconhecimento de objetos , segmentação , estimativa de profundidade e estimativa de modelo corporal são alguns importantes problemas nos quais superpixels podem ser aplicados . Porém , superpixels podem influenciar a qualidade dos resultados do sistema positiva ou negativamente , dependendo de quão bem eles respeitam as fronteiras dos objetos na imagem . Neste trabalho , é proposto um método iterativo para geração de superpixels , conhecido por IFT-SLIC , baseado em sequências de Transformadas Imagem-Floresta , começando com uma grade regular de sementes . Um procedimento de recomputação de pixels sementes é aplicado a cada iteração , gerando superpixels conexos com melhor aderência às bordas dos objetos presentes na imagem . Os superpixels obtidos via IFT-SLIC correspondem , estruturalmente , a árvores de espalhamento enraizadas nessas sementes , que naturalmente definem superpixels como regiões de pixels fortemente conexas . Comparadas ao Agrupamento Iterativo Linear Simples ( SLIC ) , o IFT-SLIC considera os custos dos caminhos mínimos entre pixels e os centros dos agrupamentos , em vez de suas distâncias diretas . Funções de conexidade não monotonicamente incrementais são exploradas em neste método resultando em melhor desempenho . Estudos experimentais indicam resultados de extração de superpixels superiores pelo método proposto em comparação com o SLIC . Também é analisada a efetividade do IFT-SLIC , em termos de medidas de eficiência e acurácia , em uma aplicação de segmentação do céu em fotos de paisagens . Os resultados mostram que o IFT-SLIC é competitivo com os melhores métodos do estado da arte e superior a muitos outros , motivando seu desenvolvimento para diferentes aplicações 
 Neste trabalho apresentamos uma análise_Bayesiana  para confiabilidade de sistemas de redes usando métodos de simulação de Monte_Carlo  via Cadeias de Markov . Assumimos diferentes densidades a priori para as confiabilidades dos componentes individuais , com o objetivo de obtermos sumários de interesse . A metodologia é exemplificada condiderando um sistema de rede com sete componentes e um caso especial de sistema complexo composto por nove componentes . Consideramos ainda confiabilidade de redes tipo k-out -- of-m com alguns exemplos 
 Servicos web sao componentes de software fracamente acoplados projetados para promover a comunicacao interoperavel entre aplicacoes na Internet . O acesso a servicos web da-se de maneira padronizada ( via protocolos definidos pelo W3C ) , independente da linguagem de programacao , sistema_operacional  ou plataforma em que sao desenvolvidos . Devido a essas caracteristicas , servicos simples podem ser combinados em servicos mais completos . Coreografia consiste em uma abordagem descentralizada para a composicao de servicos . Diferentemente de orquestracoes , abordagem na qual servicos distribuidos sao coordenados de modo centralizado , a interacao entre os servicos de uma coreografia e colaborativa e a coordenacao descentralizada . Apesar das vantagens , o desenvolvimento de coreografias , incluindo as atividades de testes , nao e algo consolidado . O objetivo desta dissertacao foi desenvolver uma arcabouco de teste que facilite o uso de Desenvolvimento Guiado por Testes ( Test-Driven Development , TDD ) em coreografias de servicos web . Rehearsal , o arcabouco proposto , fornece funcionalidades para o teste automatizado de ( i ) servicos web atomicos ( teste de unidade ) ; ( ii ) composicao de servicos ( teste de integracao ) e ( iii ) uma parte ou toda a coreografia ( teste de aceitacao ) . Esses testes podem ser implementados utilizando funcionalidades do arcabouco , tais como a criacao dinamica de clientes para servicos web , o interceptador de mensagens e a abstracao da coreografia em objetos Java . Alem disso , Rehearsal facilita a criacao e uso de Mocks de servicos web , uma importante pratica de TDD . Além de avaliar o arcabouco desenvolvido , um estudo ex- ploratorio qualitativo foi conduzido com estudantes de Ciencia da Computacao . De acordo com os resultados obtidos , as funcionalidades do Rehearsal mostraram-se adequadas para a utilizacao de TDD em coreografias de servicos web 
 Diversas pesquisas apoiam e investigam o teste de programas concorrentes , as quais objetivam , principalmente , a proposição de critérios de teste e mecanismos para execução das diferentes sincronizações entre processos ou threads . As características específicas dessas aplicações podem ocasionar diferentes_tipos  de defeitos , os quais , em sua maioria , não são facilmente identificados . Nesse contexto , a geração_automática  de dados de teste pode apoiar a atividade de teste atuando na seleção de entradas mais representativas , ou seja , aquelas com maior probabilidade de revelar defeitos . Apesar disso , poucas pesquisas abordam este tema no contexto de programas concorrentes , e as existentes não consideram aspectos importantes desse tipo de aplicação . A geração de dados de teste para programas sequenciais dispõe de uma variedade de técnicas que apoiam a seleção dos dados de teste . Essas técnicas têm sido estendidas para o contexto de programas concorrentes partindo da premissa que esses programas necessitam de abordagens mais complexas para seleção de entradas , em decorrência disso um maior custo é imposto ao teste . Considerando esse contexto , uma lacuna ainda em aberto é a avaliação das técnicas para o cenário de programas concorrentes . Neste trabalho a avaliação das técnicas foi explorada por meio da realização de estudos_experimentais  , os quais avaliaram diferentes técnicas de geração de dados de teste para o contexto de programas concorrentes , considerando a eficácia em revelar defeitos , cobertura de critérios e custo para atividade de teste . Os resultados obtidos demonstraram que as técnicas empregadas para programas sequenciais não atingem o mínimo esperado para este tipo aplicação . Apesar disso , as técnicas investigadas apresentaram características importantes que podem auxiliar a atividade de teste para programas concorrentes e a proposição de abordagens efetivas de geração de dados para esse contexto 
 Esta dissertação relata o primeiro trabalho de pesquisa em alinhamento automático de textos paralelos envolvendo o português brasileiro ( PB ) . Neste trabalho foram implementados cinco métodos de alinhamento sentencial automático bastante referenciados na literatura , incluindo métodos empíricos , lingüísticos e híbridos , avaliados com textos paralelos PB-inglês . Os resultados mostraram-se compatíveis com os relatados para outros pares de línguas , sendo que as maiores precisões ( acima de 94 % ) foram obtidas em corpora sem ruídos ( sem erros gramaticais e de tradução ) , conforme era esperado . Além_disso  , os resultados apontam muita semelhança no desempenho de todos os métodos , o que impossibilita a eleição de um deles como o melhor . Além da implementação dos métodos de alinhamento sentencial e dos corpora paralelos construídos para avaliá-los , outros recursos lingüísticos e computacionais de grande valor para as pesquisas em PLN foram gerados durante este trabalho 
 A textura é um dos principais atributos visuais para a descrição de padrões encontrados na natureza . Diversos métodos de análise de textura têm sido usados como uma poderosa ferramenta para aplicações reais que envolvem análise de imagens e visão_computacional  . Entretanto , os métodos existentes não conseguem discriminar com sucesso a complexidade dos padrões de textura . Tais métodos desconsideram a possibilidade de se descrever estruturas de imagens por meio de medidas como a dimensão_fractal  . Medidas baseadas em fractalidade permitem uma interpretação geométrica não-inteira que possui aplicações encontradas em áreas como matemática , física , e biologia . Sobre esta lacuna metodológica , a hipótese central desta tese é que texturas presentes na natureza podem ser medidas como superfícies fractais irregulares devido à sua geometria complexa , o que pode ser explorado para fins de análise de imagens e visão_computacional  . Para superar tais limitações , avançando o estado da arte , esta tese se inicia com uma análise das características de texturas baseada em caminhadas aleatórias de agentes sobre superfícies de imagens . Esta primeira análise leva a um método que combina dimensão_fractal  com caminhadas de agentes sobre a superfície de imagens . Em uma segunda abordagem , usa-se a difusão não-linear para representar imagens de texturas em diferentes escalas , as quais são descritas via dimensão_fractal  para fins de classificação de imagens . Em uma terceira proposta , emprega-se a dimensão_fractal  sobre múltiplas escalas derivadas de uma mesma imagem com o propósito de se realizar a descrição multi-escala de texturas . Um dos propósitos específicos foi a detecção automática de doenças em folhas de soja . Por último , as características de textura foram exploradas segundo uma metodologia baseada em redes complexas para análise de aglomeração de partículas em imagens de nanotecnologia . Os resultados alcançados nesta tese demonstraram o potencial do uso de características de textura . Para tanto foram usadas técnicas de dimensão_fractal  de Bouligand-Minkowski , multiagentes Artificial Crawlerse difusão não-linear de Perona-Malik , os quais alcançaram eficácia e eficiência comparáveis ao do estado da arte . As contribuições obtidas devem suportar avanços significativos nas áreas de engenharia de materiais , visão_computacional  , e agricultura 
 A relevância técnica e econômica das aplicações práticas dos Problemas de Corte e Empacotamento , bem como suas dificuldades de resolução , têm motivado o desenvolvimento de algoritmos heurísticos dos diversos problemas da área . O presente_trabalho  apresenta uma abordagem heurística para o Problema do Corte em Bobinas de Aço , onde as bobinas do estoque são cortadas em bobinas-intermediárias que por sua vez são laminadas antes de serem recortadas em bobinas finais 
 Redes_Neurais  Artificiais vêm_sendo  amplamente usada em uma variedade de áreas . Uma destas áreas é a previsão de séries_temporais  . Neste trabalho , uma investigação sobre a adequabilidade de usar os modelos de redes neurais conhecidos como Kohonen e Multi-Camadas com algoritmo Back-Propagation , na previsão de vazão , é realizada . Além_disso  , estes métodos são comparados com o Método dos Vizinhos Mais Próximos que tem sido utilizado para previsão de vazão . Uma análise comparativa é feita utilizando os dados da Bacia Hidrográfica do Rio Atibaia e os resultados mostram as vantagens e desvantagens de cada uma das técnicas utilizadas 
 A Interpolação de Imagens Tomográficas é usada durante o processo de formação de volume 3D a partir de determinado conjunto de imagens tomográficas adquiridas , processo chamado de Reconstrução 3D , tendo por objetivo a reconstrução de um volume de dados de resoluções isotrópicas . Os métodos tradicionalmente usados para tal interpolação fornecem resultados_satisfatórios  somente para conjuntos cujas imagens tomográficas possuam espaçamento pequeno entre si , falhando completamente na recuperação das formas de estruturas 3D quando o espaçamento entre as imagens é consideravelmente grande . O objetivo deste trabalho foi o estudo de métodos alternativos de interpolação de imagens tomográficas para reconstrução 3D a partir de conjuntos de imagens com grande espaçamento , buscando a definição de um método mais adequado para interpolação nessa situação . Definido o Método de Interpolação de Imagens Tomográficas através de Matching como método alternativo , e propostas alterações no processo de matching requerido , tal método foi implementado e analisado , chegando-se à conclusão de que o mesmo atinge plenamente o objetivo proposto para este trabalho 
 Esta monografia visa relacionar e discutir os conceitos , a terminologia e as implicações teóricas de um Sistema Completo de Visualizações de Imagens Médicas . A sua implementação foi idealizada para servir a hospitais e demais instituições médicas , pois insere-se neste contexto como ferramenta de auxílio ao diagnóstico por parte dos profissionais da área ; desde o processo de aquisição de imagens até a tratamento e a visualização . Há que se ressaltar a multidisciplinaridade utilizada para o total desenvolvimento deste trabalho onde se destacam a transferência de imagens entre computadores ( locais e remotos ) , na qual foi utilizado o conjunto de protocolos TCP/IP ; o processamento de imagens , e a computação gráfica , na visualização em duas e três dimensões . As estruturas Voxel e Octree Linear , além das técnicas de convolução e filtragem , foram as formas encontradas e utilizadas como estruturas básicas de armazenamento e manipulação da área . O sistema desenvolvido neste trabalho encontra aplicação em um PACS ( Picture Archiving and Communication System ) 
 A área de Dinâmica de Fluido Computacional caracteriza-se pelo amplo uso dos recursos da Computação_Gráfica  com o objetivo de analisar e validar os resultados de suas simulações . Em conexão com o Simulador de Escoamento de Fluidos FREEFLOW , foi desenvolvido um pacote gráfico que oferece facilidades para introdução de dados do domínio e parâmetros do escoamento , bem como possibilita a visualização gráfica dos resultados das simulações realizadas 
 Com o crescente aumento no número de imagens geradas em mídias digitais surgiu a necessidade do desenvolvimento de novas técnicas de recuperação desses dados . Um critério de busca que pode ser utilizado na recuperação das imagens é o da dissimilaridade , no qual o usuário deseja recuperar as imagens semelhantes à uma imagem de consulta . Para a realização das consultas são empregados vetores de características extraídos das imagens e funções de distância para medir a dissimilaridade entre pares desses vetores . Infelizmente , a busca por conteúdo de imagens em consultas simples tende a gerar resultados que não correspondem ao interesse do usuário misturados aos resultados significativos encontrados , pois em geral há uma descontinuidade semântica entre as características_extraídas  automaticamente e a subjetividade da interpretação humana . Com o intuito de tratar esse problema , diversos métodos foram propostos para a diminuição da descontinuidade semântica . O foco_principal  desta tese é o desenvolvimento de métodos escaláveis para a redução da descontinuidade semântica em sistemas recuperação de imagens por conteúdo em tempo real . Nesta sentido , são apresentados : a formalização de consultas por similaridade que permitem a utilização de múltiplos centros de consulta em espaços_métricos  como base para métodos de realimentação de relevância ; um método exato para otimização dessas consultas nesses espaços ; e um modelo para tratamento da diversidade em consultas por similaridade e heurísticas para sua 
 Redes de Quarta Geração ( 4G ) possibilitam acesso ubíquo a serviços em redes heterogêneas . Nesses ambientes , a distribuição do gerenciamento pode necessitar de semânticas para o vocabulário compartilhado , uma facilidade ainda não explorada . Por essa razão , provedores de acesso de diferentes domínios de gerenciamento possuem o desafio de compartilhar informações gerenciais nas trocas de pontos de acesso realizadas pelos usuários em redes 4G . Isso se deve principalmente à carência de um modelo de negócio com controle de acesso , com acordos mútuos , e com gerenciamento de tarifação , segurança e privacidade , o que limita a colaboração entre provedores no momento de aceitar um novo usuário . Este trabalho descreve um conjunto de ontologias para serem utilizadas por novos modelos de acesso com suporte de ontologias para redes 4G . As ontologias formalizam o vocabulário comum das entidades em redes 4G . Para avaliar as ontologias desenvolvidas , a abordagem escolhida foi desenvolver dois protótipos de serviços para redes 4G que especializaram partes das ontologias Haggle , Y-Comm , SOHand e DOHand . A principal_contribuição  deste trabalho é facilitar a troca de informações gerenciais entre moderadores , usuários e provedores de serviços . Outra contribuição é mostrar como o uso de ontologias em plataformas 4G facilita a inserção de novos serviços nos ambientes 
 Neste trabalho procuramos determinar o controle ótimo para problemas de custo médio a longo prazo ( CMLP ) de sistemas_lineares  com saltos markovianos ( SLSMs ) com observação parcial dos estados da cadeia de Markov , e , para isso , implementamos métodos computacionais heurísticos como algoritmos_evolutivos  de primeira geração - algoritmo genético ( AG ) básico - e os algoritmos UMDA ( Univariate Marginal Distribution Algorithm ) e BOA ( Bayesian Optimization Algorithm ) , de segunda geração . Utilizamos um algoritmo variacional para comparar com os métodos implementados e medir a qualidade de suas soluções . Desenvolvemos uma abordagem de transição de níveis de observação ( ATNO ) , partindo de um problema de observação completa e migrando através de problemas parcialmente observados . Cada um dos métodos mencionados acima foi implementado também no contexto da ATNO . Para realizar uma análise estatística sobre o desempenho dos métodos computacionais , utilizamos um gerador de SLSMs com importantes características da teoria de controle como : estabilidade , estabilizabilidade , observabilidade , controlabilidade e detetabilidade . Por fim , apresentamos alguns resultados sobre o CMLP com controles estabilizantes e resultados parciais a respeito da unicidade de 
 Na última_década  observou-se grande interesse pra o desenvolvimento de técnicas para Recuperação de Imagens Baseada em Conteúdo devido à explosão na quantidade de imagens capturadas e à necessidade de armazenamento e recuperação dessas imagens . A área médica especificamente é um exemplo que gera um grande fluxo de informações , principalmente imagens_digitais  para a realização de diagnósticos . Porém um problema ainda permanecia sem solução que tratava-se de como atingir a similaridade baseada na percepção do usuário , uma vez que para que se consiga uma recuperação eficaz , deve-se caracterizar e quantificar o melhor possível tal similaridade . Nesse contexto , o presente_trabalho  de Doutorado visou trazer novas contribuições para a área de recuperação de imagens por contúdo . Dessa forma , almejou ampliar o alcance de consultas por similaridade que atendam às expectativas do usuário . Tal abordagem deve permitir ao sistema CBIR a manutenção da semântica da consulta desejada pelo usuário . Assim , foram desenvolvidos três métodos principais . O primeiro método visou a seleção de características por demanda baseada na intenção do usuário , possibilitando dessa forma agregação de semântica ao processo de seleção de características . Já o segundo método culminou no desenvolvimento de abordagens para coleta e agragação de perfis de usuário , bem como novas formulações para quantificar a similaridade perceptual dos usuários , permitindo definir dinamicamente a função de distância que melhor se adapta à percepção de um determinado usuário . O terceiro método teve por objetivo a modificação dinâmica de funções de distância em diferentes ciclos de realimentação . Para tanto foram definidas políticas para realizar tal modificação as quais foram baseadas na junção de informações a priori da base de imagens , bem como , na percepção do usuário no processo das consultas por similaridade . Os experimentos_realizados  mostraram que os métodos propostos contribuíram de maneira efetiva para caracterizar e quantificar a similaridade baseada na percepção do usuário , melhorando consideravelmente a busca por conteúdo segundo as expectativas dos 
 A educação baseada na Web proliferou de forma rápida nos últimos_anos  . Além do uso do computador no ensino e suporte ao aprendizado de alunos , ele tem sido usado na construção e administração do testes . A participação do computador no processo de avaliação pode ser dar através de métodos convencionais informatizados ou métodos alternativos . A principal característica dos métodos convencionais é classificar se o conhecimento de um estudante é certo ou errado , não permitindo a expressão de conhecimento parcial . Os métodos alternativos procuram avaliar o real conhecimento de um indivíduo , isto é , quanto ele sabe e muitas_vezes  são baseados em teorias estatísticas bastante elaboradas que levam em conta o fator de adivinhação ( chute ) , o nível de dificuldade e o índice de discriminação das questões . A Medida de Probabilidade Admissível ( MPA ) um método alternativo mais simples , consiste na tentativa de qualificar o conhecimento parcial de um estudante por meio de associações numéricas ponderadas a cada opção de resposta de uma questão , estabelecendo um sistema de pontuação efetivo fixando um cenário de recompensas e penalidades associadas a cada alternativa . Uma outra questão relacionada à avaliação é como estabelecer os objetivos educacionais , isto e , quais habilidades realmente estão sendo avaliadas em um dado teste . A resposta a esta questão pode ser dada pela Taxonomia de Objetos Educacionais de Bloom . Este projeto de mestrado propôs o uso da técnica MPA e da Taxonomia de Bloom para dar sustentação a uma avaliação informatizada e de melhor qualidade no cenário do exame de proficiência em inglês para cursos de mestrado . Um sistema de avaliação baseado na Web , chamado CAPTEAP ( Cmputer Assisted Proficiency Test of English for Academic Purposes ) , que utiliza telas dinamicamente construídas e um banco de questões para a avliação do inglês instrumental foi construído para ser usado no Exame de Proficiência em Inglês do programa de mestrado do ICMC-USP . Um sistema construído nestes moldes pode ser aplicado em outros programas de pós-graduação de outras unidades a um custo baixo ( exige-se apenas uma sala com microcomputadores individuais e se acrescido da avaliação de outras habilidades pode também ser aplicado a programas de doutorado 
 A ferramenta ChipCflow vem_sendo  desenvolvida nos últimos quatro anos , inicialmente a partir de um projeto de arquitetura a fluxo de dados dinâmico em hardware_reconfigurável  , mas agora como uma ferramenta de compilação . Ela tem como objetivo a execução de algoritmos por meio do modelo de arquitetura a fluxo de dados associado ao conceito de dispositivos parcialmente reconfiguráveis . Sua característica principal é acelerar o tempo de execução de programas escritos em Linguagem de Programação de Alto Nível ( LPAN ) , do inglês , High Level Languages , em particular nas partes mais intensas de processamento . Isso é feito por meio da implementação dessas partes de código diretamente em hardware_reconfigurável  - utilizando a tecnologia Field-programmable Gate Array ( FPGA ) - aproveitando ao máximo o paralelismo considerado natural do modelo a fluxo de dados e as características do hardware parcialmente reconfigurável . Neste trabalho , o objetivo é a prova de conceito do processo de partição e do protocolo de comunicação entre as partições definidas a partir de um Grafo de Fluxo de Dados ( GFD ) , para a execução direta em hardware_reconfigurável  utilizando Reconfiguração Parcial Dinâmica ( RPD ) . Foi necessário elaborar um mecanismo de partição e protocolo de comunicação entre essas partições , uma vez que a RPD insere características tecnológicas limitantes não encontradas em hardwares reconfiguráveis mais tradicionais . O mecanismo criado se mostrou parcialmente adequado à prova de conceito , significando a possibilidade de se executar GFDs na plataforma parcialmente reconfigurável . Todavia , os tempos de reconfiguração inviabilizaram a proposta inicial de se utilizar RPD para diminuir o tempo de tag matching dos GFDs 
 Grades oportunistas são ambientes distribuídos que permitem o aproveitamento do poder de processamento ocioso de recursos_computacionais  dispersos geograficamente em diferentes domínios administrativos . São características desses ambientes a alta heterogeneidade e a variação na disponibilidade dos seus recursos . Nesse contexto , o paradigma de agentes_móveis  surge como uma alternativa promissora para superar os desafios impostos na construção de grades oportunistas . Esses agentes podem ser utilizados na construção de mecanismos que permitam a progressão de execução das aplicações mesmo na presença de falhas . Esses mecanismos podem ser utilizados isoladamente , ou em conjunto , de forma a se adequar a diferentes cenários de disponibilidade de recursos . Neste trabalho , descrevemos a arquitetura do middleware MAG ( Mobile Agents for Grid Computing Environment ) e o que ele pode fazer em ambientes de grades oportunistas . Utilizamos esse middleware como base para a implementação de um mecanismo de tolerância a falhas baseado em replicação e salvaguarda periódica de tarefas . Por fim , analisamos os resultados obtidos através de experimentos e simulações 
 Nessa tese consideramos problemas de alocação e precificação de itens , onde temos um conjunto de itens e um conjunto de compradores interessados em tais itens . Nosso objetivo é escolher uma alocação de itens a compradores juntamente com uma precificação para tais itens para maximizar o lucro obtido , considerando o valor máximo que um comprador está disposto a pagar por um determinado item . Em particular , focamos em três problemas : o Problema da Compra Máxima , o Problema da Precificação Livre de Inveja e o Leilão de Anúncios de Segundo Preço . O Problema da Compra Máxima e o Problema da Precificação Livre de Inveja modelam o problema que empresas que vendem produtos ou serviços enfrentam na realidade , onde é necessário escolher corretamente os preços dos produtos ou serviços disponíveis para os clientes para obter um lucro interessante . Já o Leilão de Anúncios de Segundo Preço modela o problema enfrentado por empresas donas de ferramentas de busca que desejam vender espaço para anunciantes nos resultados das buscas dos usuários . Ambas as questões , tanto a precificação de produtos e serviços quanto a alocação de anunciantes em resultados de buscas , são de grande relevância econômica e , portanto , são interessantes de serem atacadas dos pontos de vista teórico e prático . Nosso foco nesse trabalho é considerar algoritmos de aproximação e algoritmos de programação inteira_mista  para os problemas mencionados , apresentando novos resultados superiores àqueles conhecidos previamente na literatura , bem como determinar a complexidade computacional destes problemas ou de alguns de seus casos particulares de interesse 
 Verificação de modelos é uma das mais eficientes técnicas de verificação automática de sistemas . No entanto , apesar de poder lidar com verificações complexas , as ferramentas de verificação de modelos usualmente não fornecem informação alguma sobre como reparar inconsistências nestes modelos . Nesta dissertação , mostramos que abordagens desenvolvidas para a atualização de modelos CTL inconsistentes não são capazes de lidar com todos os tipos de alterações em modelos . Introduzimos então o conceito de revisão de modelos : uma abordagem baseada em revisão de crenças para o reparo de modelos inconsistentes em um contexto estático . Relacionamos nossa proposta com trabalhos clássicos em revisão de crenças . Definimos um operador de revisão de modelos e mostramos que este obedece postulados de racionalidade clássico de revisão de crenças . Propomos um algoritmo de revisão com base no algoritmo utilizado pela abordagem de atualização de modelos . Discutimos sobre problemas e limites do algoritmo proposto , e mostramos que essa estratégia de adaptação não é uma solução apropriada 
 A extração de conhecimento útil a partir de conjuntos de dados é um conceito chave em sistemas de informação modernos . Por conseguinte , a necessidade de técnicas eficientes para extrair o conhecimento desejado vem crescendo ao longo do tempo . Aprendizado de máquina é uma área de pesquisa dedicada ao desenvolvimento de técnicas capazes de permitir que uma máquina `` aprenda '' a partir de conjuntos de dados . Muitas técnicas já foram propostas , mas ainda há questões a serem reveladas especialmente em pesquisas interdisciplinares . Nesta tese , exploramos as vantagens da representação de dados em rede para desenvolver técnicas de aprendizado de máquina baseadas em processos dinâmicos em redes . A representação em rede unifica a estrutura , a dinâmica e as funções do sistema representado e , portanto , é capaz de capturar as relações espaciais , topológicas e funcionais dos conjuntos de dados sob análise . Desenvolvemos técnicas baseadas em rede para os três paradigmas de aprendizado de máquina : supervisionado , semissupervisionado e não supervisionado . O processo dinâmico de passeio aleatório é utilizado para caracterizar o acesso de dados não rotulados às classes de dados configurando uma nova heurística no paradigma supervisionado , a qual chamamos de facilidade de acesso . Também propomos uma técnica de classificação de dados que combina a visão de alto nível dos dados , por meio da caracterização topológica de rede , com relações de baixo_nível  , por meio de medidas de similaridade , em uma estrutura geral . Ainda no aprendizado_supervisionado  , as medidas de rede modularidade e centralidade Katz são aplicadas para classificar conjuntos de múltiplas observações , e um método de construção evolutiva de rede é aplicado ao problema de redução de dimensionalidade . O paradigma semissupervisionado é abordado por meio da extensão da heurística de facilidade de acesso para os casos em que apenas algumas amostras de dados rotuladas e muitas amostras não rotuladas estão disponíveis . É também proposta uma técnica semissupervisionada baseada em forças de interação , para a qual fornecemos heurísticas para selecionar parâmetros e uma análise de estabilidade mediante uma função de Lyapunov . Finalmente , uma técnica não supervisionada baseada em rede utiliza os conceitos de controle pontual e tempo de consenso de processos dinâmicos para derivar uma medida de similaridade usada para agrupar dados . Os dados são representados por uma rede conectada e esparsa na qual os vértices são elementos dinâmicos . Simulações com dados de referência e comparações com técnicas de aprendizado de máquina conhecidas são fornecidos para todas as técnicas propostas . As vantagens da representação de dados em rede e de processos dinâmicos para o aprendizado de máquina são evidenciadas em todos os 
 Os sistemas de recuperação de imagens baseada em conteúdo ( CBIR - Content-Based Irnage Retneval ) vêm_sendo  bastante estudados e pesquisados atualmente . Isso ocorre especialmente devido às áreas de aplicabilidade , entre as quais tem-se a área médica , onde há uma enorme_quantidade  de informação armazenada em forma de imagens . Muitas das consultas de interesse dos médicos visam procurar imagens de pacientes que tenham semelhança entre si . Desse modo , estudos de casos , diagnósticos e tratamentos podem ser verificados e comparados baseando-se apenas na imagem e não em descrições textuais sobre elas . Atualmente , os sistemas PACS ( Picture Archiving and Communication Systems ) nã , o são capazes de realizar consultas de imagens por similaridades . O trabalho aqui apresentado implementa um recurso adicional para um PACS com suporte a consultas por similaridade , ampliando o poder de recuperação de imagens através de histogramas . Este recurso permite que o sistema seja capaz de realizar buscas em imagens mesmo com algumas variações de intensidade de brilho , o que é um problema comum já que a aquisição de imagens é realizada por diversos equipamentos e mesmo com diferentes configurações 
 Algoritmos_Evolutivos  que utilizam modelos_probabilísticos  de distribuição dos valores das variáveis ( para orientar o processo de busca da solução de problemas ) são chamados Algoritmos de Estimação de Distribuição ( AEDs ) . Esses algoritmos têm apresentado resultados relevantes para lidar com problemas relativamente complexos . O desempenho deles depende diretamente da qualidade dos modelos_probabilísticos  construídos que , por sua vez , dependem dos métodos de construção dos modelos . Os melhores modelos em geral são construídos por métodos computacionalmente complexos , resultando em AEDs que requerem tempo computacional alto , apesar de serem capazes de explorar menos pontos do espaço de busca para encontrar a solução de um problema . Este trabalho_investiga  modelos_probabilísticos  obtidos por algoritmos de reconstrução de filogenias , uma vez que alguns desses métodos podem produzir , de forma computacionalmente eficiente , modelos que representam bem as principais relações entre espécies ( ou entre variáveis ) . Este trabalho propõe algumas estratégias para obter um melhor uso de modelos baseados em filogenia para o desenvolvimento de AEDs , dentre elas o emprego de um conjunto de filogenias em vez de apenas uma filogenia como modelo de correlação entre variáveis , a síntese das informações mais relevantes desse conjunto em uma estrutura de rede e a identificação de grupos de variáveis correlacionadas a partir de uma ou mais redes por meio de um algoritmo de detecção de comunidades . Utilizando esses avanços para a construção de modelos , foi desenvolvido uma nova_técnica  de busca , a Busca Exaustiva Composta , que possibilita encontrar a solução de problemas combinatórios de otimização de diferentes níveis de dificuldades . Além_disso  , foi proposta uma extensão do novo algoritmo para problemas multiobjetivos , que mostrou ser capaz de determinar a fronteira Pareto-ótima dos problemas combinatórios investigados . Por fim , o AED desenvolvido possibilitou obter um compromisso em termos de número de avaliações e tempo de computação , conseguindo resultados similares aos dos melhores algoritmos encontrados para cada um desses critérios de desempenho nos problemas testados 
 A presença de atributos não importantes , i.e. , atributos irrelevantes ou redundantes nos dados , pode prejudicar o desempenho de classificadores gerados a partir desses dados por algoritmos de aprendizado de máquina . O objetivo de algoritmos de seleção de atributos consiste em identificar esses atributos não importantes para removê-los dos dados antes da construção de classificadores . A seleção de atributos em dados monorrótulo , nos quais cada exemplo do conjunto de treinamento é associado com somente um rótulo , tem sido amplamente estudada na literatura . Entretanto , esse não é o caso para dados multirrótulo , nos quais cada exemplo é associado com um conjunto de rótulos ( multirrótulos ) . Além_disso  , como esse tipo de dados usualmente apresenta relações entre os rótulos do multirrótulo , algoritmos de aprendizado de máquina deveriam considerar essas relações . De modo similar , a dependência de rótulos deveria também ser explorada por algoritmos de seleção de atributos multirrótulos . A abordagem filtro é uma das mais utilizadas por algoritmos de seleção de atributos , pois ela apresenta um custo_computacional  potencialmente menor que outras abordagens e utiliza características gerais dos dados para calcular as medidas de importância de atributos . tais como correlação de atributo-classe , entre outras . A hipótese deste trabalho é trabalho é que algoritmos de seleção de atributos em dados multirrótulo que consideram a dependência de rótulos terão um melhor desempenho que aqueles que ignoram essa informação . Para tanto , é proposto como objetivo deste trabalho o projeto e a implementação de algoritmos filtro de seleção de atributos multirrótulo que consideram relações entre rótulos . Em particular , foram propostos dois métodos que levam em conta essas relações por meio da construção de rótulos e da adaptação inovadora do algoritmo de seleção de atributos monorrótulo ReliefF . Esses métodos foram avaliados experimentalmente e apresentam bom_desempenho  em termos de redução no número de atributos e qualidade dos classificadores construídos usando os atributos selecionados 
 O grau de similaridade entre elementos de dados é o fator primordial para a recuperação de informações em Sistemas Gerenciadores de Bases de Dados que manipulam dados complexos , como seqüências genéticas , séries_temporais  e dados multimídia ( imagens , áudios , vídeos , textos longos ) . Para responder a essas consultas em um tempo reduzido , faz-se necessário utilizar métodos que usam métricas para avaliar a similaridade entre os elementos . Esses métodos são conhecidos como Métodos de Acesso Métricos . Dentre os mais conhecidos na literatura estão a M-tree e a Slim-tree . Existem duas maneiras de executar as operações de construção de índices em qualquer método de acesso : inserindo elemento a elemento ou usando a operação de carga-rápida ( bulk-loading ) . O primeiro tipo de construção é comum e necessário para todo tipo de método de indexação dinâmico . Já as operações de carga-rápida são utilizadas para conjuntos de dados maiores , como por exemplo , na recuperação de backups em bases de dados ou na criação posterior de índices . Nessas situações , a inserção individual tende a ser mais demorada . Realizar uma carga-rápida possibilita a construção de índices com melhor eficiência e em menor tempo , pois há a disponibilidade de todos os dados no instante da criação da estrutura de índices , possibilitando explorar as propriedades do conjunto como um todo . Os Sistemas Gerenciadores de Base de Dados oferecem operações de carga-rápida dos dados nos métodos tradicionais , as quais devem ser supridas também nos Métodos de Acesso Métricos . Neste trabalho , são apresentadas três abordagens , uma técnica para carga-rápida dos dados em Métodos de Acesso Métricos e foi desenvolvido um algoritmo baseado nessa técnica para construir uma Slim-tree . Este é o primeiro algoritmo de carga-rápida baseada em amostragem que sempre produz uma Slim-tree válida , portanto é o primeiro descrito na literatura que pode ser incluído em um Sistema Gerenciador de Base de Dados . Os experimentos descritos neste trabalho mostram que o algoritmo proposto mantém bom agrupamento dos dados e supera o desempenho dos métodos de inserção seqüencial levando em conta tanto o desempenho de construção quanto à eficiência para realizar 
 Problemas com múltiplos objetivos são muito frequentes nas áreas de Otimização , Economia , Finanças , Transportes , Engenharia e várias outras . Como os objetivos são , geralmente , conflitantes , faz-se necessário o uso de técnicas apropriadas para obter boas soluções . A área que trata de problemas deste tipo é chamada de Otimização Multiobjetivo . Neste trabalho , estudamos os problemas dessa área e alguns dos métodos existentes para resolvê-los . Primeiramente , alguns conceitos relacionados ao conjunto de soluções são definidos , como o de eficiência , no intuito de entender o que seria a melhor solução para este tipo de problema . Em seguida , apresentamos algumas condições de otimalidade de primeira_ordem  , incluindo as do tipo Fritz John para problemas de Otimização Multiobjetivo . Discutimos ainda sobre algumas condições de regularidade e total regularidade , as quais desempenham o mesmo papel das condições de qualificação em Programação Não-Linear , propiciando a estrita positividade dos multiplicadores de Lagrange associados às funções objetivo . Posteriormente , alguns dos métodos existentes para resolver problemas de Otimização Multiobjetivo são descritos e comparados entre si . Ao final , aplicamos a teoria e métodos de Otimização Multiobjetivo nas áreas de Compressed Sensing e Otimização de Portfolio . Exibimos então testes computacionais realizados com alguns dos métodos discutidos envolvendo problemas de Otimização de Portfolio e fazemos uma análise dos resultados 
 O sucesso dos Sistemas de Gerenciamento de Banco de Dados ( SGBDs ) em aplicações envolvendo dados tradicionais ( números e textos curtos ) encorajou o seu uso em novos tipos de aplicações , que exigem a manipulação de dados complexos . Séries temporais , dados científicos , dados multimídia e outros são exemplos de Dados Complexos . Inúmeras áreas de aplicação têm demandado soluções para o gerenciamento de dados complexos , dentre as quais a área de informática médica . Dados complexos podem também ser estudos com técnicas de descoberta de conhecimentos , conhecidas como KDD ( Knowledge Discovery in Database ) , usando alguns algoritmos de detecção de agrupamentos apropriados . Entretanto , estes algoritmos possuem custo_computacional  elevado , o que dificulta a sua utilização em grandes_conjuntos  de dados . As técnicas já desenvolvidas na Área de Bases de Dados para indexação de espaços_métricos  usualmente consideram o conjunto de maneira uniforme sem levar em conta a existência de agrupamentos nos dados , por isso as estruturas buscam maximizar a eficiência das consultas para todo o conjunto simultaneamente . No entanto muitas_vezes  as consultas por similaridade estão limitadas a uma região específica do conjunto de dados . Neste contexto , esta dissertação_propõe  a criação de um novo método de acesso , que seja capaz de indexar de forma eficiente dados métricos , principalmente para conjuntos que contenham agrupamentos . Para atingir esse objetivo este trabalho também propõe um novo algoritmo para detecção de agrupamentos em dados métricos tornando mais eficiente a escolha do medoide de determinado conjunto de elementos . Os resultados dos experimentos mostram que os algoritmo propostos FAMES e M-FAMES podem ser utilizados para a detecção de agrupamentos em dados complexos e superam os algoritmos PAM , CLARA e CLARANS em eficácia e eficiência . Além_disso  , as consultas por similaridade realizadas com o método de acesso métrico proposto FAMESMAM mostraram ser especialmente apropriados para conjuntos de dados com 
 Inicialmente projetadas para processamento de gráficos , as placas gráficas ( GPUs ) evoluíram para um coprocessador paralelo de propósito_geral  de alto_desempenho  . Devido ao enorme potencial que oferecem para as diversas_áreas  de pesquisa e comerciais , a fabricante NVIDIA destaca-se pelo pioneirismo ao lançar a arquitetura CUDA ( compatível com várias de suas placas ) , um ambiente capaz de tirar proveito do poder_computacional  aliado à maior facilidade de programação . Na tentativa de aproveitar toda a capacidade da GPU , algumas práticas devem ser seguidas . Uma delas consiste em manter o hardware o mais ocupado possível . Este trabalho propõe uma ferramenta prática e extensível que auxilie o programador a escolher a melhor configuração para que este objetivo seja alcançado 
 Nas últimas_décadas  , o grande avanço da ciência e tecnologia com suas invenções , novos materiais , equipamentos e métodos gerou a necessidade da criação de novos nomes , chamados aqui de termos , e alterações nos seus significados , para nomear adequadamente esses avanços , principalmente em áreas dinâmicas como a Ciência da Computação , a Genética e a Medicina . Dado que o desenvolvimento de repertórios terminológicos é um trabalho difícil quando realizado manualmente , lingüistas computacionais , lingüistas aplicados , tradutores , intérpretes , jornalistas científicos têm se interessado pela extração automática de terminologias ( EAT ) de textos . O crescimento explosivo de dados do tipo texto disponíveis na Web foi um fator contribuinte para a facilidade na construção de córpus eletrônicos de textos técnicos e científicos , propiciando a implementação de métodos de EAT . A EAT tem sido de grande interesse para todos os tipos de aplicações do Processamento de Línguas_Naturais  ( PLN ) que trabalham com domínios especializados e que , conseqüentemente , necessitam de um vocabulário especial . O objetivo desse projeto de mestrado foi avaliar métodos de EAT para o português do Brasil , ainda carente do tratamento automatizado para a criação de terminologias . Especificamente , foram implementados e avaliados métodos de EAT das abordagens estatística , lingüística e híbrida para unigramas , bigramas e trigramas a partir de um córpus de textos do domínio de Revestimentos Cerâmicos . Esses métodos empregam recursos simples como ( a ) uma stoplist para eliminar palavras como advérbios , ( b ) padrões sintáticos para os termos do domínio , por exemplo & ltsubstantivo adjetivo & gt , & ltsubstantivo preposição adjetivo & gt , levantados após a aplicação de um etiquetador Part-Of-Speech , ( c ) uma lista de expressões e palavras características de definições , descrições , classificações como 'definido ( a ) ( s ) como ' , 'caracterizado ( a ) ' , 'conhecido ( a ) ( s ) como ' , 'significa ( m ) ' , entre outras que são concentradoras de termos . As medidas estatísticas utilizadas nos métodos estatísticos e híbridos para indicar a relevância de termos no domínio são a informação mútua , o log-likelihood , o coeficiente dice e a freqüência . Os métodos propostos foram avaliados pelas medidas de precisão , revocação e medida_F  , utilizando uma lista de referência da área de Revestimentos Cerâmicos . Os melhores_resultados  da precisão são do método híbrido para unigramas ( 7 % ) , bigramas ( 17 % ) e trigramas ( 26 % ) , enquanto que a revocação é melhor nos métodos puramente lingüísticos tanto para unigramas ( 95 % ) como para bigramas ( 90 % ) e trigramas ( 100 % ) . Os melhores valores da medida_F  foram dos métodos híbridos ( 11 % , 17 % e 33 % para uni , bi e trigramas , respectivamente ) . Esses valores , embora tenham se apresentado os mais relevantes , foram bastante inferiores àqueles normalmente encontrados na literatura que trata da EAT , cujo desempenho obtido para essa tarefa fica em torno de 60 % . Esses valores motivam a busca e implementação de métodos mais avançados para tratar o português , bem como a obtenção de recursos mais elaborados , a fim de encontrar resultados mais significantes para essa tarefa , facilitando , conseqüentemente o trabalho do especialista da área , que vai analisar os candidatos a termos extraídos pelos métodos automáticos , visto que é possível fornecer a ele informações mais precisas ( poucas palavras da língua geral ) e completas ( uma maior quantidade de termos ) sobre o córpus considerado 
 Diagnóstico baseado em modelos ( Model Based Diagnosis - MBD ) é uma técnica de Inteligência_Artificial  usada para encontrar componentes falhos em dispositivos físicos . MBD também tem sido utilizado para auxiliar programadores experientes a encontrarem falhas em seus programas , sendo essa técnica chamada de Depuração de Programas baseada em Modelos ( Model Based Software Debugging - MBSD ) . Embora o MBSD possa auxiliar programadores experientes a entenderem e corrigirem suas falhas , essa abordagem precisa ser aprimorada para ser usada por aprendizes de programação . Esse trabalho propõe o uso da técnica de depuração hierárquica de programas , uma extensão da técnica MBSD , para que aprendizes de programação sejam capazes de depurar seus programas raciocinando sobre componentes abstratos , tais como : padrões elementares , funções e procedimentos . O depurador hierárquico de programas proposto foi integrado ao Dr. Java e avaliado com um grupo de alunos de uma disciplina de Introdução à Programação . Os resultados mostram que a maioria dos alunos foi capaz de compreender as hipóteses de falha geradas pelo depurador automático e usar essas informações para corrigirem seus programas 
 Neste trabalho estudamos as aplicações do método do Gradiente Espectral Projetado ( SPG ) em Meteorologia nos campos de previsibilidade , estabilidade e sensibilidade . Inicialmente revisamos os Vetores Singulares Lineares ( LSVs ) e em seguida apresentamos a teoria das Perturbações Condicionais Não-Lineares Ótimas ( CNOPs ) . Enquanto os métodos clássicos estão baseados no Modelo Tangente Linear , as CNOPs são uma formulação do mesmo problema baseado em Programação Não-Linear . As CNOPs são descritas na literatura como responsáveis por melhorias em relação aos métodos anteriores . Finalmente analisamos três exemplos de aplicação do método à problemas de previsibilidade , estabilidade e sensibilidade 
 Menor custo e expectativa de menor time-to-market são os principais motivadores para melhorias de produtividade de software . Para gerir eficazmente a produtividade , é importante identificar as dificuldades mais relevantes e desenvolver estratégias para lidar com elas . Os métodos ágeis , incluindo Programação Extrema e Scrum , evoluíram como abordagens para simplificar o processo de desenvolvimento de software , potencialmente levando a uma melhor produtividade . Eles visam reduzir o tempo de desenvolvimento e lidar com as mudanças inevitáveis decorrentes da dinâmica do mercado . Embora a indústria tenha adotado amplamente métodos ágeis , há pouco entendimento científico do construto agilidade em desenvolvimento de software em relação às suas dimensões , determinantes e efeitos sobre o desempenho no desenvolvimento de software . Compreender esse construto poderia ajudar a determinar onde concentrar os esforços de gestão ( e recursos financeiros relacionados ) de um ponto de vista prático , assim como onde concentrar os esforços de investigação de uma perspectiva científica . Pesquisa considerável tem sido direcionada para identificar os fatores com impacto significativo na produtividade de desenvolvimento de software . Em geral , os fatores de produtividade estudados foram relacionadas ao produto , pessoas , projeto , processo ou questões organizacionais . Avaliar fatores de produtividade continuamente é importante , pois os fatores podem mudar quando novas práticas de engenharia de software são adotadas . No entanto , poucos_estudos  investigaram fatores influenciando a produtividade de times ágeis . O objetivo desta tese é explorar definições , fatores e monitoramento de produtividade em times ágeis e melhorar a prática baseada em evidência . Esta tese apresenta cinco novas contribuições : C1 - Verificação empírica da importância de produtividade para as empresas que adotam métodos ágeis e seus benefícios percebidos ; C2 - Justificativa para a definição da produtividade no contexto de métodos ágeis ; C3 - A verificação empírica de fatores de produtividade em times ágeis ; C4 - Um arcabouço conceitual de fatores de produtividade em times ágeis e seu impacto ; C5 - Um processo de acompanhamento de produtividade de times ágeis , considerando adaptabilidade e uma avaliação da utilidade de métricas de produtividade para esses times 
 A teoria das redes complexas é uma área altamente interdisciplinar que oferece recursos para o estudo dos mais variados tipos de sistemas complexos , desde o cérebro até a sociedade . Muitos problemas da natureza podem ser modelados como redes , tais como : as interações protéicas , organizações sociais , o mercado financeiro , a Internet e a World_Wide  Web . A organização de todos esses sistemas complexos pode ser representada por grafos , isto é , vértices conectados por arestas . Tais topologias têm uma influencia fundamental sobre muitos processos dinâmicos . Por exemplo , roteadores altamente conectados são fundamentais para manter o tráfego na Internet , enquanto pessoas que possuem um grande número de contatos sociais podem contaminar um grande número de outros indivíduos . Ao mesmo tempo , estudos têm mostrado que a estrutura do cérebro esta relacionada com doenças neurológicas , como a epilepsia , que está ligada a fenômenos de sincronização . Nesse trabalho , apresentamos como técnicas de mineração de dados podem ser usadas para estudar a relação entre topologias de redes complexas e processos dinâmicos . Tal estudo será realizado com a simulação de fenômenos de sincronização , falhas , ataques e propagação de epidemias . A estrutura das redes será caracterizada através de métodos de mineração de dados , que permitirão classificar redes de acordo com um conjunto de modelos e determinar padrões de conexões presentes na organização de diferentes_tipos  de sistemas complexos . As análises serão realizadas com aplicações em neurociências , biologia de sistemas , redes_sociais  e 
 Segmentação de imagens é ainda uma etapa desafiadora do processo de reconhecimento de padrões . Entre as abordagens de segmentação , muitas são baseadas em particionamento em grafos , as quais apresentam alguns inconvenientes , sendo um deles o tempo de processamento muito elevado . Com as recentes pesquisas na teoria de redes complexas , as técnicas de reconhecimento de padrões baseadas em grafos melhoraram consideravelmente . A identificação de grupos de vértices pode ser considerada um processo de detecção de comunidades de acordo com a teoria de redes complexas . Como o agrupamento de dados está relacionado com a segmentação de imagens , esta também pode ser abordada através de redes complexas . No entanto , a segmentação de imagens baseado em redes complexas apresenta uma limitação fundamental , que é o número excessivo de nós na rede . Neste trabalho é proposta uma abordagem de redes complexas para segmentação de imagens de grandes dimensões que é ao mesmo tempo precisa e rápida . Para alcançar este objetivo , é incorporado o conceito de Super Pixels , visando reduzir o número de nós da rede . Os experimentos mostraram que a abordagem proposta produz segmentações de boa_qualidade  em baixo tempo de processamento . Além_disso  uma das principais_contribuições  deste trabalho é a determinação dos melhores parâmetros , uma vez que torna o método bastante independente dos parâmetros , o que não fora alcançado antes em nenhuma pesquisa da 
 O contínuo aumento da complexidade no controle de sistemas robóticos , bem como a aplicação de grupos de robôs auxiliando ou substituindo seres_humanos  em atividades críticas tem gerado uma importante demanda por soluções mais robustas , flexíveis , e eficientes . O desenvolvimento convencional de algoritmos especializados , constituídos de sistemas baseados em regras e de autômatos usados para coordenar estes conjuntos físicos em um ambiente dinâmico é um desafio extremamente complexo . Diversos modelos de desenvolvimento existem , entretanto , muitos desafios da área da robótica_móvel  autônoma continuam em aberto . Esta tese se insere no contexto da busca por soluções inteligentes a serem aplicadas em robôs_móveis  autônomos com o objetivo de permitir a operação destes em ambientes dinâmicos . Buscamos , com a investigação e aplicação de estratégias inteligentes por meio de aprendizado de máquina no funcionamento dos robôs , a proposta de soluções originais que permitam uma nova visão sobre a operação de robôs_móveis  em três dos desafios da área da robótica_móvel  autônoma , que são : localização , navegação e operações com grupos de robôs . As pesquisas sobre localização e coordenação de grupos apresentam investigação e propostas originais , buscando estender o estado da arte , onde apresentam resultados inovadores . A parte sobre navegação tem como objetivo principal ser um elo entre os conceitos de localização e coordenação de grupos , sendo o foco o desenvolvimento de um veículo autônomo com maior implicação em avanços técnicos . Relacionado com a coordenação de grupos de robôs , fizemos a escolha de trabalhar sobre uma aplicação modelada como o problema de combate a incêndios florestais . Buscamos desenvolver um ambiente de simulação realístico , onde foram avaliadas quatro técnicas para busca de iii estratégias de formação do grupo : Algoritmos Genéticos , Otimização por Enxame de Partículas , Hill Climbing e ( iv ) Simulated Annealing . Com base nas diversas avaliações realizadas pudemos mostrar quais das técnicas e conjuntos de parâmetros permitem a obtenção de resultados mais acurados que os demais . Além_disso  , mostramos como uma heurística baseada em populações anteriores pode auxiliar na tolerância a falhas da operação . Relacionado com a tarefa de navegação , apresentamos o desenvolvimento de um veículo autônomo de grande porte funcional para ambientes externos . Buscamos aperfeiçoar uma arquitetura para navegação_autônoma  , baseada em visão monocular e com capacidade de seguir pontos esparsos de GPS . Mostramos como a simulação e os usos de robôs de pequeno_porte  auxiliaram no desenvolvimento do veículo de grande porte e apresentamos como as redes neurais podem ser aplicadas nos modelos de navegação_autônoma  . Na investigação sobre localização , mostramos um método utilizando informação obtida de redes sem fio para prover informação de localização para robôs_móveis  . As informações obtidas da rede sem fio são utilizadas para aprendizado da posição de um robô_móvel  por meio de uma rede_neural  . Diversas avaliações foram realizadas buscando entender o comportamento do sistema com diferentes números de pontos de acesso , com uso de filtros , com diferentes topologias . Os resultados mostram que o modelo usando redes sem fio pode ser um possível método prático e barato para localização de robôs_móveis  . Esta tese aborda temas relevantes e propostas originais relacionadas com os objetivos propostos , apresentando métodos que provenham autonomia na coordenação de grupos e nas atividades individuais dos mesmos . A busca por altos graus de eficiência na resolução de tarefas em ambientes dinâmicos ainda é um campo que carece de soluções e de um aprofundamento nas pesquisas . Sendo_assim  , esta pesquisa buscou agregar diversos avanços científicos na área de pesquisa de robôs_móveis  autônomos e coordenação de grupos , por meio da aplicação de estratégias 
 O problema de empacotamento em faixas de peças irregulares consiste em cortar um conjunto de peças bidimensionais a partir de um objeto de largura fixa utilizando o menor comprimento possível . Apesar de sua importância econômica para diversos setores industriais , há poucos trabalhos que abordam o problema de forma exata devido a sua dificuldade de resolução . Recentemente , Toledo et_al  . ( 2013 ) propuseram um modelo inteiro misto para este problema , no qual as peças são posicionadas em uma malha de pontos . Este modelo obteve bons_resultados  , provando a otimalidade para instâncias com até 21 peças . No entanto , o modelo possui um grande número de restrições de não-sobreposição , que cresce rapidamente de acordo com a discretização utilizada e a quantidade de peças distintas que devem ser alocadas . Neste trabalho , são propostas novas formulações_matemáticas  baseadas neste modelo , com o objetivo de reduzir o número de restrições . Na primeira abordagem , são propostos dois modelos reduzidos que mostraram ser eficientes para instâncias com poucas repetições de peças . Na segunda abordagem , foi proposto um modelo de cobertura por cliques para o problema . Este modelo obteve desempenho igual ou superior ao modelo da literatura para todas as instâncias avaliadas , obtendo uma solução ótima para instâncias com até 28 peças 
 O propósito desta dissertação é combinar tópicos de percolação e processo de contato para formular e obter resultados em um modelo de sistema de partículas que é inspirado no fenômeno de difusão de uma inovação em uma população estruturada . Mais precisamente , propomos uma cadeia de Markov a tempo contínuo definida na rede hipercúbica d-dimensional . Cada indivíduo da população deve estar em algum dos três estados pertencentes ao conjunto { 0 ; 1 ; 2 } . Nesse modelo , 0 representa ignorante , 1 consciente e 2 adotador . Serão estudados argumentos que permitam encontrar condições suficientes nas quais a inovação se espalha ou não com probabilidade positiva . Isto envolve o estudo de modelos de percolação e do processo de contato 
 Problemas de escalonamento cuja função objetivo é o consumo de energia tem sido cada vez mais estudados . Neste trabalho , estudamos o problema conhecido , em inglês , por Dynamic Speed Scaling , um problema de escalonamento de tarefas bem definidas em processadores de velocidade variável , cujo consumo de energia é função da velocidade . Além_disso  , relacionamos este problema com outro conhecido como MOSP , sigla em inglês para Multi-Organization Scheduling Problem . Neste , queremos escalonar tarefas de múltiplas organizações independentes respeitando certas restrições individuais . Provamos , neste trabalho , que este novo problema é NP-Completo e desenvolvemos várias heurísticas eficientes cujos testes experimentais_mostram  economia de energia significativa 
 A Web Semântica é uma maneira de explorar a associação de significados explícitos aos conteúdos de documentos presentes na Web , para que esses possam ser processados diretamente ou indiretamente por máquinas . Para possibilitar esse processamento , os computadores necessitam ter acesso a coleções estruturadas de informações e a conjuntos de regras de inferência sobre esses conteúdos . O SWRL permite a combinação de regras e termos de ontologias ( definidos por OWL ) para aumentar a expressividade de ambos . Entretanto , conforme um conjunto de regras cresce , ele torna-se de difícil compreensão e sujeito a erros , especialmente quando mantido por mais de uma pessoa . Para que o SWRL se torne um verdadeiro padrão web , deverá ter a capacidade de lidar com grandes_conjuntos  de regras . Para encontrar soluções para este problema , primeiramente , foi realizado um levantamento sobre sistemas de regras de negócios , descobrindo os principais recursos e interfaces utilizados por eles , e então , com as descobertas , propusemos técnicas que usam novas representações_visuais  em uma aplicação web . Elas permitem detecção de erro , identificação de regras similares , agrupamento , visualização de regras e o reuso de átomos para novas regras . Estas técnicas estão implementadas no SWRL Editor , um plug-in open-source para o Web-Protégé ( um editor de ontologias baseado na web ) que utiliza ferramentas de colaboração para permitir que grupos de usuários possam não só ver e editar regras , mas também comentar e discutir sobre elas . Foram realizadas duas avaliações do SWRL Editor . A primeira avaliação foi um estudo de caso para duas ontologias da área biomédica ( uma área onde regras SWRL são muito usadas ) e a segunda uma comparação com os únicos três editores de regras SWRL encontrados na literatura . Nessa comparação foi mostrando que ele implementa mais recursos encontrados em sistemas de regras em 
 Esta dissertação teve como objetivo o estudo de modelos para séries_temporais  bivariadas , que tem a estrutura de dependência determinada por meio de funções de cópulas . A vantagem desta abordagem é que as cópulas fornecem uma descrição completa da estrutura de dependência . Em termos de inferência , foi adotada uma abordagem_Bayesiana  com utilização dos métodos de Monte_Carlo  via cadeias de Markov ( MCMC ) . Primeiramente , um estudo de simulações foi realizado para verificar como os seguintes fatores , tamanho das séries e variações nas funções de cópula , nas distribuições marginais , nos valores do parâmetro de cópula e nos métodos de estimação , influenciam a taxa de seleção de modelos segundo os critérios EAIC , EBIC e DIC . Posteriormente , foram realizadas aplicações a dados reais dos modelos com estrutura de dependência estática e variante no 
 Expressões matemáticas manuscritas online estão constituídas por sequências de traços . O reconhecimento automático de tais expressões requer a solução de três subproblemas : segmentação de símbolos , classificação de símbolos e análise estrutural ( isto é , a identificação de relações espaciais , tais como sobrescrito e subscrito , entre símbolos ) . Uma das dificuldades principais do problema é a ambiguidade no nível de símbolos ou relações , que frequentemente sugere várias possíveis interpretações de uma mesma expressão . Alguns métodos de reconhecimento tratam o problema de maneira sequencial , onde um processo de segmentação e classificação de símbolos é seguido de análise estrutural . Um problema principal de tais métodos é que eles determinam interpretações no nível de símbolos sem considerar informação estrutural , a qual é importante para solucionar ambiguidades . Para solucionar esse problema , métodos mais recentes adaptaram técnicas de parsing de strings . Dado que gramáticas de strings foram originalmente projetadas para modelar arranjos lineares de tokens ( como texto , onde símbolos são arranjados de esquerda a direita ) , a estrutura não linear dos símbolos matemáticos ( dada pelos multiples tipos de relações espaciais ) é modelada como uma composição de regras de produção de estruturas lineares . Dessa maneira , o parsing de uma expressão consiste em determinar estruturas lineares na expressão que são consistentes com as estruturas das regras de produção . Esse último passo requer a introdução de restrições , baseadas na definição de uma ordem em relação ao tempo ou espaço , para linearizar os componentes da expresão . Os requerimentos das gramáticas de strings não apenas limitam a efectividade dos métodos , mas também dificultam a extensão dos métodos na inclusão de novas estruturas . Neste trabalho , o problema de reconhecimento de expressões matemáticas é modelado como um problema de parsing de grafos . A representação por meio de grafos nas regras de produção permite uma representação direta das estruturas não lineares das expressões matemáticas . O algoritmo de parsing determina partições dos traços de entrada que induzem grafos isomorfos aos grafos das regras de produção . Para mitigar o custo_computacional  , restringimos as possíveis partições a aquelas derivadas de um conjunto de possíveis símbolos e relações identificados por classificadores previamente treinados . Um conjunto de rótulos que indica interpretações alternativas é associado a cada símbolo e relação ; a decisão da melhor interpretação é realizada pelo parser . O parser construi uma floresta na qual uma árvore representa uma possível interpretação da entrada , e atribui um custo de interpretação para cada árvore , baseado nas relações e símbolos definidas na árvore . O resultado do reconhecimento é dado pela extração de uma árvore com custo mínimo . Resultados experimentais do método proposto mostram um melhor desempenho em comparação com vários métodos descritos na literatura . A pesar do parsing de grafos ser um processo computacionalmente caro , a restrição do espaço de busca proposto reduz a complexidade o suficiente para permitir uma aplicação prática da abordagem . Adicionalmente , dado que a abordagem não pressupõe estruturas particulares das expressões matemática , o método tem potencial para ser adaptado para o reconhecimento de outras estruturas bidimensionais . Uma contribuição secundaria deste trabalho é o desenvolvimento de uma framework para construção automática de bancos de dados de expressões matemáticas manuscritas . A framework tem sido implementada num sistema usado para criar parte das amostras de expressões usadas para avaliação do método de reconhecimento 
 Este trabalho aborda dois problemas de geometria computacional : união de círculos e união de ( vários ) polígonos . Para o problema da união de círculos , os principais algoritmos da literatura são revisados e um algoritmo simples , porém ineficiente , é introduzido . Este algoritmo é então adaptado para resolver o problema da união de polígonos , produzindo um algoritmo que é competitivo com o estado da arte e , dependendo da aplicação , utiliza menos armazenamento 
 Esta dissertação tem por objetivo apresentar uma abordagem baseada em técnicas de inteligência_artificial  para automatizar a etapa de particionamento de modelos em simulação distribuída . Essa abordagem utiliza os conceitos da computação evolutiva para o desenvolvimento de um algoritmo genético capaz de otimizar o processo de particionamento e auxiliar a tomada de decisões na tarefa de obtenção dos processos lógicos . Objetiva-se com sua aplicação minimizar o tempo de execução da simulação distribuída , evitando que o pior tempo de execução seja utilizado . Para alcançar esse objetivo , o particionamento apresentado como solução é caracterizado pelo balanceamento de carga e pela baixa latência de comunicação entre processos . Isso é possível porque o algoritmo genético utiliza informações contidas no modelo e na arquitetura de onde a simulação será executada . Esses padrões são utilizados para obter informações sobre a comunicação entre processos , a carga de processamento por centro de serviço e a capacidade de processamento das 
 Métricas de código-fonte não são novidade , mas ainda não têm sido bem exploradas no desenvolvimento de software . A maioria das ferramentas de métricas mostra valores numéricos isolados , que não são fáceis de entender porque a interpretação deles depende do contexto da implementação . Esta dissertação apresenta o software_livre  Kalibro Metrics , que foi desenvolvido com o objetivo de melhorar a legibilidade de métricas de código-fonte . Kalibro , ao contrário das outras ferramentas , permite que o próprio usuário crie configurações de intervalos associados a avaliações qualitativas , incluindo comentários e recomendações . Usando essas configurações , o Kalibro mostra resultados de métricas de modo amigável , ajudando : arquitetos de software a detectar falhas de projeto ; gerentes de projetos a controlar a qualidade de código-fonte ; usuários de software e pesquisadores a comparar características específicas do código-fonte de vários projetos . Essas configurações podem ser compartilhadas e utilizadas para intermediar discussões voltadas à evolução de critérios de avaliação baseados em métricas de código-fonte 
 Este trabalho apresenta o desenvolvimento de um processador open source baseado no processador Nios_II  da Altera . O processador desenvolvido permite a customização de instruções , a inclusão de componentes que possibilitem um estudo detalhado da memória cache , tal como um monitor de cache , definir o tamanho da cache , dentre outras características . Além_disso  , o processador é baseado na arquitetura do Nios_II  e implementa 90 % do ISA do Nios_II  , o mesmo está integrado aos ambientes Qsys e SOPC Builder da ferramenta Quartus II da Altera , sendo possível utilizar todo o conjunto de IP ( Propriedade Intelectual ) e ferramentas disponíveis pela Altera . Assim , este trabalho tem como propósito colaborar com o desenvolvimento de arquiteturas de hardware com uma unidade de processamento configurável e customizável facilmente pelo usuário , uma vez que o seu código fonte em Bluespec SystemVerilog está aberto a todos os usuários , diferente do Nios_II  da Altera , que tem o código encriptado , inviabilizando fornecer qualquer mudança no processador a nível RTL ( Register Transfer Level ) . Para o desenvolvimento do processador foi utilizada a Linguagem de Descrição de Hardware Bluespec SystemVerilog , pelo fato de ser uma ESL ( Electronic System Level ) que acelera o processo de desenvolvimento de 
 Determinar quais são os aparelhos eletrônicos de uma residência que possuem maior influência na conta de luz não é tarefa trivial . As Redes de Sensores Sem Fios ( RSSF ) auxiliam os usuários nessa tarefa , permitindo descobrir se há algum tipo de desperdício no ambiente monitorado e assim , auxiliá-los a fazer as devidas correções . Por isso , é fundamental usar nas smart grids métodos que detectam novidades , também conhecido como anomalias , de forma individual e autônoma , para os usuários quando algo anômalo surge no consumo de energia dos equipamentos eletrônicos . Tais anomalias podem surgir , por exemplo , quando um equipamento consome energia acima do esperado , o que pode indicar um defeito . Nesse contexto , este trabalho propõe um método inteligente , nomeado como Novelty Detection Power Meter ( NodePM ) , para detectar as novidades no consumo de energia dos equipamentos eletrônicos monitorados por uma smart grid . O NodePM detecta as novidades considerando a entropia de cada equipamento monitorado , a qual é calculada com base em um modelo de cadeia de markov que é gerado através de um algoritmo de aprendizado de máquina . Para tanto , o NodePM é integrado a uma plataforma de monitoramento remoto de consumo de energia , que consiste de uma RSSF associada a uma aplicação em nuvem . Para validar o desempenho do NodePM foram feitos experimentos utilizando a análise de variância e testes paramétricos e não-paramétricos . Os resultados de tais experimentos , obtidos mediante a análise estatística , evidenciou a viabilidade do NodePM na plataforma 
 Simulações de escoamentos multifásicos são de grande interesse em aplicações práticas na indústria , em particular na indústria petrolífera , entre outras . Vários processos dependem do entendimento físico de escoamentos envolvendo iteração com partículas , sedimentação e separação de fluidos . Dos muitos métodos existentes para a simulação dos processos acima descritos , há um crescente_interesse  no aumento de precisão , o que levou ao desenvolvimento de estratégias que utilizam esquemas de elementos finitos discretizados em malhas dinâmicas e adaptativas , usando uma formulação ALE ( do inglês , Arbitrary Lagrangian-Eulerian ) , juntamente com uma representação geométrica da interface . Neste sentido , este trabalho tem o objetivo de estudar e implementar estratégias robustas de controle e adaptação de malhas , em situações onde a malha dinâmica é sujeita a grandes deformações . Uma biblioteca de algoritmos e rotinas foi então desenvolvida para este fim , implementando técnicas de controle e otimização da qualidade dos elementos da malha , técnicas de adaptação da interface entre fluidos com esquemas de conservação de massa , técnicas de mudanças topológicas e preservação de propriedades materiais , além de uma comunicação facilitada destas rotinas com códigos de simulação_numérica  de escoamentos multifásicos 
 Dentre as diversas tecnologias de Assistência Avançada ao Condutor ( ADAS ) que têm sido adicionadas aos automóveis modernos estão os sistemas de detecção de pedestres . Tais sistemas utilizam sensores , como radares , lasers e câmeras de vídeo para captar informações do ambiente e evitar a colisão com pessoas no contexto do trânsito . Câmeras de vídeo têm se apresentado como um ótima opção para esses sistemas , devido ao relativo baixo custo e à riqueza de informações que capturam do ambiente . Muitas técnicas para detecção de pedestres baseadas em visão têm surgido nos últimos_anos  , tendo como característica a necessidade de um grande poder_computacional  para que se possa realizar o processamento das imagens em tempo real , de forma robusta , confiável e com baixa taxa de erros . Além_disso  , é necessário que sistemas que implementem essas técnicas tenham baixo consumo de energia , para que possam funcionar em um ambiente embarcado , como os automóveis . Uma tendência desses sistemas é o processamento de imagens de múltiplas câmeras presentes no veículo , de forma que o sistema consiga perceber potenciais perigos de colisão ao redor do veículo . Neste contexto , este trabalho aborda o coprojeto de hardware e software de uma arquitetura para detecção de pedestres , considerando a presença de quatro câmeras em um veículo ( uma frontal , uma traseira e duas laterais ) . Com este propósito , utiliza-se a flexibilidade dos dispositivos FPGA para a exploração do espaço de projeto e a construção de uma arquitetura que forneça o desempenho necessário , o consumo de energia em níveis adequados e que também permita a adaptação a novos cenários e a evolução das técnicas de detecção de pedestres por meio da programabilidade . O desenvolvimento da arquitetura baseouse em dois algoritmos amplamente_utilizados  para detecção de pedestres , que são o Histogram of Oriented Gradients ( HOG ) e o Integral Channel Features ( ICF ) . Ambos introduzem técnicas que servem como base para os algoritmos de detecção modernos . A arquitetura implementada permitiu a exploração de diferentes_tipos  de paralelismo das aplicações por meio do uso de múltiplos processadores softcore , bem como a aceleração de funções críticas por meio de implementações em hardware . Também foi demonstrada sua viabilidade no atendimento a um sistema contendo quatro câmeras de vídeo 
 A navegação_autônoma  em ambientes externos não estruturados é um dos maiores desafios no campo da robótica . Uma das suas aplicações , os veículos inteligentes autônomos , tem o potencial de diminuir o número de acidentes nas estradas e rodovias , aumentar a eficiência do tráfego nas grandes cidades e contribuir para melhoria da mobilidade de deficientes e idosos . Para que um robô/veículo navegue com segurança , uma detecção precisa de áreas navegáveis é essencial . Neste trabalho , abordamos a tarefa de detecção visual de ruas onde , dada uma imagem , o objetivo é classificar cada um de seus pixels em rua ou não-rua . Ao invés de tentar derivar manualmente uma solução analítica para a tarefa , usamos aprendizado de máquina ( AM ) para aprendê-la a partir de um conjunto de amostras criadas manualmente . Nós utilizamos tanto modelos tradicionais ( superficiais ) quanto modelos profundos para a tarefa . A nossa principal_contribuição  em relação aos modelos tradicionais é uma forma eficiente e versátil de agregar características espacialmente distantes , fornecendo efetivamente um contexto espacial para esses modelos . Quanto aos modelos de aprendizagem profunda , propusemos uma nova arquitetura de rede_neural  focada no tempo de processamento e uma nova camada de rede_neural  , chamada camada semi-global , que fornece eficientemente um contexto global ao modelo . Toda a metodologia_proposta  foi avaliada no benchmark de detecção de ruas do Instituto de Tecnologia de Karlsruhe , alcançando , em todos os casos , resultados competitivos 
 Este trabalho tem como objetivo desenvolver um espelho virtual interativo para simulação de maquiagem com a intenção de melhorar a experiência do usuário durante a experimentação de produtos de maquiagem , tornando essa tarefa mais rápida ( aplicação e remoção da maquiagem simulada ocorre de forma instantânea ) e barata ( sem gasto de produtos ) . A interação com o simulador foi baseada na metáfora de um espelho . A interface do sistema permite a escolha da maquiagem e sua livre aplicação sobre a imagem da própria pessoa . O sistema renderiza em tempo real o vídeo resultante da simulação exibido no espelho sem a necessidade de marcas fiduciais na face ou a criação de um modelo 3D da face antes do uso do sistema . A interface proposta é composta por um monitor sensível a toque e uma câmera RGBD . A câmera RGBD facilita o rastreamento de características faciais do usuário e fornece a imagem a ser utilizada no espelho virtual . A interação ocorre por meio do toque no monitor , permitindo que o usuário escolha a maquiagem e também faça sua aplicação . A simulação da maquiagem considera produtos comuns que modificam propriedades da superfície da pele como cor e textura , sob condições controladas de iluminação . O método permite a livre aplicação de tipos diferentes de maquiagem ( base , batom e sombra ) em qualquer área do rosto de forma independente ( flexibilidade ) , não necessita de equipamentos especiais e pode ser computado em tempo real . Para isso , a imagem é decomposta em camadas de cor e textura que podem ser tratadas independentemente , por meio de filtros e transformações no espaço de cor . Um protótipo em tempo real foi desenvolvido utilizando um Microsoft Kinect , que demonstra a viabilidade do método 
 A textura de uma imagem apresenta informações importantes sobre as características de um objeto . Usar essa informação para reconhecimento de padrões vem_sendo  uma tarefa bastante pesquisada na área de processamento de imagens e aplicado em atividades como indústria têxtil , biologia , análise de imagens médicas , imagens de satélite , análise de peças industriais , entre outros . Muitos pesquisadores focam em criar mecanismos que convertam a imagem em um vetor de características a fim de utilizar um classificador sobre esse vetores . No entanto , as imagens podem ser transformadas para que que características peculiares sejam evidenciadas fazendo com que extratores de características já existentes explorem melhor as imagens . Esse trabalho tem como objetivo estudar a influência da aplicação de métodos de pré-processamento em imagens de textura para a posterior análise das imagens . Os métodos escolhidos são seis : difusão isotrópica , difusão anisotrópica clássica , dois métodos de regularização da difusão anisotrópica , um método de difusão morfológica e a transformada de distância . Além_disso  , os métodos foram aliados a sete descritores já conhecidos da literatura para que as características das imagens tranformadas sejam extraídas . Resultados mostram um aumento significativo no desempenho dos classificadores KNN e Naive Bayes quando utilizados nas imagens transformadas de quatro bases de textura : Brodatz , Outex , Usptex e Vistex 
 A demanda por abordagens eficientes para o problema de reconhecer a estrutura de cada gene numa sequência genômica motivou a implementação de um grande número de programas preditores de genes . Fizemos uma análise dos programas de sucesso com abordagem probabilística e reconhecemos semelhanças na implementação dos mesmos . A maior_parte  desses programas utiliza a cadeia oculta generalizada de Markov ( GHMM - generalized hiddenMarkov model ) como um modelo de gene . Percebemos que muitos preditores têm a arquitetura da GHMM fixada no código-fonte , dificultando a investigação de novas abordagens . Devido a essa dificuldade e pelas semelhanças entre os programas atuais , implementamos o sistema MYOP ( Make Your Own Predictor ) que tem como objetivo fornecer um ambiente flexível o qual permite avaliar rapidamente cada modelo de gene . Mostramos a utilidade da ferramenta através da implementação e avaliação de 96 modelos de genes em que cada modelo é formado por um conjunto de estados e cada estado tem uma distribuição de duração e um outro modelo probabilístico . Verificamos que nem sempre um modelo probabilísticomais sofisticado fornece um preditor melhor , mostrando a relevância das experimentações e a importância de um sistema como o MYOP 
 Com a grande quantidade de informações existentes nas aplicações computacionais hoje em dia , cada vez mais tornam-se necessários mecanismos que facilitem e aumentem o desempenho da recuperação dessas informações . Nesse contexto vem surgindo os bancos de dados chamados de NOSQL , que são bancos de dados tipicamente não relacionais que , em prol da disponibilidade e do desempenho em ambientes com enormes quantidades de dados , abrem mão de requisitos antes vistos como fundamentais . Neste trabalho iremos lidar com esse cenário ao implementar o módulo de consultas distribuídas do JBoss Infinispan , um sistema de cache distribuído que funciona também como um banco de dados NOSQL em memória . Além de apresentar a implementação desse módulo , iremos falar do surgimento do movimento NOSQL , de como se caracterizam esses bancos e de onde o Infinispan se insere nesse movimento 
 A Análise de Conceitos Formais ( FCA ) é uma teoria matemática que formaliza a noção de conceitos e hierarquias conceituais . De importância central a esta teoria é uma estrutura algébrica denominada reticulado de conceitos . Esta estrutura é definida em função de um conjunto de objetos , outro de atributos e uma relação que indica os atributos apresentados por cada objeto . Uma representação gráfica de um reticulado de conceitos , por meio de uma interface computacional , é capaz de expor regularidades presentes em dados a um usuário , e este pode então realizar tarefas de análise exploratória de dados . Este tipo de aplicação de FCA vem_sendo  empregado em dezenas de projetos pertencentes a áreas diversas , como medicina , serviços de inteligência , engenharia de software e bioinformática . Mostramos neste trabalho um sistema de análise exploratória de dados baseado em FCA , e sua utilização sobre dados reais . Também é mostrado como reticulados de conceitos podem ser empregados em interfaces de recuperação de informação . Do ponto de vista algorítmico , analisamos métodos computacionais para a determinação do reticulado de conceitos , e também de uma subestrutura simplificada , o conjunto de conceitos . O tamanho de um reticulado de conceitos pode ser exponencial em função dos tamanhos dos conjuntos de objetos e de atributos . Assim , é de vital interesse o estabelecimento de cotas superiores para o número de conceitos de um reticulado . Neste trabalho , apresentamos as cotas já conhecidas presentes na literatura . Também estabelecemos uma nova cota superior , e mostramos famílias de casos em que nossa cota superior é mais justa que as demais . Para algumas famílias particulares , nossa cota é polinomial , enquanto que as demais são exponenciais 
 A educação enfrenta , nesse início de milênio , profundas mudanças . Existe hoje uma grande variedade de ferramentas de ensino à distância à disposição do educador , especialmente quando este faz_uso  de tecnologias atuais como a Internet e a Web . Frente a este fato , esforços têm sido direcionados para a adequação das técnicas tradicionais de ensino aos novos tempos . O ensino à distância via Internet têm o potencial de por a força da rede mundial a serviço da educação , seja no ensino educação , seja no ensino fundamental , de graduação ou , como no caso deste trabalho , na educação continuada . O objetivo deste trabalho foi , utilizando ferramentas desenvolvidas ao laboratório Intermídia ( ICMC-USP ) , implantar e administrar um curso à distância via Internet , em nível de curso de extensão universitária ( educação continuada ) , coletando e analisando dados sobre seu andamento e sobre sua comunidade alunos . O trabalho foi realizado atravéz de pesquisa bibliográfica , pesquisa em sites especializados na Internet , e também pela própria experimentação através do oferecimento do curso à distância intitulado `` Distributed Programing Using Java '' ( http : //java.icmc.sc.usp.br/java_course ) . Nesse trabalho são discutidas técnicas e ferramentas de software para educação à distância e , principalmente , é descrito o curso propriamente dito : sua natureza , sua implementação , sua descrição e a interação com os usuários . Também é feita uma análise dos dados conseguidos com vistas ao aprimoramento dessa técnica de ensino 
 Atualmente uma grande quantidade de conteúdo é produzida e publicada todos os dias na Internet . São documentos publicados por diferentes pessoas , por diversas organizações e em inúmeros formatos sem qualquer tipo de padronização . Por esse motivo , a informação relevante sobre um mesmo domínio de interesse acaba espalhada pela Web nos diversos portais , o que dificulta uma visão ampla , centralizada e objetiva sobre esta informação . Nesse contexto , a integração dos dados espalhados na rede torna-se um problema de pesquisa relevante , para permitir a realização de consultas mais inteligentes , de modo a obter resultados mais ricos de significado e mais próximos do interesse do usuário . No entanto , tal integração não é trivial , sendo por muitas_vezes  custosa devido à dependência do desenvolvimento de sistemas e mão de obra especializados , visto que são poucos os modelos reaproveitáveis e facilmente integráveis entre si . Assim , a existência de um modelo padronizado para a integração dos dados e para o acesso à informação produzida por essas diferentes entidades reduz o esforço na construção de sistemas específicos . Neste trabalho é proposta uma arquitetura baseada em ontologias para a integração de dados publicados na Internet . O seu uso é ilustrado através de casos de uso reais para a integração da informação na Internet , evidenciando como o uso de ontologias pode trazer resultados mais relevantes 
 O desenvolvimento de aplicações paralelas é uma tarefa complexa . Esse tipo de aplicação possui um grande potencial para comportamento não determinístico . Para a resolução deste problema podem ser utilizadas ferramentas de auxilio à programação paralela . Também são necessárias metodologias de desenvolvimento de software para a obtenção de programas com qualidade e desempenho satisfatórios . Este trabalho apresenta a UMP2D , uma metodologia orientada a objetos para o desenvolvimento de aplicações paralelas . Esta metodologia faz o uso da UML ( Unified Modeling Language ) como linguagem de modelagem e introduz novos processos de desenvolvimento de aplicações paralelas . A UMP2D atende aos principais requisitos de desenvolvimento de programas paralelos tais como o balanceamento eficiente de carga , a minimização da comunicação entre os elementos de processamento e a maximização da eficiência computacional . Neste trabalho também é proposto um ambiente CASE ( Computer Aided Software Engineering ) para o desenvolvimento de aplicações paralelas utilizando a metodologia UMP2D 
 Esta dissertação de mestrado descreve a implementação de um módulo de simulação de redes de computadores para o ASiA ( Ambiente de Simulação Automático ) . Este módulo permite ao usuário a simulação de redes já definidas ( com a possibilidade de alteração dos parâmetros de entrada ) ou definir novas redes através da utilização dos recursos da barra de ferramentas . Para aumentar a gama de sistemas que o usuário pode modelar foram implementados e incluídos na barra de ferramentas novas_funcionalidades  que permitem a modelagem de sistemas mais complexos . Este trabalho apresenta também uma revisão bibliográfica sobre simulação , redes de computadores e ambientes de simulação 
 Aprendizado de Máquina AM é uma área de Inteligência_Artificial  IA que estuda métodos computacionais para adquirir novos conhecimentos bem como meios de organizar o conhecimento já existente . Para isso , são necessárias linguagens de descrição de objetos e de conceitos aprendidos . Elas podem ser divididas em dois tipos : baseadas em atributos , ou proposicionais , e relacionais . Sistemas de AM proposicional têm sido aplicados com relativo sucesso , utilizando dados no formato_atributo-valor  . No entanto , são incapazes de aprender relações em função da linguagem que utilizam . Programação Lógica Indutiva PLI é uma abordagem recente dentro de AM que faz_uso  de uma linguagem de descrição relacional baseada em lógica de primeira_ordem  , de modo que consegue aprender relações entre os objetos . Todo problema que pode ser resolvido por um sistema de aprendizado proposicional pode , em princípio , ser resolvido por um sistema de aprendizado relacional , desde que os fiados estejam devidamente formatados . Uma série de ferramentas foi por nós implementada para converter os dados do formato_atributo-valor  para o formato relacional apropriado de dois sistemas de PLI , FOIL ( Quinlan , 1990 ) e PROGOL ( Muggleton , 1995 ) . A partir dessas representações , tornou-se possível analisar o comportamento de cada um deles em bases de dados naturais com características diferentes . Como estudo de caso do mundo_real  , utiliza-se uma base de dados disponibilizada pelo Programa de Melhoramento Genético da Raça Nelore PMGRN da Universidade de São Paulo em Ribeirão Preto ( Làbo et al. , 1999 ) . Utilizando os sistemas PLI , adquire-se um conhecimento sobre essa base de dados de gado . Para tanto segue-se uma metodologia baseada no processo KDD ( Knowledge Discovery in Databases ) descrito em ( Fayyad , 1996 ) 
 Existem vários_tipos  de benchmarks , cada tipo está relacionado ao tipo de informação que se deseja obter . Os benchmarks de baixo nivel , assim como os sintéticos , medem o desempenho de componentes do sistema , tais como memória , E/S , comunicação , etc . Já os de alto nível medem o desempenho do sistema como um todo , e são mais complexos . O que dificulta a escolha do benchmark mais adequado para o tipo de objeto que se deseja avaliar é a grande quantidade de benchmarks existentes , sendo que muitos nem ao menos possuem uma documentação adequada para que se possa compreendê-los . Para superar essa dificuldade esta dissertação apresenta uma avaliação dos benchmarks quanto a sua área de utilização , quanto aos componentes do sistema que ele pode avaliar com eficácia , facilidade de utilização , etc . A avaliação dos benchmarks se baseia não apenas no estudo teórico , mas também em sua execução em diferentes plataformas de hardware e software . Os resultados obtidos são analisados e classificados de acordo com a área em que ele melhor atua , ou seja , a área em que ele poderá ser aplicado com eficácia 
 Este trabalho foi projetado com o objetivo de auxiliar o Administrador de Web no monitoramento do acesso da rede , através da análise do histórico de acesso ( Logs ) dos servidores_Web  , Proxy e Firewall , das características dos hosts e servidores , e detalhes sobre os usuários que realizaram os acessos . Para suportar a gama de dados a serem armazenados e analisados é proposta a utilização da tecnologia de Data Warehouse para fornecer informações atualizadas e que possam servir como base em relatórios estatísticos e gráficos . Para a construção do Data Warehouse , foram escolhidos o banco de dados relacional Oracle 8 Enterprise e o pacote Business Intelligence Tools , desenvolvidos pela empresa Oracle Corporation 
 O vetor de sufixo é uma estrutura de dados importante utilizada em muitos problemas que envolvem cadeias de caracteres . Na literatura , muitos trabalhos têm sido_propostos  para a construção de vetores de sufixo em memória externa . Entretanto , esses trabalhos não enfocam conjuntos de cadeias , ou seja , não consideram vetores de sufixo generalizados . Essa limitação motiva esta dissertação , a qual avança no estado da arte apresentando o algoritmo eGSA , o primeiro algoritmo proposto para a construção de vetores de sufixo generalizados aumentado com o vetor de prefixo comum mais longo ( LCP ) e com a transformada de Burrows-Wheeler ( BWT ) em memória externa . A dissertação foi desenvolvida dentro do contexto de bioinformática , já que avanços tecnológicos recentes têm aumentado o volume de dados biológicos disponíveis , os quais são armazenados como cadeias de caracteres . O algoritmo eGSA foi validado por meio de testes de desempenho com dados reais envolvendo sequências grandes , como DNA , e sequências pequenas , como proteínas . Com relação aos testes comparativos com conjuntos de grandes cadeias de DNA , o algoritmo proposto foi comparado com o algoritmo correlato mais eficiente na literatura de construção de vetores de sufixo , o qual foi adaptado para construção de vetores generalizados . O algoritmo eGSA obteve um tempo_médio  de 3,2 a 8,3 vezes menor do que o algoritmo correlato e consumiu 50 % menos de memória . Para conjuntos de cadeias pequenas de proteínas , foram realizados testes de desempenho apenas com o eGSA , já que no melhor do nosso conhecimento , não existem trabalhos correlatos que possam ser adaptados . Comparado com o tempo_médio  para conjuntos de cadeias grandes , o eGSA obteve tempos competitivos para conjuntos de cadeias pequenas . Portanto , os resultados dos testes demonstraram que o algoritmo proposto pode ser aplicado eficientemente para indexar tanto conjuntos de cadeias grandes quanto conjuntos de cadeias 
 Sistemas embarcados estão presentes em diversas_áreas  de aplicação em que falhas podem ser críticas . Tais sistemas frequentemente possuem características que tornam a fase de teste particularmente desafiadora , como a produção de grande quantidade de dados e requisitos temporais que precisam ser validados de acordo com a especificação . Existem ferramentas que auxiliam no desenvolvimento de modelos para análise e simulação do comportamento de sistemas embarcados ainda na fase de design . Após ser avaliado , o modelo pode ser usado como base para a implementação . Neste caso , deve-se buscar garantir que um modelo esteja de acordo com a especificação . Do contrário , tal divergência será propagada para a implementação . Portanto , e importante que o modelo seja testado antes da fase de implementação . Simulink e uma ferramenta-padrão de desenvolvimento e simulação de modelos de sistemas embarcados . Sua ampla aplicação na indústria incentivou a criação de alternativas de software livres como XCos . Na literatura , existem pesquisas que visam a aprimorar a atividade de teste de modelos Simulink-like . As soluções propostas geralmente focam em estratégias de seleção de casos de teste . Mas pouco esforço tem sido direcionado ao problema do oráculo , isto e , na dificuldade em avaliar se a execução está de acordo com a especificação . O objetivo desta proposta de doutorado é prover uma abordagem de geração de oráculos de teste para modelos simulink-like que contemple as características previamente resumidas . Especificamente , é proposto um processo , métodos , procedimentos e uma ferramenta que viabilizem a geração parcialmente automatizada de oráculos de teste para modelos Simulink-like . Como contribuição principal , é esperada a melhora da qualidade , custo e tempo do processo de validação de sistemas embarcados suportados por modelagem em Simulink e ferramentas 
 Algoritmos de aprendizado de máquina são amplamente empregados na indução de modelos para descoberta de conhecimento em conjuntos de dados . Como grande parte desses algoritmos assume que os dados são gerados por uma função de distribuição estacionária , um modelo é induzido uma única vez e usado indefinidamente para a predição do rótulo de novos dados . Entretanto , atualmente , diversas aplicações , como gerenciamento de transportes e monitoramento por redes de sensores , geram fluxos contínuos de dados que podem mudar ao longo do tempo . Consequentemente , a eficácia do algoritmo escolhido para esses problemas pode se deteriorar ou outros algoritmos podem se tornar mais apropriados para as características dos novos dados . Nesta tese é proposto um método baseado em meta-aprendizado para gerenciar o processo de aprendizado em ambientes dinâmicos de fluxos contínuos de dados com o objetivo de melhorar o desempenho preditivo do sistema de aprendizado . Esse método , denominado MetaStream , seleciona regularmente o algoritmo mais promissor para os dados que estão chegando , de acordo com as características desses dados e de experiências passadas . O método proposto emprega técnicas de aprendizado de máquina para gerar o meta-conhecimento , que relaciona as características_extraídas  dos dados em diferentes instantes do tempo ao desempenho preditivo dos algoritmos . Entre as medidas usadas para extrair informação relevante dos dados , estão aquelas comumente empregadas em meta-aprendizado convencional com diferentes conjuntos de dados , que são adaptadas para as especificidades do cenário de fluxos , e de áreas correlatas , que consideram , por exemplo , a ordem de chegada dos dados . O MetaStream é avaliado para três conjuntos de dados reais e seis algoritmos de aprendizado diferentes . Os resultados mostram a aplicabilidade do MetaStream e sua capacidade de melhorar o desempenho preditivo geral do sistema de aprendizado em relação a um método de referência para a maioria dos problemas investigados . Deve ser observado que uma combinação de modelos mostrou-se superior ao MetaStream para dois conjuntos de dados . Assim , foram analisados os principais fatores que podem ter influenciado nos resultados observados e são indicadas possíveis melhorias do método 
 Os programas de Ensaios de Prociência ( EP ) são utilizados pela sociedade para avaliar a competência e a confiabilidade de laboratórios na execução de medições específicas . Atualmente , diversos grupos de EP foram estabelecidos pelo INMETRO , entre estes , o grupo de testes de motores . Cada grupo é formado por diversos laboratórios que medem o mesmo artefato e suas medições são comparadas através de métodos estatísticos . O grupo de motores escolheu um motor gasolina 1.0 , gentilmente cedido pela GM Powertrain , como artefato . A potência do artefato foi medida em 10 pontos de rotação por 6 laboratórios . Aqui , motivados por este conjunto de dados , estendemos o modelo de calibração comparativa de Barnett ( 1969 ) para avaliar a compatibilidade dos laboratórios considerando a distribuição t de Student e apresentamos os resultados obtidos das aplicações e simulações a este conjunto de 
 A Sumarização Automática de Textos tem considerável importância nas tarefas de localização e utilização de conteúdo relevante em meio à quantidade enorme de informação disponível atualmente em meio digital . Nessa área , procura-se desenvolver técnicas que possibilitem obter o conteúdo mais relevante de documentos , de maneira condensada , sem alterar seu significado original , e com mínima intervenção humana . O objetivo deste trabalho de mestrado foi investigar de que maneira conceitos desenvolvidos na área de Redes Complexas podem ser aplicados à Sumarização Automática de Textos , mais especificamente à sumarização extrativa . Embora grande parte das pesquisas em sumarização tenha se voltado para a utilização de técnicas extrativas , ainda é possível melhorar o nível de informatividade dos extratos gerados automaticamente . Neste trabalho , textos foram representados como redes , das quais foram extraídas medidas tradicionalmente utilizadas na caracterização de redes complexas ( por exemplo , coeficiente de aglomeração , grau hierárquico e índice de localidade ) , com o intuito de fornecer subsídios à seleção das sentenças mais significativas de um texto . Essas redes são formadas pelas sentenças ( representadas pelos vértices ) de um determinado texto , juntamente com as repetições ( representadas pelas arestas ) de substantivos entre sentenças após lematização . Cada método de sumarização proposto foi aplicado no córpus TeMário , de textos jornalísticos em português , e em córpus das conferências DUC , de textos jornalísticos em inglês . A avaliação desse estudo foi feita por meio da realização de quatro experimentos , fazendo-se uso de métodos de avaliação automática ( Rouge-1 e Precisão/Cobertura de sentenças ) e comparando-se os resultados com os de outros sistemas de sumarização extrativa . Os melhores sumarizadores propostos referem-se aos seguintes conceitos : d-anel , grau , k-núcleo e caminho mínimo . Foram obtidos resultados comparáveis aos dos melhores métodos de sumarização já propostos para o português , enquanto que , para o inglês , os resultados são menos expressivos 
 O problema de dimensionamento de lotes , objeto desse estudo , considera um ambiente composto por múltiplas plantas independentes , múltiplos itens e múltiplos períodos . O ambiente de produção tem capacidade limitada e as plantas podem produzir os mesmos itens . Cada planta tem uma demanda própria e é permitida a transferência de lotes entre as plantas , o que envolve um certo custo . Este problema tem como caso particular o de dimensionamento de lotes com máquinas_paralelas  . O objetivo desta dissertação é propor uma heurística baseada na meta-heurística GRASP ( Greedy Randomized Adaptive Search Procedures ) . Além_disso  , uma estratégia path relinking foi incorporada ao GRASP como uma fase de melhoria do algoritmo . Para verificar a eficiência da heurística proposta , os seus resultados são comparados aos da literatura tanto no caso de máquinas_paralelas  quanto no de múltiplas plantas . Como resultado , o problema de múltiplas plantas obteve melhores_resultados  quando comparado aos da heurística da literatura . Com relação ao problema de máquinas_paralelas  , a heurística proposta se mostrou 
 Modelos de regressão com resposta binária são utilizados na solução de problemas nas mais diversas_áreas  . Neste trabalho enfocamos dois problemas comuns em certos conjuntos de dados e que requerem técnicas apropriadas que forneçam inferências satisfatórias . Primeiro , em certas aplicações uma mesma unidade amostral é utilizada mais de uma vez , acarretando respostas positivamente correlacionadas , responsáveis por uma variância na variável resposta superior ao que comporta a distribuição binomial , fenômeno conhecido como sobredispersão . Por outro_lado  , também encontramos situações em que a variável explicativa contém erros de medição . É sabido que utilizar técnicas que desconsideram esses erros conduz a resultados inadequados ( estimadores viesados e inconsistentes , por exemplo ) . Considerando um modelo com resposta binária , utilizaremos a distribuição beta-binomial para representar a sobredispersão . Os métodos de máxima_verossimilhança  , SIMEX , calibração da regressão e máxima pseudo-verossimilhança foram usados na estimação dos parâmetros do modelo , que são comparados através de um estudo de simulação . O estudo de simulação sugere que os métodos de máxima_verossimilhança  e calibração da regressão são melhores no sentido de correção do viés , especialmente para amostras de tamanho 50 e 100 . Também estudaremos testes de hipóteses assintóticos ( como razão de verossimilhanças , Wald e escore ) a fim de testar hipóteses de interesse . Apresentaremos também um exemplo com dados 
 Diversos problemas envolvem a classificação de dados em categorias , também denominadas classes . A partir de um conjunto de dados cujas classes são conhecidas , algoritmos de Aprendizado de Máquina ( AM ) podem ser utilizados na indução de um classificador capaz de predizer a classe de novos dados do mesmo domínio , realizando assim a discriminação desejada . Dentre as diversas técnicas de AM utilizadas em problemas de classificação , as Máquinas de Vetores de Suporte ( Support Vector Machines - SVMs ) se destacam por sua boa capacidade de generalização . Elas são originalmente concebidas para a solução de problemas com apenas duas classes , também denominados binários . Entretanto , diversos problemas requerem a discriminação dos dados em mais que duas categorias ou classes . Nesta Tese são investigadas e propostas estratégias para a generalização das SVMs para problemas com mais que duas classes , intitulados multiclasses . O foco deste trabalho é em estratégias que decompõem o problema multiclasses original em múltiplos subproblemas binários , cujas saídas são então combinadas na obtenção da classificação final . As estratégias propostas visam investigar a adaptação das decomposições a cada aplicação considerada , a partir de informações do desempenho obtido em sua solução ou extraídas de seus dados . Os algoritmos implementados foram avaliados em conjuntos de dados gerais e em aplicações reais da área de Bioinformática . Os resultados obtidos abrem várias possibilidades de pesquisas futuras . Entre os benefícios verificados tem-se a obtenção de decomposições mais simples , que requerem menos classificadores binários na solução multiclasses 
 Simulação de fluidos tem sido um dos focos principais de pesquisa em computação gráfica nos últimos_anos  . O interesse por tal assunto é motivado pelas aplicações na indústria cinematográfica , jogos e sistemas voltados para simulação de fenômenos físicos realísticos em tempo real . Neste trabalho atacamos um problema ainda pouco explorado pela comunidade de computação gráfica , a simulação de fluidos em imagens_digitais  . Adotamos uma abordagem relacionando fluidos multifásicos , onde propriedades da imagem são incorporadas às equações de Navier-Stokes a fim de permitir que objetos contidos nas imagens `` escoem '' interagindo a forças que agem no 
 A computação ubíqua explora o fato de que é possível embutir em um ambiente sistemas computacionais que transparentemente apóiem tarefas cotidianas do usuário . Sistemas desse tipo podem ser aplicados a diferentes domínios contribuindo , por exemplo , com atividades educacionais , médicas ou administrativas . A captura automática de informação em ambientes de computação ubíqua visa documentar atividades cotidianas de modo a possibilitar , posteriormente , o acesso às informações capturadas . O nível de transparência da interação do usuário com as chamadas aplicações ubíquas de captura e acesso é um desafio para projetistas , uma vez que os requisitos da aplicação devem ser conhecidos em profundidade para que a interação usuário-ambiente seja eficiente , eficaz e satisfatória . O trabalho realizado teve por objetivo especificar uma versão da aplicação para comunicação síncrona entre usuários que participem de reuniões distribuídas usando o DiGaE ( Distributed Gathering Environment ) , desenvolver protótipos de interfaces , avaliá-los e sugerir soluções que facilitem a interação do usuário , aproximando o modelo_conceitual  da aplicação de seu modelo mental . Foram utilizados conceitos de Engenharia de Software para especificação e modelagem do sistema , e de Interação Humano-Computador para o desenvolvimento e a avaliação das interfaces . O projeto realizado orientou a implementação de um protótipo do ambiente DiGaE pela equipe do Projeto TIDIA-Ae , em cujo contexto este trabalho se insere . A principal_contribuição  do trabalho é um projeto que considera usabilidade em ambientes ubíquos para comunicação remota entre usuários , de modo a tornar a interação fácil , eficiente , eficaz e satisfatória até mesmo para usuários não especialistas em 
 Métodos tipo simplex são a base dos principais softwares utilizados na resolução de problemas de otimização_linear  . A implementação computacional direta destes métodos , assim como são descritos na teoria , leva a resultados indesejáveis na resolução de problemas reais de grande porte . Assim , a utilização de técnicas computacionais adequadas é fundamental para uma implementação eficiente e estável . Neste trabalho , as principais técnicas são discutidas , com enfoque naquelas que buscam proporcionar a estabilidade numérica do método : utilização de tolerâncias , estabilização do teste da razão , mudança de escala e representação da matriz básica . Para este último tópico , são apresentadas duas técnicas , a Forma Produto da Inversa e a Decomposição LU . A análise das abordagens é feita baseando-se na resolução dos problemas da biblioteca 
 O uso da computação em uma variedade cada vez maior de aplicações fez com que os Sistemas de Gerenciamento de Bases de Dados ( SGBD ) passassem a ser utilizados para armazenar os mais diversos tipos de dados complexos , como imagens , sons e cadeias de DNA entre outros . Consultas baseadas em relações de ordem total ou igualdade não podem ser aplicadas ou tem aplicações limitadas quando executadas nestes conjuntos de dados . Logo , efetua-se consultas por similaridade baseadas no conteúdo de dados desses tipos . Se tais conjuntos de dados podem ser representados em um espaço métrico , é possível utilizar os Métodos de Acesso Métricos ( MAM ) , como a Slim-Tree , a M-Tree e a DBM-Tree , para otimizar as consultas por similaridade . Porém , os MAM são muito difíceis de compreender e analisar devido à complexidade de suas estruturas . Esta dissertação apresenta um sistema de visualização que permite a inspeção visual da organização e do comportamento de MAM , provendo aos desenvolvedores e administradores de SGBD uma forma rápida e fácil para obter informações essenciais sobre estas estruturas que podem levar a melhorias no desempenho de consultas e outras operações 
 Esta tese propõe um ambiente automático para desenvolvimento de simulação distribuída ASDA ( Ambiente de Simulação Distribuída Automático ) , que tem como objetivo principal facilitar a utilização e desenvolvimento de simulação distribuída . As funcionalidades definidas no ASDA tornam-o diferente de todos os outros ambientes encontrados na literatura . A especificação do ASDA foi realizada através de um diagrama modular composto por sete módulos e também com o auxílio da ferramenta UML ( Unified Modelling Language ) , através da utilização de três de seus diagramas : de casos de uso , de classes e de atividades . O ASDA permite aos usuários a utilização de simulação distribuída através da definição de uma nova simulação ou da replicação de um programa de simulação já desenvolvido . Se a opção for pelo desenvolvimento de um novo programa de simulação , o usuário deve fornecer o modelo e os parâmetros e o ambiente se encarrega de gerar o código do programa de simulação utilizando a abordagem que proporciona o melhor desempenho , levando em consideração as características do modelo e da plataforma . Além da especificação do ASDA , esta tese definiu um protótipo do ambiente com o objetivo de mostrar sua viabilidade de utilização . Neste protótipo , três_módulos  foram implementados , destacando-se o módulo Replicador , que utiliza a abordagem MRIP ( Multiple Replication in Parallel ) . Esta tese contribui também com a definição de algumas diretrizes para a utilização da abordagem MRIP . A base para essa definição foram os resultados obtidos com a utilização do módulo 
 Neste trabalho apresentamos o desenho e desenvolvimento de um novo sistema de apoio ao ensino/aprendizagem via Web , o iComb - Combinatória Interativa na Internet . Também apresentamos alguns experimentos didáticos com a atual proposta do iComb . Este sistema é baseado na experiência do Combien ? , sistema desenvolvido pela Université Pierre et Marie Curie - LIP6 , que tem bons recursos para auxílio à resolução de exercícios ligados a análise combinatória , mas apresenta limitações quanto a incorporação de novos exercícios e principalmente apresenta restrições para seu uso via Web . Uma inovação advinda deste projeto é que o iComb pode ser integrado a sistemas gerenciadores de cursos , como o SAW- Sistema de Aprendizagem pelaWeb . Além_disso  , o iComb pode ser incorporado a páginas pessoais , redes_sociais  e blogs , na forma de um widget 
 Uma família de esquemas upwind denominada FUS-RF ( Family of Upwind_Scheme  via Rational Functions ) , que é derivada via funções racionais e dependentes de parâmetros , é proposta para o cálculo de soluções aproximadas de equações de conservação . A fim de ilustrar a capacidade dos novos esquemas , vários resultados computacionais para sistemas hiperbólicos de leis de conservação são apresentados . Esses testes mostram a inflluência dos parâmetros escolidos sobre a qualidade dos resultados numéricos . Fazendo o uso de alguns testes de padrões , comparação dos novos limitadores de fluxo correspondentes com o esquema bem estabelecido van Albada e esquema atual EPUS ( Eight-degree Polynomial Upwind_Scheme  ) é também realizada . Os testes numéricos realizados em transporte de escalares e problemas de dinâmica dos gases confirmam que alguns esquemas da família FUS-RF são não oscilatórios e fornecem resultados confiáveis quando perfis descontínuos são transportados . Um esquema particular dessa nova família de esquemas upwind é então selecionado e utilizado para resolver escoamentos complexos com superfícies_livres  
 A anotação de papéis semânticos ( APS ) é uma subárea do Processamento de Línguas_Naturais  ( PLN ) que começou a ser explorada para a língua inglesa a partir de 2002 . Seu objetivo é detectar estruturas de predicador e argumentos em sentenças escritas , que correspondem a descrições de eventos ( normalmente feitas por verbos ) ; seus participantes , como agente e paciente ; e circunstâncias , como tempo , local , etc . Diversas aplicações de PLN , como tradução automática e recuperação de informação , têm obtido melhorias em seu desempenho ao empregar a APS como uma etapa de pré-processamento . Para a língua_portuguesa  , os avanços na pesquisa de APS são ainda muito incipientes . Dado que a grande maioria dos trabalhos encontrados na literatura desta área emprega aprendizado de máquina supervisionado , um fator limitante tem sido a ausência de dados rotulados em português , problema que apenas recentemente foi parcialmente resolvido com a criação do PropBank-Br . Este recurso segue o modelo de anotação usado no Prop- Bank , o principal conjunto de dados rotulados empregado na tarefa de APS para a língua inglesa . Ainda assim , o PropBank-Br contém menos de um décimo do total de instâncias de dados presentes no PropBank original . Outro ponto a ser observado é que a abordagem mais comum para a APS baseia-se na extração de uma grande quantidade de informação linguística das sentenças de entrada para ser usada por classificadores automáticos . Tal abordagem mostra-se extremamente dependente de outras ferramentas de PLN , característica particularmente indesejável no caso da língua_portuguesa  , que não possui muitos recursos livremente disponíveis . Em contrapartida , uma outra abordagem bem sucedida encontrada na literatura abre mão do uso de conhecimento linguístico explícito e associa palavras a sequências numéricas , cujos valores são ajustados durante o treinamento de uma rede_neural  artificial . Estas sequências são então empregadas pela rede para realizar a APS , e podem servir também para outras tarefas de PLN . O presente_trabalho  seguiu o segundo método descrito acima . Foram implementadas alterações nesse método que permitiram um ganho de desempenho em comparação com sua versão original quando testada no PropBank-Br . A versão final do sistema desenvolvido está pronta para uso e poderá auxiliar pesquisas de PLN em 
 A área de detecção de outliers ( ou detecção de anomalias ) possui um papel fundamental na descoberta de padrões em dados que podem ser considerados excepcionais sob alguma perspectiva . Uma importante distinção se dá entre as técnicas supervisionadas e não supervisionadas . O presente_trabalho  enfoca as técnicas de detecção não supervisionadas . Existem dezenas de algoritmos desta categoria na literatura , porém cada um deles utiliza uma intuição própria do que deve ser considerado um outlier ou não , que é naturalmente um conceito subjetivo . Isso dificulta sensivelmente a escolha de um algoritmo em particular e também a escolha de uma configuração adequada para o algoritmo escolhido em uma dada aplicação prática . Isso também torna altamente complexo avaliar a qualidade da solução obtida por um algoritmo/configuração em particular adotados pelo analista , especialmente em função da problemática de se definir uma medida de qualidade que não seja vinculada ao próprio critério utilizado pelo algoritmo . Tais questões estão inter-relacionadas e se referem respectivamente aos problemas de seleção de modelos e avaliação ( ou validação ) de resultados em aprendizado de máquina não supervisionado . Neste trabalho foi desenvolvido um índice pioneiro para avaliação não supervisionada de detecção de outliers . O índice , chamado IREOS ( Internal , Relative Evaluation of Outlier Solutions ) , avalia e compara diferentes soluções ( top-n , i.e. , rotulações binárias ) candidatas baseando-se apenas nas informações dos dados e nas próprias soluções a serem avaliadas . O índice também é ajustado estatisticamente para aleatoriedade e extensivamente avaliado em vários experimentos envolvendo diferentes coleções de bases de dados sintéticas e reais 
 Aplicações como comércio_eletrônico  , redes de computadores , redes_sociais  e biologia ( interação proteica ) , entre outras , levaram a produção de dados que podem ser representados como grafos à escala planetária { podendo possuir milhões de nós e bilhões de arestas . Tais aplicações apresentam problemas desafiadores quando a tarefa consiste em usar as informações contidas nos grafos para auxiliar processos de tomada de decisão através da descoberta de padrões não triviais e potencialmente utéis . Para processar esses grafos em busca de padrões , tanto pesquisadores como a indústria tem usado recursos de processamento distribuído organizado em clusters computacionais . Entretanto , a construção e manutenção desses clusters pode ser complexa , trazendo tanto problemas técnicos como financeiros que podem ser proibitivos em diversos casos . Por isso , torna-se desejável a capacidade de se processar grafos em larga_escala  usando somente um nó computacional . Para isso , foram desenvolvidos processos e algoritmos seguindo três abordagens diferentes , visando a definição de um arcabouço de análise capaz de revelar padrões , compreensão e auxiliar na tomada de decisão sobre grafos em escala planetária 
 O desempenho global dos sistemas computacionais é limitado , geralmente , pelo componente de menor desempenho . Os processadores e a memória principal têm experimentado um aumento de desempenho bem maior que o da memória secundária , como os discos magnéticos . Em 1984 , Johnson introduziu o conceito de fragmentação , onde um dado é gravado em uma matriz de discos , de forma que os seus fragmentos podem ser recuperados em paralelo e , por conseqüência , de forma mais rápida . O principal problema da fragmentação é a redução da confiabilidade da matriz pois , a falha de um dos discos torna o dado inacessível . Patterson , Gibson e Katz propuseram , em 1988 , 5 formas de armazenar informação redundante na matriz de discos e , dessa forma , aumentar sua confiabilidade . A essas formas foi dado o nome de RAID - Redundant Arrays of Independent Disks . Com o passar do tempo , outras formas de armazenamento de redundância foram criadas , tornando complexa a taxonomia da área . Além_disso  , alterações de parâmetros na matriz implicam em variações de desempenho nem sempre fáceis de se perceber em um primeiro momento . Com o objetivo de facilitar a compreensão da taxonomia e permitir que sejam feitos experimentos na matriz buscando um melhor desempenho , esta dissertação_propõe  um ambiente de simulação e aprendizado para RAID , onde o usuário pode interagir com diversos modelos de RAID , ou até criar o seu próprio , para avaliar seu desempenho em várias situações , além de oferecer ao usuário acesso ao conhecimento da área , agindo como um tutor . Esta dissertação apresenta , ainda , um protótipo de um simulador de discos magnéticos que pode ser utilizado como base para o desenvolvimento de um simulador de RAID para ser utilizado pelo ambiente 
 Este trabalho tem como objetivo analisar a influência 
 pelas políticas de substituição de objetos em caches na Web . 
 é feito por meio da investigação das políticas existentes 
 literatura , considerando um estudo de caracterização de carga , 
 avaliação de desempenho e de comparação do uso dessas políticas 
 Para realizar a avaliação das políticas é utilizado um 
 de caches para a Web 

 Durante a pesquisa , foi desenvolvida uma nova política , 
 MeMoExP . Ela utiliza os conceitos de Média Móvel para otimizar 
 e BHR . As simulações realizadas mostraram que a MeMoExP segue 
 mesma tendência da política FBR , tida como eficiente 
 literatura 

 Finalmente , são expostas algumas ponderações sobre as 
 apresentadas nos capítulos componentes desta dissertação , além 
 serem apresentadas as contribuições provenientes desta pesquisa 
 alguns trabalhos futuros propostos a partir desta dissertação 
 Neste trabalho propomos a distribuição Weibull-Binomial-Negativa ( WBN ) considerando uma estrutura de ativação latente para explicar a ocorrência do evento de interesse , em que o número de causas competitivas é modelado pela distribuição Binomial Negativa , e os tempos não observados devido às causas seguem a distribuição Weibull . Em geral , as causas competitivas podem ter diferentes mecanismos de ativação , sendo assim os casos de primeira ativação , última ativação e ativação aleatória foram considerados no estudo . Desse modo o modelo proposto inclui uma ampla distribuição , tais como Weibull-Geométrico ( WG ) e Exponencial-Poisson Complementar ( EPC ) , introduzidas por Barreto-Souza et_al  . ( 2011 ) e G. et_al  . ( 2011 ) , respectivamente . Baseando-nos na mesma estrutura , consideramos o modelo de regressão locação-escala baseado na distribuição proposta ( WBN ) e o modelo para dados de sobrevivência com fração de cura . Os principais objetivos deste trabalho é estudar as propriedades matemáticas dos modelos propostos e desenvolver procedimentos de inferências desde uma perspectiva clássica e Bayesiana . Além_disso  , as medidas de diagnóstico Bayesiana baseadas na 'psi'-divergência ( Peng & Dey , 1995 ; Weiss , 1996 ) , que inclui como caso particular a medida de divergência Kullback-Leibler ( K-L ) , foram consideradas para detectar observações 
 A ideia de se permitir que diversos músicos possam interagir remotamente utilizando-se de redes de computadores não é nova , mas nunca se popularizou . O que antes era limitado tecnologicamente devido às condições disponibilizadas por conexões de internet pela linha telefônica , hoje apresenta-se muito mais viável tecnicamente com a massicação da internet de banda larga , seja por cabo ou por tecnologias DSL . Apesar disso , encontrar um ambiente virtual em que a performance musical pareça completamente natural para o músico é uma das maiores diculdades nesta área . Embora a latência obtida na conexão entre computadores esteja vinculada principalmente à distância entre os usuários e à qualidade do serviço oferecido pelos provedores de internet , outros aspectos , como a largura de banda , podem ser controlados pelo software . Para lidar com as limitações de largura de banda no acesso à internet disponíveis comercialmente , um método de compactação de áudio eciente e que alie baixa latência algorítmica com alta qualidade sonora é imprescindível . Para evitar que atrasos muito grandes na transmissão do áudio impossibilitem uma interação musical entre os participantes , uma técnica em que a própria resposta sonora do instrumento seja atrasada localmente torna-se uma alternativa válida na busca de um ambiente para performances musicais via rede . Esta dissertação busca apresentar uma alternativa exível e de fácil utilização para que a realização de performances musicais distribuídas por usuários caseiros seja possível , mesmo que estes não tenham acesso a redes dedicadas para tal nalidade 
 Os Sistemas de Apoio à Decisão ( SAD ) organizam e processam dados e informações para gerar resultados que apoiem a tomada de decisão em um domínio especifico . Eles integram conhecimento de especialistas de domínio em cada um de seus componentes : modelos , dados , operações matemáticas ( que processam os dados ) e resultado de análises . Nas metodologias de desenvolvimento tradicionais , esse conhecimento deve ser interpretado e usado por desenvolvedores de software para implementar os SADs . Isso porque especialistas de domínio não conseguem formalizar esse conhecimento em um modelo computável que possa ser integrado aos SADs . O processo de modelagem de conhecimento é realizado , na prática , pelos desenvolvedores , parcializando o conhecimento do domínio e dificultando o desenvolvimento ágil dos SADs ( já que os especialistas não modificam o código diretamente ) . Para solucionar esse problema , propõe-se um método e ferramenta web que usa ontologias , na Web Ontology Language ( OWL ) , para representar o conhecimento de especialistas , e uma Domain Specific Language ( DSL ) , para modelar o comportamento dos SADs . Ontologias , em OWL , são uma representação de conhecimento computável , que permite definir SADs em um formato entendível e accessível a humanos e máquinas . Esse método foi usado para criar o Framework Decisioner para a instanciação de SADs . O Decisioner gera automaticamente SADs a partir de uma ontologia e uma descrição naDSL , incluindo a interface do SAD ( usando uma biblioteca de Web Components ) . Um editor online de ontologias , que usa um formato simplificado , permite que especialistas de domínio possam modificar aspectos da ontologia e imediatamente ver as consequência de suasmudanças no SAD.Uma validação desse método foi realizada , por meio da instanciação do SAD SustenAgro no Framework Decisioner . O SAD SustenAgro avalia a sustentabilidade de sistemas produtivos de cana-de-açúcar na região centro-sul do Brasil . Avaliações , conduzidas por especialistas em sustentabilidade da Embrapa Meio ambiente ( parceiros neste projeto ) , mostraram que especialistas são capazes de alterar a ontologia e DSL usadas , sem a ajuda de programadores , e que o sistema produz análises de sustentabilidade corretas 
 Métodos de representação de superfícies a partir de pontos não-organizados se mantêm como uma das principais vertentes científicas que aquecem o estado-da-arte em Computação_Gráfica  e , significativamente , estão sendo reconhecidos como uma ferramenta interessante para definição de interfaces móveis no contexto de simulações numéricas de escoamento de fluidos . Não é difícil encontrar motivos para tais fatos : pelo lado da computação gráfica , por exemplo , a manipulação de conjuntos de pontos massivos com geometrias complexas e sujeitos a informações ruidosas ainda abre margem para novas metodologias . Já no âmbito da mecânica dos fluidos , onde os dados não são originados de \emph tridimensionais , mas sim de interfaces entre fluidos imiscíveis , mecanismos de representação de superfícies a partir de pontos não-organizados podem apresentar características computacionais e propriedades geométricas que os tornem atrativos para aplicações em simulação de fenômenos físicos . O objetivo principal dessa tese de doutorado foi , portanto , o desenvolvimento de técnicas de representação de superfícies a partir de pontos não-organizados , que sejam capazes de suprir restrições de importantes trabalhos prévios . Nesse sentido , primeiramente focalizamos a elaboração de técnicas baseadas em formulações de mínimos-quadrados-móveis e de uma técnica robusta de partição da unidade implícita adaptativa em duas vias . Além de mecanismos de representação de superfícies a partir de pontos não-organizados , também propusemos um método promissor para representação de interfaces em simulação_numérica  de escoamento de fluidos multifásicos . Para isso , embasamo-nos numa abordagem Lagrangeana ( livre-de-malhas ) , fundamentada no método dos mínimos-quadrados-móveis algébricos e apresentamos diversos resultados numéricos , estudos de convergências e comparações que evidenciam o potencial dessa metodologia para simulações numéricas de fenômenos físicos . Apesar de a contribuição principal deste trabalho ser o desenvolvimento de métodos para representação de superfícies a partir de pontos não-organizados , a experiência que adquirimos no desenvolvimento dessas técnicas nos conduziu à elaboração de mecanismos para representação de dados volumétricos não-organizados . Por conta disso , apresentamos dois mecanismos de representação a partir de dados volumétricos não-organizados com o intuito de serem aplicáveis a informações oriundas de malhas contendo células arbitrárias , isto é , propusemos a definição de um método de rendering 
 Esta dissertação aborda o escalonamento de processos em sistemas de clusters de computadores , tanto em plataformas homogêneas quanto heterogêneas . As heterogeneidades abordadas incluem a potência computacional dos processadores , quantidade de memória principal do sistema e o tempo_médio  de acesso ao disco . Neste trabalho são propostas quatro novas políticas destinadas a realizar o compartilhamento de carga nesses ambientes , considerando cargas de trabalho com aplicações variando entre CPU-bound e memoryintensive . Dentre as quatro políticas , uma utiliza apenas índices de CPU , enquanto as demais utilizam também índices de memória . Os resultados foram obtidos através de simulações baseadas em trace e mostram reduções significativas das perdas de desempenho observadas nos resultados obtidos com as políticas de escalonamento propostas . Como referências foram utilizadas políticas de escalonamento tradicionais encontradas na 
 Uma adaptação de um processo de software específico para linhas de produtos de software é proposta . Esse processo tem o objetivo de ser ágil , de apoiar as atividades de projetar e desenvolver características com re-trabalho mínimo e de facilitar a engenharia de aplicações . A fase de engenharia de domínio é iterativa e incremental e propõe uma arquitetura baseada em componentes . As aplicações são geradas por um gerador de aplicações configurável a partir de uma linguagem de modelagem de aplicações baseada no modelo de características . Adicionalmente , é apresentado um estudo detalhado de alternativas para projeto de componentes em uma linha de produtos , considerando componentes do tipo caixa-preta e caixa-branca visando a facilitar a composição e o reúso de componentes . Uma linha de produtos para controle de Bilhetes Eletrônicos em Transporte urbano ( BET ) foi projetada e implementada usando o processo proposto . Alternativas para a implementação baseada em aspectos de requisitos transversais e de variabilidades da linha de produtos BET , bem como sua geração_automática  , são apresentadas e 
 A triangulação de Delaunay de um conjunto de pontos é uma importante entidade geométrica cujas aplicações abrangem diversas_áreas  da ciência . Triangulações regulares , que podem ser vistas como uma generalização da triangulação de Delaunay , onde pesos são associados aos vértices , também têm sido aplicadas em diversos problemas como reconstrução a partir de nuvens de pontos [ 5 ] , geração de malha [ 12 ] , modelagem molecular [ 7 ] e muitos outros . Apesar de ser muito utilizada , a fundamentação teórica referente à triangulação regular ainda não está tão desenvolvida quanto para triangulação de Delaunay . Por exemplo , pouco se sabe a respeito da dinâmica de uma triangulação regular [ 22 ] quando os pesos associados aos vértices mudam . Este trabalho tem como objetivo principal desenvolver um arcabouço teórico e computacional que permita representar uma triangulação qualquer como uma triangulação regular . Para isso , um estudo da dinâmica das operações de flip frente à variação de pesos nos vértices deve ser realizado . Este estudo tem como base o mapeamento da triangulação em um politopo que define os possíveis pesos para os vértices . Tal politopo pode ser obtido por meio de um sistema de inequações que gera um problema de programação linear cuja solução fornece os pesos adequados . A transformação de uma triangulação qualquer em triangulação regular permite o desenvolvimento de novas técnicas de morphing entre malhas e algoritmos para modelar níveis de detalhe , sendo este mais um objetivo deste 
 Características de texturas atuam como bons descritores de imagens e podem ser empregadas em diversos problemas , como classificação e segmentação . Porém , quando o número de características é muito elevado , o reconhecimento de padrões pode ser prejudicado . A seleção de características contribui para a solução desse problema , podendo ser empregada tanto para redução da dimensionalidade como também para descobrir quais as melhores características de texturas para o tipo de imagem analisada . O objetivo deste trabalho é avaliar métodos ótimos e subótimos de seleção de características em problemas que envolvem texturas de imagens . Os algoritmos de seleção avaliados foram o branch and bound , a busca exaustiva e o sequential oating forward selection ( SFFS ) . As funções critério empregadas na seleção foram a distância de Jeffries-Matusita e a taxa de acerto do classificador de distância mínima ( CDM ) . As características de texturas empregadas nos experimentos foram obtidas com estatísticas de primeira_ordem  , matrizes de co-ocorrência e filtros de Gabor . Os experimentos_realizados  foram a classificação de regiôes de uma foto aérea de plantação de eucalipto , a segmentação não-supervisionada de mosaicos de texturas de Brodatz e a segmentação supervisionada de imagens médicas ( MRI do cérebro ) . O branch and bound é um algoritmo ótimo e mais efiiente do que a busca exaustiva na maioria dos casos . Porém , continua sendo um algoritmo lento . Este trabalho apresenta uma nova estratégia para o branch and bound , nomeada floresta , que melhorou significativamente a eficiência do algoritmo . A avaliação dos métodos de seleção de características mostrou que os melhores subconjuntos foram aqueles obtidos com o uso da taxa de acerto do CDM . A busca exaustiva e o branch and bound , mesmo com a estratégia floresta , foram considerados inviáveis devido ao alto tempo de processamento nos casos em que o número de característica é muito grande . O SFFS apresentou os melhores_resultados  , pois , além de mais rápido , encontrou as soluções_ótimas  ou próximas das ótimas . Pôde-se concluir também que a precisão no reconhecimento de padrões aumenta com a redução do número de características e que os melhores subconjuntos freqüentemente são formados por características de texturas obtidas com técnicas 
 Em NGN ( Next Generation Networks ) , os usuários podem se conectar em diferentes tecnologias de rede e desejam , além de uma comunicação transparente , novos serviços personalizados . Neste sentido , este trabalho explora informações de contexto em NGN . O principal objetivo é capturar as informações de contexto envolvidas no momento da utilização da rede sem fio , na escolha do novo ponto de acesso e no procedimento do handover . Este contexto capturado é aplicado para serviços cientes de contexto . A proposta é validada por meio de dois cenários , o primeiro é um mashup que exibe as redes sem fio disponíveis de um determinado local e o segundo um protótipo gerenciador de conexões para atender as preferências do usuário . Adicionalmente , são realizadas duas avaliações do impacto do uso de contexto em handovers . Para tanto , foi implantado um testbed NGN com o protocolo Mobile 
 Os robôs_móveis  e de serviço vêm assumindo um papel cada vez mais amplo e importante junto à sociedade moderna . Um tipo importante de robô_móvel  autônomo são os robôs voltados para a vigilância e segurança em ambientes internos ( indoor ) . Estes robôs_móveis  de vigilância permitem a execução de tarefas repetitivas de monitoramento de ambientes , as quais podem inclusive apresentar riscos à integridade física das pessoas , podendo assim ser executadas de modo autônomo e seguro pelo robô . Este trabalho teve por objetivo o desenvolvimento dos principais módulos que compõem a arquitetura de um sistema robótico de vigilância , que incluem notadamente : ( i ) a aplicação de sensores com percepção 3D ( Kinect ) e térmica ( Câmera FLIR ) , de relativo baixo custo , junto a este sistema robótico ; ( ii ) a detecção de intrusos ( pessoas ) através do uso conjunto dos sensores 3D e térmico ; ( iii ) a navegação de robôs_móveis  autônomos com detecção e desvio de obstáculos , para a execução de tarefas de monitoramento e vigilância de ambientes internos ; ( iv ) a identificação e reconhecimento de elementos do ambiente que permitem ao robô realizar uma navegação baseada em mapas topológicos . Foram utilizados métodos de visão_computacional  , processamento de imagens e inteligência computacional para a realização das tarefas de vigilância . O sensor de distância Kinect foi utilizado na percepção do sistema robótico , permitindo a navegação , desvio de obstáculos , e a identificação da posição do robô em relação a um mapa topológico utilizado . Para a tarefa de detecção de pessoas no ambiente foram utilizados os sensores Kinect e câmera térmica FLIR , integrando os dados fornecidos por ambos sensores , e assim , permitindo obter uma melhor percepção do ambiente e também permitindo uma maior confiabilidade na detecção de pessoas . Como principal resultado deste trabalho foi desenvolvido um iii sistema , capaz de navegar com o uso de um mapa topológico global , capaz de se deslocar em um ambiente interno evitando colisões , e capaz de detectar a presença de seres_humanos  ( intrusos ) no ambiente . O sistema proposto foi testado em situações reais com o uso de um robô_móvel  Pioneer P3AT equipado com os sensores Kinect e com uma Câmera FLIR , realizando as tarefas de navegação definidas com sucesso . Outras funcionalidades foram implementadas , como o acompanhamento da pessoa ( follow me ) e o reconhecimento de comandos gestuais , onde a integração destes módulos com o sistema desenvolvido constituem-se de trabalhos futuros 
 Com a popularidade e o crescimento das Redes Sociais Online , o interesse pelo entendimento de como seus usuários interagem entre si também tem crescido , configurando assim um cenário rico no qual são criadas oportunidades para melhorar o design de interfaces , compreender a organização de movimentos sociais , o marketing viral e a distribuição de conteúdos multimídia , dentre outros . Na investigação dessas oportunidades de pesquisa , um modelo de representação da atividade dos usuários amplamente empregado é baseado na construção de um grafo . Embora as atividades dos usuários em uma Rede Social Online sejam variadas , assim como as possibilidades de interação entre usuários , a construção de um grafo normalmente considera uma interação específica , que é então analisada a partir da interpretação de medidas baseadas em grafos ou de medidas estatísticas . Técnicas de mineração de dados podem ser empregadas de forma alternativa e complementar ao modelo baseado em grafos , possibilitando a extração e a avaliação de padrões das atividades de usuários . Entretanto , tanto na análise baseada em grafos quanto na utilização de técnicas de mineração de dados , a literatura reporta trabalhos em que são implicitamente representados elementos associados com a atividade dos usuários como ações executadas , mídias compartilhadas , aplicações e tipos de dispositivos utilizados . Nesse cenário , existe uma demanda por um modelo descritivo que permita a representação explícita dos elementos associados com as atividades dos usuários representação essa que possa ser utilizada na extração e na avaliação das interações entre usuários . Para atender essa demanda , nesta tese é apresentada uma técnica e um método para representar , extrair e avaliar interações entre usuários de Redes Sociais Online . São também reportados resultados de experimentos da aplicação da técnica e do método a partir de dados obtidos de uma Rede Social 
 A extração de Interações Proteína-Proteína ( IPPs ) a partir de texto é um problema relevante na área biomédica e um desafio na área de aprendizado de máquina . Na área biomédica , as IPPs são fundamentais para compreender o funcionamento dos seres vivos . No entanto , o número de artigos relacionados com IPPs está aumentando rapidamente , sendo impraticável identicá-las e catalogá-las manualmente . Por exemplo , no caso das IPPs humanas apenas 10 % foram catalogadas . Por outro_lado  , em aprendizado de máquina , métodos baseados em kernels são frequentemente empregados para extrair automaticamente IPPs , atingindo resultados considerados estado da arte . Esses métodos usam informações léxicas , sintáticas ou semânticas como características . Entretanto , os resultados ainda são insuficientes , atingindo uma taxa relativamente baixa , em termos da medida_F  , devido à complexidade do problema . Apesar dos esforços em produzir kernels , cada vez mais sofisticados , usando árvores sintáticas como árvores constituintes ou de dependência , pouco é conhecido sobre o desempenho de outras abordagens de aprendizado de máquina como , por exemplo , as redes bayesianas . As àrvores constituintes são estruturas de grafos que contêm informação importante da gramática subjacente as sentenças de textos contendo IPPs . Por outro_lado  , a rede bayesiana permite modelar algumas regras da gramática e atribuir para elas uma distribuição de probabilidade de acordo com as sentenças de treinamento . Neste trabalho de mestrado propõe-se um método para construção automática de redes bayesianas a partir de árvores contituintes para extração de IPPs . O método foi testado em cinco corpora padrões da extração de IPPs , atingindo resultados competitivos , em alguns_casos  melhores , em comparação a métodos do estado da 
 Estudamos o problema de famílias intersectantes extremais em um subconjunto aleatório da família dos subconjuntos com exatamente k elementos de um conjunto dado . Obtivemos uma descrição quase completa da evolução do tamanho de tais famílias . Versões semelhantes do problema foram estudadas por Balogh , Bohman e Mubayi em 2009 , e por Hamm e Kahn , e Balogh , Das , Delcourt , Liu e Sharifzadeh de maneira concorrente a este trabalho 
 No presente documento são reunidas as colaborações de inúmeros trabalhos das áreas 
 Bancos de Dados , Descoberta de Conhecimento em Bases de Dados , Mineração de Dados 
 e Visualização de Informações Auxiliada por Computador que , juntos , estruturam o tema 
 pesquisa e trabalho da dissertação de Mestrado : a Visualização de Informações . A 
 relevante é revista e relacionada para dar suporte às atividades conclusivas teóricas e 
 relatadas no trabalho 
 O referido trabalho , embasado pela substância teórica pesquisada , faz 
 contribuições à ciência em voga , a Visualização de Informações , apresentando-as através 
 propostas formalizadas no decorrer deste texto e através de resultados práticos na forma 
 softwares habilitados à exploração visual de informações . As idéias apresentadas se 
 na exibição visual de análises numéricas estatísticas básicas , frequenciais ( Frequency Plot ) 
 e de relevância ( Relevance Plot ) . São relatadas também as contribuições à 
 FastMapDB do Grupo de Bases de Dados e Imagens do ICMC-USP em conjunto com 
 resultados de sua utilização . Ainda , é apresentado o Arcabouço , previsto no projeto original 
 para construção de ferramentas visuais de análise , sua arquitetura , características e utilização 
 Por fim , é descrito o Pipeline de visualização decorrente da junção entre o Arcabouço 
 visualização e a ferramenta FastMapDB 
 O trabalho se encerra com uma breve análise da ciência de Visualização 
 Informações com base na literatura estudada , sendo traçado um cenário do estado da 
 desta disciplina com sugestões de futuros trabalhos 
 Nos últimos_anos  , a área de educação vem passando por mudanças importantes em relação à introdução das tecnologias da informação no processo de ensino/aprendizagem . Como resultado dessas mudanças e dos esforços para aproveitar os benefícios dessas tecnologias , diversos sistemas ( ou ambientes virtuais ) de apoio à educação foram desenvolvidos . Apesar do vasto conjunto de ferramentas oferecidas por esses ambientes , muitos usuários ainda optam pelo desenvolvimento de suas próprias aplicações de apoio à educação . Isso acontece em função da diversidade das técnicas de ensino e avaliação existentes entre diferentes professores . Devido ao fato dos ambientes educacionais tradicionais não serem projetados para oferecer muitas possibilidades de adaptação e extensão , principalmente em termos de funcionalidades , os usuários são desmotivados a utilizar esses sistemas e acabam criando suas próprias aplicações . Como uma alternativa para essa falta de flexibilidade , nesta tese , um framework modular baseado em componentes ( WebMODE - Web MOdular DEvelopment ) foi proposto para o desenvolvimento de aplicações Web mais adaptáveis e extensíveis ( em especial as aplicações de apoio à educação ) . Além_disso  , também foi proposto um processo de software para a instanciação de aplicações sobre esse framework . Tal processo enfatiza as atividades de especificação dos módulos , que fazem parte dessas aplicações , de forma independente da tecnologia que será utilizada na sua implementação . Os pontos principais deste framework são o uso de padrões de projeto , como o MVC ( Model View Controller ) e o uso de sistemas modulares e baseados em componentes no desenvolvimento de aplicações Web que são compostas de módulos com funcionalidades configuráveis . O WebMODE foi projetado como um framework conceitual e , posteriormente , implementado e avaliado utilizando a tecnologia J2EE ( Java 2 Enterprise Edition 
 Esta tese analisa o método de colocação polinomial , para uma classe de equações integro-diferenciais singulares em espaços ponderados de funções contínuas e condições de fronteira não nulas . A convergência do método_numérico  em espaços com norma uniforme ponderada , é demonstrada , e taxas de convergências são determinadas , usando a suavidade dos dados das funções envolvidas no problema . Exemplos numéricos confirmam as 
 A UML é uma notação gráfica utilizada na modelagem de sistemas orientados a objetos , em diferentes domínios da computação . Por ser simples de utilizar , em relação a outras formas de modelagem , a UML é amplamente difundida entre os desenvolvedores de software , tanto na academia quanto na indústria . Entre as suas vantagens , encontram-se : ( i ) a representação visual das relações entre classes e entidades , pois ao se utilizar de diagramas , a UML facilita o entendimento e a visualização das relações dentro do sistema modelado ; ( ii ) a legibilidade e usabilidade , sem que seja necessário a leitura do código do sistema , uma vez que um desenvolvedor pode compreender quais partes do código são redundantes ou reutilizadas ; e ( iii ) uma ferramenta de planejamento , ao auxiliar na definição do que deve ser feito , antes que a implementação comece de fato , além de poder produzir código e reduzir o tempo de desenvolvimento . Todavia , a UML possui desvantagens , tais como : ( i ) ambiguidade entre elementos UML diferentes , devido a sobreposição dos diagramas ; e ( ii ) falta de uma semântica clara , o qual geralmente faz com que a semântica da linguagem de programação seja adotada . Para mitigar essas desvantagens , pesquisadores buscam atribuir uma semântica formal à UML . Esse tipo de semântica é encontrado em modelos formais , onde o sistema modelado é livre de ambiguidades e possui uma semântica clara e precisa . Por sua vez , os modelos formais não são simples de serem criados e compreendidos por desenvolvedores . O grau de conhecimento em formalismo necessário para utilizar tal modelo é alto , o que faz com que seu uso seja menos difundido , comparado com a notação gráfica não formal da UML . Apesar dos esforços dos pesquisadores , as técnicas de formalização semântica da UML apresentam , no geral , um problema pouco abordado : apesar de utilizar a UML para modelar o sistema , o artefato final dessas técnicas é um trace formal . Considerando o conhecimento comum de um desenvolvedor de software , esse trace dificulta a análise dos problemas , encontrados pelos model checkers , e a correção dos mesmos no modelo UML . Com o objetivo de auxiliar o desenvolvedor na compreensão dos resultados formais ( o trace citado ) , esta tese de doutorado apresenta uma abordagem baseada em Model-driven Architecture ( MDA ) capaz de representar as informações dos resultados formais dentro de um modelo UML . Por meio de transformações do modelo UML , essas representações , definidas utilizando a abordagem , auxiliam o desenvolvedor a visualizar o fluxo de execução do model checker dentro do modelo UML . Assim , acredita-se que as vantagens obtidas pela formalização semântica da UML podem ser mais difundidas e utilizadas pelos desenvolvedores , principalmente na indústria 
 Com o crescimento do volume de dados manipulado por aplicações de data warehousing , soluções centralizadas tornam-se muito custosas e enfrentam dificuldades para tratar a escalabilidade do volume de dados . Nesse sentido , existe a necessidade tanto de se armazenar grandes volumes de dados quanto de se realizar consultas analíticas ( ou seja , consultas OLAP ) sobre esses dados volumosos de forma eficiente . Isso pode ser facilitado por cenários caracterizados pelo uso de bancos de dados NoSQL gerenciados em ambientes paralelos e distribuídos . Dentre os desafios relacionados a esses cenários , destaca-se a necessidade de se promover uma análise de desempenho de aplicações de data warehousing que armazenam os dados do data warehouse ( DW ) em bancos de dados NoSQL orientados a colunas . A análise experimental e padronizada de diferentes sistemas é realizada por meio de ferramentas denominadas benchmarks . Entretanto , benchmarks para DW foram desenvolvidos majoritariamente para bancos de dados relacionais e ambientes centralizados . Nesta pesquisa de mestrado são investigadas formas de se estender o Star Schema Benchmark ( SSB ) , um benchmark de DW centralizado , para o banco de dados NoSQL distribuído e orientado a colunas HBase . São realizadas propostas e análises principalmente baseadas em testes de desempenho experimentais considerando cada uma das quatro etapas de um benchmark , ou seja , esquema e carga de trabalho , geração de dados , parâmetros e métricas , e validação . Os principais resultados obtidos pelo desenvolvimento do trabalho são : ( i ) proposta do esquema FactDate , o qual otimiza consultas que acessam poucas dimensões do DW ; ( ii ) investigação da aplicabilidade de diferentes esquemas a cenários empresariais distintos ; ( iii ) proposta de duas consultas adicionais à carga de trabalho do SSB ; ( iv ) análise da distribuição dos dados gerados pelo SSB , verificando se os dados agregados pelas consultas OLAP estão balanceados entre os nós de um cluster ; ( v ) investigação da influência de três importantes parâmetros do framework Hadoop MapReduce no processamento de consultas OLAP ; ( vi ) avaliação da relação entre o desempenho de consultas OLAP e a quantidade de nós que compõem um cluster ; e ( vii ) proposta do uso de visões materializadas hierárquicas , por meio do framework Spark , para otimizar o desempenho no processamento de consultas OLAP consecutivas que requerem a análise de dados em níveis progressivamente mais ou menos detalhados . Os resultados obtidos representam descobertas importantes que visam possibilitar a proposta futura de um benchmark para DWs armazenados em bancos de dados NoSQL dentro de ambientes paralelos e distribuídos 
 Atualmente , existe um grande número de ferramentas para montagem de fragmentos de DNA disponíveis . Neste trabalho apresentamos um estudo comparativo das ferramentas CAP2 , FAKtory , TIGR e PHRAP . Para realizarmos este estudo , primeiramente executamos esses sistemas de montagem sobre 12 casos de testes distintos . Após isso , tomamos os resultados obtidos por cada um deles e os comparamos com as seqüências de onde os fragmentos foram originalmente obtidos . Os testes utilizados avaliam a eficiências dos programas com relação a três problemas associados ao processo de montagem ( erros no sequenciamento , fragmentos quimeras e regiões repetidas ) e pudemos ver que nenhum dos sistemas é claramente superior aos demais no tratamento de todos eles . Cada ferramenta de montagem parece tratar de melhor forma um problema em especial.Além de avaliarmos os resultados , realizamos também um estudo 
 Nesta tese , estudamos o problema de Empacotamento de Bicliques . Um biclique é um grafo bipartido completo . No problema de Empacotamento de Bicliques são dados um inteiro k e um grafo bipartido G e deseja-se encontrar um conjunto de k bicliques , subgrafos de G , dois a dois disjuntos nos vértices , tal que a quantidade total de arestas dos bicliques escolhidos seja máxima . No caso em que k=1 , temos o problema de Biclique máximo . Esses dois problemas possuem aplicações na área de Bioinformática . Mantemos neste trabalho um enfoque prático , no sentido de que nosso interesse é resolver instâncias desses dois problemas com tamanho razoavelmente grande . Para isso , utilizamos técnicas de Programação Linear Inteira . Para avaliar os métodos propostos aqui , mostramos resultados de experimentos computacionais feitos com instâncias vindas de aplicações e também com instâncias geradas aleatoriamente 
 Doenças vasculares são um problema mundial , que representa 28 % das mortes no mundo e 66 % do total de doenças que acometem os brasileiros . Dessa forma , há um grande interesse em pesquisar formas de prevenção e tratamento dessas doenças . Algumas medidas são relevantes no auxílio de diagnóstico , tal como : tamanho médio dos ramos , diâmetro médio das seções transversais dos vasos e padrões de divisão de ramos . Calcular essas medidas de forma_manual  é uma tarefa demorada e trabalhosa . Assim , esta Tese tem como objetivo , propor um método computacional de rastreamento e extração de atributos em redes vasculares a partir de imagens 3D de angiografia por ressonância_magnética  e por tomografia computadorizada . Trata-se de uma abordagem de rastreamento e identificação de bifurcações que difere das técnicas anteriores , utilizando a Transformada de Hough para identificar o diâmetro do vaso em cortes transversais num dado ponto ao longo de um vaso sanguíneo . Mais detalhadamente , essa abordagem utiliza um campo vetorial advindo do cálculo de uma matriz formada por derivadas parciais de segunda_ordem  , obtida da intensidade luminosa da imagem , para identificar a direção de um ramo de vaso . Além_disso  , durante o processo de rastreamento de um ramo de vaso , são calculados vários descritores de forma com o objetivo de classificar regiões como pertencentes a uma bifurcação ou não . Em adição a estes descritores , desenvolvemos uma nova medida chamada de variância do raio que permite distinguir , bifurcações , não-bifurcações e segmentos de vaso com stents ( aparelho metálico usado para aumentar o diâmetro dos vasos ) . Para a classificação de bifurcações , criamos a medida de bifurcação , que trata-se de uma combinação linear de todos os descritores de forma apresentados neste trabalho . Testes foram realizados para atestar a eficácia da abordagem proposta , utilizando tanto imagens sintéticas quantoimagens reais . Os resultados mostraram que o método é capaz de rastrear 91 % de uma rede vascular sintética variando o ponto de inicialização e 76 % variando o nível de ruído . Também foi observado por meio de testes que o método proposto consegue rastrear vasos e identificar bifurcações em imagens reais sem avaliação numérica . Essa abordagem permite a extração da relação hierárquica entre os ramos em uma rede vascular e a extração do padrão de divisão dos vasos , o que contribui sobremaneira para o estudo do comportamento do fenômeno da angiogênese e no auxílio no diagnóstico de anomalias vasculares 
 O Brasil é considerado o país da megadiversidade por abrigar diversas espécies de flora e fauna . Dessa forma preservar essa diversidade é extremamente importante , pois a vida no planeta depende dos muitos ecossistemas que compõem essa biodiversidade . Atualmente , vários estudos sobre formas de recuperar e acessar informações sobre biodiversidade vem_sendo  discutidos na comunidade_científica  . Muitas instituições importantes têm disponibilizado gratuitamente seus registros de coletas disponíveis abertamente em repositórios online . No entanto , os dados disponibilizados nesses repositórios contêm informações geográficas imprecisas ou ausentes . Isso acarreta vários problemas como , por exemplo , a inviabilidade da realização de planos sistemáticos para preservar áreas para conservação de espécies ameaçadas . O problema principal para a realização desse plano é determinar com precisão a distribuição dessas espécies . Nesse contexto , o problema de pesquisa identificado é a necessidade de melhorar as informações geográficas contidas em dados sobre biodiversidade disponíveis em repositórios online . Para atacar esse problema , o SWI Gazetteer foi desenvolvido . Ele usa tecnologias da Web Semântica r técnicas de Recuperação de Informação Geográfica para associar coordenadas geográficas a nomes de lugares . Quando procuram por lugares , usuários podem realizar buscas semânticas que conseguem melhores_resultados  ( em relação à precisão e cobertura de dados ) que buscas tradicionais por palavras chaves . O Gazetteer também permite a difusão de suas informações usando formatos dos padrões Linked Open Data . Os resultados dos experimentos mostram que o SWI Gazetteer é capaz de aumentar , em até 102 % , o número de registros com coordenadas geográficas em amostras representativas de repositórios de dados sobre biodiversidade bem conhecidos ( como GBIF e SpecielLink ) 
 Sistemas de recomendação surgiram da necessidade de selecionar e apresentar conteúdo relevante a usuários de acordo com suas preferências . Dentre os diversos métodos existentes , aqueles baseados em conteúdo faz em uso exclusivo da informação inerente aos itens . Estas informações podem ser criadas a partir de técnicas de indexação automática e manual . Enquanto que as abordagens automáticas necessitam de maiores recursos_computacionais  e são limitadas á tarefa específica que desempenham , os métodos manuais são caros e propensos a erros . Por outro_lado  , com a expansão da Web e a possibilidade de usuários comuns criarem novos conteúdos e anotações sobre diferentes itens e produtos , uma alternativa é obter esses metadados criados colaborativamente pelos próprios usuários . Entretanto , essas informações , em especial revisões e comentários , podem conter ruídos , além de estarem em uma forma desestruturada . Deste modo , este trabalho1 tem como objetivo desenvolver métodos de construção de representações de itens baseados em descrições colaborativas para um sistema de recomendação . Objetiva-se analisar o impacto que diferentes técnicas de extração de características , aliadas à análise de sentimento , causam na precisão da geração de sugestões , avaliando-se os resultados em dois cenários de recomendação : predição de notas e geração de ranques . Dentre as técnicas analisadas , observa-se que a melhor apresenta um ganho no poder descritivo dos itens , ocasionando uma melhora no sistema de recomendação 
 Esta tese de doutorado analisa o impacto gerado pela virtualização assistida por hardware , de terceira geração , no desempenho de aplicações baseadas em SOA do tipo CPU-bound e memory-intensive , bem como nas operações de marshalling e unmarshalling de mensagens SOAP executadas nas bibliotecas dos servidores de aplicação . A partir dessa análise , concluiu-se que os recursos disponibilizados pela plataforma de hardware , principalmente no gerenciamento de memória , eliminam a sobrecarga gerada pela virtualização baseada em tradução binária , elevando os hypervisores bare-metal a um novo patamar , onde aplicações executadas sobre tais virtualizadores obtém , frequentemente , desempenho_superior  àquelas executadas em plataformas não virtualizadas . Assim , modificações nos modelos de desempenho de aplicações baseadas em SOA atualmente em uso são propostas , de acordo com a análise desenvolvida , para que reflitam os resultados obtidos de forma 
 Este trabalho tem por objetivo estudar os problemas computacionais que surgem ao se relacionar grafos com superfícies bidimensionais , dando especial atenção aos problemas do número de cruzamentos mínimo no plano ( CROSSING NUMBER ) e a problemas relacionados ao desenho de grafos em livros . Apresentamos uma redução do problema MULTICUT para CROSSING NUMBER , além de um resultado de complexidade em grafos de comparabilidade baseado em um resultado conhecido para desenhos em livros 
 O milho é uma cultura anual importante para diferentes setores da economia mundial . Sua produção é principalmente utilizada para consumo humano e animal , além de indústrias química e de biocombustíveis . Com o aumento da demanda mundial , há a necessidade de aumento da produtividade com diminuição de custos , tanto econômicos como ambientais . Isso leva a investimentos para maior tecnificação do cultivo , seleção de diferentes cultivares e busca por maior eficiência nutricional . Nesse contexto insere-se este doutorado , o qual propõe metodologia para diagnóstico nutricional precoce em culturas de milho . Para tanto , objetiva-se melhorar um sistema de classificação nutricional por imagem de folhas , analisar a invariância deste diagnóstico a diferentes híbridos de milho e identificar características que permitam uma análise em estágio precoce do ciclo de crescimento . Além_disso  , os estudos são realizados sobre experimentos de cultivos em ambiente controlado e em ambiente comercial . A análise das imagens é realizada pela extração de características de textura e , consequentemente , resulta no desenvolvimento de metodologias inovadoras nesta área . Especificamente , são propostas duas metodologias na área de descritores fractais e usando transformação local jet . Os resultados da diagnose nutricional demonstram como é promissora a pesquisa , uma vez que obtém-se 98 % de acerto na classificação de níveis nutricionais de nitrogênio ou potássio em etapa chave para correção nutricional em um mesmo ciclo da cultura . Outra abordagem proposta , ainda promove a identificação e visualização de sintomas em estágio em que esses sintomas são considerados ocultos , comprovando a eficácia do reconhecimento de padrões de textura 
 Em um ambiente de computação em nuvem , as empresas têm a capacidade de alocar recursos de acordo com a demanda . No entanto , há um atraso que pode_levar  alguns minutos entre o pedido de um novo recurso e o mesmo estar pronto para uso . Por esse motivo , as técnicas reativas , que solicitam um novo recurso apenas quando o sistema atinge um determinado limiar de carga , não são adequadas para o processo de alocação de recursos . Para resolver esse problema , é necessário prever as requisições que chegam ao sistema , no próximo período de tempo , para alocar os recursos necessários antes que o sistema fique sobrecarregado . Existem vários modelos de previsão de séries_temporais  para calcular as previsões de carga de trabalho com base no histórico de dados de monitoramento . No entanto , é difícil saber qual é o melhor modelo de previsão a ser utilizado em cada caso . A tarefa se torna ainda mais complicada quando o usuário não tem muitos dados históricos a serem analisados . A maioria dos trabalhos_relacionados  , considera apenas modelos de previsão isolados para avaliar os resultados . Outros trabalhos propõem uma abordagem que seleciona modelos de previsão adequados para um determinado contexto . Mas , neste caso , é necessário ter uma quantidade significativa de dados para treinar o classificador . Além_disso  , a melhor solução pode não ser um modelo específico , mas sim uma combinação de modelos . Neste trabalho propomos um método de previsão adaptativo , usando técnicas de otimização multiobjetivo , para combinar modelos de previsão de séries_temporais  . O nosso método não requer uma fase prévia de treinamento , uma vez que se adapta constantemente a medida em que os dados chegam ao sistema . Para avaliar a nossa proposta usamos quatro logs extraídos de servidores reais . Os resultados mostram que a nossa proposta frequentemente converge para o melhor resultado , e é suficientemente genérica para se adaptar a diferentes_tipos  de séries_temporais  
 O problema de corte de estoque consiste em cortar objetos maiores , disponíveis em estoque , para produzir uma quantidade especificada de peças menores , de modo que uma certa função objetivo seja otimizada . Um modelo de otimização_linear  tem sido amplamente utilizado na solução deste problema desde os anos 60 , que incorpora parte da estrutura combinatória inerente ao problema na construção das colunas da matriz de restrições . As colunas são construídas a cada iteração do Método Simplex , chamando-se geração de colunas . Apesar do método Simplex ser largamente utilizado para este tipo de problema , apresenta baixa convergência quando próximo da otimalidade , pouco melhorando a função objetivo . Assim , estratégias para aceleração do Método Simplex faz-se necessário , uma maneira consiste na redução do espaço dual , com a introdução de restrições ( colunas no primal ) que evite grandes variações nas variáveis duais , chamadas cortes duais . Neste trabalho , generalizamos duas famílias de cortes duais recentemente publicadas e analisamos o impacto computacional desses cortes duais sobre a convergência do Método 
 Contexto : Apesar de existirem muitas informações sobre a área de teste de software , elas se encontram de forma dispersa e sem conexão , o que aumenta a já existente dificuldade por parte de usuários em compreender os conceitos e as tecnologias dessa área e , conseqüentemente , em tomar a decisão de onde e quando usálas . Objetivo : O objetivo deste trabalho foi criar o arcabouço KITest ( Knowledge and Improvement on Test ) capaz de agregar o conhecimento em teste e disponibilizá-lo para a comunidade com a intenção de facilitar a sua transferência , a definição e a melhoria de processos de teste , com mais qualidade . Metodologia : Para facilitar a transferência de conhecimento modelou-se o conhecimento em teste por meio de um processo genérico de teste organizado em um mapa mental ( KITMap ) . Para contemplar a questão de qualidade estabeleceu-se como base as práticas do modelo TMMi , distribuídas no processo de teste genérico . Para permitir que a comunidade interaja com essa base de conhecimento em teste criou-se uma ferramenta ( KITTool ) que permita acesso a essas informações , faça diagnóstico do processo de teste vigente e sugira melhorias . Para gerenciar toda essa estrutura utilizou-se uma estratégia de melhoria para que essa estrutura esteja sempre em evolução com base na realimentação da comunidade que a utiliza . Resultados e Conclusões : os resultados e as conclusões sobre a aplicação do arcabouço KITest estão apresentados no relatório técnico anexo a esta 
 Este trabalho apresenta a construção de um sistema embarcado para detectar pedestres , utilizando computação reconfigurável com captura de imagens através de uma única câmera acoplada a um veículo que trafega em ambiente urbano . A principal motivação é a necessidade de reduzir o número vítimas causadas por acidentes de trânsito envolvendo pedestres . Uma das causas está relacionada com a velocidade de resposta do cérebro humano para reconhecer situações de perigo e tomar decisões . Como resultando , há um interesse mundial de cientistas para elaborar soluções economicamente viáveis que venham a contribuir com inovações tecnológicas direcionadas a auxiliar motoristas na condução de veículos . A implementação em hardware deste sistema foi desenvolvida em FPGA e dividida em blocos interconectados . Primeiramente , no pré-tratamento do vídeo , foi construído um bloco para conversão de dados da câmera para escala de cinza , em seguida , um bloco simplificado para a estabilização vertical dinâmica de vídeo . Para a detecção foram construídos dois blocos , um para detecção binária de movimento e um bloco de detecção BLOB . Para fazer a classificação , foi construído um bloco para identificação do tamanho do objeto em movimento e fazendo a seleção pela proporcionalidade . Os testes em ambiente real deste sistema demonstraram ótimos resultados para uma velocidade máxima de 30 
 A Web Semântica tem como meta fazer com que os conteúdos disponibilizados na Web tenham significado não apenas para pessoas , mas também que possam ser processados por máquinas . Essa meta está sendo realizada com o desenvolvimento e uso de ontologias para criar dados anotados semanticamente . Entre as distintas formas de anotação semântica , a Semantic Web Rule Language ( SWRL ) torna possível criar anotações no formato de regras que combinam regras com conceitos definidos em ontologias , especificadas em Web Ontology Language ( OWL ) , para representar conhecimento sobre dados por meio de afirmações condicionais . Todavia , à medida que o número dessas regras crescem , seus desenvolvedores podem enfrentar dificuldades para gerenciá-las adequadamente . Um grande conjunto de regras torna-se difícil de entender e propício a erros , principalmente quando usado e mantido de forma colaborativa . Neste trabalho é apresentado um conjunto de soluções para aprimorar o uso e gerenciamento de regras SWRL , que compreendem o desenvolvimento de novas representações_visuais  , técnicas de classificação de regras e ferramenta de detecção de erros . Essas soluções resultaram no SWRL Editor , uma ferramenta Web de visualização e composição de regras que roda como um plug-in para o Web Protégé . Como estudo de caso , foi utilizada a Autism Phenologue Rules , uma ontologia para caracterizar fenótipos de autismo , para exemplificar um conjunto grande e complexo de regras SWRL . A partir desse estudo , uma nova representação visual específica para as regras dessa ontologia foi elaborada , permitindo que um especialista em autismo , sem grandes conhecimentos computacionais , seja capaz de ver e editar regras sem ter de se preocupar com a sintaxe da linguagem SWRL . Os resultados obtidos indicam que o SWRL Editor é uma ferramenta clara e intuitiva , contribuindo para um melhor entendimento , criação e gerenciamento de regras SWRL 
 Com o advento da computação em nuvem , não é mais necessário ao usuário investir grandes quantidades de recursos financeiros em equipamentos computacionais . Ao invés disto , é possível adquirir recursos de processamento , armazenamento ou mesmo sistemas completos por demanda , usando um dos diversos serviços disponibilizados por provedores de nuvem como a Amazon , o Google , a Microsoft , e a própria USP . Isso permite um controle maior dos gastos operacionais , reduzindo custos em diversos casos . Por exemplo , usuários de computação de alto_desempenho  podem se beneficiar desse modelo usando um grande número de recursos durante curtos períodos de tempo , ao invés de adquirir um aglomerado computacional de alto custo inicial . Nosso trabalho analisa a viabilidade de execução de aplicações de alto_desempenho  , comparando o desempenho de aplicações de alto_desempenho  em infraestruturas com comportamento conhecido com a nuvem pública oferecida pelo Google . Em especial , focamos em diferentes configurações de paralelismo com comunicação interna entre processos no mesmo nó , chamado de intra-nós , e comunicação externa entre processos em diferentes nós , chamado de entre-nós . Nosso caso de estudo para esse trabalho foi o NAS Parallel Benchmarks , um benchmark bastante popular para a análise de desempenho de sistemas paralelos e de alto_desempenho  . Utilizamos aplicações com implementações puramente MPI ( para as comunicações intra e entre-nós ) e implementações mistas onde as comunicações internas foram feitas utilizando OpenMP ( comunicação intra-nós ) e as comunicações externas foram feitas usando o MPI ( comunicação entre-nós ) 
 Atualmente a qualidade deve ser uma característica presente em qualquer produto ou serviço , inclusive em todos os tipos de softwares . As aplicações_hipermídia  , por sua vez , são compostas por milhares de links e nós de uma forma bastante flexível , que as tomam facilmente passíveis de erros e merecem atenção para um efetivo controle de qualidade . Além_disso  , as aplicações_hipermídia  na Internet , por meio de páginas da Web , estão aumentando de forma considerável . Uma análise estrutural foi proposta com o objetivo de avaliar a qualidade das ligações de um hiperdocumento Web e uma classificação de casos de reuso de links foi definida . O objetivo deste trabalho é aplicar essa análise estrutural e avaliar os casos de links no domínio educacional , ou seja , identificar o que significa cada caso para esse domínio especifico e fornecer indicativos para que o autor possa melhorar sua aplicação hipermídia educacional 
 Os requisitos de largura de banda para satisfazer as necessidades dos sistemas de vídeo sob_demanda  são críticos para a qualidade do serviço fornecido pelo servidor . Muitas técnicas tem sido propostas para reduzir estes requisitos . Este projeto modela uma aplicação que implementa a técnica do SNAPSHOT para reunir vários streams de vídeo permitindo o uso do multicasting . É baseada no método adaptive piggybacking que ajusta a taxa de exibição dos quadros dos streams novos ( tornando-os mais rápidos ) e velhos ( tornando-os mais lentos ) até que eles possam ser mesclados em um único stream . A modelagem foi feita em UML e alguns cenários de implementações são propostos 
 A busca pela qualidade de processo de software é uma constante cada vez mais crescente entre as empresas produtoras de software . Para auxiliar na obtenção dessa qualidade , modelos de melhoria de processo de software vêm_sendo  desenvolvidos em todo o mundo . A Estratégia para Iniciar Melhoria de Processo de Software [ Endo , 1998 ] define , relaciona e organiza as principais atividades que devem ser realizadas para avaliar um Processo de Software com relação ao nível 2 do SW-CMM1 . Este trabalho de mestrado seleciona e adiciona à Estratégia as atividades das áreas-chave do nível 3 do SW-CMM , propiciando às empresas a avaliação de seu processo de software para que atinjam esse nível . Também a ferramenta SProQ 2 foi desenvolvida por este trabalho de mestrado para auxiliar na coleta de dados e reporte de resultados existente na Estratégia , configurando-se um importante sistema de informação destinado entre outras coisas a auxiliar nas tomadas de decisão quanto à melhoria do processo de software das empresas 
 A aplicação da : informática na educação tem sido alvo de intensas pesquisas nos últimos_anos  . Em ambientes de educação , o computador pode ser visto como um instrumento que facilita o acesso às informações , podendo ser considerado como um parceiro para as tarefas de orientação do professor . Adicionalmente , um crescente_interesse  tem sido observado em termos do desenvolvimento de aplicações , ferramentas e ambientes para suporte 4. produção e publicação de materiais didáticos via Internet - mais especificamente no ambiente WWW , criando uma variedade de cursos disponíveis neste ambiente . Genericamente , o processo para o oferecimento de cursos no ambiente WWW pode ser dividido em três etapas principais , que são a autoria , a disponibilização e o gerenciamento . Atualmente , as duas primeiras etapas podem ser realizadas com a utilização de ferramentas para autoriaidisponibilização existentes . Por outro_lado  , a etapa de gerenciamento do curso ainda é uma tarefa pouco explorada . Assim , o objetivo deste trabalho está centrado no criação de ferramentas para ambientes de ensino distribuídos que promovam a intenção entre estudantes e professores para o gerenciamento das atividades didáticas envolvidas em um curso , tais como formação de grupos de trabalho , entrega de-trabalhos , visualização de notas , calendários , dentre outras . Este trabalho apresenta um conjunto de ferramentas WebCoM ( Web Course Manager ) baseado no conceito de agentes para a criação de um ambiente de gerenciamento e definição das atividades programadas para um curso via Internet . As ferramentas do WebCoM são fwadamentadas no princípio do isolamento de professores e estudantes das tarefas de baixo_nível  do gerenciamento , como é o caso do acesso às bases de dados e ao volume de documentos gerados durante o curso 
 A dinâmica e a flexibilidade da autoria de hiperdocumentos na Web , por um lado popularizam a cada dia o uso da Internet , mas por outro , propiciam que facilmente muitas informações fiquem inconsistentes . Basta uma definição errônea de um hiperlinki , para que o usuário se depare com uma inconsistência e se sinta `` perdido '' . Um procedimento comum durante o desenvolvimento de um site2 é a reutilização dos componentes de link , seja por haver na mesma página origem mais de um link , ou o mesmo rótulo de link em diversas páginas , ou ainda vários links para uma mesma página destino . Como um site , geralmente , contém uma grande quantidade de links , torna-se inviável a verificação manual da reusabilidade de seus links . A ferramenta DB-LiOS foi desenvolvida com o objetivo de automatizar a verificação da reusabilidade de links de um site da Web , através de processos de extração e classificação de links . Com a utilização de DBLiOS , os autores de um site podem obter um auxilio efetivo para avaliação da consistência de seus links 
 O modo tradicional de criar o layout de circuitos ASICs ( Application Specific Integrated Circuits ) requer um projetista humano para interagir com um programa . Ele utiliza uma metodologia de layout baseada em bibliotecas de células-padrão ( standard cells ) . Essa metodologia é boa para os projetos ASIC porque os processos realizados para geração de layouts podem ser automatizados , diminuindo o tempo gasto no projeto e aumentando a sua confiabilidade . Porém , essa metodologia possui inconvenientes em relação à manutenção da biblioteca de células e em relação ao número total e variedade de células existentes , pois , algumas células requeridas num projeto , podem não existir na biblioteca forçando uma adaptação do projeto . O sistema Agents 2 foi desenvolvido para automatizar a geração de células para circuitos integrados , mais precisamente células-padrão . O sistema é composto por vários agentes servidores , o Placer ( que posiciona os componentes do circuito ) e vários Routers ( que conectam os componentes do circuito ) . Os servidores trabalham de forma distribuída em uma rede de computadores e a escalabilidade do sistema aumenta à medida que novos computadores são introduzidos na rede . Entretanto , o sistema não é capaz de explorar os recursos_computacionais  de máquinas multiprocessadas . Para resolver esse problema foi desenvolvido nesse trabalho um novo servidor de roteamento , chamado de RouterServer . Ele foi desenvolvido usando a linguagem Java e programação multithreaded , permitindo que o sistema Agents 2 possa explorar o paralelismo existente em computadores multiprocessados com memória_compartilhada  , mas mantendo sua escalabilidade em sistemas distribuídos em rede 
 Atualmente tem crescido o uso de aparelhos móveis com mais de uma interface de rede para o acesso à Internet , caracterizando em parte as Redes de Próxima Geração ( NGN ) . Outras características da NGN são o acesso sob_demanda  , no qual o cliente de acesso não seria , necessariamente , usuário de um único provedor e usaria a rede de outros provedores conforme a necessidade , e o gerenciamento de handover ( trocas de rede ) centrado no usuário , no qual o usuário é quem decide à qual rede vai se conectar e em que momento . Existem algumas implicações relacionadas ao acesso sob_demanda  que podem melhorar ou piorar a experiência de acesso do usuário e causar certos impactos relacionados à vazão recebida , valor gasto , número de handovers , entre outros . Para o dispositivo móvel gerenciar os handovers , ele precisa obter informações sobre as redes disponíveis como , por exemplo , preço e incentivos , além das informações já obtidas atualmente como força do sinal recebido e identificador do provedor ( SSID ) . Porém , os provedores não possuem um sistema automatizado para fornecer tais informações aos dispositivos_móveis  . Esta tese apresenta uma avaliação dos impactos das trocas dinâmicas entre provedores Wi-Fi para o usuário móvel , comparando o acesso sob_demanda  com o acesso tradicional ( como é feito atualmente ) . Como metodologia para esta avaliação , foram realizadas simulações de redes em um cenário com provedores Wi-Fi com perfis diferentes , nas quais foram analisados o número de handovers realizados , a quantidade de bytes recebidos e o valor gasto com o acesso . Nestas simulações o usuário pôde priorizar o custo da conexão , a força do sinal recebido e a carga de trabalho no ponto de acesso para decidir a troca de rede . Esta tese também apresenta uma arquitetura para provimento de informações adicionais sobre as redes disponíveis em um ambiente de acesso sob_demanda  com gerenciamento de handover centrado no usuário , chamada MYHand ( MIH-based and Y-Comm-based Handover Management ) . Para isso foi utilizada a arquitetura para acesso sob_demanda  chamada Y-Comm e o padrão IEEE 802.21 ( MIH Media Independent Handover ) . Os resultados desta tese contribuem no gerenciamento das trocas de rede e ubiquidade do acesso à Internet em NGN . A arquitetura MYHand auxiliará o dispositivo móvel a obter mais informações necessárias à decisão de handover , podendo otimizá-la . Com os resultados das simulações , o dispositivo móvel poderá prever o quanto determinada decisão poderá beneficiar a experiência do usuário em termos de qualidade da conexão e 
 Grafos são empregados com sucesso em uma grande variedade de problemas e aplicações , sendo objeto de estudo na modelagem , análise e na construção de representações_visuais  . Embora existam diferentes formas para a visualização de grafos , a maioria delas sofrem pela desorganização do espaço_visual  quando o número de vértices ou arestas é alto . Entre as abordagens que lidam com este problema , as técnicas de agrupamentos visuais de arestas obtiveram sucesso na melhora da representação visual pelo encurvamento e agrupamento de arestas que aperfeiçoam a organização da representação . Apesar deste sucesso , a maioria das técniques criam grupos de arestas baseados apenas na informação do espaço_visual  , não existindo conexão explícita entre o desenho no espaço_visual  e o conjunto de dados original . Dessa forma , estas técnicas produzem agrupamentos de arestas com baixa significância e podem levar o usuário a uma interpretação incorreta da informação . Esta pesquisa de mestrado apresenta uma nova_técnica  de agrupamento visual de arestas baseado nas relações de similaridade entre os vértices . Nós desenvolvemos esta técnica com base em duas premissas . Primeiro , ela defende a hipótese que a representação por agrupamento de arestas pode representar melhor o conjunto de dados se existir uma conexão inerente entre a proximidade dos elementos no espaço de informação e a proximidade entre arestas no desenho de arestas agrupadas . Nós atendemos esta questão apresentando um arcabouço para o agrupamento de arestas baseado em similaridade , que considera a similaridade entre vértices para realizar o encurvamento das arestas . Para guiar este encurvamento , nós criamos uma estrutura de similaridade , denominada backbone . Esta estrutura é baseada em um particionamento multi-nível do conjunto de dados , que agrupa arestas de vértices similares . A segunda premissa , nós também defendemos que uma representação multiescala melhora a escalabilidade computacional e visual da representação visual de arestas agrupadas . Nós apresentamos um agrupamento visual multi-nível de arestas que permite uma exploração generalizada e detalhada , revelando detalhes em múltiplos níveis da visualização . Nosso processo de avaliação mostra que a construção do backbone produz uma hierarquia balanceada e com boa representação das relações de similaridade entre os vértices . Além_disso  , a visualização com arestas guiadas pelo backbone reduz a desordem visual e melhora as técnicas do estado-da-arte na identificação de padrões de arestas globais e locais 
 Informação sobre o estágio de câncer num paciente é fundamental quando médicos avaliam o progresso de seu tratamento . A determinação do estágio de câncer ( cancer staging ) é um processo que leva em consideração a descrição , localização , características e possíveis metástases dos tumores cancerosos de um paciente . Esse processo deve seguir um padrão de classificação como , por exemplo , o padrão TNM . Porém , na prática clínica , a execução desse processo pode ser tediosa , propensa a erros e gerar incertezas . Com o intuito de amenizar esses problemas , este trabalho tem como objetivo auxiliar radiologistas fornecendo uma segunda opinião na avaliação do estágio de câncer de um paciente . Para isso , tecnologias da Web Semântica , como ontologias e reasoning , foram usadas para classificar automaticamente estágios de câncer . Essa classificação usou anotações semânticas feitas por radiologistas , usando a ferramenta ePAD , e armazenadas no formato AIM . Um protótipo de classificador , baseado no padrão TNM , foi criado . Ele transforma anotações AIM em indivíduos da ontologia AIM4-O e , usando axiomas e regras ( escritos na linguagens OWL-SWRL ) representando o padrão TNM , ele automaticamente calcula o estágio de câncer de fígado de pacientes . A ontologia AIM4-O foi desenvolvida , como parte desse trabalho , para representar anotações AIM 4 em OWL . Esse classificador TNM foi avaliado , usando-se dados reais de relatórios radiológicos de pacientes do NCIs Genomic Data Commons ( GDC ) , em termos de precisão e revocação , com resultados respectivos de 85,7 % e 81,0 % ( quando comparado aos valores reais de estágio de câncer dos relatórios ) . Todo o processo foi validado com radiologistas do Radiology Dept . of the Stanford University 
 Um problema de otimização_linear  consiste no problema de maximizar ou minimizar uma função linear sujeita a um conjunto de restrições_lineares  . Tal problema é , sem dúvidas , o modelo de otimização mais freqüentemente usado seja em planejamento da produção , alocação de recursos , bem como em muitos outros aspectos da tomada de decisão em indústrias , economia , transportes , engenharias , etc . Nesse trabalho o Método Simplex e alguns de seus variantes são investigados com ênfase na compreensão do seu desempenho computacional 
 A Teoria das equações integrais , desde a segunda metade do século XX , tem assumido um papel cada vez maior no âmbito de problemas aplicados . Com isso , surge a necessidade do desenvolvimento de métodos_numéricos  cada vez mais eficazes para a resolução deste tipo de equação . Isso tem como consequência a possibilidade de resolução de uma gama cada vez maior de problemas . Nesse sentido , outros_tipos  de equações integrais estão sendo objeto de estudos , dentre elas as chamadas equações integro-diferenciais . O presente_trabalho  tem como objetivo o estudo das equações integro-diferenciais singulares lineares e não-lineares . Mais especificamente , no caso linear , apresentamos os principais resultados necessários para a obtenção de um método_numérico  e a formulação de suas propriedades de convergência . O caso não-linear é apresentado através de um modelo matemático para tubulações em um tipo específico de reator nuclear ( LMFBR ) no qual origina-se a equação integro-diferencial . A partir da equação integro-diferencial um modelo numérico é proposto com base nas condições físicas do 
 Neste trabalho são apresentados os resultados do desenvolvimento e teste de esquemas upwind de alta_resolução  para o controle da difusão numérica em leis de conservação gerais e problemas em dinâmica dos fluidos . Em particular , são derivados dois novos esquemas : o ALUS ( Adaptive Linear Upwind_Scheme  ) e o TOPUS ( Third-Order Polynomial Upwind_Scheme  ) . Esses esquemas são testados no transporte de escalares , em equações 1D tipo convecção-difusão , em sistemas hiperbólicos 1D , nas equações de Euler 2D da dinâmica dos gases e nas equações de Navier-Stokes incompressíveis 2D/3D . Os esquemas são então associados a uma modelagem algébrica não linear para a simulação de problemas de escoamentos_incompressíveis  turbulentos 2D com/sem superfícies_livres 
 Nas últimas_décadas  , testemunhou-se um crescente_interesse  no estudo de sistemas complexos . Tais sistemas são compostos por pelo menos dois componentes fundamentais : elementos dinâmicos individuais e uma estrutura de organização definindo a forma de interação entre estes . Devido a dinâmica de cada elemento e a complexidade de acoplamento , uma grande variedade de fenômenos espaço-temporais podem ser observados . Esta tese tem como objetivo principal explorar o uso da dinâmica espaço-temporal em redes visando a solução de alguns problemas computacionais . Com relação aos mecanismos dinâmicos , a sincronização entre osciladores acoplados , a caminhada aleatória-determinística e a competição entre elementos na rede foram considerados . Referente à parte estrutural da rede , tanto estruturas regulares baseadas em reticulados quanto redes com estruturas mais gerais , denominadas redes complexas , foram abordadas . Este estudo é concretizado com o desenvolvimento de modelos aplicados a dois domínios específicos . O primeiro refere-se à utilização de redes de osciladores acoplados para construção de modelos de atenção visual . Dentre as principais características desses modelos estão : a seleção baseada em objetos , a utilização da sincronização/ dessincronização entre osciladores neurais como forma de organização perceptual , a competição entre objetos para aquisição da atenção . Além_disso  , ao comparar com outros modelos de seleção de objetos baseados em redes osciladores , um número maior de atributos visuais é utilizado para definir a saliência dos objetos . O segundo domínio está relacionado ao desenvolvimento de modelos para detecção de comunidades em redes complexas . Os dois modelos desenvolvidos , um baseado em competição de partículas e outro baseado em sincronização de osciladores , apresentam alta precisão de detecção e ao mesmo tempo uma baixa complexidade computacional . Além_disso  , o modelo baseado em competição de partículas não só oferece uma nova_técnica  de detecção de comunidades , mas também apresenta uma abordagem alternativa para realização de aprendizado competitivo . Os estudos realizados nesta tese mostram que a abordagem unificada de dinâmica e estrutura é uma ferramenta promissora para resolver diversos problemas 
 Bases de imagens armazenadas em sistemas computacionais da área médica correspondem a uma valiosa fonte de conhecimento . Assim , a mineração de imagens pode ser aplicada para extrair conhecimento destas bases com o propósito de apoiar o diagnóstico auxiliado por computador ( Computer Aided Diagnosis - CAD ) . Sistemas CAD apoiados por mineração de imagens tipicamente realizam a extração de características visuais relevantes das imagens . Essas características são organizadas na forma de vetores de características que representam as imagens e são utilizados como entrada para classificadores . Devido ao problema conhecido como lacuna semântica , que corresponde à diferença entre a percepção da imagem pelo especialista médico e suas características automaticamente extraídas , um aspecto desafiador do CAD é a obtenção de um conjunto de características que seja capaz de representar de maneira sucinta e eficiente o conteúdo visual de imagens médicas . Foi desenvolvido neste trabalho o extrator de características FFS ( Fast Fractal Stack ) que realiza a extração de características de forma , que é um atributo visual que aproxima a semântica esperada pelo ser humano . Adicionalmente , foi desenvolvido o algoritmo de classificação Concept , que emprega mineração de regras de associação para predizer a classe de uma imagem . O aspecto inovador do Concept refere-se ao algoritmo de obtenção de representações de imagens , denominado MFS-Map ( Multi Feature Space Map ) e também desenvolvido neste trabalho . O MFS-Map realiza agrupamento de dados em diferentes espaços de características para melhor aproveitar as características_extraídas  no processo de classificação . Os experimentos_realizados  para imagens de tomografia pulmonar e mamografias indicam que tanto o FFS como a abordagem de representação adotada pelo Concept podem contribuir para o aprimoramento de sistemas 
 O desenvolvimento de novas técnicas capazes de produzir modelos de predição com erros de generalização relativamente baixos é uma constante em aprendizado de máquina e áreas correlatas . Nesse sentido , a composição de um conjunto de modelos no denominado ensemble merece destaque por seu potencial teórico e empírico de minimizar o erro de generalização . Diversos métodos para construção de ensembles de modelos são encontrados na literatura . Dentre esses , o método baseado em rotação ( RB ) tem apresentado desempenho_superior  a outros clássicos . O método RB utiliza a técnica de extração de características da análise de componentes principais ( PCA ) como estratégia de rotação para provocar acurácia e diversidade entre os modelos componentes . Contudo , essa estratégia não assegura que a direção resultante será apropriada para a técnica de aprendizado_supervisionado  ( SLT ) escolhida . Adicionalmente , o método RB não é adequado com SLTs invariantes à rotação e não foi amplamente validado com outras estáveis . Esses aspectos tornam-no inadequado e/ou restrito a algumas SLTs . Nesta tese , é proposta uma nova abordagem de extração baseada na concatenação de rotação e projeção otimizadas em prol da SLT ( denominada roto-projeção otimizada ) . A abordagem utiliza uma metaheurística para otimizar os parâmetros da transformação de roto-projeção e minimizar o erro da técnica diretora da otimização . Mais enfaticamente , propõe-se a roto-projeção otimizada como parte fundamental de um novo método de ensembles , denominado ensemble baseado em roto-projeção otimizada ( ORPE ) . Os resultados obtidos_mostram  que a roto-projeção otimizada pode reduzir a dimensionalidade e a complexidade dos dados e do modelo , além de aumentar o desempenho da SLT utilizada posteriormente . O método ORPE superou , com relevância estatística , o RB e outros com SLTs estáveis e instáveis em bases de classificação e regressão de domínio público e privado . O ORPE mostrou-se irrestrito e altamente eficaz assumindo a primeira posição em todos os ranqueamentos de dominância 
 A população idosa cresce no Brasil e cada vez mais se faz necessário desenvolver tecnologias de informação e comunicação adequadas a esse público . Com o barateamento dos telefones celulares , muitas famílias gostariam que seus idosos fossem usuários desses dispositivos visando ter contato com os mesmos quando esses estão fora de suas casas . No entanto , o design atual de celulares privilegia o público jovem , não levando em consideração as diferentes necessidades da população idosa . No mais , mesmo na população idosa , há diferenças com relação à escolaridade , experiência com tecnologias , habilidades cognitivas e destreza física . Este trabalho argumenta a favor do design , da implementação e da avaliação de interfaces que sejam flexíveis para atender à diversidade de requisitos dos idosos na interação com celulares . Uma abordagem para o design de interfaces de usuário flexíveis foi aplicada em um experimento exploratório e , considerando-se resultados de uma atividade estudo empírico com usuários idosos , um conjunto de normas que definem o design do comportamento flexível do sistema foi especificado . Esta dissertação_propõe  e apresenta um framework que propicia a reconfiguração de interfaces em tempo de interação , denominado de FlexInterface . Esse framework aborda o conceito de elementos da interface sendo , componentes que são carregados , instanciados e destruídos quando solicitados . Além_disso  , esta dissertação apresenta também uma abordagem que apoia a avaliação de interfaces flexíveis para idosos em telefones celulares . A abordagem analítica proposta , apresenta heurísticas específicas para esse contexto de uso . Por fim , uma avaliação com idosos foi realizada para verificar a viabilidade da proposta . Esse estudo constatou que houve uma redução no tempo de interação com o uso da interface flexível e um aumento na satisfação do 
 Seja G um grafo conexo e k um inteiro positivo . Um subconjunto D de vértices de G é um conjunto dominante conexo de k-saltos se o subgrafo de G induzido por D é conexo e se , para todo vértice v em G , existe um vértice u em D a uma distância não maior do que k de v. Estudamos neste trabalho o problema de se encontrar um conjunto dominante conexo de k-saltos com cardinalidade mínima ( Mink-CDS ) . Provamos que Mink-CDS é NP-difícil em grafos planares bipartidos com grau máximo 4 . Mostramos que Mink-CDS é APX-completo em grafos bipartidos com grau máximo 4 . Apresentamos limiares de inaproximabilidade para Mink-CDS para grafos bipartidos e ( 1 , 2 ) -split , sendo que um desses é expresso em função de um parâmetro independente da ordem do grafo . Também discutimos a complexidade computacional do problema de se computar tal parâmetro . No lado positivo , propomos um algoritmo de aproximação para Mink-CDS cuja razão de aproximação é melhor do que a que se conhecia para esse problema . Finalmente , quando k = 1 , apresentamos dois novos algoritmos de aproximação para a versão do problema com pesos nos vértices , sendo que um deles restrito a classes de grafos com um número polinomial de separadores minimais . Além_disso  , discutimos uma formulação de programação linear_inteira  para essa versão do problema e provamos resultados poliédricos a respeito de algumas das desigualdades que constituem o politopo associado à formulação 
 Um espaço métrico é definido por um conjunto de objetos e uma função de distância métrica , que é utilizada para avaliar o nível de similaridade entre estes objetos . Isto permite a elaboração de Métodos de Acesso Métricos ( MAMs ) capazes de responder consultas por similaridade nesses conjuntos em um tempo reduzido . Em geral , esses MAMs são materializados através de uma estrutura hierárquica chamada de árvore métrica . Normalmente essas árvores são mantidas balanceadas , pois isto tende a manter a altura da árvore mínima , reduzindo o número de acessos a disco necessários para responder às consultas . No entanto , é difícil manter as estruturas balanceadas sem a existência de sobreposição entre os nós que cobrem regiões de alta densidade de objetos . O efeito disto é a degradação do tempo das consultas , pois várias subárvores devem ser analisadas para compor as consultas . Em outras_palavras  , minimizar a sobreposição entre os nós aumenta a eficiência das árvores métricas . Um meio efetivo para isto é flexibilizar o balanceamento das árvores métricas . Este trabalho apresenta um novo MAM dinâmico , chamado de DBM-tree ( Density-Based Metric tree ) , que permite flexibilizar o balanceamento da estrutura , minimizando o grau de sobreposição entre os nós em regiões densas e , conseqüentemente , aumentando o seu desempenho para responder às consultas . Essa flexibilização é ajustada pelo usuário e é rigidamente controlada pela estrutura . A profundidade da árvore é maior em regiões de alta densidade , procurando um equilíbrio entre o número de acessos a disco para avaliar múltiplas subárvores e para a busca em profundidade em cada subárvore . A DBM-tree possui um algoritmo de otimização chamado de DBM-Slim-Down , que melhora o desempenho das árvores através da reorganização de elementos entre os seus nós . Os experimentos feitos com dados reais e sintéticos mostram que a DBM-tree supera em desempenho os MAMs tradicionais . Ela é , em média , 50 % mais rápida que os MAMs tradicionais e reduz o número de acessos a disco e cálculos de distância em até 50 % . Depois de executado o algoritmo DBM-Slim-Down , o seu desempenho melhorou em até 30 % para as consultas por abrangência e aos vizinhos mais próximos . Ainda , a DBM-tree é escalável considerando tempo total de processamento , número de acessos a disco e de cálculos de distância em relação ao tamanho do conjunto de dados indexado 
 O grande volume de dados coletados pelas empresas nas últimas_décadas  tornou-se uma fonte de informações valiosas , permitindo às empresas obter maior competitividade . Entretanto , a análise desses dados - a transformação deles em informações úteis - tornou-se uma tarefa_difícil  . Em muitos_casos  , além desses dados existirem em grande número de itens , eles são compostos por um grande número de dimensões ( ou atributos ) , dificultando a sua compreensão . Nesses dados , uma eliminação de atributos correlacionados poderia diminuir a complexidade de diversas técnicas de análise existentes . Embora a existência de correlações entre atributos possa ser encontrada por diversas técnicas estatísticas , essas correlações também podem ser observadas visualmente . Este trabalho apresenta a técnica Visualização por Blocos Verticais - BV que permite a sobreposição de diferentes mapeamentos de um mesmo conjunto de dados de alta dimensão , tornando possível a observação visual das correlações existentes entre os atributos . A técnica Visualização por Blocos Horizontais - BH , também apresentada neste trabalho , permite a sobreposição de mapeamentos de eventos orientados por um atributo separador , permitindo observar a evolução dos objetos como um todo . Ambas as técnicas utilizam o algoritmo FastMap para realizar a redução de dimensionalidade dos dados para um espaço euclidiano tridimensional para viabilizar sua visualização em um ambiente interativo e intuitivo 
 O uso de técnicas de replicação de dados em dispositivos_móveis  permite que uma aplicação móvel compartilhe dados com um servidor e possa atuar sobre tais dados durante períodos de desconexão . Embora essa característica seja fundamental em diversos_domínios  , a reconciliação das transações que foram aplicadas sobre a réplica móvel dos dados apresenta-se como um desafio a ser superado . O uso de bloqueios apresenta-se impraticável em determinados domínios de aplicação . Por outro_lado  , ao permitir a execução de operações de escrita em diversas réplicas sem uma sincronização a priori , o sistema se torna suscetível a conflitos de atualização , sendo necessário a implementação de um mecanismo de resolução de conflitos . Resolver conflitos é uma tarefa complexa e propensa a erros , em especial nos casos em que há a necessidade de intervenção humana . Diante desse cenário , foi desenvolvido um modelo para controle de transações em bancos de dados autônomos que faz_uso  de metadados e multiversão de banco de dados de forma a permitir a auditoria e retificação de resoluções de conflitos . Isso torna a resolução de conflitos uma operação não destrutiva , reduzindo , assim , o impacto de uma resolução de conflito incorreta . Neste trabalho é apresentado também um arcabouço para reconciliação de transações que implementa o modelo proposto . Como estudo de caso , o arcabouço desenvolvido foi utilizado para implementar a integração entre dois sistemas reais que possuem necessidades de replicação de dados e atualizações desconectadas 
 Nesta tese de doutorado , apresentamos uma abordagem para a observação das métricas de código-fonte , estudando-as através de suas distribuições e associações , além de discutir as relações de causalidade e implicações práticas-gerenciais para monitoramento das mesmas . Em nossos estudos avaliamos a distribuição e correlações dos valores das métricas de 38 projetos de software_livre  , dentre os com mais contribuidores ativos em seus repositórios . Para tal , coletamos e analisamos os valores para cada métrica em mais de 344.872 classes e módulos dos projetos avaliados . Complementarmente , para mostrarmos a utilidade do monitoramento de métricas , descrevemos uma extensão e adaptação do modelo de causalidade do conceito de atratividade de projetos de software_livre  , que indica uma relação estatística entre os valores das métricas de código-fonte e a quantidade de downloads , contribuidores e atualizações ( commits ) nos repositórios dos projetos . Para isso , realizamos estudos empíricos com milhares de projetos de software_livre  . Do ponto de vista prático , também contribuímos com um conjunto de ferramentas inovador para a automação da avaliação de projetos de software_livre  , com ênfase nos estudos e na seleção de métricas , o que permite a análise de código-fonte de acordo com a percepção de qualidade das comunidades de software_livre  . Entre as principais_contribuições  desta tese está uma análise detalhada , em relação ao comportamento , valores e estudos de caso , de 15 métricas de código-fonte , o que representa um avanço em comparação a literatura relacionada ao ampliar o número de métricas avaliadas e propor uma abordagem que visa diminuir as contradições das análises das métricas 
 Técnicas de modelagem baseada em imagens têm recebido considerável atenção da comunidade de visualização computacional devido ao potencial de criar cenas realistas a partir de um pequeno conjunto de imagens bi-dimensionais . Entretanto , a qualidade dos modelos gerados pelas ferramentas atualmente disponíveis é extremamente dependente de entradas fornecidas pelo usuário . Este trabalho propõe a execução do projeto de uma ferramenta de auxílio para sistemas de modelagem baseada em imagens que utiliza o conceito de contornos ativos para aumentar o grau de automação do processo de localização do contorno do objeto presente na fotografia , que servirá de guia para a posterior localização dos vértices desse objeto . Através desta abordagem , figuras geométricas mais simples , como pirâmides e hexaedros , puderam ser reconstruídas após a recuperação das coordenadas de seus 
 A análise de sentimentos é um campo de estudo com recente popularização devido ao crescimento da Internet e do conteúdo que é gerado por seus usuários , principalmente nas redes_sociais  , nas quais as pessoas publicam suas opiniões em uma linguagem coloquial e em muitos_casos  utilizando de artifícios gráficos para tornar ainda mais sucintos seus diálogos . Esse cenário é observado no Twitter , uma ferramenta de comunicação que pode facilmente ser usada como fonte de informação para várias ferramentas automáticas de inferência de sentimentos . Esforços de pesquisas têm sido direcionados para tratar o problema de análise de sentimentos em redes_sociais  sob o ponto de vista de um problema de classificação , com pouco consenso sobre qual é o classificador com melhor poder preditivo , bem como qual é a configuração fornecida pela engenharia de atributos que melhor representa os textos . Outro problema é que em um cenário supervisionado , para a etapa de treinamento do modelo de classificação , é imprescindível se dispor de exemplos_rotulados  , uma tarefa árdua e que demanda esforço humano em grande parte das aplicações . Esta tese tem por objetivo investigar o uso de agregadores de classificadores ( classifier ensembles ) , explorando a diversidade e a potencialidade de várias abordagens supervisionadas quando estas atuam em conjunto , além de um estudo detalhado da fase que antecede a escolha do classificador , a qual é conhecida como engenharia de atributos . Além destes aspectos , um estudo mostrando que o aprendizado não supervisionado pode fornecer restrições complementares úteis para melhorar a capacidade de generalização de classificadores de sentimento é realizado , fornecendo evidências de que ganhos já observados em outras áreas do conhecimento também podem ser obtidos no domínio em questão . A partir dos promissores resultados experimentais obtidos no cenário de aprendizado_supervisionado  , alavancados pelo uso de técnicas não supervisionadas , um algoritmo existente , denominado de C3E ( Consensus between Classification and Clustering Ensembles ) foi adaptado e estendido para o cenário semissupervisionado . Este algoritmo refina a classificação de sentimentos a partir de informações adicionais providas pelo agrupamento em um procedimento de autotreinamento ( self-training ) . Tal abordagem apresenta resultados promissores e competitivos com abordagens que representam o estado da arte em outros domínios 
 Este trabalho tem como principal objetivo o desenvolvimento de um método_numérico  para simulação das grandes escalas de escoamentos turbulentos tridimensionais utilizando uma modelagem de turbulência baseada em filtragem temporal ( denominada TLES - Temporal Large Eddy Simulation ) . O método desenvolvido combina discretizações temporais com ordem de mínima precisão 2 ( Adams-Bashforth , QUICK , Runge-Kutta ) , um método de projeção de ordem 2 , com discretizações espaciais também de ordem 2 obtidas pelo método de volumes finitos . Esta metodologia foi empregada na simulação de problemas teste turbulentos como o canal e a cavidade impulsionada , sendo este último resultado simulado pela primeira vez com modelagem TLES . Os resultados mostram uma excelente concordância quando comparado com resultados de simulações diretas ( DNS ) e dados experimentais , superando resultados clássicos obtidos com formulação LES com filtragem espacial 
 Nessa dissertação apresentamos um problema de programação da produção , motivado por uma indústria alimentícia caracterizada pela perecibilidade dos produtos , sequenciamento da produção dos lotes e pela necessidade de sincronização de recursos escassos para operação das linhas de produção . Em indústrias desse ramo , existem altos custos associados a estocagem dos produtos , a fim de evitar sua perda , de modo que é essencial a boa gestão dos processos industriais e do estoque . Modelos matemáticos de programação inteira_mista  foram desenvolvidos para tratar o problema , bem como o estudo da inclusão de diversas restrições da literatura para o tratamento da perecibilidade . Testes computacionais foram realizados para as validações dos modelos matemáticos , entretanto , devido à dificuldade de determinar soluções de boa_qualidade  pelo solver de otimização , foram propostos métodos heurísticos baseados na formulação_matemática  . Com o objetivo de mostrar o desempenho das heurísticas , comparamos as suas performances na resolução de instâncias da literatura e exemplares baseados no cenário produtivo da indústria com os resultados do solver 
 O teste de software é considerado uma atividade importante para a revelação de falhas . Apesar desta vantagem , tem sido pouco explorado no âmbito de aplicações de Realidade Virtual ( RV ) . Dentre as lacunas existentes , a definição e automatização de critérios de teste de software para esse domínio foi identificada , uma vez que esses sistemas possuem características próprias que requerem definição ou adaptação de técnicas de teste , fazendo com que aplicações nesse domínio constituam sistemas de alta complexidade . Diante disso , o objetivo desta tese é apresentar uma abordagem denominada Virtual Reality-Requirements Specification and Testing ( VR-ReST ) que visa apoiar a especificação de requisitos de aplicações de RV com base na descrição de casos de uso e conceitos do domínio de RV e Grafo de Cena ( GC ) , derivar requisitos de teste e gerar dados de teste a partir dos requisitos especificados . Além_disso  , é apresentado um apoio ferramental chamado de Virtual Requirements Specification and Testing ( ViReST ) , que permite automatizá-las . A abordagem é composta por três_módulos  : ( i ) especificação dos requisitos por meio do auxílio de um modelo denominado Virtual Requirements Specification ( ViReS ) ; ( ii ) mapeamento dos requisitos por meio de uma linguagem semi-formal chamada Behavior Language Requirement Specification ( BeLaRS ) para garantir uma especificação padronizada ; e ( iii ) geração_automática  dos requisitos de teste e dos dados de teste . Foi realizado um estudo de caso para avaliar a conformidade e a usabilidade da BeLaRS em auxiliar a especificação de requisitos de uma aplicação de RV . Além_disso  , também foi realizado um experimento para avaliar a eficácia da abordagem VR-ReST por meio da ferramenta ViReST . Usando teste de mutação neste último experimento , a abordagem VR-ReST alcançou um escore de mutação médio de 15,49 % maior que o teste aleatório . Portanto , os resultados mostraram que a abordagem , bem como o apoio ferramental , podem auxiliar o projetista durante a atividade de especificação de requisitos e o testador na geração dos testes para aplicações de RV 
 Estudos indicam que a utilização de aplicativos educacionais móveis vêm crescendo continuamente , possibilitando a alunos e professores maior flexibilidade e comodidade na execução de atividades e práticas educacionais . Embora várias instituições já tenham aderido à modalidade de aprendizagem móvel ( m-learning ) , sua adoção ainda traz problemas e desafios organizacionais , culturais e tecnológicos . Um destes problemas consiste em como avaliar adequadamente a qualidade dos aplicativos educacionais desenvolvidos . De fato , os métodos existentes para avaliação da qualidade de software ainda são muito genéricos , não contemplando aspectos específicos aos contextos pedagógico e móvel . Nesse cenário , o presente_trabalho  apresenta o método MoLEva , desenvolvido para avaliar a qualidade de aplicativos educacionais móveis . O método tem como base a norma ISO/IEC 25000 , sendo composto por : ( i ) modelo de qualidade ; ( ii ) métricas ; e ( iii ) critérios de julgamento . Para validar o método , foram realizados dois estudos de caso ; o primeiro consistiu na aplicação do MoLEva para avaliar o aplicativo do ENEM ; o segundo consistiu na aplicação do método para avaliação de aplicativos para o ensino de idiomas . A partir dos resultados obtidos , foi possível identificar problemas e pontos de melhoria nos aplicativos avaliados . Além_disso  , os estudos de caso conduzidos forneceram bons indicativos a respeito da viabilidade de uso do método MoLEva na avaliação de aplicativos educacionais móveis 
 Modelagem e controle de execução são duas_abordagens  do gerenciamento de processos de negócio que , embora complementares , têm se desenvolvido independentemente . Por um lado , a modelagem é normalmente conduzida por especialistas de negócio e explora aspectos semânticos do processo . Por outro_lado  , o controle de execução estuda mecanismos consistentes e eficientes de implementação . Este trabalho apresenta um método algorítmico que relaciona modelagem e controle de execução , por meio da geração de expressões algébricas a partir de digrafos acíclicos . Por hipótese , assumimos que modelos de processos de negócio são formados por estruturas baseadas em grafos , e mecanismos de controle de execução são baseados na interpretação de expressões de álgebra de processos . Para a geração de expressões algébricas , esta tese apresenta as propriedades topológicas de digrafos série-paralelo e define um sistema de transformação baseado em redução de digrafos . Além_disso  , um algoritmo de identificação de digrafos série-paralelo e geração de expressões algébricas é apresentado . O texto também discute o tratamento de digrafos que não são série-paralelo e apresenta , para alguns desses casos , soluções baseadas em mudanças topológicas . Finalmente , o algoritmo é ilustrado com o estudo de caso de uma aplicação real 
 A metodologia LES ( Large Eddy Simulation ) é uma alternativa viável para a solução_numérica  de escoamentos de interesse prático em virtude da limitação computacional imposta pela resolução direta de todas as escalas presentes em escoamentos turbulentos . Entretanto , a compreensão detalhada do fenômeno da turbulência é ainda uma tarefa desafiadora em consequência do seu comportamento não linear e alta sensibilidade às condições iniciais e de contorno . Dessa forma , o sucesso de simulações LES está associado à utilização de um código computacional eficiente , com modelagem submalha que represente corretamente a dinâmica do escoamento , juntamente com a especificação de condições iniciais turbulentas fisicamente consistentes . Nesse contexto , o presente_trabalho  tem como objetivo o desenvolvimento de um código LES de alta ordem aliado a um método de geração de perturbações para o estudo de escoamentos turbulentos em camada limite sobre superfície plana . Foi adotada a formulação vorticidadevelocidade . A metodologia numérica baseia-se no método de diferenças_finitas  em malhas colocalizadas , onde as derivadas nas direções longitudinal e normal ao escoamento são aproximadas usando diferenças compactas de alta ordem . Esse estudo assume periodicidade na direção transversal do escoamento e então um método espectral é adotado nessa direção . A integração temporal é feita através do método Runge-Kutta de 4a ordem e a solução da equação de Poisson se dá por meio de um método multigrid . Para a modelagem submalha é adotado o modelo WALE ( Wall-Adapting Local Eddy-viscosity ) . O método RFG ( Random Flow Generation ) foi responsável pela geração das flutuações de velocidade . Os resultados obtidos mostraram-se em boa concordância com os dados DNS ( Direct Numerical Simulation ) e LES presentes na literatura 
 Vehicular Ad hoc Network ( VANET ) é um tipo especial de Mobile Ad hoc Network ( MANET ) e é uma tecnologia chave para apoiar os Sistemas Inteligentes de Transporte ( ITSs ) . Ela desempenha um papel_importante  na implantação em grande escala das aplicações do tipo safety e non-safety . Dentre as aplicações non-safety , uma área importante e desafiadora é descoberta e divulgação de informação sobre serviços . Neste trabalho , nós adotamos a estratégia de beaconing e as comunicações veiculares oportunistas para resolver o problema da descoberta e divulgação de serviços ao longo de rodovias . Nossa abordagem oferece várias vantagens em relação aos métodos tradicionais usando painéis , painéis eletrônicos digitais e outdoors , por exemplo . Baixo custo , atinge um grande número de clientes em tempo real , maior controle sobre a campanha de propaganda , atualização dinâmica de conteúdo e descoberta de serviços antes de alcançar a região de negócio são algumas delas . Para a divulgação de serviços ao longo de rodovias , seguimos a estratégia de publicidade push-based e usamos somente comunicações do tipo Infrastructure-to-Vehicle ( I2V ) . Por outro_lado  , para a descoberta destes serviços pelos motoristas , utilizamos a estratégia de publicidade pull-based e comunicações oportunistas do tipo Vehicle-to-Vehicle ( V2V ) . A fim de contribuir para a campanha de propagandas , foi utilizado o mecanismo store-carry-and-response . Nossa primeira contribuição é o Extended Opportunistic Service Discovery Protocol ( EOSDP ) . O EOSDP implementa e estende a versão original do Opportunistic Service Discovery Protocol ( OSDP ) via experimentos por simulação . Nós utilizamos os simuladores Simulation of Urban Mobility ( SUMO ) , Objective Modular Network Testbed in C++ ( OMNET++ ) e o Vehicles in network simulation ( Veins ) . Nossa segunda contribuição é o Service Advertisement Protocol ( SADP ) . SADP é um protocolo leve baseado na estratégia de beaconing para divulgação de serviços em VANETs . Ele não requer conexão com a Internet para anunciar os serviços nas margens das rodovias . Além_disso  , a modelagem do SADP é baseada no padrão de protocolos VANETs . O desempenho do SADP foi avaliado em um cenário de rodovia congestionada , com velocidades variáveis dos veículos e sob diferentes frequências de comunicação broadcast . Por fim , apresentamos o nosso último protocolo chamado Commercial Services Discovery Protocol ( CSDP ) . Como o protocolo anterior , o CSDP é um protocolo leve para descoberta de serviços baseado em beaconing para VANETs . O CSDP depende somente de comunicações V2V oportunistas para a descoberta de serviços ao longo de rodovias . O protocolo foi implementado no cenário de uma rodovia congestionada e avaliado com um extenso conjunto de experimentos de simulação , sob diferentes parâmetros como , por exemplo , Query Interval ( QI ) , Query Attempts ( QA ) e Time to Live ( TTL ) 
 A modelagem computacional do olho humano tem sido largamente estudada por diferentes setores da comunidade_científica  e tecnológica . Uma das principais razões para esse crescente_interesse  é a possibilidade de reproduzir propriedades ópticas por meio de simulações computacionais , tornando possível o desenvolvimento de dispositivos eficientes para tratar e corrigir os problemas da visão . Os resultados dessa intensa pesquisa já podem ser observados nos equipamentos para medições de distorções do sistema visual , como os topógrafos de córnea , que sofreram uma rápida evolução nos últimos_anos  . Diferente do que ocorreu em outras áreas da ciência , como na medicina por exemplo , o aprimoramento dos dispositivos de medição ocular não tem influenciado de forma significativa a modelagem computacional do sistema visual . Como pode ser notado a partir da literatura ( Gullstrand , 1909 ) , ( Emsley , 1952 ) , ( Kooijman , 1983 ) , 4 ( L.N . Thibos , 1999a ) , a maioria dos trabalhos descritos ainda emprega modelos puramente teóricos na simulação dos processos visuais , não fazendo_uso  das informações adquiridas pelos equipamentos modernos desenvolvidos para a oftalmologia . Esse projeto de mestrado explora exatamente esse aspecto ainda pouco investigado da modelagem do sistema visual . Em conjunto com membros do grupo de óptica oftalmológica do Departamento de Física da USP de São Carlos , o projeto propõe um arcabouço computacional que viabiliza a utilização de dados reais « a modelagem e simulação do sistema visual humano . Essa nova abordagem possibilita a investigação individualizada do sistema óptico , possibilitando a construção de novas técnicas as quais possibilitarão inferir dados de vital importância em exames médicos , exames estes anteriormente apenas disponíveis a um alto custo , isto quando existentes 
 Este trabalho relata replicações do experimentos e uma experiência de transferência de tecnologia desenvolvida no âmbito acadêmico para o contexto industrial . No escopo deste trabalho foram conduzidas duas replicações do experimento para avaliar a efetividade e eficiência da técnica de leitura PBB , sendo uma realizada em ambiente acadêmico e a outra em ambiente industrial , e foi iniciada a transferência tecnológica da técnica PBR para uma empresa , utilizando o processo de experimentação como base em pacotes de laboratório . Os principais resultados deste estudo são sintetizados e analisados sob a ótica do modelo de compartilhamento do conhecimento no contexto de experimentação ( EKSM - Experimentation Knowledge Sharing Model ) e do paradigma de melhoria da experimentação ( EIP - Experimentation Improvement Paradigm ) . Em função dessa experiência observa-se que a experimentação traz contribuições na perspectiva da cooperação entre a indústria e a academia , possibilitando a validação de novas tecnologias , a evolução de pacotes do laboratório o a obtenção de resultados de projetos de uso real 
 A popularização dos dispositivos_móveis  em todas as camadas sociais tem motivado o desenvolvimento de aplicações educacionais móveis , denominadas aplicações de m-learning . Neste cenário , as aplicações existentes , mesmo possuindo diversos benefícios e facilidades no que diz_respeito  ao ensino e aprendizagem , apresentam problemas e desafios relacionados , sobretudo no que se refere ao desenvolvimento , reuso e padronização arquitetural . Por outro_lado  , na vertente do reúso de software , percebe-se uma crescente adoção do conceito de Linha de Produtos de Software ( LPS ) . Esse paradigma possibilita às organizações explorar as similaridades e variabilidades de seus produtos , aumentando a reutilização de artefatos e , como consequência , diminuindo custos e tempo de desenvolvimento . Neste trabalho é apresentada uma LPS voltada ao domínio das aplicações de m-learning , denominada M-SPLearning . A proposição da M-SPLearning envolveu desde o estudo inicial para a obtenção de uma análise de domínio efetiva , até a implementação dos componentes previamente analisados . A LPS concebida teve seus respectivos produtos avaliados experimentalmente no cenário industrial , fornecendo evidências de que sua utilização pode acelerar o time-to-market de produtos de m-learning , com um número reduzido de defeitos 
 Contexto : A Anotação de Papéis Semânticos ( APS ) é uma tarefa da área de Processamento de Línguas_Naturais  ( PLN ) que permite detectar os eventos descritos nas sentenças e os participantes destes eventos ( Palmer et al. , 2010 ) . A APS responde perguntas como Quem ? , Quando ? , Onde ? , O quê ? , e Por quê ? , dentre outras e , sendo assim , é importante para várias aplicações de PLN . Para anotar automaticamente um texto com papéis semânticos , a maioria dos sistemas atuais emprega técnicas de Aprendizagem de Máquina ( AM ) . Porém , alguns papéis semânticos são previsíveis e , portanto , não necessitam ser tratados via AM . Além_disso  , a grande maioria das pesquisas desenvolvidas em APS tem dado foco ao inglês , considerando as particularidades gramaticais e semânticas dessa língua , o que impede que essas ferramentas e resultados sejam diretamente transportados para outras línguas . Revisão da Literatura : Para o português do Brasil , há três trabalhos finalizados recentemente que lidam com textos jornalísticos , porém com performance inferior ao estado da arte para o inglês . O primeiro ( Alva- Manchego , 2013 ) obteve 79,6 de F1 na APS sobre o córpus PropBank.Br ; o segundo ( Fonseca , 2013 ) , sem fazer uso de um treebank para treinamento , obteve 68,0 de F1 sobre o córpus PropBank.Br ; o terceiro ( Sequeira et al. , 2012 ) realizou anotação apenas dos papéis Arg0 ( sujeito prototípico ) e Arg1 ( paciente prototípico ) no córpus CETEMPúblico , com performance de 31,3 pontos de F1 para o primeiro papel e de 19,0 de F1 para o segundo . Objetivos : O objetivo desse trabalho de mestrado é avançar o estado da arte na APS do português brasileiro no gênero jornalístico , avaliando o desempenho de um sistema de APS treinado com árvores sintáticas geradas por um parser automático ( Bick , 2000 ) , sem revisão humana , usando uma amostragem do córpus PLN-Br . Como objetivo adicional , foi avaliada a robustez da tarefa de APS frente a gêneros diferentes , testando o sistema de APS , treinado no gênero jornalístico , em uma amostra de revisões de produtos da web . Esse gênero não foi explorado até então na área de APS e poucas de suas características foram formalizadas . Resultados : Foi compilado o primeiro córpus de opiniões sobre produtos da web , o córpus Buscapé ( Hartmann et al. , 2014 ) . A diferença de performance entre um sistema treinado sobre árvores revisadas e outro sobre árvores não revisadas ambos no gênero jornalístico foi de 10,48 pontos de F1 . A troca de gênero entre as fases de treinamento e teste , em APS , é possível , com perda de performance de 3,78 pontos de F1 ( córpus PLN-Br e Buscapé , respectivamente ) . Foi desenvolvido um sistema de inserção de sujeitos não expressos no texto , com precisão de 87,8 % no córpus PLN-Br e de 94,5 % no córpus Buscapé . Foi desenvolvido um sistema , baseado em regras , para anotar verbos auxiliares com papéis semânticos modificadores , com confiança de 96,76 % no córpus PLN-Br . Conclusões : Foi mostrado que o sistema de Alva-Manchego ( 2013 ) , baseado em árvores sintáticas , desempenha melhor APS do que o sistema de Fonseca ( 2013 ) , independente de árvores sintáticas . Foi mostrado que sistemas de APS treinados sobre árvores sintáticas não revisadas desempenham melhor APS sobre árvores não revisadas do que um sistema treinado sobre dados gold-standard . Mostramos que a explicitação de sujeitos não expressos nos textos do Buscapé , um córpus do gênero de opinião de produtos na web , melhora a performance da sua APS . Também mostramos que é possível anotar verbos auxiliares com papéis semânticos modificadores , utilizando um sistema baseado em regras , com alta confiança . Por fim , mostramos que o uso do sentido do verbo , como feature de AM , para APS , não melhora a perfomance dos sistemas treinados sobre o PLN-Br e o Buscapé , por serem córpus pequenos 
 A robótica tem passado por uma notável evolução ao longo dos últimos_anos  , juntamente com um crescente_interesse  por parte da sociedade . Robôs não são mais exclusivamente produzidos para realizar atividades repetitivas em fábricas , eles têm sido projetados para apoiar humanos em diversos e importantes domínios de aplicação . Os sistemas robóticos utilizados para controlar tais robôs têm , portanto , se tornado maiores , mais complexos e difíceis de desenvolver . Nesse cenário , a Arquitetura Orientada a Serviços ( do inglês , Service-Oriented Architecture - SOA ) tem sido investigada como um promissor estilo arquitetural para o desenvolvimento de sistemas robóticos de forma mais flexível , reusável e produtiva . Embora um número considerável de Sistemas Robóticos Orientados a Serviços ( do inglês , Service-Oriented Robotic Systems - SORS ) já exista , grande parte deles têm sido desenvolvida de maneira ad hoc . A pouca atenção e o suporte limitado ao projeto das arquiteturas de software de SORS pode não só impedir a obtenção dos benefícios associados à adoção da SOA , mas também reduzir a qualidade dos sistemas robóticos que , frequentemente , são utilizados em contextos de segurança crítica . Essa tese tem por objetivo aprimorar o entendimento e a sistematização do projeto arquitetural de SORS . Para isso , é proposta uma taxonomia de serviços para o domínio de robótica , bem como um processo e uma arquitetura de referência para sistematizar o projeto das arquiteturas de software de SORS . Os resultados obtidos evidenciam que tanto o processo quanto a arquitetura de referência podem impactar positivamente na qualidade das arquiteturas de software de SORS e , consequentemente , contribuir para o desenvolvimento de sistemas robóticos 
 Aplicativos educacionais podem promover diversos benefícios a professores e alunos , desde a redução de tarefas repetitivas à realização de atividades impossíveis com o uso de apenas lousa e papel . Módulos de Aprendizagem Interativa ( iMA ) são uma família de aplicativos educacionais que fornecem atividades interativas integradas a Sistemas de Gerenciamento de Cursos . O desenvolvimento de iMA , similarmente ao de outros sistemas criados em contexto de projetos acadêmicos , enfrenta problemas relacionados a manutenção e evolução de software , que afetam suas contribuições à sociedade . Isso é provocado principalmente pela degradação do código com o tempo e dos métodos ad-hoc utilizados , sem sistematização do processo ou procedimentos explícitos para prevenção desses problemas . Com o objetivo de aprimorar esse processo , esta dissertação apresenta uma Linha de Produto de Software ( LPS ) criada para o desenvolver iMA . O método usado para criá-la envolveu análise do domínio , formado pelos iMA existentes , a elaboração de um modelo de sistema para definir as responsabilidades das características obrigatórias , variantes e opcionais aos aplicativos , e a implementação de um arcabouço de aplicação . Assim , essa LPS consiste nesse arcabouço e seus manuais de funcionamento interno , utilização e evolução , promovendo reúso de código , arquitetura e processo . A avaliação deste trabalho foi conduzida com uma prova de conceito e um estudo de caso . A prova de conceito descreve o desenvolvimento de um novo iMA , chamado iTangran , apresentando a factibilidade da utilização da LPS para essa tarefa . O estudo de caso investigou com maior profundidade o impacto da LPS sobre a criação da nova versão de um aplicativo existente , chamado iVProg . Os resultados obtidos_mostram  que o oferecimento de um processo e uma arquitetura que guiam as tarefas do programador de iMA teve grande influência na sua percepção de produtividade e satisfação , além de contribuírem para a qualidade do código criado e sua documentação , fatores essenciais para a prevenção dos problemas de desenvolvimento enfrentados atualmente 
 A base de dados do Programa de Melhoramento Genético da Raça Nelore está crescendo consideravelmente , com isso , a criação de um ambiente que dê apoio à análise dos dados do Programa é de fundamental_importância  . As tecnologias que são utilizadas para a criação de um ambiente analítico são os processos de Data Warehousing e de Data_Mining  . Neste trabalho , foram construídos um Data Warehouse e consultas OLAP para fornecer visões multidimensionais dos dados . Além das análises realizadas com as consultas , também foi utilizada uma ferramenta de Data_Mining  Visual . O ambiente analítico desenvolvido proporciona aos pesquisadores e criadores do Programa um maior poder de análise de seus dados . Todo o processo de desenvolvimento desse ambiente é aqui apresentado 
 O desenvolvimento de arquiteturas de controle para múltiplos robôs em ambientes dinâmicos tem sido tema de pesquisas na área de robótica . A complexidade deste tema varia de acordo com as necessidades exigidas da equipe de robôs . Em geral , espera-se que os robôs colaborem uns com os outros na execução de uma tarefa . Além_disso  , cada robô deve ser capaz de planejar trajetórias e replanejá-las em caso de situações inesperadas . No presente_trabalho  , propomos uma Arquitetura de Controle Inteligente para múltiplos robôs denominada ACIn . Para esta finalidade , foram investigadas algumas técnicas utilizadas para o controle inteligente de robôs , tais como , Redes_Neurais  Artificiais , Campos Potenciais e Campos Potenciais baseados em Problema do Valor de Contorno ( PVC ) . Tais técnicas , normalmente utilizadas para um único robô , foram adaptadas para tornar possível o controle de múltiplos robôs sob arquitetura ACIn . Uma outra contribuição deste trabalho refere-se ao aperfeiçoamento da técnica de Campos Potenciais baseada PVC denominada Campos Potenciais Localmente Orientados ( CPLO ) . Este aperfeiçoamento foi proposto para suprir a deficiência das técnicas baseadas em PVC quando estas são aplicadas em ambientes com múltiplos robôs . Além_disso  , um Sistema Baseado em Regras ( SBR ) também foi proposto como parte integrante da arquitetura ACIn . O objetivo do SBR é caracterizar a funcionalidade de cada robô para uma determinada tarefa . Isto se faz necessário para que o comportamento dos integrantes da equipe de robôs não seja competitivo e sim colaborativo . Por fim , através dos experimentos utilizando o simulador oficial de futebol de robôs da FIRA , observou-se que a arquitetura de controle inteligente ( ACIn ) implementada com a técnica de planejamento CPLO e SBR propostos , mostrou-se robusta e eficiente no controle de múltiplos 
 Esse projeto implementa urna Interface Básica para um Servidor Universal ( IBSU ) . A IBSU provê um ambiente para executar agentes de software e interfaces seguras entre estes e o banco de dados de um servidor universal . Os agentes têm acesso a roots ( pontos de entrada para objetos ) armazenados no banco de dados . A IBSU provê um ambiente aberto e seguro para a execução de agentes . Suas principais funções são receber os agentes , autenticá-los e prover acesso aos roots do banco de dados e aos recursos do sistema . Contudo , a IBSU não permitirá que um agente acesse um root ou recurso do sistema se este não tiver permissão para tal . A IBSU é composta por quatro partes que garantem a abertura e segurança do ambiente de execução dos agentes : a Interface do Banco de Dados , o Gateway , o Pool de agentes e o Gerenciador de Segurança . A Interface do Banco de Dados define métodos que permitem aos agentes manipular grupos , roots e associar permissões de acesso entre roots e grupos no banco de dados . O Gateway recebe os agentes de hosts remotos e os autentica . Essa autenticação é feita verificando-se a assinatura digital , os certificados associados ao agente e a qual grupo ele pertence . Se o processo de autenticação tiver êxito , o agente pode se juntar ao Pool . O Pool de agentes executa os agentes e controla seu tempo total de vida . Esse tempo depende do grupo do qual esse agente faz parte . O Gerenciador de Segurança garante que os agentes que estão executando no Pool não conseguirão acesso a recursos que eles não tenham permissão para utilizar . A IBSU e os agentes de software que executam no Pool são implementados em Java . Esses agentes têm , no servidor universal , o mesmo papel que as linguagens de consulta ( como SQL ) têm nos bancos de dados relacionais . Mas como eles têm a vantagem de possuir todo o poder de computação do ambiente Java ( incluindo o maior poder de programação , abertura e segurança ) , eles podem realizam esse papel de forma muito mais eficiente 
 Tarefas envolvendo Reconhecimento de Padrões vêm se tornando mais freqüentes em diferentes domínios de aplicação . A maioria destas tarefas tem sido eficientemente tratada através da utilização de Redes_Neurais  Artificiais . Entre os modelos de Redes_Neurais  mais difundidos , destaca-se o modelo Perceptron Multi-Camadas ( Multi-Layer Perceptron ou MLP ) . Entretanto , o desempenho de uma Rede_Neural  MLP em um determinado problema depende diretamente da topologia adotada , que deve ser determinada no inicio do processo de treinamento . A escolha da topologia de uma Rede_Neural  não é trivial , normalmente resultando em uma busca exaustiva pela configuração mais apropriada . Com o objetivo de auxiliar a determinação da topologia de uma Rede_Neural  , vários métodos foram desenvolvidos para a automação deste processo , entre os quais encontram-se as Redes_Neurais  Construtivas . Estas redes utilizam Algoritmos Construtivos que , a partir de uma rede mínima , inserem gradualmente novos neurônios e conexões durante o treinamento , procurando melhorar o desempenho da mesma . Contudo , a avaliação da melhor aplicação de diferentes Algoritmos Construtivos em um mesmo problema depende da homogeneidade do seu ambiente de treinamento . Este trabalho fornece a definição de um conjunto de classes abstratas para permitir que diferentes algoritmos de treinamento , incluindo Algoritmos Construtivos , sejam criados como componentes com acesso estritamente definido para futura utilização em diferentes aplicações . Através do uso destes componentes em uma nova versão do Simulador para Redes_Neurais  Artificiais Kipu , a análise da eficiência de Redes_Neurais  Construtivas em tarefas reais de Reconhecimento de Padrões teve início 
 Neste trabalho de mestrado foi estudado um processo de geração de imagens a partir de dados volumétricos dentários . Para isso foi elaborado e implementado um método de Rendering Volumétrico Direto que faz_uso  de texturas volumétricas ( DVRT Direct Volume Rendering witk-Texture ) . Esse método funciona criando um mapa de textura tridimensional a partir do volume de dados e então amostrando-o com planos de cortes paralelos entre si e ao plano de visão , a imagem ftnal é então obtida pela composição desses planos . Para facilitar a compreensão do volume submetido à visualização foi implementada a possibilidade de gerar imagens tonalizadas , obtidas a partir da interação do vota= de dados com as luzes direcionais presentes na cena . O método DVRT foi implementado na forma de classes incorporadas à biblioteca de visualização orientada a objetos de propósito_geral  , o VTK ( Visualization Toolkit ) , apresentando resultados_satisfatórios  
 Os Jogos de Empresa são amplamente_utilizados  para treinamento na área de administração de empresas por proporcionarem uma forma prática e experimental de aprendizado . Esta dissertação explora o uso de modelos baseados na Dinâmica de Sistemas para criação de Jogos de Empresa para o treinamento de gerentes de projeto de software . Isso é feito apresentando-se um jogo para a gestão de recursos humanos em projetos de software baseado em um modelo dinâmico desenvolvido por Abdel-Hamid e Madnick na década de 1980 . Este trabalho apresenta também a extensão de uma ferramenta desenvolvida para edição e simulação de sistemas , Cycles , para dar_apoio  automatizado à criação de jogos baseados na Dinâmica de Sistemas . Essa ferramenta estendida permite que várias equipes possam jogar e analisar suas decisões . O coordenador das equipes jogadoras pode estabelecer os parâmetros e metas do jogo e acompanhar on-line o desenrolar do jogo 
 Este trabalho propõe uma metodologia para avaliação de desempenho de políticas de escalonamento de processos . Essa metodologia , projetada para ser simples e flexível , é composta de dez passos , cada um definindo características da política ou abordagem da avaliação . A modelagem e simulação da DPWP Dynamical Politics Without Preempition , uma política de escalonamento `` CPU-Bound '' , são usadas como um exemplo do uso da metodologia_proposta  . Essa política é modelada usando Redes de Petri Estocásticas Generalizadas , e simulada usando o ALPHA/Sim , um ambiente para simulação de Redes de Petri . Os resultados obtidos_mostram  que a metodologia atinge plenamente seus objetivos , permitindo uma avaliação simples e útil de políticas de escalonamento de processos 
 Este trabalho envolve estudos na área de Modelagem Geométrica , e descreve a implementação de um Modelador de Superflcies Implicitas em R3 . Esse modelador tem objetivo de anyiliar o grupo de Mecânica de Fluidos Computacional do ICMC na definição de domínios complexos para realização de simulações de escoamentos de fluidos . O Modelador utiliza a técnica de Modelagem Implicita . Um modelo é definido por uma árvore CSG ( Constructive Solid Geometty ) . As primitivas que podem constituir essa árvore são : primitivas pré-definidas como esfera , cilindros , etc . , e. primitivas baseadas em esqueletos definidos por pontos , segmentos de reta ou poligonos planares . O Modelador é constituído por três componentes : a Interface de Modelagem , responsável pela definição da árvore CSG que representa o modelo implícito ; o Poligonalizador , que transforma o modelo implícito em uma malha de polígonos ; e o Visnalizador que apresenta essa malha de polígonos 
 Diante das inúmeras opções de pacotes de software que existem atualmente para as aplicações de Sistemas de Informação e da dificuldade que um cliente tem para escolher aquele pacote que atenda melhor suas necessidades , neste trabalho é apresentado um procedimento para escolha de pacote de software na área de Sistemas de Informação . Nesse procedimento são utilizadas a norma NBR 12119 da Associação Brasileira de Normas Técnicas ( ABNT ) e o Quality Function Deployment QFD do American Supplier lnstitute ( ASI ) . Também são apresentados dois estudos de caso da aplicação do procedimento para escolha de pacote de software 
 O problema de dimensionamento de lotes em sistemas de produção multiestágios , no qual os produtos dependem da produção e/ou compra de outros componentes , consiste na determinação das quantidades de itens a serem produzidas em diferentes períodos , de tal modo que a demanda seja atendida . O problema é formulado como um problema de programação matemática inteiro-misto e utiliza o conceito de estoque de escalão . O objetivo é de minimizar os custos de produção , estoque e de preparação ( setup ) sujeito a um conjunto de restrições , no caso , limitações de capacidade e de balanço de estoque . Para a resolução desse problema , foram desenvolvidos métodos heurísticos utilizando a técnica de relaxação Lagrangiana com o objetivo de obter planos factíveis e buscar soluções de qualidade . Os resultados dos testes computacionais são comparados com a solução ótima em um conjunto de exemplos de pequeno_porte  e , para exemplos maiores , os resultados são comparados com um limitante inferior obtido por relaxação Lagrangiana 
 Este trabalho apresenta um modelacior de movimentos para integrar os sistemas Freeflow- 2D e Freefiow-3D . Estes sistemas compreendem um ambiente de modelagem , simulação e visualização de escoamentos do tipo incompressíveis com superfícies_livres  . O movimento de cada objeto utilizado durante a simulação é definido a partir do seu centro de massa . Foram implementados três tipos de movimento : rotação , harmônico simples e interpolação por splines cúbica . Exemplos de simulações que envolvem problemas com objetos em movimento são apresentados , confirmando o desempenho do modelador de movimentos 
 O problema de estimação do número de espécies em uma população é um tema de grande relevância em várias áreas de pesquisa , principalmente em estudos ecológicos . Nesta dissertação apresentamos uma metodologia Baye , siana , via amostrador de Gibbs e Metropolis-Hastings , para os casos onde as espécies são equiprováveis e não equiprováveis . Formulamos um procedimento Bayesiano baseado em estatísticas suficientes e discutimos as suas vantagens . Ilustrações numéricas da metodologia_proposta  , baseada em dados reais , foram apresentadas 
 O aumento da complexidade dos problemas resolvidos por computador tem tornado importante a utilização de processamento paralelo . O uso de computadores convencionais conectados por redes de comunicação de alta velocidade é uma alternativa viável e de baixo custo em relação aos MPPs ( Massively Parallel Processors ) em aplicações que demandam alto poder_computacional  . Uma biblioteca de alto nível muito utilizada para a programação paralela , que utiliza o paradigma de passagem de mensagens , é o PVM ( Parallel Virtual Machine ) . PVM utiliza TCP/IP ( sockets ) para comunicação entre os processos paralelos . Este trabalho de mestrado demonstra que pode-se atingir um desempenho maior em comunicação se Fast Messages for utilizado em vez de TCP/IP no PVM . Uma biblioteca , denominada Sock2Fm , que utiliza a semântica de sockets e usa Fast Messages em baixo_nível  , foi desenvolvida e foram realizados testes para aferição de seu desempenho . Essa biblioteca também pode ser utilizada diretamente para troca de mensagens na rede , sendo mais adequada para o programador acostumado com sockets do que com o uso das primitivas de Fast Messages . Testes mostram que Sock2Fm apresenta um desempenho melhor que TCP/IP para mensagens maiores que 250 bytes ( 79 % melhor para alguns tamanhos de pacotes ) .Também foi aferido o desempenho de PVM sobre Ethernet a 10 Mbps , 100 Mbps e sobre Myrinet com TCP/IP para comparação dos resultados 
 Esse trabalho aborda o problema de dimensionamento lotes com restrições de capacidade em sistemas multiestágios de produção , que consiste basicamente em determinar a quantidade e o período para produzir determinados produtos num dado horizonte de tempo de tal modo que uma certa demanda seja atendida . Em sistemas multiestágios de produção o planejamento de cada produto ainda depende do planejamento de outros , situados em níveis hierárquicos inferiores . Os modelos apresentados incluem custos e tempos de preparação , com o objetivo de melhor representar as características dos problemas reais . Devido a complexidade envolvida na sua resolução , desenvolvemos métodos baseados em meta-heurísticas evolutivas , mais especificamente algoritmos genéticos e meméticos . As técnicas propostas foram avaliadas em conjuntos de exemplos numéricos gerados aleatoriamente . Os resultados obtidos foram comparados com a solução ótima para os exemplos de pequeno_porte  , e com um limitante inferior obtido pela aplicação da Relaxaçã ' oLagrangiana ao problema , para os exemplos de médio porte 
 Este trabalho consiste no estudo de métodos de otimização aplicados em um problema de controle para sistemas_lineares  com saltos markovianos ( SLSM ) . SLSM formam uma importante classe de sistemas que têm sido muito úteis em aplicações envolvendo sistemas sujeitos a falhas e outras alterações abruptas de comportamento . Este estudo enfoca diferentes métodos para resolução deste problema . Comparamos o método variacional com o de Newton , sob o ponto de vista do número de problemas resolvidos e pelo nível de sub-otimalidade obtido ( relação entre os custos obtidos por estes métodos ) . Também propomos um novo método , o qual pode ser inicializado com soluções de equações de Riccati acopladas , e o comparamos com o método variacional . Além_disso  , para a comparação dos métodos , propomos um algoritmo que gerou dez mil 
 Este trabalho apresenta um método_numérico  para simular_escoamentos  viscoelásticos bidimensionais governados pela equação_constitutiva  Giesekus [ Schleiniger e Weinacht 1991 ] . As equações governantes são resolvidas pelo método de diferenças_finitas  numa malha_deslocada  . A superfície_livre  do fluido é modelada por partículas marcadoras possibilitando assim a sua visualização e localização . O cálculo da velocidade é efetuado por um método implícito enquanto a pressão é calculada por um método explícito . A equação_constitutiva  de Giesekus é resolvida pelo método de Euler modificado explícito . O método_numérico  desenvolvido nesse trabalho é verificado comparando-se a solução_numérica  com a solução analítica para o escoamento de um fluido Giesekus em um canal . Resultados de convergência são obtidos pelo uso de refinamento de malha . Os resultados alcançados incluem um estudo da aplicação do modelo Giesekus para simular o escoamento numa contração planar 4:1 e o problema de um jato incidindo sobre uma placa rígida , em que o fenômeno jet buckling é simulado 
 Este projeto de doutorado tem como objetivo definir modelos de negócio para ambientes de computação em nuvem que consideram desempenho e segurança como atributos de qualidade de serviço durante a definição do contrato . Para isso , foi necessário quantificar o impacto causado no desempenho de um ambiente em nuvem quando diferentes mecanismos de segurança foram utilizados . Para a quantificação da sobrecarga foram utilizadas técnicas e metodologias disponíveis na literatura que visam garantir a integridade , disponibilidade e confidencialidade dos dados , abordando desafios que envolvem o acesso , armazenamento e manipulação de dados em serviços oferecidos por meio de máquinas virtuais . Experimentos executados possibilitaram analisar o comportamento das variáveis de resposta na utilização de cenários com diferentes mecanismos de segurança e cargas . Dessa forma , foi possível confrontar a sobrecarga imposta pelos mecanismos de segurança com a alteração da quantidade de recursos aplicada por um módulo proposto , chamado ReMM . De acordo com os resultados , o ReMM alterou a quantidade de recursos virtuais alocados utilizando dois algoritmos de escalabilidade , garantindo as exigências definidas no contrato de níveis de serviço . No entanto , a alteração dos recursos_computacionais  para contrapor a sobrecarga imposta pelos mecanismos de segurança impactou nos custos finais dos serviços . Dessa forma , a sobrecarga de segurança , desempenho e custo foram considerados na definição dos modelos de negócios em diferentes ambientes de computação em nuvem 
 Devido ao grande número de otimizações fornecidas pelos compiladores modernos e à ampla possibilidade de ordenação dessas transformações , uma eficiente Exploração do Espaço de Projeto ( DSE ) se faz necessária para procurar a melhor sequência de otimização de uma determinada função ou fragmento de código . Como esta exploração é uma tarefa complexa e dispendiosa , apresentamos uma nova abordagem de DSE capaz de reduzir esse tempo de exploração e selecionar sequências de otimização que melhoraram o desempenho dos códigos transformados . Nossa abordagem utiliza um conjunto de funções de referência , para as quais uma representação simbólica do código ( DNA ) e a melhor sequência de otimização são conhecidas . O DSE de novas funções é baseado em uma abordagem de agrupamento aplicado sobre o código DNA que identifica similaridades entre funções . O agrupamento utiliza três técnicas para a mineração de dados : distância de compressão normalizada , algoritmo de reconstrução de árvores filogenéticas ( Neighbor Joining ) e identificação de grupos por ambiguidade . As otimizações das funções de referência identificadas como similares formam o espaço que é explorado para encontrar a melhor sequência para a nova função . O DSE pode utilizar o conjunto reduzido de otimizações de duas formas : como o espaço de projeto ou como a configuração inicial do algoritmo . Em ambos os casos , a adoção de uma pré-seleção baseada no agrupamento permite o uso de algoritmos de busca simples e rápidos . Os resultados experimentais revelam que a nova abordagem resulta numa redução significativa no tempo total de exploração , ao mesmo tempo que alcança um desempenho próximo ao obtido através de uma busca mais extensa e dispendiosa baseada em algoritmos genéticos 
 Vários fenômenos naturais e artificiais compostos de partes interconectadas vem_sendo  estudados pela teoria de redes complexas . Tal representação permite o estudo de processos dinâmicos que ocorrem em redes complexas , tais como propagação de epidemias e rumores . A evolução destes processos é influenciada pela organização das conexões da rede . O tamanho das redes do mundo_real  torna a análise da rede inteira computacionalmente proibitiva . Portanto , torna-se necessário representá-la com medidas topológicas ou amostrá-la para reduzir seu tamanho . Além_disso  , muitas redes são amostras de redes maiores cuja estrutura é difícil de ser capturada e deve ser inferida de amostras . Neste trabalho , ambos os problemas são estudados : a influência da estrutura da rede em processos de propagação e os efeitos da amostragem na estrutura da rede . Os resultados obtidos sugerem que é possível predizer o tamanho da epidemia ou do rumor com base em um modelo de regressão beta com dispersão variável , usando medidas topológicas como regressores . A medida mais influente em ambas as dinâmicas é a informação de busca média , que quantifica a facilidade com que se navega em uma rede . Também é mostrado que a estrutura de uma rede amostrada difere da original e que o tipo de mudança depende do método de amostragem utilizado . Por fim , quatro métodos de amostragem foram aplicados para estudar o comportamento do limiar epidêmico de uma rede quando amostrada com diferentes taxas de amostragem . Os resultados sugerem que a amostragem por busca em largura é a mais adequada para estimar o limiar epidêmico entre os métodos comparados 
 A engenharia de requisitos em projetos de software_livre  é uma atividade de segunda classe , ao menos em face ao estado da arte da área . Um exemplo claro disso é a inexistência de especificações de requisitos nesses projetos . No entanto , softwares livres são reconhecidos como produtos de elevada qualidade e não é possível produzir softwares de sucesso sem que os requisitos de seus usuários sejam satisfeitos . Portanto , existe um processo de engenharia de requisitos , ainda que não formalmente definido . De fato , recentes estudos sobre o processo de desenvolvimento de software_livre  demonstraram que os requisitos são publicamente declarados a posteriori do desenvolvimento do código , dependendo das habilidades do desenvolvedor para a correta elicitação , análise e especificação dos requisitos . A natureza iterativa e aberta do desenvolvimento , com ciclos rápidos e resultados publicamente discutidos , permite que erros sejam detectados prematuramente , o que diminui o esforço necessário para as correções , viabilizando o processo de produção de software_livre  . Porém , existe a constante preocupação da documentação apenas do código-fonte e não dos requisitos . Uma das causas é que não existe uma ferramenta apropriada para armazenar esses requisitos e disponibilizá-los ao público , precisando os desenvolvedores recorrer a arquivos textos ou páginas_Web  cujo gerenciamento é trabalhoso ao ponto de sua constante atualização ser comprometida . Uma solução para o problema é a adoção de ferramentas ágeis de edição colaborativa para a Web , que permitam a rápida atualização dos documentos de requisitos por qualquer pessoa envolvida no desenvolvimento . Ademais , ela deve facilitar a associação dos requisitos com as discussões a seu respeito , geralmente armazenadas nos arquivos das listas de discussão e ferramentas de gerenciamento de alterações ( como o Bugzilla ) . A Wiki/RE , proposta neste trabalho , visa disponibilizar um ambiente com tais características , voltado especificamente para a engenharia de requisitos . Ela é uma ferramenta wiki que permite a criação de hiperdocumentos de requisitos , provendo capacidades de gerenciamento do documento e permitindo a rápida avaliação da qualidade do mesmo 
 A programação orientada a aspectos é uma abordagem que utiliza conceitos da separação de interesses para modularizar o software de maneira mais adequada . Com o surgimento dessa abordagem vieram também novos desafios , dentre eles o teste de programas orientados a aspectos . Duas estratégias de ordenação de classes e aspectos para apoiar o teste de integração orientado a aspectos são propostas nesta tese . As estratégias de ordenação tem o objetivo de diminuir o custo da atividade de teste por meio da diminuição do número de stubs implementados durante o teste de integração . As estratégias utilizam um modelo de dependências aspectuais e um modelo que descreve dependências entre classes e aspectos denominado AORD ( Aspect and Oriented Relation Diagram ) também propostos neste trabalho . Tanto o modelo de dependências aspectuais como o AORD foram elaborados a partir da sintaxe e semântica da linguagem AspectJ . Para apoiar as estratégias de ordenação , idealmente aplicadas durante a fase de projeto , um processo de mapeamento de modelos de projeto que usam as notações UML e MATA para o AORD é proposto neste trabalho . O processo de mapeamento é composto de regras que mostram como mapear dependências advindas da programação orientada a objetos e também da programação orientada a aspectos . Como uma forma de validação das estratégias de ordenação , do modelo de dependências aspectuais e do AORD , um estudo exploratório de caracterização com três sistemas implementados em AspectJ foi conduzido . Durante o estudo foram coletadas amostras de casos de implementação de stubs e drivers de teste . Os casos de implementação foram analisados e classificados . A partir dessa análise e classificação , um catálogo de stubs e drivers de teste é 
 O problema de dimensionamento de lotes com restrições de capacidade ( CLSP ) consiste em determinar um plano de produção que satisfaça a demanda requerida , respeitando as limitações de capacidade , com o menor_custo  possível , ou seja , minimizando os custos de produção , estocagem e preparação de máquina . Encontrar uma solução factível para o CLSP , considerando tempo de preparação de máquina , é NP-completo . Nesta dissertação , para a resolução do CLSP , utiliza-se a decomposição de Dantzig-Wolfe e o procedimento de geração de colunas , encontrando bons limitantes inferiores . Duas diferentes estratégias de decomposição são exploradas , decomposição por itens e períodos . Para a obtenção de uma solução inteira para o problema ( limitante superior ) foram exploradas heurísticas lagrangianas , onde a solução inicial para as heurísticas provém da geração de colunas . Os limitantes obtidos podem ser utilizados em métodos exatos , como por exemplo , em algoritmos do tipo branch-and-price . Experimentos computacionais , baseados em exemplares gerados aleatoriamente , foram realizados e os resultados analisados , as variações dos parâmetros das instâncias foram sugeridas na 
 Um dos grandes desafios em mineração de dados é a integração de dados temporais ao seu processo . Existe um grande número de aplicações emergentes que envolvem dados temporais , incluindo a identificação de transações fraudulentas em cartões de crédito e ligações telefônicas , a detecção de intrusão em sistemas computacionais , a predição de estruturas secundárias de proteínas , a análise de dados provenientes de sensores , entre muitas outras . Neste trabalho , tem-se interesse na classificação de séries_temporais  que representam sinais de áudio . Como aplicação principal , tem-se interesse em classificar sinais de insetos coletados por um sensor óptico , que deve ser capaz de contar e classificar os insetos de maneira automática . Apesar de serem coletados opticamente , os sinais capturados se assemelham a sinais de áudio . O objetivo desta pesquisa é comparar métodos de classificação por similaridade e por extração de atributos que possam ser utilizados no contexto da classificação de insetos . Para isso , foram empregados os principais métodos de classificação de sinais de áudio , que têm sido_propostos  para problemas como reconhecimento de instrumentos musicais , fala e espécies animais . Neste trabalho , é mostrado que , de modo geral , a abordagem por extração de atributos é mais eficaz do que a classificação por similaridade . Mais especificamente , os melhores_resultados  são obtidos com a utilização de coeficientes mel-cepstrais . Este trabalho apresenta contribuições significativas em outras aplicações , também relacionadas à análise de séries_temporais  e sinais de áudio , por similaridade e por extração de 
 Devido à falta de meios para a representação da Abstração de Classificação , situações do mundo_real  onde ela ocorre a são representadas através de outros mecanismos ou mesmo desprezadas , induzindo falhas semânticas no projeto , dificilmente reparáveis em fases subsequentes . Tal fato é devido principalmente à pouca exploração e suporte dessa abstração . Por exemplo o Modelo Entidade Relacionamento Estendido ( ME-RX ) , o mais usado na prática profissional , não possui mecanismos para representá-la . Este trabalho caracteriza precisamente essa abstração , rerenci a nd o-a de outras abstrações , prineipalniente a generalização , c fornecendo subsídios nos projetistas para a escolha e uso correto da mesma para representar as várias situações de um projeto . Para tal , utilizam-se as construções teóricas de modelos de dados conhecidos aliado ao conhecimento adquirido em um estudo de caso real o desenvolvimento de um sistema de automação de autorizações para cooperativas médicas . É feita uma nova extensão ao ME-RX para introduzir um construtor semântico destinado à representação da classificação , sendo o novo modelo denominado Modelo Entidade Relacionamento Classificação ( ME-R-C ) . A partir desse novo modelo são analisadas as interações da classificação com os demais construtores existentes , e pela primeira vez na literatura da área são definidas as regras de mapeamento da classificação para o Modelo Relacional 
 A etiquetagem morfossintática é uma tarefa básica , bem conhecida e bastante explorada em diversas aplicações de Processamento de Línguas_Naturais  ( PLN ) , como análise sintática e extração e recuperação de informações . Os etiquetadores para a língua inglesa atingiram um estado da arte entre 96-99 % de precisão geral . Diferentemente do inglês , para o português do Brasil não foram ainda exploradas todas as técnicas para a etiquetagem , nem se atingiu a precisão dos melhores etiquetadores para a língua inglesa . Com estas motivações , quatro etiquetadores disponíveis na WWW foram treinados Unigrama ( TreeTagger ) , Trigrama ( TreeTagger ) , baseado em transformações ( TBL ) e baseado em máxima entropia ( MXPOST ) , e um etiquetador simbólico foi desenvolvido ( PoSiTagger ) . Todos os etiquetadores adaptados foram treinados com um corpus com cerca de 100.000 palavras formado por textos didáticos , jornalísticos e literários , e etiquetado com o Nilc tagset . A maior precisão geral obtida foi a do MXPOST 89,66 % . Foram também implementados quatorze métodos para a combinação dos etiquetadores , dos quais sete superaram a precisão do MXPOST . A maior precisão obtida com os métodos de combinação foi 90,91 % . A precisão geral sofreu a influência do tamanho do corpus manualmente etiquetado disponível para treinamento , do conjunto de etiquetas e dos tipos de texto utilizados 
 Este trabalho descreve um servidor que suporta visualização interativa na Internet por meio de applets Java que interfaceiam com o Visualization Toolkit . Como parte da revisão bibliográfica , são discutidas algumas abordagens para fornecer recursos de visualização na WWW explorando as tecnologias disponíveis para programação de aplicações baseadas na Web . Esse .estudo foi fundamental para o desenvolvimento do servidor , denominado Vis Web , que oferece um arcabouço genérico para disponibilizar recursos de visualização de alta qualidade na WWW . O VisWeb é composto por três_módulos  para visualização volumétrica : um módulo de extração de superfícies , um módulo para rendering volumétrico direto e um módulo de visualização de campos vetoriais . Cada módulo é uma applet que implementa uma técnica de visualização , e permite a definição interativa e o controle local de parâmetros da visualização , além de oferecer recursos para a manipulação direta dos modelos gerados . Para garantir às applets o acesso a recursos locais foram estudados os mecanismos de segurança de Java . Também são discutidos alguns trabalhos descritos na literatura que oferecem recursos de visualização na Web , destacando as vantagens e limitações do Vis Web em relação a esses trabalhos 
 Este trabalho descreve uma interface_gráfica  para a utilização da técnica de mapeamento por cores em tarefas de Visualização Científica . Apresentamos , como parte da revisão bibliográfica , um estudo geral sobre cores e sua percepção pelo olho humano , bem como sobre o processo de geração de visualizações e uma classificação dos sistemas de visualização existentes . No escopo do estudo do processo de geração de visualizações , discutimos sua complexidade e algumas abordagens que tentam auxiliar um usuário nesse processo . Enfatizamos a abordagem de visualização baseada em regras , que utiliza regras heurísticas que para incorporar conhecimentos sobre percepção visual de cores e informações sobre os dados ao processo de visualização . Essa proposta serviu de base para a implementação de um módulo de apoio à geração de tabelas de cores para tarefas de visualização . Como parte deste trabalho foi desenvolvida uma interface_gráfica  interativa para o módulo de mapeamento por cores baseado em regras . A interface oferece ao usuário acesso à biblioteca de tabelas sugerida pelo módulo , e apóia a sua aplicação no contexto de visualização volumétrica por extração de superfícies e por rendering volumétrico direto . Permite , também , a alteração das tabelas , de forma interativa . No final deste trabalho apresentamos algumas visualizações obtidas por meio da interface 
 Orientar o desenvolvimento de aplicações para a Web é um desafio para pesquisadores da área de Hipermídia . O trabalho apresentado nesta dissertação tem como objetivo apoiar a construção de aplicações que se preocupam com o intercâmbio de informações através da utilização da especificação XML ( Extensible MarkUp Language ) . Apresenta-se xRot , um roteiro para apoiar as etapas de definição , geração e apresentação de documentos estruturados manipulados por aplicações para a Internet . O roteiro inclui um algoritmo para geração de documentos XML em um ambiente apoiado por servidores de banco de dados e World_Wide  Web . Também é apresentada a ArqGDE , uma arquitetura que suporta as aplicações desenvolvidas com o xRot . Como estudos de caso da utilização do roteiro , foram desenvolvidas duas aplicações : AulaML e C2000ML 
 Este trabalho apresenta uma abordagem_bayesiana  para fazer inferência sobre os parâmetros de modelos auto-regressivos . Neste contexto , quando os parâmetros variara de forma aleatória e independente adotamos um modelo hierárquico para descrever a densidade a posteriori dos parâmetros . Unia segunda abordagem supõe que os parâmetros variam de acordo com um modelo auto-regressivo de primeira_ordem  , nesse caso a abordagem proposta é vista como uma extensão do filtro de Kalman onde as variâncias dos ruídos são conhecidas . Os modelos foram analisados usando-se técnicas de simulação de Monte_Carlo  e a geração de amostras das densidades a posteriori permitiram fazer previsões de séries através das densidades preditivas . Ilustrações de séries financeiras com dados reais são apresentadas e avaliadas pela qualidade da previsão obtida , salientando-se o modelo que melhor representa os dados 
 Atualmente , o inglês é a língua dominante para a escrita e divulgação de pesquisas científicas na forma de artigos científicos . Porém , existem muitos usuários da língua inglesa que sofrem interferência da língua materna quando escrevem este gênero de texto em inglês . Estes usuários se defrontam com problemas nas regras da gramática e estilo , e/ou se sentem incapazes de gerar expressões e orações padrões , e composições lingüísticas maiores que são convencionais neste gênero . Para amenizar os problemas desses usuários foi desenvolvido um ambiente modular de auxílio e ensino de escrita técnica chamado AMADEUS ( Amiable Article Development for User Support ) . AMADEUS consiste de várias ferramentas inter-relacionadas ferramenta de referência , ferramenta de suporte , ferramenta de crítica e ferramenta tutorial e fornece o contexto que está inserido este trabalho de mestrado . O objetivo principal deste mestrado é implementar AMADEUS como uma arquitetura de agentes que se comunicam através da troca de mensagem com um agente especial que contém um modelo dinâmico do usuário interagindo com o ambiente . Para tanto , introduzimos o conceito de adaptatividade em sistemas computacionais e descrevemos vários shells para modelar o usuário . Também fornecemos detalhes sobre agentes inteligentes que foram usados para implementar o modelo de usuário para o ambiente AMADEUS 
 A segmentação de imagens consiste em dividir uma imagem em regiões ou objetos que a compõem , como , por exemplo , para isolar os pixels de um objeto alvo de uma dada aplicação . Em segmentação de imagens médicas , o objeto de interesse comumente apresenta transições em suas bordas predominantemente do tipo claro para escuro ou escuro para claro . Métodos tradicionais por região , como a conexidade fuzzy relativa ( RFC - Relative Fuzzy Connectedness ) , não distinguem bem entre essas bordas similares com orientações opostas . A especificação da polaridade de contorno pode ajudar a amenizar esse problema , o que requer uma formulação_matemática  em grafos dirigidos . Uma discussão_sobre  como incorporar essa propriedade no arcabouço do RFC é apresentada neste trabalho . Uma prova teórica da otimalidade do novo algoritmo , chamado conexidade fuzzy relativa com orientação ( ORFC - Oriented Relative Fuzzy Connectedness ) , em termos de uma função de energia em grafos dirigidos sujeita as restrições de sementes é apresentada , bem como a sua apli- cação em poderosos métodos híbridos de segmentação . O método híbrido proposto ORFC & Graph Cut preserva a robustez do ORFC em relação à escolha de sementes , evitando o problema do viés de encolhimento do método de Corte em Grafo ( GC - Graph Cut ) , e mantém o forte controle do GC no delineamento de contornos de bordas irregulares da imagem . Os métodos propostos são avaliados usando imagens médicas de ressonáncia magnética ( RM ) e tomografia computadorizada ( TC ) do cérebro humano e de estudos torácicos 
 Esta dissertação apresenta uma solução para a integração do serviço de diretório LDAP com o serviço de nomes CORBA . Descrevemos a implementação de um servidor de nomes CORBA que armazena , num diretório LDAP , as associações entre nomes e referências para objetos . O servidor de nomes CORBA é , portanto , um cliente do serviço de diretório LDAP 
 Eficiência e flexibilidade são as vantagens desta abordagem . As associações nomereferência ficarão acessíveis tanto para clientes CORBA ( através das interfaces do serviço de nomes ) como para clientes LDAP ( através da API do LDAP ) . Atributos descritivos poderão ser adicionados às entradas do diretório que representam associações nome-referência . Clientes LDAP poderão utilizar as facilidades de busca no diretório para obter referências cujas entradas satisfaçam determinadas condições . Essas condições podem envolver o nome associado à referência ou outros atributos presentes nas entradas do diretório 
 Recentes pesquisas têm investigado modelos de garantia de desempenho baseados em restrições temporais , parametrizadas pela especificação de limites superiores de tempo_médio  de resposta . Este trabalho estende o desenvolvimento da política de escalonamento de temporeal EBS , aplicável a esse problema , apresentando um mecanismo de controle de admissão de requisições em aplicações com tais requisitos . A abordagem baseia-se em um método adaptativo capaz de administrar o nível de degradação do sistema , de forma a isolar o efeito do comportamento de um usuário sobre a qualidade de serviço oferecida aos demais usuários . Também é proposta uma modificação na implementação do algoritmo originalmente definido para a EBS , de forma a diminuir sua complexidade temporal . Resultados de simulação demonstram a efetividade dos mecanismos 
 A inteligência_artificial  não busca somente entender mas construir entidades inteligentes . A inteligência pode ser dividida em vários fatores e um deles é conhecido como aprendizado . A área de aprendizado de máquina visa o desenvolvimento de técnicas para aprendizado automático de máquinas , que incluem computadores , robôs ou qualquer outro dispositivo . Entre essas técnicas encontra-se o Aprendizado por Reforço , foco_principal  deste trabalho . Mais especificamente , o aprendizado por reforço relacional ( ARR ) foi investigado , que representa na forma relacional o aprendizado obtido através da interação direta com o ambiente . O ARR é bem interessante no campo de robótica , pois , em geral , não se dispôe do modelo do ambiente e se requer econômia de recursos utilizados . A técnica ARR foi investigada dentro do contexto de aprendizado de uma cabeça robótica . Uma modificação no algoritmo ARR foi proposta , denominada por ETG , e incorporada em uma arquitetura de controle de uma cabeça robótica . A arquitetura foi avaliada no contexto de um problema real não trivial : o aprendizado da atenção compartilhada . Os resultados obtidos_mostram  que a arquitetura é capaz de exibir comportamentos apropriados durante uma interação social controlada , através da utilização do ETG . Uma análise comparativa com outros métodos foi realizada que mostram que o algoritmo proposto conseguiu obter um desempenho_superior  na maioria dos experimentos_realizados 
 Este trabalho propõe mecanismos de diferenciação de serviços para servidores_Web  , visando a melhorar o desempenho desses sistemas quando são consideradas as características das requisições Web nas políticas de atendimento . Optou-se por adotar o contexto do comércio_eletrônico  para a realização das pesquisas , uma vez que esse ambiente é um dos mais impactados negativamente quando há um comportamento inadequado do servidor em situações de sobrecarga . Para isso , foi realizada uma investigação das características das requisições Web típicas do e-commerce , para que tais características pudessem ser usadas como diretrizes para os mecanismos e melhorar o desempenho dos servidores . Em seguida , foram propostos um modelo de carga de trabalho e um modelo de simulação para a realização dos experimentos . Com isso , foi possível avaliar os resultados obtidos com a inserção dos diversos mecanismos no Servidor Web com Diferenciação de Serviços ( SWDS ) , um modelo de servidor cuja arquitetura o torna capaz de fornecer serviços diferenciados a seus usuários e aplicações . Foram propostos novos mecanismos de escalonamento de requisições bem como novos mecanismos de controle de admissão . Diversas simulações foram realizadas e os resultados obtidos_mostram  que a exploração das características das requisições Web , além de ser fundamental para um bom entendimento do comportamento do servidor , possibilita a melhoria de desempenho do 
 Problemas de projeto de redes ( PPRs ) são muito importantes uma vez que envolvem uma série de aplicações em áreas da engenharia e ciências . Para solucionar as limitações de algoritmos convencionais para PPRs que envolvem redes complexas do mundo_real  ( em geral modeladas por grafos completos ou mesmo esparsos de larga-escala ) , heurísticas , como os algoritmos_evolutivos  ( EAs ) , têm sido investigadas . Trabalhos recentes têm mostrado que estruturas de dados adequadas podem melhorar significativamente o desempenho de EAs para PPRs . Uma dessas estruturas de dados é a representação nó-profundidade ( NDE , do inglês Node-depth Encoding ) . Em geral , a aplicação de EAs com a NDE tem apresentado resultados relevantes para PPRs de larga-escala . Este trabalho_investiga  o desenvolvimento de uma nova representação , baseada na NDE , chamada representação nó-profundidade-grau ( NDDE , do inglês Node-depth-degree Encoding ) . A NDDE é composta por melhorias nos operadores existentes da NDE e pelo desenvolvimento de novos operadores de reprodução possibilitando a recombinação de soluções . Nesse sentido , desenvolveu-se um operador de recombinação capaz de lidar com grafos não-completos e completos , chamado EHR ( do inglês , Evolutionary History Recombination Operator ) . Foram também desenvolvidos operadores de recombinação que lidam somente com grafos completos , chamados de NOX e NPBX . Tais melhorias tem como objetivo manter relativamente baixa a complexidade computacional dos operadores para aumentar o desempenho de EAs para PPRs de larga-escala . A análise de propriedades de representações mostrou que a NDDE possui redundância , assim , foram propostos mecanismos para evitá-la . Essa análise mostrou também que o EHR possui baixa complexidade de tempo e não possui tendência , além de revelar que o NOX e o NPBX possuem uma tendência para árvores com topologia de estrela . A aplicação de EAs usando a NDDE para PPRs clássicos envolvendo grafos completos , tais como árvore geradora de comunicação ótima , árvore geradora mínima com restrição de grau e uma árvore máxima , mostrou que , quanto maior o tamanho das instâncias do PPR , melhor é o desempenho relativo da técnica em comparação com os resultados obtidos com outros EAs para PPRs da literatura . Além desses problemas , um EA utilizando a NDE com o operador EHR foi aplicado ao PPR do mundo_real  de reconfiguração de sistemas de distribuição de energia elétrica ( envolvendo grafos esparsos ) . Os resultados mostram que o EHR possibilita reduzir significativamente o tempo de convergência do 
 Esta dissertação trata do problema da k-árvore de custo mínimo ( kMST ) : dados um grafo conexo G , um custo não-negativo c_e para cada aresta e e um número inteiro positivo k , encontrar uma árvore com k vértices que tenha custo mínimo . O kMST é um problema NP-difícil e portanto não se conhece um algoritmo polinomial para resolvê-lo . Nesta dissertação discutimos alguns_casos  em que é possível resolver o problema em tempo polinomial . Também são estudados algoritmos de aproximação para o kMST . Entre os algoritmos de aproximação estudados , apresentamos a 2-aproximação desenvolvida por Naveen Garg , que atualmente é o algoritmo com melhor fator de aproximação 
 Neste trabalho estudamos um controlador denominado rastreador linear quadrático ( RLQ ) com custo médio de longo prazo ( CMLP ) para sistemas_lineares  com saltos markovianos ( SLSM ) . Mostramos que o conceito de detetabilidade uniforme , juntamente com a hipótese de que o regulador linear quadrático associado ao RLQ tenha custo uniformemente limitado , são suficientes para que o controle obtido seja estabilizante em um certo sentido . A partir deste resultado , e considerando as mesmas hipóteses , demonstramos a existência do CMLP . Com isto , estendemos os resultados dispostos na literatura desde que consideramos um sistema variante no tempo e uma estrutura mais geral para a cadeia deMarkov . Além disto , avaliamos a aplicação deste controlador no planejamento da operação de um sistema hidrotérmico . Para isto , utilizamos o sistema de usinas do rio São Francisco , em dois casos de estudo , para comparar o desempenho do controlador estudado em relação à solução ótima para o problema , encontrada com o uso da programação_dinâmica  estocástica , e em relação à solução obtida via programação_dinâmica  determinística . Os resultados sugerem que o RLQ pode representar uma alternativa interessante para o problema de planejamento 
 Este trabalho tem como objetivo desenvolver um método de segmentação de cenas em vídeos digitais que trate segmentos semânticamente complexos . Como prova de conceito , é apresentada uma abordagem multimodal que utiliza uma definição mais geral para cenas em telejornais , abrangendo tanto cenas onde âncoras aparecem quanto cenas onde nenhum âncora aparece . Desse modo , os resultados obtidos da técnica multimodal foram signifiativamente melhores quando comparados com os resultados obtidos das técnicas monomodais aplicadas em separado . Os testes foram executados em quatro grupos de telejornais brasileiros obtidos de duas emissoras de TV diferentes , cada qual contendo cinco edições , totalizando vinte 
 A crescente adoção do paradigma experimental na pesquisa em Engenharia de Software visa a obtenção de evidências experimentais sobre as tecnologias propostas para garantir sua correta avaliação e para a construção de um corpo de conhecimento sólido da disciplina . Uma das abordagens de pesquisa experimental é a revisão_sistemática  , um método rigoroso , planejado e auditável para a realização da coleta e análise crítica de dados experimentais disponíveis sobre um determinado tema de pesquisa . Apesar de produzir resultados confiáveis , a condução de uma revisão_sistemática  pode ser trabalhosa e muitas_vezes  demorada , principalmente quando existe um grande volume de estudos a serem considerados , selecionados e avaliados . Uma solução encontrada na literatura é a utilização de ferramentas de Mineração Visual de Textos ( VTM ) como a Projection Explorer ( PEx ) para apoiar a fase de seleção e análise de estudos primários no processo de revisão_sistemática  . Neste trabalho foi realizada uma reengenharia de software na ferramenta PEx com dois objetivos principais : apoiar , utilizando VTM , a fase de seleção e análise de estudos primários no processo de revisão_sistemática  e implementar novos requisitos não-funcionais relativos à melhoria da manutenibilidade e escalabilidade da ferramenta . Como resultado foi construída uma plataforma modular para a instanciação de ferramentas de visualização e , a partir da mesma , uma ferramenta de revisão_sistemática  apoiada por VTM . Os resultados de um estudo de caso executado com a ferramenta mostraram que a abordagem de aplicação de técnicas VTM usada nesse contexto é viável e promissora , melhorando tanto a performance quanto a efetividade da 
 O sistema de TV digital interativa está em fase de implantação no Brasil . O middleware Ginga , responsável por permitir a apresentação de programas interativos , prevê que usuários possam interagir com aplicações apresentadas na TV ao pressionar de teclas em um controle remoto . Considerando que controles remotos tradicionais apresentam limitações de usabilidade , este trabalho teve o objetivo investigar a aplicação de conceitos de computação ubíqua , em particular interfaces naturais e multimodais , como alternativas para prover interatividade entre usuários e programas de TV digital . Como resultado , um dispositivo móvel alternativo ao controle remoto tradicional foi utilizado no projeto de novos mecanismos de interação que incluem interfaces baseadas em telas sensíveis ao toque , interfaces sensíveis a gestos capturados por dispositivos que contêm acelerômetros , e interfaces que contêm microfones que permitem entrada de dados por voz . A construção de protótipos correspondentes foi beneficiada pela ( assim como beneficiou ) implementação prévia de um componente que oferece funcionalidades para envio de dados multimodais para um receptor de TV digital contendo o middleware Ginga , e de um componente que , instalado no receptor , permite a comunicação peer-to-peer entre dispositivos sem 
 Neste trabalho foi investigado o problema de autoria automatizada de informação multimídia sob a perspectiva da computação ubíqua de modo geral , e da interação do usuário com aplicações de captura e accesso ( C & A ) de modo particular . O objetivo do projeto foi a definição de operadores sobre interação do usuário em ambientes e em aplicações para permitir a geração_automática  de documentos_multimídia  interativos , um dos temas de pesquisa da área de engenharia de documentos . A abordagem da proposta foi a generalização dos operadores Inkteractors , definidos sobre a interação do usuário com aplicações baseadas em tinta eletrônica , considerando a interação do usuário na voz , mensagens de texto , vídeo e lousa . Como resultado foram definido os novos Interactors : operadores de interação sobre informação capturada em aplicações que envolvem interação do usuário com as mídias . Os Interactors foram validados no contexto de engenharia de documentos ao serem utilizados para a geração_automática  de documentos_multimídia  interativos , associados a aplicações de C & A para oferecer novas possibilidades de indexar , visualizar e acessar os documentos_multimídia 
 O objetivo deste trabalho é apresentar os conceitos relacionados a Teste Adaptativo Informatizado , ou abreviadamente TAI , para o modelo logístico unidimensional da Teoria de Resposta ao Item . Utilizamos a abordagem_bayesiana  para a estimação do parâmetro de interesse , chamado de traço latente ou habilidade . Apresentamos os principais algoritmos de seleção de itens em TAI e realizamos estudos de simulação para comparar o desempenho deles . Para comparação , usamos aproximações numéricas para o Erro Quadrático Médio e para o Vício e também calculamos o tempo_médio  para o TAI selecionar um item . Além_disso  , apresentamos como instalar e usar a implementação de TAI desenvolvida neste projeto chamada de TAI2U , que foi desenvolvido no VBA-Excel usando uma interface com o 
 A ciência tem feito uso frequente de recursos_computacionais  para execução de experimentos e processos científicos , que podem ser modelados como workflows que manipulam grandes volumes de dados e executam ações como seleção , análise e visualização desses dados segundo um procedimento determinado . Workflows científicos têm sido usados por cientistas de várias áreas , como astronomia e bioinformática , e tendem a ser computacionalmente intensivos e fortemente voltados à manipulação de grandes volumes de dados , o que requer o uso de plataformas de execução de alto_desempenho  como grades ou nuvens de computadores . Para execução dos workflows nesse tipo de plataforma é necessário o mapeamento dos recursos_computacionais  disponíveis para as atividades do workflow , processo conhecido como escalonamento . Plataformas de computação em nuvem têm se mostrado um alternativa viável para a execução de workflows científicos , mas o escalonamento nesse tipo de plataforma geralmente deve considerar restrições específicas como orçamento limitado ou o tipo de recurso computacional a ser utilizado na execução . Nesse contexto , informações como a duração estimada da execução ou limites de tempo e de custo ( chamadas aqui de informações de suporte ao escalonamento ) são importantes para garantir que o escalonamento seja eficiente e a execução ocorra de forma a atingir os resultados esperados . Este trabalho identifica as informações de suporte que podem ser adicionadas aos modelos de workflows científicos para amparar o escalonamento e a execução eficiente em plataformas de computação em nuvem . É proposta uma classificação dessas informações , e seu uso nos principais Sistemas Gerenciadores de Workflows Científicos ( SGWC ) é analisado . Para avaliar o impacto do uso das informações no escalonamento foram realizados_experimentos  utilizando modelos de workflows científicos com diferentes informações de suporte , escalonados com algoritmos que foram adaptados para considerar as informações inseridas . Nos experimentos_realizados  , observou-se uma redução no custo financeiro de execução do workflow em nuvem de até 59 % e redução no makespan chegando a 8,6 % se comparados à execução dos mesmos workflows sendo escalonados sem nenhuma informação de suporte disponível 
 Moldagem por injeção é um dos mais importantes processos industriais para produção de produtos plásticos finos . Esse processo é dividido essencialmente em quatro estágios : plastificação , preenchimento , empacotamento e resfriamento 

 O escoamento de um fluido caracterizado por alta viscosidade em uma cavidade estreita é um problema tipicamente encontrado em processos de moldagem por injeção.Neste caso , o escoamento pode ser descrito por uma formulação conhecida como aproximação de Hele-Shaw . Tal formulação pode ser derivada das equações de conservação tridimensionais usando um número de suposições a respeito do polímero injetado e da geometria da cavidade do molde , juntamente com a integração e o acoplamento das equações da conservação da quantidade de movimento e da continuidade . Essa formulação , referindo às limitações da geometria do molde como sendo canais estreitos e quase sem curvatura , é comumente denominada formulação 2 1/2D 

 Neste trabalho , é apresentada uma técnica para a simulação da fase de preenchimento de um processo de moldagem por injeção , usando essa formulação 2 1/2D , com um método de volumes finitos e malhas não estruturadas . O modelo de Cross modificado com dependência da temperatura de Arrhenius é empregado para descrever a viscosidade do polímero fundido . O campo de distribuição de temperatura é tridimensional e é resolvido usando um esquema semi-Lagrangeano baseado em volumes finitos . As malhas não estruturadas utilizadas são geradas por triangulação de Delaunay e o método_numérico  implementado usa a estrutura de dados topológica SHE - Singular Handle Edge , que é capaz de lidar com condições de contorno e singularidades , aspectos comumente encontrados em simulações numéricas de escoamento de fluidos 
 Sob o modelo de criptografia de chave_pública  baseada em identidades ( ID-PKC ) , a própria identidade dos usuários é usada como chave_pública  , de modo a dispensar a necessidade de uma infra-estrutura de chaves públicas ( ICP ) , na qual o gerenciamento de certificados digitais é complexo . Por outro_lado  , sistemas nesse modelo requerem uma entidade capaz de gerar chaves secretas . Essa entidade é conhecida por PKG ( Private Key Generator ) ; ela possui uma chave-mestra e mantém custódia das chaves secretas geradas a partir dessa chave-mestra . Naturalmente , a custódia de chaves é indesejável em muitas aplicações . O conceito de Criptografia de Chave Pública sem Certificado , ou Certificateless Public Key Cryptography ( CL-PKC ) , foi proposto para que a custódia de chaves fosse eliminada , mantendo , porém , as características de interesse : a não necessidade de uma ICP e a eliminação de certificados digitais . CL-PKC deixa de ser um sistema baseado em identidades , pois é introduzida uma chave_pública  , gerada a partir de uma informação secreta do usuário . Nesta dissertação , apresentamos a construção de dois esquemas , um CL-PKE e um CL-PKS , baseados em emparelhamentos bilineares sobre curvas elípticas . Ambas propostas : ( 1 ) eliminam custódia de chaves ; ( 2 ) dispensam certificados digitais ; ( 3 ) são mais eficientes , sob certos aspectos , que esquemas anteriormente publicados ; ( 4 ) e são seguros contra ataques adaptativos de texto cifrado escolhido ( em CL-PKE ) e contra ataques adaptativos de mensagem escolhida ( em CL-PKS ) , sob o modelo de oráculos aleatórios 
 Uma das principais motivações da Inteligência_Artificial  no contexto dos sistemas de entretenimento digital é criar personagens adaptáveis a novas situações , pouco previsíveis , com aprendizado rápido , memória de situações passadas e uma grande diversidade de comportamentos consistente e convincente ao longo do tempo . De acordo com recentes estudos desenvolvidos nos campos da Neurociência e da Psicologia , a capacidade de resolução de problemas não está unicamente atrelada à facilidade na manipulação de símbolos , mas também à exploração das características do ambiente e à interação social , que pode ser expressa na forma de fenômenos emocionais . Os resultados desses estudos confirmam o papel fundamental que cumprem a personalidade e as emoções nas atividades de percepção , planejamento , raciocínio , criatividade , aprendizagem , memória e tomada de decisão . Quando módulos para a manipulação de personalidade e emoções são incorporados à teoria de agentes , é possível a construção de Agentes com Comportamento Convincente ( Believable Agents ) . O objetivo principal deste trabalho é desenvolver e implementar uma arquitetura de agentes inteligentes para construir personagens sintéticos cujos estados afetivos influenciam em suas atividades cognitivas . Para o desenvolvimento de tal arquitetura utilizou-se o modelo BDI ( Beliefs , Desires e Intentions ) como base e aos módulos existentes em uma implementação desse modelo foi incluído um Módulo Afetivo . Esse Módulo Afetivo é constituído por três submódulos ( Personalidade , Humor e Emoção ) e deve impactar nas atividades cognitivas de percepção , memória e tomada de decisão do agente . Duas provas de conceito ( experimentos ) foram construídas : a simulação do problema do `` Dilema do Prisioneiro Iterado '' e a versão computadorizada do `` Jogo da Memória '' . A construção desses experimentos permitiu avaliar empiricamente a influência da personalidade , humor e emoção nas atividades cognitivas dos agentes , e consequentemente no seu comportamento . Os resultados evidenciam que a utilização da nova arquitetura permite a construção de agentes com comportamentos mais coerentes , adaptativos e cooperativos quando comparados aos de agentes construídos com arquiteturas cujas atividades cognitivas não consideram o estado afetivo , e também produz um comportamento mais próximo de um agente humano que de um comportamento ótimo ou aleatório . Essa evidência de sucesso , apresentada nos resultados , mostra que os agentes construídos com a arquitetura proposta nessa dissertação indicam um avanço na direção do desenvolvimento dos Agentes com Comportamento Convincente 
 A evolução da tecnologia da informação popularizou o uso de sistemas computacionais para a automação de tarefas operacionais . As tarefas de implantação e manutenção desses sistemas computacionais , por outro_lado  , não acompanharam essa tendência de forma ágil , tendo sido , por anos , efetuadas de forma_manual  , implicando alto custo , baixa produtividade e pouca qualidade de serviço . A fim de preencher essa lacuna foi proposta uma iniciativa denominada Computação Autônoma , a qual visa prover capacidade de autogerenciamento a sistemas computacionais . Dentre os aspectos necessários para a construção de um sistema autônomo está a detecção de intrusão , responsável por monitorar o funcionamento e fluxos de dados de sistemas em busca de indícios de operações maliciosas . Dado esse contexto , este trabalho apresenta um sistema autônomo de detecção de intrusões em aplicações Web , baseado em técnicas de aprendizado de máquina com complexidade computacional próxima de linear . Esse sistema utiliza técnicas de agrupamento de dados e de detecção de novidades para caracterizar o comportamento normal de uma aplicação , buscando posteriormente por anomalias no funcionamento das aplicações . Observou-se que a técnica é capaz de detectar ataques com maior autonomia e menor dependência sobre contextos específicos em relação a trabalhos 
 A necessidade de extrair conhecimento útil e inovador de grandes massas de dados textuais , tem motivado cada vez mais a investigação de métodos para Mineração de Textos . Dentre os métodos existentes , destacam-se as iniciativas para organização de conhecimento por meio de hierarquias de tópicos , nas quais o conhecimento implícito nos textos é representado em tópicos e subtópicos , e cada tópico contém documentos relacionados a um mesmo tema . As hierarquias de tópicos desempenham um papel_importante  na recupera ção de informação , principalmente em tarefas de busca exploratória , pois permitem a análise do conhecimento de interesse em diversos níveis de granularidade e exploração interativa de grandes coleções de documentos . Para apoiar a construção de hierarquias de tópicos , métodos de agrupamento hierárquico têm sido utilizados , uma vez que organizam coleções textuais em grupos e subgrupos , de forma não supervisionada , por meio das similaridades entre os documentos . No entanto , a maioria dos métodos de agrupamento hierárquico não é adequada em cenários que envolvem coleções textuais dinâmicas , pois são exigidas frequentes atualizações dos agrupamentos . Métodos de agrupamento que respeitam os requisitos existentes em cenários dinâmicos devem processar novos documentos assim que são adicionados na coleção , realizando o agrupamento de forma incremental . Assim , neste trabalho é explorado o uso de métodos de agrupamento incremental para o aprendizado não supervisionado de hierarquias de tópicos em coleções textuais dinâmicas . O agrupamento incremental é aplicado na construção e atualização de uma representação condensada dos textos , que mantém um sumário das principais características dos dados . Os algoritmos de agrupamento hierárquico podem , então , ser aplicados sobre as representa ções condensadas , obtendo-se a organização da coleção textual de forma mais eficiente . Foram avaliadas experimentalmente três estratégias de agrupamento incremental da literatura , e proposta uma estratégia alternativa mais apropriada para hierarquias de tópicos . Os resultados indicaram que as hierarquias de tópicos construídas com uso de agrupamento incremental possuem qualidade próxima às hierarquias de tópicos construídas por métodos não incrementais , com significativa redução do custo_computacional 
 Este trabalho analisa os efeitos das propriedades dinâmicas de sistemas distribuídos de larga_escala  e seu impacto no desempenho , e introduz uma abordagem para o planejamento de experimentos de referência capazes de expor essa influência . Especialmente em aplicações complexas com múltiplas camadas de software ( multi-tier ) , o efeito total de pequenos atrasos introduzidos por buffers , latência na comunicação e alocação de recursos , pode resultar gerando inércia significativa ao longo do funcionamento do sistema . A fim de detetar estas propriedade dinâmica , o experimento de execução do benchmark deve excitar o sistema com carga de trabalho não-estacionária sob um ambiente controlado . A presente pesquisa discorre sobre os elementos essenciais para este fim e ilustra a abordagem de desenvolvimento com um estudo de caso . O trabalho também descreve como a metodologia de instrumentação pode ser explorada em abordagens de modelagem dinâmica para sobrecargas transientes devido a distúrbios na carga de trabalho 
 A análise de texturas dinâmicas tem se apresentado como uma área de pesquisa crescente e em potencial nos últimos_anos  em visão_computacional  . As texturas dinâmicas são sequências de imagens de textura ( i.e . vídeo ) que representam objetos dinâmicos . Exemplos de texturas dinâmicas são : evolução de colônia de bactérias , crescimento de tecidos do corpo humano , escada rolante em movimento , cachoeiras , fumaça , processo de corrosão de metal , entre outros . Apesar de existirem pesquisas relacionadas com o tema e de resultados promissores , a maioria dos métodos da literatura possui limitações . Além_disso  , em muitos_casos  as texturas dinâmicas são resultado de fenômenos complexos , tornando a tarefa de caracterização um desafio ainda maior . Esse cenário requer o desenvolvimento de um paradigma de métodos baseados em complexidade . A complexidade pode ser compreendida como uma medida de irregularidade das texturas dinâmicas , permitindo medir a estrutura dos pixels e quantificar os aspectos espaciais e temporais . Neste contexto , o objetivo deste mestrado é estudar e desenvolver métodos para caracterização de texturas dinâmicas baseado em metodologias de complexidade advindas da área de sistemas complexos . Em particular , duas metodologias já utilizadas em problemas de visão_computacional  são consideradas : redes complexas e caminhada determinística parcialmente auto-repulsiva . A partir dessas metodologias , três métodos de caracterização de texturas dinâmicas foram desenvolvidos : ( i ) baseado em difusão em redes - ( ii ) baseado em caminhada determinística parcialmente auto-repulsiva - ( iii ) baseado em redes geradas por caminhada determinística parcialmente auto-repulsiva . Os métodos desenvolvidos foram aplicados em problemas de nanotecnologia e tráfego de veículos , apresentando resultados potenciais e contribuindo para o desenvolvimento de ambas áreas 
 Este trabalho apresenta um método_numérico  para simular_escoamentos  viscoelásticos tridimensionais com superfícies_livres  governados pela equação_constitutiva  de Maxwell . O método_numérico  é uma extensão da técnica newtoniana GENSMAC3D para escoamentos viscoelásticos . As equações governantes para escoamentos_incompressíveis  cartesianos isotérmicos são apresentadas em detalhes . O tratamento do tensor não-newtoniano em contornos rígidos tridimensionais é apresentado em detalhes , bem como o cálculo da condição de contorno na superfície_livre  . As equações governantes são resolvidas pelo método de diferenças_finitas  numa malha_deslocada  tridimensional . O fluido é modelado pela técnica das partículas marcadoras e tratado como uma superfície linear por partes . O método_numérico  desenvolvido foi implementado no sistema Freeflow3D , e resultados numéricos obtidos na simulação de escoamentos tridimensionais governados pela equação_constitutiva  de Maxwell são apresentados . Adicionalmente , apresentamos uma validação mostrando a convergência do método desenvolvido nesse 
 Problemas de otimização em caixas são de grande_importância  , não só por surgirem naturalmente na formulação de problemas da vida prática , mas também por aparecerem como subproblemas de métodos de penalização ou do tipo Lagrangiano Aumentado para resolução de problemas de programação não-linear . O objetivo do trabalho é estudar um algoritmo de restrições ativas para problemas de otimização em caixas recentemente apresentado chamado ASA e compará-lo à versão mais recente de GENCAN , que é também um método de restrições ativas . Para tanto , foi elaborada uma metodologia de testes robusta e minuciosa , que se propõe a remediar vários dos aspectos comumente criticados em trabalhos anteriores . Com isso , puderam ser extraídas conclusões que levaram à melhoria de GENCAN , conforme ficou posteriormente comprovado por meio da metodologia aqui introduzida 
 O cálculo dos vetores de movimento é utilizado em vários processos na área de visão_computacional  . Problemas como estabelecer rotas de colisão e movimentação da câmera ( egomotion ) utilizam os vetores como entrada de algoritmos complexos e que demandam muitos recursos_computacionais  e consequentemente um consumo maior de energia . O fluxo ótico é uma aproximação do campo gerado pelos vetores de movimento . Porém , para aplicações móveis e de baixo consumo de energia se torna inviável o uso de computadores de uso geral . Um sistema embarcado é definido como um computador desenvolvido com um propósito específico referente à aplicação na qual está inserido . O objetivo principal deste trabalho foi elaborar um módulo em sistema embarcado que realiza o cálculo do fluxo ótico . Foi elaborado um co-projeto de hardware e software dedicado e implementados em FPGAs Cyclone II e Stratix IV para a prototipação do sistema . Desta forma , a implementação de um projeto que auxilia a detecção e medição do movimento é importante não só como aplicação isolada , mas para servir de base no desenvolvimento de outras aplicações como tracking , compressão de vídeos , predição de colisão , 
 Cerný , em 1964 , conjecturou que um autômato sincronizado com n estados possui uma palavra sincronizadora mínima de tamanho no máximo ( n-1 ) ². Esta conjectura permanece em aberto . Neste trabalho são apresentados algoritmos para obter palavras sincronizadoras e é feito um experimento comparativo entre os resultados obtidos por estes algoritmos em relação a algumas séries infinitas de autômatos . Por fim , é feito um breve histórico sobre os resultados parciais obtidos até a presente data e alguns destes trabalhos são apresentados em mais detalhes 
 A ambiguidade lexical é considerada uma das principais barreiras para melhoria de aplicações do Processamento de Língua Natural ( PLN ) . Neste contexto , tem-se a área de Desambiguação Lexical de Sentido ( DLS ) , cujo_objetivo  é desenvolver e avaliar métodos que determinem o sentido correto de uma palavra em um determinado contexto por meio de um conjunto finito de possíveis significados . A DLS é empregada , principalmente , no intuito de prover recursos e ferramentas para diminuir problemas de ambiguidade e , consequentemente , contribuir para melhorias de resultados em outras áreas do PLN . Para o Português do Brasil , pouco se tem pesquisado nesta área , havendo alguns trabalhos bem específicos de domínio . Outro fator importante é que diversas_áreas  do PLN engajam-se no cenário multidocumento , onde a computação é efetuada sobre uma coleção de textos , todavia , não há relato de trabalhos de DLS direcionados a este cenário , tampouco experimentos de desambiguação neste domínio . Portanto , neste trabalho de mestrado , objetivou-se o desenvolvimento de métodos de DLS de domínio geral voltado à língua_Portuguesa  do Brasil e o desenvolvimento de algoritmos de desambiguação que façam uso de informações multidocumento , bem como a experimentação e avaliação destes no cenário multidocumento . Para tanto , a fim de subsidiar experimentos , desenvolvimento e avaliação deste projeto , anotou-se manualmente o córpus CSTNews , caracterizado como um córpus multidocumento , utilizando a WordNet de Princeton como repositório de sentidos , que organiza os significados por meio de conjuntos de sinônimos ( synsets ) e relações linguísticas entre estes . Foram desenvolvidos quatro métodos de DLS e algumas variações , sendo : um método heurístico ( para aferir valores de baseline ) ; variações do algoritmo de Lesk ( 1986 ) ; adaptação do algoritmo de Mihalcea and Moldovan ( 1999 ) ; e uma variação do método de Lesk para o cenário multidocumento . Foram realizados três experimentos para avaliação dos métodos , cujos objetivos foram : determinar o desempenho geral dos algoritmos em todo o córpus ; avaliar a qualidade de desambiguação de palavras mais ambíguas no córpus ; e verificar o ganho de qualidade da desambiguação ao empregar informação multidocumento . Após estes experimentos , pôde-se observar que o método heurístico apresenta um melhor resultado geral . Contudo , é importante ressaltar que a maioria das palavras anotadas no córpus tiveram apenas um synset , que , normalmente , era o mais frequente , o que , consequentemente , apresenta um cenário mais propício ao método heurístico . Outro fato importante foi que , neste cenário , a diferença de desempenho entre o método de DLS multidocumento e o heurístico é estatisticamente irrelevante . Já para a desambiguação de palavras mais ambíguas , o método heurístico foi inferior , evidenciando que , para a desambiguação de palavras mais ambíguas , são necessários métodos mais sofisticados de DLS . Por fim , verificou-se que a utilização de informação multidocumento auxilia o processo de desambiguação . As contribuições deste trabalho podem ser agrupadas entre teóricas e técnicas . Nas teóricas , tem-se a investigação e análises da DLS no cenário multidocumento . Entre as contribuições técnicas , foram desenvolvidos métodos de DLS , um córpus anotado e uma ferramenta de anotação direcionados à língua_Portuguesa  do Brasil , que podem avançar as pesquisas em DLS para o 
 Algumas técnicas e métodos orientados a objetos são discutidos e avaliados neste trabalho , bem como propostas de técnicas para elicitação e anrálise de requisitos.Uma proposta de um modelo de requisitos baseado em cenários , integrável e compatível com o método zuSION é feita . A notação introduzida , o processo de construção do modelo e a passagem para a construção do modelo de análise do FUSION são apresentados . Um estudo de caso relativo a um sistema de agendamento de reuniões ilustra a construção do modelo de requisitos nos moldes propostos 
 Este trabalho apresenta um estudo de técnicas para matching de imagens . Matching de imagens pode ser definido como o problema de avalíar a similaridade entre os objetos em diferentes imagens , sendo usado também para detectar mudanças em uma cena , estimar a movimentação de objetos e integrar informações de diferentes_tipos  de imagens . Esta dissertação apresenta um estudo teórico de técnicas de matching , a implementagão de um sistema de matching - SisMatch , e a análise de desempenho e resultados sobre as técnicas implementadas . O sistema de Matching , utíIiza as imagens produzidas pelo tomógrafo por Ressonância Magnética desenvolvido no Instituto de Física de São Carlos 
 A expansão vertiginosa do uso de Sistemas Web como ferramenta de negócio colocou grande pressão sobre o desenvolvimento de software , exigindo entrega deresultado tangível cada vez mais rápido , num ambiente altamente instável e dinâmico . Em resposta a essas necessidades , surgiu uma nova classe de metodologias de desenvolvimento de software , conhecidas como Metodologias Ágeis . Este trabalho apresenta as principais características desta nova classe de metodologias , analisando em detalhes três dos principais Métodos Ágeis existentes . O objetivo primordial deste trabalho é a definição de um Método Ágil especializado para as características dos Sistemas Web , ou usando uma terminologia mais alinhada com a base filosófica que permeia o trabalho , o objetivo é a criação de um Ecossistema de Desenvolvimento Ágil de software , especializado para Sistemas 
 Os Sistemas de Gerenciamento de Bases de Dados ( SGBD ) foram desenvolvidos para armazenar e recuperar de maneira eficiente dados formados apenas por números ou cadeias de caracteres . Entretanto , nas últimas_décadas  houve um aumento expressivo , não só da quantidade , mas da complexidade dos dados manipulados em bases de dados , dentre eles os de natureza multimídia ( como imagens , áudio e vídeo ) , informações geo-referenciadas , séries_temporais  , entre outros . Assim , surgiu a necessidade do desenvolvimento de novas técnicas que permitam a manipulação eficiente de tipos de dados complexos . Para atender às buscas necessárias às aplicações de base de dados modernas é preciso que os SGBD ofereçam suporte para buscas por similaridade ? consultas que realizam busca por objetos da base similares a um objeto de consulta , de acordo com uma certa medida de similaridade . Outro fator importante que veio contribuir para a necessidade de suportar a realização de consultas por similaridade em SGBD está relacionado à integração de técnicas de mineração de dados . É fundamental para essa integração o fornecimento de recursos pelos SGBD que permitam a realização de operações básicas para as diversas técnicas de mineração de dados existentes . Uma operação básica para várias dessas técnicas , tais como a técnica de detecção de agrupamentos de dados , é justamente o cálculo de medidas de similaridade entre pares de objetos de um conjunto de dados . Embora haja necessidade de fornecer suporte para a realização desse tipo de consultas em SGBD , o atual padrão da linguagem SQL não prevê a realização de consultas por similaridade . Esta tese pretende contribuir para o fornecimento desse suporte , incorporando ao SQL recursos capazes de permitir a realização de operações de consulta por similaridade sobre grandes bases de dados complexos de maneira totalmente integrada com os demais recursos da 
 A crescente oferta de programas de código_aberto  na rede mundial de computadores expõe potenciais usuários a muitas possibilidades de escolha . Em face da pluralidade de interesses desses indivíduos , mecanismos eficientes que os aproximem daquilo que buscam trazem benefícios para eles próprios , assim como para os desenvolvedores dos programas . Este trabalho apresenta o AppRecommender , um recomendador de aplicativos GNU/Linux que realiza uma filtragem no conjunto de programas disponíveis e oferece sugestões individualizadas para os usuários . Tal feito é alcançado por meio da análise de perfis e descoberta de padrões de comportamento na população estudada , de sorte que apenas os aplicativos considerados mais suscetíveis a aceitação sejam oferecidos aos usuários 
 Esta tese visa apresentar uma contribuição ao desenvolvimento de sistemas computacionais , baseados principalmente em visão_computacional  , usados para o auxílio a navegação de robôs_móveis  e veículos inteligentes . Inicialmente , buscou-se apresentar uma proposição de uma arquitetura de um sistema computacional para veículos inteligente que permita a construção de sistemas que sirvam tanto para o apoio ao motorista , auxiliando-o em sua forma de condução , quanto para o controle autônomo , proporcionando maior segurança e autonomia do tráfego de veículos em meio urbano , em rodovias e inclusive no meio rural . Esta arquitetura vem_sendo  aperfeiçoada e validada junto as plataformas CaRINA I e CaRINA II ( Carro Robótico Inteligente para Navegação Autônoma ) , que também foram alvo de desenvolvimentos e pesquisas junto a esta tese , permitindo também a experimentação prática dos conceitos propostos nesta tese . Neste contexto do desenvolvimento de veículos inteligentes e autônomos , o uso de sensores para a percepção 3D do ambiente possui um papel muito importante , permitindo o desvio de obstáculos e navegação_autônoma  , onde a adoção de sensores de menor_custo  tem sido buscada a m de viabilizar aplicações comerciais . As câmeras estéreo são dispositivos que se enquadram nestes requisitos de custo e percepção 3D , destacando-se como sendo o foco da proposta de um novo método automático de calibração apresentado nesta tese . O método proposto permite estimar os parâmetros extrínsecos de um sistema de câmeras estéreo através de um processo evolutivo que considera apenas a coerência e a qualidade de alguns elementos do cenário quanto ao mapa de profundidade . Esta proposta apresenta uma forma original de calibração que permite a um usuário , sem grandes conhecimentos sobre visão estéreo , ajustar o sistema de câmeras para novas configurações e necessidades . O sistema proposto foi testado com imagens reais , obtendo resultados bastante promissores , se comparado aos métodos tradicionais de calibração de câmeras estéreo que fazem_uso  de um processo interativo de estimação dos parâmetros através da apresentação e uso de um padrão xadrez . Este método apresenta-se como uma abordagem promissora para realizar a fusão dos dados de câmeras e sensores , permitindo o ajuste das matrizes de transformação ( parâmetros extrínsecos do sistema ) , a m de obter uma referência única onde são representados e agrupados os dados vindos dos diferentes sensores 
 A área de aprendizado de máquina passa por uma grande expansão em seu universo de aplicações . Algoritmos de indução de modelos preditivos têm sido responsáveis pela realização de tarefas que eram inviáveis ou consideradas exclusividade do campo de ação humano até recentemente . Contudo , ainda é necessária a supervisão humana durante a construção de conjuntos de treinamento , como é o caso da tarefa de classificação . Tal construção se dá por meio da rotulação manual de cada exemplo , atribuindo a ele pelo menos uma classe . Esse processo , por ser manual , pode ter um custo elevado se for necessário muitas_vezes  . Uma técnica sob investigação corrente , capaz de mitigar custos de rotulação , é o aprendizado ativo . Dado um orçamento limitado , o objetivo de uma estratégia de amostragem ativa é direcionar o esforço de treinamento para os exemplos essenciais . Existem diversas abordagens efetivas de selecionar ativamente os exemplos mais importantes para consulta ao supervisor . Entretanto , não é possível , sem incorrer em custos adicionais , testá-las de antemão quanto à sua efetividade numa dada aplicação . Ainda mais crítica é a necessidade de que seja escolhido um algoritmo de aprendizado para integrar a estratégia de aprendizado ativo antes que se disponha de um conjunto de treinamento completo . Para lidar com esses desafios , esta tese apresenta como principais_contribuições  : uma estratégia baseada na inibição do algoritmo de aprendizado nos momentos menos propícios ao seu funcionamento ; e , a experimentação da seleção de algoritmos de aprendizado , estratégias ativas de consulta ou pares estratégia-algoritmo baseada em meta-aprendizado , visando a experimentação de formas de escolha antes e durante o processo de rotulação . A estratégia de amostragem proposta é demonstrada competitiva empiricamente . Adicionalmente , experimentos iniciais com meta-aprendizado indicam a possibilidade de sua aplicação em aprendizado ativo , embora tenha sido identificado que investigações mais extensivas e aprofundadas sejam necessárias para apurar sua real efetividade prática . Importantes contribuições metodológicas são descritas neste documento , incluindo uma análise frequentemente negligenciada pela literatura da área : o risco devido à variabilidade dos algoritmos . Por fim , são propostas as curvas e faixas de ranqueamento , capazes de sumarizar , num único gráfico , experimentos de uma grande coleção de conjuntos de dados 
 A avaliação de trabalhos práticos de programação é uma tarefa dispendiosa . Diversas ferramentas têm sido propostas e desenvolvidas a fim de automatizar as tarefas repetitivas realizadas pelos professores na avaliação de trabalhos práticos de programação e fornecer um feedback mais rápido e adequado aos alunos . No entanto , a adição de uma nova ferramenta aumenta a sobrecarga de novas informações e ambientes que os alunos têm que lidar . De forma análoga , a ferramenta de avaliação é mais um recurso que o professor tem de configurar , manter e ensinar os alunos a utilizar . Por este motivo , vários trabalhos vêm_sendo  conduzidos a fim de integrar ferramentas de avaliação para trabalhos de programação em sistemas de gestão da aprendizagem ( Learning Management Systems LMSs ) . A integração de ferramentas em LMSs promove sua adoção em disciplinas de computação , uma vez que estarão em concordância com o LMS já familiar aos alunos e professores , dispensando a necessidade de adotar , aprender e gerenciar a submissão e correção de trabalhos em um outro ambiente . No entanto , tais trabalhos consistem apenas em modelos conceituais abstratos , não propõem uma integração com vários LMSs ou não tratam certas dificuldades específicas encontradas quando integrando ferramentas de avaliação . Nessa perspectiva , este trabalho tem como objetivo o desenvolvimento da arquitetura IMPACTLE , uma solução que permite a integração de diferentes ferramentas de avaliação para trabalhos de programação em vários LMSs . A ideia é que os alunos e professores possam acessar as funcionalidades das ferramentas por meio dos LMSs em que já estão habituados , sem a necessidade de aprender a utilizar e adotar uma nova ferramenta . Protótipos da arquitetura IMPACTLE foram instanciados e experimentos foram realizados envolvendo a utilização de diferentes ferramentas de avaliação por meio de LMSs . De modo geral , observou-se que a utilização da IMPACTLE possibilita que professores e alunos realizem as tarefas relacionadas às atividades de programação de forma mais eficiente , eficaz e efetiva por meio dos LMSs 
 O teste de mutação é um critério de teste poderoso para detectar falhas e medir a eficácia de um conjunto de dados de teste . No entanto , é uma técnica de teste computacionalmente cara . O alto custo provém principalmente do esforço para gerar dados de teste adequados para matar os mutantes e pela existência de mutantes equivalentes . Nesse contexto , o objetivo desta tese é apresentar uma abordagem chamada de Reach , Infect and Propagation to Mutation Testing ( RIPMuT ) que visa gerar dados de teste e sugerir mutantes equivalentes . A abordagem é composta por dois módulos : ( i ) uma geração automatizada de dados de teste usando subida da encosta e um esquema de fitness de acordo com as condições de alcançabilidade , infeção e propagação ( RIP ) ; e ( ii ) um método para sugerir mutantes equivalentes com base na análise das condições RIP durante o processo de geração de dados de teste . Os experimentos foram conduzidos para avaliar a eficácia da abordagem RIP-MuT e um estudo comparativo com o algoritmo genético e testes aleatórios foi realizado . A abordagem RIP-MuT obteve um escore médio de mutação de 18,25 % maior que o AG e 35,93 % maior que o teste aleatório . O método proposto para detecção de mutantes equivalentes se mostrou viável para redução de custos relacionado a essa atividade , uma vez que obteve uma precisão de 75,05 % na sugestão dos mutantes equivalentes . Portanto , os resultados indicam que a abordagem gera dados de teste adequados capazes de matar a maioria dos mutantes em programas C e , também auxilia a identificar mutantes equivalentes corretamente 
 Em diferentes aplicações de Aprendizado de Máquina podemos estar interessados na minimização do valor esperado de certa função de perda . Para a resolução desse problema , Otimização estocástica e Sample Size Selection têm um papel_importante  . No presente_trabalho  se apresentam as análises teóricas de alguns algoritmos destas duas áreas , incluindo algumas variações que consideram redução da variância . Nos exemplos práticos pode-se observar a vantagem do método Stochastic Gradient Descent em relação ao tempo de processamento e memória , mas , considerando precisão da solução obtida juntamente com o custo de minimização , as metodologias de redução da variância obtêm as melhores soluções . Os algoritmos Dynamic Sample Size Gradient e Line Search with variable sample size selection apesar de obter soluções melhores que as de Stochastic Gradient Descent , a desvantagem se encontra no alto custo_computacional  deles 
 Tendo em vista a grande atenção e aceitação que vem_sendo  dadas ao Paradigma Orientado a Objetos ( OO ) por parte de pesquisadores e desenvolvedores de software , uma das preocupações atuais do grupo de Engenharia de Software do ICMC-USP é o estabelecimento e validação de estratégias de teste de programas_OO  . Entre os interesses de pesquisa do grupo de Engenharia de Software do ICMC estão o Teste Baseado em Fluxo de Dados e o Teste de Mutação , que tiveram sua origem na década de 70 . Considerando que o paradigma OO vem se destacando como uma das tendências de desenvolvimento de software , o que pode ser observado pelo grande número de programas desenvolvidos à luz desse paradigma , o presente_trabalho  visa a caracterizar o estado atual do teste de software OO tanto em termos de técnicas e critérios correntemente utilizados como em termos de ferramentas de teste de software disponíveis . A identificação e avaliação de critérios e ferramentas de suporte para o teste de software OO darão subsídios para a definição de novos estratégias de teste e para a especificação e implementação de ferramentas de apoio no contexto de software OO 
 Em geral , quando modelamos problemas de planejamento probabilístico do mundo_real  , usando o arcabouço de Processos de Decisão Markovianos ( MDPs ) , é difícil obter uma estimativa exata das probabilidades de transição . A incerteza surge naturalmente na especificação de um domínio , por exemplo , durante a aquisição das probabilidades de transição a partir de um especialista ou de dados observados através de técnicas de amostragem , ou ainda de distribuições de transição não estacionárias decorrentes do conhecimento insuficiente do domínio . Com o objetivo de se determinar uma política robusta , dada a incerteza nas transições de estado , Processos de Decisão Markovianos com Probabilidades Imprecisas ( MDP-IPs ) têm sido usados para modelar esses cenários . Infelizmente , apesar de existirem diversos algoritmos de solução para MDP-IPs , muitas_vezes  eles exigem chamadas externas de rotinas de otimização que podem ser extremamente custosas . Para resolver esta deficiência , nesta tese , introduzimos o MDP-IP fatorado e propomos métodos eficientes de programação matemática e programação_dinâmica  que permitem explorar a estrutura de um domínio de aplicação . O método baseado em programação matemática propõe soluções aproximadas eficientes para MDP-IPs fatorados , estendendo abordagens anteriores de programação linear para MDPs fatorados . Essa proposta , baseada numa formulação multilinear para aproximações robustas da função valor de estados , explora a representação fatorada de um MDP-IP , reduzindo em ordens de magnitude o tempo consumido em relação às abordagens não-fatoradas previamente propostas . O segundo método proposto , baseado em programação_dinâmica  , resolve o gargalo computacional existente nas soluções de programação_dinâmica  para MDP-IPs propostas na literatura : a necessidade de resolver múltiplos problemas de otimização não-linear . Assim , mostramos como representar a função valor de maneira compacta usando uma nova estrutura de dados chamada de Diagramas de Decisão Algébrica Parametrizados , e como aplicar técnicas de aproximação para reduzir drasticamente a sobrecarga computacional das chamadas a um otimizador não-linear , produzindo soluções_ótimas  aproximadas com erro limitado . Nossos resultados mostram uma melhoria de tempo e até duas ordens de magnitude em comparação às abordagens tradicionais enumerativas baseadas em programação_dinâmica  e uma melhoria de tempo de até uma ordem de magnitude sobre a extensão de técnicas de iteração de valor aproximadas para MDPs fatorados . Além_disso  , produzimos o menor erro de todos os algoritmos de aproximação avaliados 
 A utilização de códigos QR por deficientes visuais e robôs expande o horizonte de emprego dessa tecnologia e viabiliza várias aplicações cogitadas por um número crescente de trabalhos acadêmicos . Para que os códigos QR possam ser decodificados eles precisam ser apropriadamente enquadrados . A detecção desses símbolos em imagens adquiridas ao acaso é a primeira etapa na construção de um sistema de enquadramento assistido . O presente_trabalho  apresenta uma proposta para a detecção de códigos QR em imagens adquiridas ao acaso . A abordagem proposta é baseada em componentes e dividida em estágios , nos quais primeiramente partes do símbolo são detectadas através da técnica de detecção rápida de objetos proposta por Viola-Jones e em seguida uma análise do conjunto formado por essas partes é realizada para determinar a localização exata do símbolo na cena . A adaptação da técnica proposta por Viola e Jones para esse novo domínio é descrita e detalhada . É realizada uma discussão_sobre  a influência dos muitos parâmetros de treinamento e detecção presentes na abordagem proposta com base em resultados experimentais detalhados . Por fim é apresentado o desempenho em termos da qualidade dos resultados e tempo de processamento em vídeo e conclui-se que a abordagem proposta aponta um caminho promissor na resolução do problema de detecção de códigos QR em imagens adquiridas arbitrariamente na maioria das situações práticas 
 Neste documento apresentamos uma cifra multicanal inspirada nas técnicas de telecomunicações de espalhamento espectral ( Spread Spectrum ) , ou como também conhecido , difusão espectral , para prevenir escuta em uma rede de computadores TCP/IP . Mostramos que dividindo um texto-ilegível ciphertext em blocos de tamanho fixo e transmitindo-os aleatoriamente através de diversos canais estabelecidos entre os agentes ( transmissor - receptor ) é possível aumentar a complexidade da criptanálise por um suposto adversário não autorizado que esteja escutando a comunicação , dessa forma aumentando relativamente a segurança . Mostramos teoricamente que o tempo de quebra da cifra multicanal cresce em ordem fatorial ou exponencial em função do número de canais utilizados na comunicação . Além_disso  , adaptado a um esquema de difusão binária herdado do AES ( Advanced Encryption Standard ) é possível incrementar a segurança da cifra multicanal para maior resiliência contra a criptanálise diferencial 
 Narrativas clínicas são normalmente escritas em linguagem_natural  devido a seu poder descritivo e facilidade de comunicação entre os especialistas . Processar esses dados para fins de descoberta de conhecimento e coleta de estatísticas exige técnicas de extração de informações , com alguns resultados já apresentados na literatura para o domínio jornalístico , mas ainda raras no domínio médico . O presente_trabalho  visa desenvolver um classificador de laudos de anatomia patológica que seja capaz de inferir a topografia e a morfologia de um câncer na Classificação Internacional de Doenças para Oncologia ( CID-O ) . Dados fornecidos pelo A.C. Camargo Cancer Center em São Paulo foram utilizados para treinamento e validação . Técnicas de processamento de linguagem_natural  ( PLN ) aliadas a classificadores bayesianos foram exploradas na busca de qualidade da recuperação da informação , avaliada por meio da medida-F2 . Valores acima de 74 % para o grupo topográfico e de 61 % para o grupo morfológico são relatados , com pequena contribuição das técnicas de PLN e suavização . Os resultados corroboram trabalhos similares e demonstram a necessidade de retreinamento das ferramentas de PLN no domínio médico 
 O objetivo desta dissertação de mestrado é estudar técnicas numéricas para simular_escoamentos  incompressíveis multifásicos e implementar uma ferramenta computacional utilizando malhas hierárquicas e discretizações por diferenças_finitas  . São apresentados a formulação_matemática  e o desenvolvimento do método_numérico  , levando em consideração o caráter multifásico do escoamento . Foi adotado o modelo de força superficial contínua e a representação da interface foi feita pelo método de acompanhamento de fronteira . São expostos todos os testes realizados durante o desenvolvimento da ferramenta para checar cada etapa do método . Finalmente , testes visando verificar o código foram feitos e os resultados obtidos foram considerados satisfatórios para a verificação da ferramenta aqui desenvolvida 
 Este trabalho diz_respeito  à solução_numérica  de equação algébrico-diferencial ( E.A.D . ) com singularidades . Inicialmente realizamos um estudo sobre a teoria geral de E.A.D . e dos métodos_numéricos  a ela aplicado . Uma nova classe de métodos para E.A.D . com singularidades , desenvolvida em [ Castelo-Tavares-94 ] , é introduzida . Na implementação dessa nova classe de métodos , utilizamos o método BDF com ordem e amplitude do passo variáveis . A utilização do método BDF , bem como as técnicas de seleção de ordem e amplitude do passo foram baseadas nas utilizadas no código DASSL [ Brenan-89 ] . Finalizamos o trabalho com alguns exemplos numéricos que ilustram o funcionamento dos métodos considerados 
 Este trabalho é parte de um empreendimento maior que tem por objetivo o desenvolvimento de um Sistema Gerenciador de Bases de dados Orientado a Objetos ( GEO - Gerenciador de Objetos ) , baseado em um Modelo de Representação de Dados Orientado a Objetos ( MRO - Modelo de Representação de Objetos ) . Neste trabalho foi desenvolvido um conjunto de softwares e especificações para manipulação de imagens em Bases de Dados para compor o Sistema Gerenciador de Base de Dados GEO , permitindo que o mesmo ao ser integrado a um Sistema de Reconstrução , Manipulação , AnáIise e Visualização de Imagens Médicas seja classificado entre os sistemas chamados de Picture Archiving and Communications Systems ( PACS ) Com a realização deste trabalho , então , o GEO passou a suportar a armazenagem de imagens , permitindo de forma eficiente a manipulação das mesmas 
 Este trabalho propõe o desenvolvimento de um toolkit para a construção de interfaces gráficas apoiado no paradigma de Orientação a Objeto . O toolkit , denominado ICMSCKit , consiste de uma biblioteca de classes implementadas para a linguagem C++ e visa facilitar a criação de objetos de interface tais como janelas , botões e menus . A comunicação entre a interface e a aplicação é feita através de um mecanismo de troca de mensagens e chamadas de rotinas do tipo callback procedures , tornando os códigos da aplicação e interface independentes . O toolkit foi desenvolvido inicialmente para ambiente Microsoft Windows 3.1 , porém `` esconde '' todas as características da plataforma `` nativa '' tornando-o desta forma portável para outros ambientes operacionais . Além da característica de portabilidade , o ICMSCKit é extensível via o mecanismo de sub-classificação da linguagem C++ ou via os recursos de composição de objetos oferecidos pelas classes do toolkit 
 O objetivo do presente_trabalho  é definir e implementar o Módulo de Interface Gráfica para o Ambiente de Simulação Automático ( ASiA ) , desenvolvido pelo Grupo de Sistemas_Distribuídos  e Programação Concorrente do Departamento de Ciências de Computação e Estatística do Instituto de Ciências_Matemáticas  da Universidade de São Paulo , Campus de São Carlos . Através do Módulo Visual , o ASiA apresenta informações sobre o modelo em simulação , como : rastreamento do programa , apresentação de relatórios , geração de gráficos e edição de valores de parâmetros internos do modelo . Além disto , o Módulo Visual permite que o usuário controle todas as opções disponíveis , através de menus e ícones . O sistema apresentado nesta dissertação é desenvolvido e implementado , segundo o paradigma de orientação a objeto , utilizando-se o compilador Borland C++ , versão 4.02 , e um conjunto de ferramentas para o desenvolvimento de sistemas orientados a objetos , chamado OWL ( Object Windows Library ) . O trabalho fornece um exemplo completo de simulação , através da utilização do Módulo Visual , tornando clara a manipulação e as opções que o Módulo Visual proporciona aos seus usuários . É interessante salientar que , embora seja desenvolvido para trabalhar em conjunto com o ASiA , o Módulo Visual pode ser utilizado separadamente . Para tanto , o usuário deve ter conhecimento de programação orientada a objeto e dominar a extensão funcional SMPL ( Simulation Program Language ) 
 A experiência com autoria de material didático multimídia para propósitos educacionais mostra um grande problema : como prover uma maneira de tratar objetos multimídia de modo que usuários inexperientes ( como professores ) possam estar aptos a projetar e construir suas próprias apresentações ? A criação de tais apresentações envolve fatores como armazenamento , entrega , busca e apresentação de material multimídia ( vídeo em especial ) . Uma infra-estrutura básica que armazene e entregue eficientemente os dados de vídeo é necessária , porém , outro ponto importante é organizar esses dados armazenados no servidor de forma a facilitar seu acesso por parte dos usuários . Neste trabalho , isto é alcançado através do uso de um sistema interativo de recuperação e gerenciamento de informações projetado para facilitar o acesso a itens ( ou parte deles ) armazenados no servidor . A principal característica de tal sistema é o uso de uma base de metadados contendo os atributos dos vídeos armazenados no servidor . Buscas podem ser feitas por título , assunto , tamanho , autor , conteúdo ou , mais importante no caso de material didático , por cenas ou frames específicos . O sistema foi implementado segundo uma abordagem cliente/servidor utilizando a linguagem de programação JAVA . A comunicação entre clientes e servidores é realizada através do uso do Visibroker 3.0 , que é uma ferramenta de programação para Objetos Distribuídos segundo o padrão CORBA . O acesso aos dados a partir da base de metadados é realizado através do uso de um driver PostgreSQL que segue a API JDBC . Para propósitos de avaliação do sistema um player foi construído utilizando a ferramenta Java Media Framework ( JMF ) . Foi realizada uma análise para a verificação do impacto da utilização das tecnologias CORBA e JDBC no sistema . Foi detectado que a utilização da tecnologia JDBC impõe um atraso muito mais significante que a utilização da tecnologia CORBA . Outra conclusão é que a utilização de metadados provê uma melhor interatividade em buscas , permite economia de tempo durante o processo de edição e provê economia de espaço de armazenamento através do compartilhamento de objetos como vídeos , cenas e frames 

 Atualmente , devido ao incessante aumento dos documentos científicos disponíveis na rede mundial de computadores , as ferrametas de busca tornaram-se um importante auxílio para recuperação de informação a partir da Internet em todas as áreas de conhecimento para pesquisadores e usuários . Entretanto , as atuais ferramentas de busca disponíveis selecionam uma enorme lista de páginas , cabendo ao usuário a tarefa final de escolher aquelas que realmente são relevantes a sua pesquisa . Assim , é importante o desenvolvimento de técnicas e ferramentas que não apenas retornem uma lista de possíveis documentos relacionados com a consulta apresentada pelo usuário , mas que organizem essa informação de acordo com o conteúdo de tais documentos , e apresentem o resultado da busca em uma representação gráfica que auxilie a exploração e o entendimento geral dos documentos recuperados . Neste contexto , foi proposto o projeto de uma Ferramenta Inteligente de Apoio à Pesquisa ( FIP ) , do qual este trabalho é parte . O objetivo deste trabalho é analisar estratégias de recuperação automática de artigos científicos sobre uma determinada área de pesquisa a partir da Web , que poderá ser adotada pelo módulo de recuperação da FIP . Neste trabalho são considerados artigos escritos em inglês , no formato PDF , abrangendo as áreas da Ciência da Computação . Corpora de treino e teste foram usados para avaliação das abordagens simbólicas de Aprendizado de Máquina na indução de regras que poderão ser inseridas em um crawler inteligente para recuperação automática de artigos dessas áreas . Diversos experimentos foram executados para definir parâmetros de pré-processamento apropriados ao domínio , bem como para definir a melhor estratégia de aplicação das regras induzidas e do melhor algoritmo simbólico de indução 
 Dentre as tecnologias de desenvolvimento de software que promovem o reuso com o objetivo de construir sistemas com prazos e custos menores , sem sacrificar a qualidade dos produtos , está o Desenvolvimento Baseado em Componentes ( DBC ) . O Desenvolvimento Baseado em Componentes consiste na construção de sistemas pela composição de componentes de software de acordo com um processo de desenvolvimento específico . Para garantir a qualidade desses sistemas , é importante garantir a qualidade de seus componentes . A falta da garantia da qualidade dos componentes de software destinados à reutilização é um dos fatores de inibição do DBC , e existe certa carência de pesquisas sobre a qualidade de componentes de software . Desta maneira , o principal objetivo deste trabalho foi a definição de um modelo de qualidade específico para componentes de software , fornecendo a base para a especificação de requisitos de qualidade e para a avaliação de qualidade dos mesmos . O Modelo está embasado nas normas ISO/IEC 9126 e ISO/IEC 12119 , e também na literatura especializada . Uma ferramenta para apoiar avaliações de componentes ( e de produtos de software de forma geral ) também foi desenvolvida . Foram realizadas ainda quatro avaliações de componentes através de estudos de casos para verificar a aplicabilidade e utilidade do modelo de qualidade e da ferramenta desenvolvida . Dois questionários foram respondidos pelos avaliadores responsáveis pelas avaliações coletando assim , suas considerações sobre o modelo de qualidade e sobre a 
 Esta dissertação de mestrado aborda a introdução de reconhecimento de sessões http em um modelo de servidor web com serviços diferenciados ( SWDS ) . Algumas técnicas foram desenvolvidas com o objetivo de produzir diferenciação de serviços junto com garantias de que novas sessões poderiam ser aceitas no sistema . Esses objetivos constituem requisitos essenciais na Internet atual , especialmente para aplicações web modernas . Um novo esquema para controle de admissão de sessões foi desenvolvido e introduzido no modelo SWDS , considerando dois mecanismos para aceitar novas sessões , com garantia de nalização . O mecanismo que estima a capacidade do sistema de aceitar novas sessões , baseado em um modelo de sessão construído dinamicamente a partir da carga do sistema , é destacado . A proposta global deste trabalho também considera um controle de admissão de requisições , baseado em sessões , onde a nova política de atendimento criada mantém o sistema livre de sobrecargas e oferece atendimento diferenciado para as sessões . As políticas de negociação desenvolvidas para o controle de admissão de requisições tiveram um papel_importante  neste trabalho , contribuindo para a priorização do atendimento das sessões . Os resultados obtidos_mostram  que os controles propostos constituem estruturas fundamentais para a estabilidade do desempenho do sistema , tanto quanto os mecanismos desenvolvidos têm grande_importância  no atendimento das sessões e , portanto de seus clientes , através de uma abordagem baseada em diferenciação 
 As arquiteturas multiprocessadas heterogêneas têm como objetivo principal a extração de maior desempenho da execução dos processos , por meio da utilização de núcleos apropriados às suas demandas . No entanto , a extração de maior desempenho é dependente de um mecanismo eficiente de escalonamento , capaz de identificar as demandas dos processos em tempo real e , a partir delas , designar o processador mais adequado , de acordo com seus recursos . Este trabalho tem como objetivo propor e implementar o modelo de um escalonador para arquiteturas multiprocessadas heterogêneas , baseado em software e hardware , aplicado ao sistema_operacional  Linux e ao processador SPARC Leon3 , como prova de conceito . Nesse sentido , foram implementados monitores de desempenho dentro dos processadores , os quais identificam as demandas dos processos em tempo real . Para cada processo , sua demanda é projetada para os demais processadores da arquitetura e em seguida é realizado um balanceamento visando maximizar o desempenho total do sistema , distribuindo os processos entre processadores , de modo a diminuir o tempo total de processamento de todos os processos . O algoritmo de maximização Hungarian , utilizado no balanceamento do escalonador , foi desenvolvido em hardware , proporcionando paralelismo e maior desempenho na execução do algoritmo . O escalonador foi validado por meio da execução paralela de diversos benchmarks , resultando na diminuição dos tempos de execução em relação ao escalonador sem suporte à 
 Este trabalho apresenta o projeto e implementação de melhorias na ferramenta httperf , que é um benchmark para servidores_Web  . A melhoria implementada constitui na interpretação de logs para a ext.ração de parâmetros de carga de trabalho . Para tanto , foi necessária a criação de um ferramenta à parte , que trabalha o arquivo de log e retira os parâmetros a serem utilizados pelo httperf . O desenvolvimento do trabalho foi baseado em uma revisão bibliográfica cobrindo avaliação de desempenho de sistemas computacionais , características da Web e ferramentas/ benchmarks especializados em sistemas baseados na Web . Apresenta ainda uma discussão_sobre  arquiteturas de servidores_Web  e o uso dos benchmarks na avaliação desses servidores . Foi desenvolvido um estudo sobre as ferramentas , onde foram analisadas suas características , com o intuito de identificar características não abordadas , mas julgadas importantes nesse tipo de avaliação . A partir desse estudo selecionon-se algumas das características identificadas que foram incorporadas à ferramenta escolhida . Finalmente , a ferramenta que recebeu as alterações foi avaliada por meio de diversos experimentos , quando os resultados obtidos mostraram que a implementação desenvolvida era viável 
 Esta tese de doutorado apresenta um modelo de servidor Web mostrando as componentes internas básicas do software servidor , representando a comunicação completa entre cliente e servidor . O modelo construído pode ser usado para avaliação de desempenho e toma como base um modelo já existente no qual são feitas uma extensão e uma modificação inéditas . As principais técnicas usadas foram a teoria das filas , parametrização do modelo , solução do modelo por simulação e a validação através de observações obtidas em experimentos controlados de laboratório . Como contribuições citam-se aqui : ( i ) a extensão para conexões persistentes , o que diz_respeito  a uma característica já padronizada no protocolo HTTP 1.1 , porém não considerada no modelo base anterior ; ( ii ) a modificação quanto à carga de chegada , vindo esta segunda contribuição a constituir duas melhorias . A carga orientada a sessão é de modelagem mais fácil e a distribuição da chegada de sessões tem representação mais fidedigna do que a distribuição da chegada de documentos , conforme considerado no modelo base 
 Atualmente , há uma grande diversidade de aplicações que funcionam sobre a infraestrutura da Internet , as quais apresentam diferentes necessidades . Como consequência , seu modelo de serviços de melhor esforço tem sido incrementado , de modo a permitir o fornecimento de diferentes níveis 011 classes de serviço aos clientes . Contudo , de nada adianta garantir uma qualidade de serviço diferenciada 11a rede , se os elementos finais dessa cadeia , os servidores , não estiverem habilitados a reconhecê-la . Nesse contexto , este trabalho propõe uma arquitetura para um servidor web capaz de fornecer serviços diferenciados a seus clientes , segundo suas características de demanda . Esta arquitetura 6 verificada por meio de um modelo de simulação e são utilizados logs de acesso a servidores_web  como carga de trabalho . Foram implementados três mecanismos de diferenciação de serviços na arquitetura , os quais correspondem a duas_abordagens  distintas : enfileiramento baseado cm classes e escalonamento baseado em prioridades . Dentre eles , destaca-se o mecanismo de prioridades adaptativo , que realiza uma sintonia fina da qualidade de serviço fornecida , determinando quão rigoroso será o esquema de prioridades empregado . O sistema adquire , então , capacidade de adaptação a variações 11a carga de trabalho , característica essencial em um ambiente altamente dinâmico como a Web . A arquitetura proposta contempla também o controle de admissão de requisições , a fim de evitar a sobrecarga do sistema , caso a demanda dos usuários atinja níveis elevados . Foram implementados três mecanismos de controle de admissão , os quais utilizam diferentes parâmetros como referência para a tomada de decisão . O controle de admissão revelou-se de fundamental_importância  para a estabilidade do sistema , bem como para a garantia da qualidade do serviço fornecido aos clientes 
 Este trabalho apresenta uma nova política de escalonamento para aplicações paralelas Network-Bound baseada no impacto do processamento causado pela comunicação entre processos . O modelo utilizado quantifica o volume de tráfego imposto sobre a rede de comunicação por meio dos parâmetros latência e sobrecarga . Tais parâmetros representam a carga que cada processo impõe sobre a rede e o atraso sobre a CPU devido às operações na rede . Esse atraso é representado na política por meio da métrica slowdown . Equações matemáticas são definidas para a quantificação dos custos envolvidos no processamento e na troca de mensagens , do mesmo modo , são propostas equações para determinar a largura máxima de banda ( bandwidth ) utilizadas nas tomadas de decisões de escalonamento . Outra característica importante da política é a definição de uma constante k , que delimita a utilização máxima permitida da rede de comunicação . O valor de k define a adoção de duas possíveis técnicas de escalonamento : escalonamento em grupo , ou por intermédio da rede de comunicação . As técnicas propostas são incorporadas à política de escalonamento DPWP ( originalmente CPU-Bound ) gerando uma extensão Network-Bound . Resultados experimentais e de simulação confirmam o aumento de desempenho de aplicações paralelas sob supervisão da política DPWP estendida , denominada NB SP , quando comparadas às execuções supervisionadas pela DPWP original 
 O grande volume de dados armazenados em meio digital dificulta a anáalise e extração de informações por um ser humano sem que seja utilizada alguma ferramenta computacional inteligente . A área de Aprendizado de Máquina ( AM ) estuda e desenvolve algoritmos para o processamento e obtenção automática de conhecimento em dados digitais . Tradicionalmente , os algoritmos de AM modelam os dados analisados com base na abordagem proposicional ; entretanto , recentemente com a disponibilidade de conjuntos de dados relacionais novas abordagens têm sido estudadas , como a modelagem utilizando redes complexas . Redes complexas é uma área de pesquisa recente e ativa que têm atraíido a atenção de pesquisadores e tem sido aplicada em diversos_domínios  . Mais especificamente , o estudo de detecção de comunidades em redes complexas é o tema principal deste trabalho . Detectar comunidades consiste em buscar grupos de vértices densamente conectados entre si em uma rede . Detectar a melhor divisão em comunidades de uma rede é um problema NP-completo , o que requer que o desenvolvimento de soluções viáveis baseiem-se em heurísticas como , por exemplo , medidas de qualidade . Newman prop^os a medida de modularidade Q que tem se mostrado eficiiente na análise de comunidades em redes . Este trabalho apresenta o Algoritmo Multinível de Otimização de Modularidade ( AMOM ) que é baseado a na otimização da medida de modularidade e integrado na estratégia multinível . A estratégia multinível é composta de três fases : ( i ) sucessivas compactações da rede inicial com base em contrações de arestas e fus~oes de vértices , ( ii ) particionamento da rede reduzida utilizando Algoritmo de Otimização de Modularidade ( AOM ) modificado , e ( iii ) sucessivas descompactações das redes intermediárias até que se retorne a rede inicial . O principal atrativo da estratégia é viabilizar a utilização de algoritmos custosos no particionamento do grafo compactado , uma vez que neste grafo a quantidade de vértices e arestas é uma fração reduzida em relação ao grafo inicial . O trabalho também propõe dois novos métodos para refinamento dos particionamentos durante a fase de uncoasening . A fiim de avaliar a escalabilidade e eficiiência da metodologia_proposta  foram realizados_experimentos  empíricos em redes consideradas benchmark . Os resultados demonstram um significativo ganho de desempenho , mantendo bons_resultados  
 Devido ao crescimento do volume de imagens e , consequentemente , da grande quantidade e complexidade das características que as representam , surge a necessidade de selecionar características mais relevantes que minimizam os problemas causados pela alta dimensionalidade e correlação e que melhoram a eficiência e a eficácia das atividades que utilizarão o conjunto de dados . Existem diversos métodos tradicionais de seleção que se baseiam em análises estatísticas dos dados ou em redes neurais_artificiais  . Este trabalho propõe a inclusão de técnicas de mineração visual de dados , particularmente , projeção de dados multidimensionais , para apoiar o processo de seleção . Projeção de dados busca mapear dados de um espaço m-dimensional em um espaço p-dimensional , p < m e geralmente igual a 2 ou 3 , preservando ao máximo as relações de distância existentes entre os dados . Tradicionalmente , cada imagem é representada por um ponto e pontos projetados próximos uns aos outros indicam agrupamentos de imagens que compartilham as mesmas propriedades . No entanto , este trabalho propõe a projeção de características . Dessa forma , ao selecionarmos apenas algumas amostras de cada agrupamento da projeção , teremos um subconjunto de características , configurando um processo de seleção . A qualidade dos subconjuntos de características selecionados é avaliada comparando-se as projeções obtidas para estes subconjuntos com a projeção obtida com conjunto original de dados . Isto é feito quantitativamente , por meio da medida de silhueta , e qualitativamente , pela observação visual da projeção . Além da seleção apoiada por projeção , este trabalho propõe um aprimoramento no seletor de características baseado no cálculo de saliências de uma rede_neural  Multilayer Perceptron . Esta alteração , que visa selecionar características mais discriminantes e reduzir a quantidade de cálculos para se obter as saliências , utiliza informações provenientes dos agrupamentos de características , de forma a alterar a topologia da rede_neural  em que se baseia o seletor . Os resultados mostraram que a seleção de características baseada em projeção obtém subconjuntos capazes de gerar novas projeções com qualidade visual satisfatória . Em relação ao seletor por saliência proposto , este também gera subconjuntos responsáveis por altas taxas de classificação de imagens e por novas projeções com bons valores de 
 Um dos principais interesses na descoberta do conhecimento e mineração de dados é a indução de regras de associação . Regras de associação caracterizam as relações entre os dados a partir de um conjunto de dados estruturado com transações , onde cada transação contém um subconjunto de itens . Seja X e Y dois conjuntos de itens disjuntos , então a regra X → Y define um relacionamento , isto é , a dependência ou a co-ocorrência entre os conjuntos X e Y. Um dos algoritmos mais conhecidos para geração de regras de associação é o algoritmo Apriori . Ele explora regras de associação que respeitam o limiar suporte mínimo , ou seja , as regras devem aparecer em uma quantidade mínima de transações . Esse limiar tem a capacidade de controlar a quantidade de regras extraídas durante a mineração . Entretanto , a frequência ou suporte não consegue medir o nível de interesse de uma regra . Para medir a importância ou interesse de uma regra em relação a outras foram desenvolvidas medidas de interesse . Tais medidas são calculadas a partir das frequências dos conjuntos de itens X , Y e do par XY . Apesar das medidas de interesse realizarem uma filtragem das regras desinteressantes , elas não acarretam na diminuição no tempo de execução da mineração . Para vencer essa dificuldade , técnicas que exploram diretamente regras de associação ótimas foram desenvolvidas . Um conjunto de regras de associação ótimas é um conjunto de regras que otimiza uma determinada medida de interesse . Na literatura existem muitos trabalhos que buscam esse tipo de conjunto de regras de forma direta e eficiente . O trabalho corrente segue esta mesma direção e visou a melhoria dessa tarefa por descobrir uma quantidade arbitrária de regras de associação ótimas . As abordagens anteriores apresentam um entrave em especial , que é a utilização do algoritmo Apriori . Tal técnica realiza uma busca em largura sobre os conjuntos de itens . No entanto , as técnicas mais promissoras que descobrem regras ótimas realizam busca em profundidade sobre o espaço de busca de regras . Em virtude dessa característica , neste trabalho foi adotada a técnica FP-growth , que realiza uma busca em profundidade sobre os conjuntos de itens explorados . Além da adoção da técnica FP-growth , foram desenvolvidas novas estratégias de poda e uma nova estratégia de busca na travessia do espaço de regras . Todas essas inovações foram adicionadas aos algoritmos desenvolvidos no corrente trabalho e proporcionaram melhor eficiência ( tempo de execução ) em relação ao algoritmo baseline em todos os testes . Tais testes foram realizados sobre conjuntos de dados reais e artificiais 
 O tratamento multidocumento mostra-se indispensável no cenário atual das mídias eletrônicas , em que são produzidos diversos documentos sobre um mesmo tópico , principalmente quando se considera a explosão de informação permitida pela web . Tanto leitores quanto aplicações computacionais se beneficiam da análise discursiva multidocumento por meio da qual são explicitadas relações entre as porções dos documentos , por exemplo , relações de equivalência , contradição ou de contextualização de alguma informação . A fim de realizar o tratamento automático multidocumento , adota-se neste trabalho a teoria linguístico-computacional CST ( Cross-document Structure Theory , Radev , 2000 ) . Esse tipo de conhecimento multidocumento permite que ( i ) se tratem mais apropriadamente fenômenos como redundância , complementariedade e contradição de informações e , consequentemente , ( ii ) produzam-se sistemas melhores de processamento textual , como buscadores web mais inteligentes e sumarizadores automáticos . Neste trabalho é apresentada uma metodologia de identificação dessas relações explorando-se técnicas de aprendizado automático do paradigma tradicional e hierárquico . Para relações que não são passíveis de identificação por aprendizado automático foram desenvolvidas regras para sua identificação . Por fim , um parser é gerado contendo classificadores e 
 Métodos de Lagrangianos aumentados são muito utilizados para resolver problemas de minimização de funções sujeitas a restrições gerais . Em particular , estudamos um método de Lagrangianos aumentados que utiliza a função PHR , implementado em ALGENCAN , e observamos seu comportamento quando o aplicamos na resolução de um problema encontrado na área de Computação_Gráfica  . O problema estudado é um problema encontrado na geração de malhas de superfícies , na etapa de pós-processamento , para o qual propomos uma técnica de otimização visando a melhoria dos elementos da malha . Quando se trata de geração de malhas de superfícies em 'R POT . 3 ' , parametrizações de malhas triângulares que representam superfícies são usadas em muitas aplicações de processamento de malhas para vários fins . Muitas vezes é necessário preservar a métrica da superfície e , assim , minimizar a deformação do ângulo e da área . A técnica que propomos de otimização visa melhorar as distorções de ângulos e áreas impostas por uma parametrização . Para verificar o comportamento da técnica proposta , implementamo-na em C++ e utilizamos algumas malhas de modelos clássicos da literatura para realizar os experimentos numéricos . Os resultados obtidos foram 
 Aplicações têm tradicionalmente utilizado o paradigma de programação sequencial . Com a recente expansão da computação_paralela  , em particular os processadores multinúcleo e ambientes distribuídos , esse paradigma tornou-se um obstáculo para a utilização dos recursos disponíveis nesses sistemas , uma vez que a maior_parte  das aplicações tornam-se restrita à execução sobre um único núcleo de processamento . Nesse sentido , este trabalho de mestrado introduz uma abordagem para paralelizar programas sequenciais de forma automática e transparente , diretamente sobre o código-binário , de forma a melhor utilizar os recursos disponíveis em computadores multinúcleo . A abordagem consiste na desmontagem ( disassembly ) de aplicações Intel x86 e sua posterior tradução para uma linguagem intermediária . Em seguida , são produzidos grafos de fluxo e dependências , os quais são utilizados como base para o particionamento das aplicações em unidades paralelas . Por fim , a aplicação é remontada ( assembly ) e traduzida novamente para a arquitetura original . Essa abordagem permite a paralelização de aplicações sem a necessidade de esforço suplementar por parte de desenvolvedores e usuários 
 O teste de programas concorrentes é uma atividade que envolve diferentes perspectivas . Uma das mais conhecidas refere-se ao desenvolvimento de novos conhecimentos sobre critérios , modelos e ferramentas de teste que auxiliem o testador nessa atividade . Outra perspectiva , igualmente importante , porém , ainda incipiente , é a avaliação da atividade de teste de programas concorrentes com relação à sua eficiência e eficácia para revelar defeitos de difícil detecção . O projeto TestPar em desenvolvimento no ICMC/USP tem abordado essas duas perspectivas ao longo dos últimos_anos  , onde novas tecnologias de teste vêm_sendo  desenvolvidas e avaliadas sistematicamente . Este trabalho inseriu-se no contexto do projeto TestPar e teve por objetivo principal contribuir para melhorar a avaliação da atividade de teste de programas concorrentes , através do desenvolvimento de benchmarks específicos para este contexto . Essa avaliação representa um desafio para a área de teste , sendo essencial a existência de benchmarks simples o bastante para serem validados manualmente , se necessário , e complexos o bastante para exercitar aspectos não triviais de comunicação e sincronização , encontrados de fato nos programas concorrentes . Assim , neste trabalho de mestrado foram desenvolvidos benchmarks livres de defeitos conhecidos e algumas versões de benchmarks com defeitos intencionalmente inseridos , baseados em taxonomias de defeitos . Esses benchmarks seguiram uma série de características bem definidas , contando ainda com uma documentação padronizada e completa . Os benchmarks foram validados através da condução de estudos_experimentais  , do uso em diferentes projetos de pesquisa e também com a verificação da sua aplicabilidade para fins educacionais . Os resultados obtidos demonstram que os benchmarks atingiram os objetivos para os quais foram propostos , gerando uma demanda controlada e qualificada sobre modelo , critérios e a ferramenta de teste desenvolvidos no projeto TestPar . Os experimentos_realizados  permitiram destacar pontos positivos e limitações desses artefatos . Outra aplicação dos benchmarks foi como recurso educacional para o ensino em disciplinas como programação concorrente 
 A terapia do espelho , técnica utilizada no tratamento de pacientes em reabilitação neurológica , tem mostrado bons_resultados  a um custo relativamente baixo . Nessa terapia , um espelho é colocado ao longo do plano sagital mediano do paciente entre os dois membros , superiores ou inferiores , com o intuito de transmitir estímulos visuais para o cérebro para induzir a ilusão dos membros em questão estarem se deslocando de forma síncrona . Considerando a simplicidade , os resultados obtidos e o baixo custo da terapia do espelho , neste trabalho exploramos recursos tipicamente disponíveis em dispositivos_móveis  do tipo tablet e smartphones para propor um modelo de acompanhamento remoto baseado na gravação e no compartilhamento de vídeos de sessões de terapia do espelho considerando dois tipos de usuários : terapeutas e pacientes . O modelo é apresentado na forma de um protótipo de um sistema de acompanhamento remoto de pacientes que utilizam a terapia do espelho em sua reabilitação . Especialistas avaliaram que o sistema apresenta grande potencial no auxilio à reabilitação de membros superiores , podendo ser utilizado por qualquer paciente em reabilitação pela terapia do espelho devido à sua facilidade de uso . Entre os trabalhos futuros viabilizados pela versão atual do sistema estão testes com pacientes em tratamento para verificação das implicações resultantes de sua utilização 
 O armazenamento de arquivos e dados está seguindo um paradigma de mudança para a Internet utilizando a nuvem . Porém , este processo ainda gera algumas dúvidas em relação à segurança e a disponibilidade dos arquivos . Este projeto de mestrado_propõe  criar uma camada de abstração entre diversos servidores de armazenamento público , sem infraestrutura adicional , e possibilitar ao usuário comum um método de armazenamento distribuído , com maior redundância e segurança se comparada às abordagens atuais de armazenamento . Outro aspecto importante deste projeto foi criar uma ferramenta que seja flexível o suficiente para ser fácil de ser utilizada por outros pesquisadores para avaliar novas técnicas de armazenamento e seleção automática de estratégias de dispersão . Este projeto foi feito utilizando conceitos de dispersão de arquivos e de mecanismos de seleção automática utilizando atributos de QoS . A primeira contribuição foi um protótipo denominado FlexSky que implementa os mecanismos de dispersão utilizados . Devido a quantidade grande de parâmetros para se escolher no processo de dispersão , foi necessário desenvolver um mecanismo que reduzisse a quantidade de parâmetros para que um usuário comum consiga utilizar a ferramenta . Este mecanismo foi a segunda contribuição ( MSSF ) , no qual foi criado um modelo baseado em otimização_linear  inteira para realizar a seleção automática de estratégias de armazenamento multinuvem . Para se avaliar as contribuições foram propostos testes qualitativos de usabilidade da ferramenta e teste quantitativos de desempenho dos processos da ferramenta . Os resultados sugerem que a ferramenta FlexSky facilitou o estudo destes mecanismos de dispersão e possibilita uma avaliação de desempenho deles . Jà o MSSF consegue a partir de atributos de QoS escolher uma estratégia ótima que maximize os parâmetros desejados do processo de dispersão para diversos casos considerando diferentes quantidades de módulos e de provedores 
 Técnicas de projeção desempenham papel_importante  na análise e exploração de dados multidimensionais , já que permitem visualizar informações muitas_vezes  ocultas na alta dimensão . Esta tese explora o potencial destas técnicas para resolver problemas relacionados à : 1 ) identificação de agrupamentos e 2 ) busca por similaridade em dados multidimensionais . Para identificação de agrupamentos foi desenvolvida uma técnica de projeção local e interativa que , além de projetar dados com ótima preservação de distâncias , permite que o usuário modifique o layout da projeção , agrupando um número reduzido de amostras representativas no espaço_visual  , de acordo com suas características . Os mapeamentos produzidos tendem a seguir o layout das amostras organizadas pelo usuário , facilitando a organização dos dados e identificação de agrupamentos . Contudo , nem sempre é possível selecionar ou agrupar amostras com base em suas características visuais de forma confiável , principalmente quando os dados não são rotulados . Para estas situações , um novo método para identificação de agrupamentos baseado em projeção foi proposto , o qual opera no espaço_visual  , garantindo que os grupos obtidos não fiquem fragmentados durante a visualização . Além_disso  , é orientado por um mecanismo de amostragem determinístico , apto a identificar instâncias que representam bem o conjunto de dados como um todo e capaz de operar mesmo em conjuntos de dados desbalanceados . Para o segundo problema : busca por similaridade em dados multidimensionais , uma família de métricas baseada em classes foi construída para projetar os dados , com o objetivo de minimizar a dissimilaridade entre pares de objetos pertencentes à mesma classe e , ao mesmo tempo , maximizá-la para objetos pertencentes a classes distintas . As métricas classes-específicas são avaliadas no contexto de recuperação de imagens com base em conteúdo . Com o intuito de aumentar a precisão da família de métricas classes-específicas , outra técnica foi desenvolvida , a qual emprega a teoria dos conjuntos_fuzzy  para estimar um valor de incerteza que é transferido para a métrica , aumentando sua precisão . Os resultados confirmam a efetividade das técnicas desenvolvidas , as quais representam significativa contribuição na tarefa de identificação de grupos e busca por similaridade em dados multidimensionais 
 O objetivo deste trabalho é tornar possível a inserção de um robô humanoide para auxiliar pacientes em sessões de fisioterapia . Um sistema robótico é proposto que utiliza um robô humanoide , denominado NAO , visando analisar os movimentos feitos pelos pacientes e corrigi-los se necessário , além de motivá-los durante uma sessão de fisioterapia . O sistema desenvolvido permite que o robô , em primeiro lugar , aprenda um exercício correto de fisioterapia observando sua execução por um fisioterapeuta ; em segundo lugar , que ele demonstre o exercício para que um paciente possa imitá-lo ; e , finalmente , corrija erros cometidos pelo paciente durante a execução do exercício . O exercício correto é capturado por um sensor Kinect e dividido em uma sequência de estados em dimensão espaço-temporal usando k-means clustering . Estes estados então formam uma máquina de estados finitos para verificar se os movimentos do paciente estão corretos . A transição de um estado para o próximo corresponde a movimentos parciais que compõem o movimento aprendido , e acontece somente quando o robô observa o mesmo movimento parcial executado corretamente pelo paciente ; caso contrário o robô sugere uma correção e pede que o paciente tente novamente . O sistema foi testado com vários pacientes em tratamento fisioterapêutico para problemas motores . Os resultados obtidos , em termos de precisão e recuperação para cada movimento , mostraram-se muito promissores . Além_disso  , o estado_emocional  dos pacientes foi também avaliado por meio de um questionário aplicado antes e depois do tratamento e durante o tratamento com um software de reconhecimento facial de emoções e os resultados indicam um impacto emocional bastante positivo e que pode vir a auxiliar pacientes durante tratamento fisioterapêuticos 
 A qualidade das hipóteses induzidas pelos atuais sistemas de Aprendizado de Máquina depende principalmente da quantidade e da qualidade dos atributos e exemplos utilizados no treinamento . Freqüentemente , resultados experimentais obtidos sobre grandes bases de dados , que possuem muitos atributos irrelevantes , resultam em hipóteses de baixa precisão . Por outro_lado  , muitos dos sistemas de aprendizado de máquina conhecidos não estão preparados para trabalhar com uma quantidade muito grande de exemplos . Assim , uma das áreas de pesquisa mais ativas em aprendizado de máquina tem girado em torno de técnicas que sejam capazes de ampliar a capacidade dos algoritmos de aprendizado para processar muitos exemplos de treinamento , atributos e classes 

 Para que conceitos sejam aprendidos a partir de grandes bases de dados utilizando Aprendizado de Máquina , pode-se utilizar duas_abordagens  . A primeira realiza uma seleção de exemplos e atributos mais relevantes , e a segunda ´e a abordagem de ensembles . Um ensemble ´e um conjunto de classificadores cujas decisões individuais são combinadas de alguma_forma  para classificar um novo caso . Ainda que ensembles classifiquem novos exemplos melhor que cada classificador individual , eles se comportam como caixas pretas , no sentido de nao oferecer ao usuário alguma explicação relacionada à classificação por eles fornecida 

 O objetivo deste trabalho é propor uma forma de combinação de classificadores simbólicos , ou seja , classificadores induzidos por algoritmos de AM simbólicos , nos quais o conhecimento é descrito na forma de regras if-then ou equivalentes , para se trabalhar com grandes bases de dados . A nossa proposta é a seguinte : dada uma grande base de dados , divide-se esta base aleatoriamente em pequenas bases de tal forma que é viável fornecer essas bases de tamanho menor a um ou vários algoritmos de AM simbólicos . Logo após , as regras que constituem os classificadores induzidos por esses algoritmos são combinadas em um único classificador . Para analisar a viabilidade do objetivo proposto , foi implementado um sistema na linguagem de programação lógica Prolog , com a finalidade de ( a ) avaliar regras de conhecimento induzidas por algoritmos de Aprendizado de Máquina simbólico e ( b ) avaliar diversas formas de combinar classificadores simbólicos bem como explicar a classificação de novos exemplos realizada por um ensemble de classificares simbólicos . A finalidade ( a ) é implementada pelo Módulo de Análise de Regras e a finalidade ( b ) pelo Módulo de Combinação e Explicação . Esses módulos constituem os módulos principais do RuleSystem 

 Neste trabalho , são descritos os métodos de construção de ensembles e de combinação de classificadores encontrados na literatura , o projeto e a documentação do RuleSystem , a metodologia desenvolvida para documentar o sistema RuleSystem , a implementação do Módulo de Combinação e Explicação , objeto de estudo deste trabalho , e duas aplicações do Módulo de Combinação e Explicação . A primeira aplicação utilizou uma base de dados artificiais , a qual nos permitiu observar necessidades de modificações no Módulo de Combinação e Explicação . A segunda aplicação utilizou uma base de dados reais 

 Este trabalho propõe um modelo de teste para programas concorrentes que utilizam memória_compartilhada  . O modelo é inovador em três aspectos principais : ( 1 ) tratar a sincronização e a comunicação de threads de forma separada , ( 2 ) considerar a sincronização decorrente da inicialização/finalização de threads , e ( 3 ) apresenta um método baseado em timestamps para determinar as comunicações exercitadas em uma dada execução do programa . Os critérios de cobertura existentes para programas concorrentes foram adaptados ao contexto de programas baseados no paradigma de memória_compartilhada  . A ferramenta chamada ValiPThread foi implementada neste trabalho para apoiar a aplicação do modelo e dos critérios definidos . Com essa ferramenta é possível criar sessões de teste que podem ser salvas , interrompidas e retomadas a qualquer momento . Também é possível adicionar e executar casos de teste , avaliando a cobertura do código fonte em relação aos critérios de teste . A implementação da ferramenta mostra que é possível instanciar o modelo proposto em um software que auxilie a atividade de teste no contexto de programas com memória_compartilhada  . O trabalho apresenta soluções significativas para os principais desafios impostos pela programação concorrente para a atividade de teste , destacando-se dentre eles : ( 1 ) desenvolvimento de novas técnicas de análise estática para analisar programas concorrentes no contexto de memória_compartilhada  ; ( 2 ) testar aspectos cruciais à programação concorrente como : sincronização , comunicação e fluxo de dados ; ( 3 ) reproduzir uma execução de maneira controlada ; ( 4 ) mapeamento de critérios de teste já existentes para programas concorrentes com passagem de mensagem para o contexto de memória_compartilhada  ; ( 5 ) projetar critérios de fluxo de dados para programas concorrentes , considerando variáveis compartilhadas e ( 6 ) desenvolvimento de uma ferramenta de apoio a essas 
 A área de reconhecimento de objetos tem assistido a um impressionante progresso na última_década  . O estudo de descritores , aliado à estratégias de amostragem usando quantizações espaciais e a combinação de classificadores têm permeado o estado da arte nos últimos_anos  . Neste trabalho é proposta uma nova quantização espacial com número arbitrário de níveis e subdivisões arbitrárias de regiões . Regiões adjacentes possuem sobreposição gerando redundância na representação destas regiões de fronteiras e , assim , evitando as quebras que acontecem nas pirâmides espaciais tradicionais que prejudicam a interpretação das formas . Apesar de melhorar o desempenho da abordagem do saco de palavras , as pirâmides espaciais não são robustas a variações na orientação dos objetos na imagem . Foi também proposto neste trabalho , uma divisão espacial utilizando regiões circulares concêntricas que aumentam a robustez a rotação dos objetos na imagem em aproximadamente 80 % quando comparada às pirâmides espaciais . Além das novas divisões espaciais , é proposto neste trabalho um novo descritor baseado na aplicação de granulometria morfológica no mapa de bordas da imagem original . Este descritor foi utilizado na criação de modelos de classes em aplicações de categorização de objetos utilizando uma base de dados pública com resultados superiores aos do melhor descritor baseado em bordas reportado pela literatura . Todas estas novas técnicas propostas foram utilizadas em um problema desafiador de categorização de objetos de classes muito parecidas . Foi utilizado um subconjunto da base de pássaros Caltech-UCSD Birds-200 2011 com resultados comparáveis aos melhores_resultados  reportados pela literatura . A abordagem proposta cria uma classificação de dois_níveis  e utiliza modelos específicos por classe o que é intuitivo , pois cada espécie de pássaro possui características muito sutis que as diferenciam das demais espécies testadas . Vários descritores são utilizados na criação dos modelos de classes e uma combinação de classificadores gera a rotulação final para a amostra . O descritor proposto neste trabalho esteve presente no melhor modelo de 11 das 13 classes testadas e o resultado final obtido pela técnica de categorização proposta é o melhor resultado utilizando a abordagem do saco de palavras 
 A análise de cenas complexas por computadores não é uma tarefa trivial , entretanto , o cérebro humano pode realizar esta função de maneira eficiente . A evolução natural tem desenvolvido formas para otimizar nosso sistema visual de modo que apenas partes importantes da cena sejam analisadas a cada instante . Este mecanismo de seleção é denominado por atenção visual . A atenção visual opera sob dois aspectos : bottom-up e top-down . A atenção bottom-up é dirigida por conspicuidades baseadas na cena , como o contraste de cores , orientação , etc . Por outro_lado  , a atenção top-down é controlada por tarefas , memórias , etc . A atenção top-down pode ainda modular o mecanismo bottom-up através do enviesamento de determinadas características de acordo com a tarefa . Além do mecanismo de modulação considerado , o que é selecionado a partir da cena também representa uma importante parte para o processo de seleção . Neste cenário , diversas teorias têm sido propostas e podem ser agrupadas em duas linhas principais : atenção baseada no espaço e atenção baseada em objetos . Modelos baseados em objeto , ao invés de apenas direcionar a atenção para locais ou características específicas da cena , requerem que a seleção seja realizada a nível de objeto , significando que os objetos são a unidade básica da percepção . De modo a desenvolver modelos de acordo com a teoria baseada em objetos , deve-se considerar a integração de um módulo de organização perceptual . Este módulo pode segmentar os objetos do fundo da cena baseado em princípios de agrupamento tais como similaridade , proximidade , etc . Esses objetos competirão pela atenção . Diversos modelos de atenção visual baseados em objetos tem sido_propostos  nos últimos_anos  . Pesquisas em modelos de atenção visual têm sido desenvolvidas principalmente relacionadas à atenção bottom-up guiadas por características visuais primitivas , desconsiderando qualquer informação sobre os objetos . Por outro_lado  , trabalhos recentes têm sido realizados em relação ao uso do conhecimento sobre o alvo para influenciar a seleção da região mais saliente . Pesquisas nesta área são relativamente novas e os poucos modelos existentes encontram-se em suas fases iniciais . Aqui , nós propomos um novo modelo para atenção visual com modulações bottom-up e top-down . Comparações qualitativas e quantitativas do modelo proposto são realizadas em relação aos mapas de fixação humana e demais modelos estado da arte 
 Problemas de planejamento da produção são de suma importância no planejamento da cadeia de suprimentos , dando suporte às decisões da transformação de matérias-primas em produtos acabados . O dimensionamento de lotes em planejamento de produção é definido pelas decisões tático-operacionais relacionadas com o tamanho das ordens de produção e quando fabricá-las para satisfazer a demanda . Os objetivos destes problemas são geralmente de cunho econômico , tais como a redução de custos ou o aumento de lucros , embora outros aspectos possam ser considerados , tais como a qualidade do serviço ao cliente e a redução dos níveis de estoque . Problemas de dimensionamento de lotes são muito comuns em atividades de produção e um planejamento eficaz de tais atividades , estabelece uma clara vantagem à empresa em relação à concorrência . Para este objetivo , é necessária a consideração de características realistas do ambiente industrial e do produto . Para a modelagem matemática do problema , estas considerações são cruciais , embora sua inclusão resulte em formulações mais complexas . Embora os problemas de dimensionamento de lotes sejam bem conhecidos e amplamente estudados , várias características reais importantes não foram estudadas . Esta tese aborda , no contexto de dimensionamento de lotes , duas características muito relevantes : ( a ) preservação da preparação total e parcial ; e ( b ) produtos perecíveis . A primeira permite que o estado de preparação de uma linha de produção seja mantido entre dois períodos consecutivos , mesmo que a linha de produção ainda não esteja totalmente pronta para o processamento de ordens de produção . A ultima característica determina que alguns produtos tem prazo de validade fixo , menor ou igual do que o horizonte de planejamento , o que afeta o planejamento da produção . Além_disso  , de acordo com a duração de sua vida útil , foram considerados dois tipos de produtos perecíveis : produtos com tempo de vida de médio e curto prazo . O ultimo caso resulta em um problema mais apertado do que o anterior , o que implica em planos de produção mais restritos . Isto pode exigir uma integração com outros processos da cadeia de suprimentos , tais como o planejamento de distribuição dos produtos acabados . Pesquisas sobre formulações_matemáticas  mais fortes e abordagens de solução para problemas de dimensionamento de lotes fornecem ferramentas valiosas para os planejadores de produção . O foco da tese reside no desenvolvimento de formulações de programação linear inteiro-mistas ( MILP ) para os problemas de dimensionamento de lotes , considerando as características mencionadas anteriormente . Novas técnicas de modelagem foram introduzidas , como a proposta de variáveis de preparação desagregadas e a consideração de decisões de dimensionamento de lotes ao invés de decisões de agrupamento de ordens de produção no problema integrado de planejamento de produção e distribuição . Estas formulações foram submetidas a experimentos computacionais em MILP-solvers de ponta . No entanto , a complexidade inerente destes problemas pode exigir abordagens de solução orientadas ao problema . Nesta tese , abordagens heurísticas , metaheurísticas e matheurísticas ( híbrido de métodos exatos e heurísticos ) foram propostas para os problemas discutidos . Uma heurística lagrangeana aborda o problema de dimensionamento de lotes com restrições de capacidade , preservação da preparação total e produtos perecíveis . Um novo procedimento de programação_dinâmica  e utilizado para encontrar a solução ótima do problema de dimensionamento de lotes de um único produto perecível , sem restrições de capacidade e preservação da preparação total . Uma heurística , um procedimento x-and-optimize e uma abordagem por buscas adaptativas em grande vizinhanças são propostas para o problema integrado de planejamento de produção e distribuição . Resultados computacionais em conjuntos de instâncias geradas com base na literatura mostram que os métodos propostos obtiveram performances competitivas com relação a outras abordagens da literatura 
 O uso de padrões de workflow para controle de fluxo em aplicações de e-Science resulta em maior produtividade por parte do cientista , permitindo que se concentre em sua área de especialização . Todavia , o uso de padrões de workflow para paralelização em grades permanece uma questão em aberto . Este texto apresenta uma arquitetura de baixo acoplamento e extensível , para permitir a execução de padrões com ou sem a presença de grade , de modo transparente ao cientista . Descreve também o Padrão Junção Combinada , que atende a diversos cenários de paralelização comumente encontrados em aplicações de e-Science . Com isso , espera-se auxiliar o trabalho do cientista , oferecendo maior flexibilidade na utilização de grades e na representação de cenários de paralelização 
 Plâncton são organismos microscópicos que constituem a base da cadeia alimentar de ecossistemas aquáticos . Eles têm importante papel no ciclo do carbono pois são os responsáveis pela absorção do carbono na superfície dos oceanos . Detectar , estimar e monitorar a distribuição das diferentes espécies são atividades importantes para se compreender o papel do plâncton e as consequências decorrentes de alterações em seu ambiente . Parte dos estudos deste tipo é baseada no uso de técnicas de imageamento de volumes de água . Devido à grande quantidade de imagens que são geradas , métodos computacionais para auxiliar no processo de análise das imagens estão sob_demanda  . Neste trabalho abordamos o problema de identificação da espécie . Adotamos o pipeline convencional que consiste dos passos de detecção de alvo , segmentação ( delineação de contorno ) , extração de características , e classificação . Na primeira parte deste trabalho abordamos o problema de escolha de um algoritmo de segmentação adequado . Uma vez que a avaliação de resultados de segmentação é subjetiva e demorada , propomos um método para avaliar algoritmos de segmentação por meio da avaliação da classificação no final do pipeline . Experimentos com esse método mostraram que algoritmos de segmentação distintos podem ser adequados para a identificação de espécies de classes distintas . Portanto , na segunda parte do trabalho propomos um método de classificação que leva em consideração múltiplas segmentações . Especificamente , múltiplas segmentações são calculadas e classificadores são treinados individualmente para cada segmentação , os quais são então combinados para construir o classificador final . Resultados experimentais_mostram  que a acurácia obtida com a combinação de classificadores é superior em mais de 2 % à acurácia obtida com classificadores usando uma segmentação fixa . Os métodos propostos podem ser úteis para a construção de sistemas de identificação de plâncton que sejam capazes de se ajustar rapidamente às mudanças nas características das imagens 
 Os sistemas de recuperação de imagens por conteúdo ( CBIR -Content-based Image Retrieval ) possuem a habilidade de retornar imagens utilizando como chave de busca outras imagens . Considerando uma imagem de consulta , o foco de um sistema CBIR é pesquisar no banco de dados as `` n '' imagens mais similares à imagem de consulta de acordo com um critério dado . Este trabalho de pesquisa foi direcionado na geração de vetores de características para um sistema CBIR considerando bancos de imagens médicas , para propiciar tal tipo de consulta 
 Um vetor de características é uma representação numérica sucinta de uma imagem ou parte dela , descrevendo seus detalhes mais representativos . O vetor de características é um vetor `` n '' -dimensional contendo esses valores . Essa nova representação da imagem pode ser armazenada em uma base de dados , e assim , agilizar o processo de recuperação de imagens . Uma abordagem alternativa para caracterizar imagens para um sistema CBIR é a transformação do domínio . A principal vantagem de uma transformação é sua efetiva caracterização das propriedades locais da imagem 
 Recentemente , pesquisadores das áreas de matemática aplicada e de processamento de sinais desenvolveram técnicas práticas de `` wavelet '' para a representação multiescala e análise de sinais . Estas novas ferramentas diferenciam-se das tradicionais técnicas de Fourier pela forma de localizar a informação no plano tempo-freqüência ; basicamente , elas têm a capacidade de mudar de uma resolução para outra , o que faz delas especialmente adequadas para a análise de sinais não estacionários . A transformada `` wavelet '' consiste de um conjunto de funções base que representa o sinal em diferentes bandas de freqüência , cada uma com resoluções distintas correspondentes a cada escala . Estas foram aplicadas com sucesso na compressão , melhoria , análise , classificação , caracterização e recuperação de imagens . Uma das áreas beneficiadas , onde essas propriedades têm encontrado grande relevância , é a área médica , através da representação e descrição de imagens médicas 
 Este trabalho descreve uma abordagem para um banco de imagens médicas , que é orientada à extração de características para um sistema CBIR baseada na decomposição multiresolução de `` wavelets '' utilizando os filtros de Daubechies e Gabor . Essas novas características de imagens foram também testadas utilizando uma estrutura de indexação métrica `` Slim-tree '' . Assim , pode-se aumentar o alcance semântico do sistema cbPACS ( Content-Based Picture Archiving and Comunication Systems ) , atualmente em desenvolvimento conjunto entre o Grupo de Bases de Dados e Imagens do ICMC -- USP e o Centro de Ciências de Imagens e Física Médica do Hospital das Clínicas de Riberão Preto-USP 
 A educação a distância é uma realidade bem vista por muitas instituições de ensino e pelo Ministério da Educação , sendo , portanto , fortemente incentivada . Essa modalidade vai muito além da alternativa de oferecer cursos de maneira confortável para alunos que não têm disponibilidade de tempo em horário comercial . Em alguns_casos  , esse é o único meio de atingir uma parcela da população que , apesar de encontrar-se isolada geograficamente , tem direito à educação básica de qualidade e gratuita como forma de garantir os seus direitos de cidadão . Este trabalho teve como objetivo investigar alternativas de software que visam oferecer apoio para a oferta de cursos na modalidade a distância . Como estudo de caso , foi delimitado um cenário real : o do estado do Amazonas , que conta com diversos esforços bem sucedidos no uso de educação a distância como alternativas para reduzir o isolamento geográfico . Para auxiliar os alunos e professores foi escolhido o ambiente Tidia-Ae , sendo que o maior enfoque foi dado à ferramenta DigaE e suas extensões , por possibilitarem a autoria de documentos_multimídia  
 Um processo de software é definido pelas diversas atividades comumente efetuadas durante o desenvolvimento de software , considerando que tais atividades sejam realizadas sob políticas bem definidas e procedimentos bem estabelecidos . Assim , existem diversos modelos de processo de desenvolvimento de software , que visam garantir a qualidade do produto desenvolvido por meio de tal processo . Identificando a necessidade de processos de software específicos para o desenvolvimento de software_livre  , devido aos modelos tradicionais não considerarem as características do desenvolvimento deste tipo de software , foi proposto no contexto do Projeto QualiPSo o modelo OMM ( Open Source Maturity Model ) . Com o intuito de contribuir com o modelo OMM e a comunidade de software_livre  , esse trabalho teve como um de seus objetivos a análise do emprego de wikis no processo de desenvolvimento de software . Outro objetivo dessa pesquisa foi o desenvolvimento de uma ferramenta para mensurar o quanto de documentação e registro de Design Rationale tem sido realizado em uma 
 A modelagem evolutiva de bancos de dados é necessária devido às frequentes mudanças de requisitos das aplicações . O desafio é ainda maior quando o banco de dados tem de atender simultaneamente a várias aplicações . A solução proposta por Scott Ambler para evolução utiliza refatorações e define um período de transição , durante o qual tanto o esquema antigo quanto o novo coexistem e os dados são replicados por meio de um processo síncrono que apresenta várias dificuldades , como a interferência no funcionamento normal das aplicações . Para minimizar essas dificuldades , esta tese propõe um processo assíncrono para manter atualizados esses esquemas e apresenta um protótipo de uma ferramenta para auxiliar as evoluções dos bancos de dados . A proposta foi validada por meio de um experimento em laboratório que comparou a solução aqui apresentada com a proposta por Ambler 
 O aprendizado de máquina consiste de conceitos e técnicas que permitem aos computadores melhorar seu desempenho com a experiência , ou , em outras_palavras  , aprender com dados . Um dos principais tópicos do aprendizado de máquina é o agrupamento de dados que , como o nome sugere , procura agrupar os dados de acordo com sua similaridade . Apesar de sua definição relativamente simples , o agrupamento é uma tarefa computacionalmente complexa , tornando proibitivo o emprego de algoritmos exaustivos , na busca pela solução ótima do problema . A importância do agrupamento de dados , aliada aos seus desafios , faz desse campo um ambiente de intensa pesquisa . Também a classe de fenômenos naturais conhecida como comportamento coletivo tem despertado muito interesse . Isso decorre da observação de um estado organizado e global que surge espontaneamente das interações locais presentes em grandes grupos de indivíduos , caracterizando , pois , o que se chama auto-organização ou emergência , para ser mais preciso . Os desafios intrínsecos e a relevância do tema vêm motivando sua pesquisa em diversos ramos da ciência e da engenharia . Ao mesmo tempo , técnicas baseadas em comportamento coletivo vêm_sendo  empregadas em tarefas de aprendizado de máquina , mostrando-se promissoras e ganhando bastante atenção . No presente_trabalho  , objetivou-se o desenvolvimento de técnicas de agrupamento baseadas em comportamento coletivo . Faz-se cada item do conjunto de dados corresponder a um indivíduo , definem-se as leis de interação local , e então os indivíduos são colocados a interagir entre si , de modo que os padrões que surgem reflitam os padrões originalmente presentes no conjunto de dados . Abordagens baseadas em dinâmica de troca de energia foram propostas . Os dados permanecem fixos em seu espaço de atributos , mas carregam certa informação a energia , a qual é progressivamente trocada entre eles . Os grupos são estabelecidos entre dados que tomam estados de energia semelhantes . Este trabalho abordou também o aprendizado semissupervisionado , cuja tarefa é rotular dados em bases parcialmente rotuladas . Nesse caso , foi adotada uma abordagem baseada na movimentação dos próprios dados pelo espaço de atributos . Procurou-se , durante todo este trabalho , não apenas propor novas técnicas de aprendizado , mas principalmente , por meio de muitas simulações e ilustrações , mostrar como elas se comportam em diferentes cenários , num esforço em mostrar onde reside a vantagem de se utilizar a dinâmica coletiva na concepção dessas 
 Com o aumento da capacidade de armazenamento , as bases de dados são cada vez maiores e , em muitas situações , apenas um pequeno subconjunto de itens de dados pode ser rotulado . Isto acontece devido ao processo de rotulagem ser frequentemente caro , demorado e necessitar do envolvimento de especialistas humanos . Com isso , diversos algoritmos semissupervisionados foram propostos , mostrando que é possível_obter  bons_resultados  empregando conhecimento prévio , relativo à pequena fração de dados rotulados . Dentre esses algoritmos , os que têm ganhado bastante destaque na área têm sido aqueles baseados em redes . Tal interesse , justifica-se pelas vantagens oferecidas pela representação via redes , tais como , a possibilidade de capturar a estrutura topológica dos dados , representar estruturas hierárquicas , bem como modelar manifolds no espaço multi-dimensional . No entanto , existe uma grande quantidade de dados representados em tabelas atributo-valor , nos quais não se poderia aplicar os algoritmos baseados em redes sem antes construir uma rede a partir desses dados . Como a geração das redes , assim como sua relação com o desempenho dos algoritmos têm sido pouco estudadas , esta tese investigou esses aspectos e propôs novos métodos para construção de redes , considerando características ainda não exploradas na literatura . Foram propostos três métodos para construção de redes com diferentes topologias : 1 ) S-kNN ( Sequential k Nearest Neighbors ) , que gera redes regulares ; 2 ) GBILI ( Graph Based on the Informativeness of Labeled Instances ) e RGCLI ( Robust Graph that Considers Labeled Instances ) , que exploram os rótulos disponíveis gerando redes com distribuição de grau lei de potência ; 3 ) GBLP ( Graph Based on Link Prediction ) , que se baseia em medidas de predição de links gerando redes com propriedades mundo-pequeno . As estratégias de construção de redes propostas foram analisadas por meio de medidas de teoria dos grafos e redes complexas e validadas por meio da classificação semissupervisionada . Os métodos foram aplicados em benchmarks da área e também na classificação de gêneros musicais e segmentação de imagens . Os resultados mostram que a topologia da rede influencia diretamente os algoritmos de classificação e as estratégias propostas alcançam boa acurácia 
 Pesquisas em Lingüística e Lingüística Computacional têm comprovado há tempos que um texto é mais do que uma simples seqüência de sentenças justapostas . Um texto possui uma estrutura subjacente altamente elaborada que relaciona todo o seu conteúdo , atribuindo-lhe coerência . A essa estrutura dá-se o nome de estrutura discursiva , sendo ela objeto de estudo da área de pesquisa conhecida como Análise de Discurso . Diante da grande utilidade desse conhecimento para diversas aplicações de Processamento de Línguas_Naturais  , por exemplo , sumarização_automática  de textos e resolução de anáforas , a análise discursiva automática tem recebido muita atenção . Para o português do Brasil , em particular , há poucos recursos e pesquisas nessa área de pesquisa . Neste cenário , esta tese de doutorado visa a investigar , desenvolver e implementar métodos para análise discursiva automática , adotando como principal teoria discursiva a Rhetorical Structure Theory , uma das teorias mais difundidas atualmente . A partir da anotação retórica e da análise de um corpus de textos científicos da Computação , produziu-se o primeiro analisador retórico automático para a língua_portuguesa  do Brasil , chamado DiZer ( DIscourse analyZER ) , além de uma grande quantidade de conhecimento discursivo . Apresentam-se modelos estatísticos inéditos para o reconhecimento de relações discursivas baseados em unidades de conteúdo de crescente complexidade , abordando palavras , conceitos e estruturas argumentais . Em relação a este último item , é apresentado um modelo para o aprendizado não supervisionado das estruturas argumentais dos verbos , o qual foi aplicado para os 1.500 verbos mais freqüentes do inglês , resultando em um repositório chamado ArgBank . O DiZer e os modelos propostos são avaliados , produzindo resultados_satisfatórios  
 A Tradução Automática ( TA ) -- tradução de uma língua natural ( fonte ) para outra ( alvo ) por meio de programas de computador -- é uma tarefa árdua devido , principalmente , à necessidade de um conhecimento lingüístico aprofundado das duas ( ou mais ) línguas envolvidas para a construção de recursos , como gramáticas de tradução , dicionários bilíngües etc . A escassez de recursos lingüísticos , e mesmo a dificuldade em produzi-los , geralmente são fatores limitantes na atuação dos sistemas de TA , restringindo-os , por exemplo , quanto ao domínio de aplicação . Neste contexto , diversos métodos vêm_sendo  propostos com o intuito de gerar , automaticamente , conhecimento lingüístico a partir dos recursos multilíngües e , assim , tornar a construção de tradutores automáticos menos trabalhosa . O projeto ReTraTos , apresentado neste documento , é uma dessas propostas e visa à indução automática de léxicos bilíngües e de regras de tradução a partir de corpora paralelos etiquetados morfossintaticamente e alinhados lexicalmente para os pares de idiomas português -- espanhol e português -- inglês . O sistema proposto para a indução de regras de tradução apresenta uma abordagem inovadora na qual os exemplos de tradução são divididos em blocos de alinhamento e a indução é realizada para cada bloco , separadamente . Outro fator inovador do sistema de indução é uma filtragem mais elaborada das regras induzidas . Além dos sistemas de indução de léxicos bilíngües e de regras de tradução , implementou-se também um módulo de tradução automática para permitir a validação dos recursos induzidos . Os léxicos bilíngües foram avaliados intrinsecamente e os resultados obtidos estão de acordo com os relatados na literatura para essa área . As regras de tradução foram avaliadas direta e indiretamente por meio do módulo de TA e sua utilização trouxe um ganho na tradução palavra-a-palavra em todos os sentidos ( fonte -- alvo e alvo -- fonte ) para a tradução dos idiomas em estudo . As traduções geradas com os recursos induzidos no ReTraTos também foram comparadas às geradas por sistemas comerciais , apresentando melhores_resultados  para o par de línguas português -- espanhol do que para o par português -- inglês 
 Com o vislumbramento de aplicações que exigiam representações em espaços multidimensionais , surgiu a necessidade de desenvolvimento de métodos de acessos eficientes a estes dados representados em R^d . Dentre as aplicações precursoras dos métodos de acessos multidimensionais , podemos citar os sistemas de geoprocessamento , aplicativos 3D e simuladores . Posteriormente , os métodos de acessos multidimensionais também apresentaram-se como uma importante ferramenta no projeto de classificadores , principalmente classificadores pelos vizinhos mais próximos . Com isso , expandiu-se o espaço de representação , que antes se limitava no máximo a quatro dimensões , para dimensionalidades superiores a mil . Dentre os vários métodos de acesso multidimensional existentes , destaca-se uma classe de métodos baseados em árvores balanceadas com representação em R^d . Estes métodos constituem evoluções da árvore de acesso unidimenisonal B-tree e herdam várias características deste último . Neste trabalho , apresentamos alguns métodos de acessos dessa classe de forma a ilustrar a idéia central destes algoritmos e propomos e implementamos um novo método de acesso , a PCA-tree . A PCA-tree utiliza uma heurística de quebra de nós baseada na extração da componente principal das amostras a serem divididas . Um hiperplano que possui essa componente principal como seu vetor normal é definido como o elemento que divide o espaço associado ao nó . A partir dessa idéia básica geramos uma estrutura de dados e algoritmos que utilizam gerenciamento de memória secundária como a B-tree . Finalmente , comparamos o desempenho da PCA-tree com o desempenho de alguns outros métodos de acesso da classe citada , e apresentamos os prós e contras deste novo método de acesso através de análise de resultados práticos 
 Depurar sistemas distribuídos continua uma tarefa_difícil  , mesmo após 30 anos de pesquisa intensa . Embora essa situação possa ser parcialmente atribuída à complexidade das execuções concorrentes , o rápido passo de desenvolvimento das plataformas e tecnologias para computação distribuída também carrega a sua parcela de culpa , por encurtar a vida de muitas ferramentas potencialmente úteis . Neste trabalho , apresentamos uma análise dos principais problemas , técnicas e ferramentas ligados à depuração de sistemas concorrentes e discutidos na literatura . Baseados nessa análise , desenvolvemos e apresentamos uma nova_técnica  , simples e portátil , que pode ser aplicada a sistemas distribuídos que utilizam chamadas síncronas e bloqueantes . Essa técnica , concebida para sobreviver à heterogeneidade , é validada por meio da implementação de um arcabouço escrito para plataforma Eclipse e instanciado para sistemas de objetos distribuídos baseados em Java/CORBA 
 Em econometria um dos tópicos que vem se tornando ao longo dos anos primordial e a análise de ultra-frequência , ou seja , a análise da transação negócio a negócio . Ela tem se mostrado fundamental na modelagem da microestrutura do mercado intraday . Ainda assim temos uma teoria escassa que vem crescendo de forma humilde a cerca deste tema . Buscamos desenvolver um teste de hipótese para verificar se os dados de ultra-frequência apresentam incrementos independentes e estacionários , pois neste cenário saber disso é de grande_importância  , ja que muitos trabalhos tem como base essa hipótese . Além_disso  Grimshaw et . al . ( 2005 ) [ 6 ] mostrou que ao utilizarmos uma distribuição de probabilidade contínua para modelarmos dados econômicos , em geral , estimamos uma função de intensidade crescente , devido a resultados viciados obtidos como consequência do arredondamento , em nosso trabalho buscamos trabalhar com distribuições discretas para que contornar esse problema acarretado pelo uso de distribuições 
 Recentemente uma rede_neural  multi-camadas , baseada no algoritmo Back- Propagation , foi proposta para resolver problemas de otimização não lineares . Esta rede tem apresentado bons_resultados  na solução de problemas não lineares restritos e irrestritos . Este trabalho mostra as facilidades e beneficios da aplicação de técnicas de otimização ao algoritmo de aprendizado desta particular rede_neural  . O termo momentum , o gradiente com busca linear e o método do gradiente conjugado foram incorporados no esquema de aprendizado desta rede_neural  . Resultados computacionais são apresentados mostrando as vantagens da incorporação destas técnicas nesta rede . Além_disso  , para verificar a adequabilidade de uso desta particular rede_neural  , com algumas modificações incorporadas no algoritmo de aprendizado , na solução de problemas de dimensão maior do que os até então testados , uma aplicação é , resolvida usando este modelo para otimização de um sistema hidroelétrico de potência . O sistema é constituído de uma usina térmica , uma usina hidroelétrica e com a possibilidade de transferência de energia de sistema vizinho . Resultados são apresentados e mostram a utilidade desta abordagem quando comparada com resultados obtidos por técnicas tradicionais 
 A atividade de teste é de grande_importância  para a eliminação dos defeitos que possam persistir no software após a sua conclusão . São encontrados na literatura estudos propondo estratégias de teste de softwares Orientados a Objetos que se baseiam na experiência já adquirida no teste de programas procedimentais , principalmente na utilização de critérios baseados em fluxo de dados ; entretanto não foram encontradas , ainda , ferramentas específicas para o teste de softwares Orientados a Objetos . Deve-se observar que qualquer estratégia de teste sem o suporte de ferramentas automatizadas tende a ser trabalhosa e propensa a erros na sua condução . Os Grupos de Teste de Software do ICMSC - USP - São Carlos e DCA - FEEC - UNICAMP especificaram e implementaram duas ferramentas multilinguagens de teste de programas procedimentais baseadas em uma Linguagem Intermediária ( LI ) . Na mesma direção , este trabalho apresenta a definição de uma Linguagem Intermediária para Programas Orientados a Objetos ( denominada LI++ ) com a finalidade de viabilizar o desenvolvimento de ferramentas multilinguagens que apoiem esse paradigma , dada a diversidade de linguagens de programação Orientadas a Objetos existentes , tais como C++ , Smalltalk , Eiffel , etc . A LI++ é uma extensão da LI , visto que os corpos dos métodos que compõem um ambiente Orientado a Objetos é basicamente composto por comandos procedimentais ( mapeados na LI ) . Ainda , foi definido e implementado o mapeamento de programas C++ para a LI++ , baseando-se fortemente na implementação do mapeamento C /LI , pois a linguagem C++ é um superconjunto da linguagem C 
 Neste trabalho , um sistema para transformar texto na língua_portuguesa  falado no Brasil em voz é , desenvolvido . O método adotado para a transformação é Texto-Fonema-Voz . Este método tem algumas vantagens : ( i ) ele cria uma Biblioteca de Voz ( BV ) para qualquer língua ou para todas , independentemente do mapeamento texto para fonemas ; ( ii ) ele pode interconectar-se com outros sistemas que realizem tarefa semelhante . O uso do sistema Máquina Falante é muito diversificado . As pessoas podem usar facilmente os computadores , porque eles tornam-se mais interativos . A técnica utilizada para mapear texto para fonemas é as Redes_Neurais  Artificiais ( RNA ) . O uso de RNA tem algumas vantagens sobre técnicas convencionais , como dicionário fonético : ( i ) a quantidade de informação armazenada pela RNA é menor que a quantidade de informação armazenada pelas outras técnicas ; ( ii ) o usuário não necessita fazer manutenção para novas palavras ; ( iii ) o Perceptron Multi-Camadas tem a característica de generalização . Para uma boa generalização , o conjunto de treinamento deve caracterizar todo o universo de discurso . Para a síntese , nós codificamos a , voz humana pela técnica Waveform . Esta técnica foi escolhida porque é simples e sua característica de qualidade é muito relevante . A BV é um conjunto de informações sobre como o processo de síntese produz fisicamente os fonemas ou grupos de fonemas . Cada arquivo da BV representa uma sílaba . A performance da rede utilizada é de aproximadamente 94 % de acerto usando um conjunto de teste que foi construído com palavras diferentes das usadas no treinamento , que é considerado , na área , um bom índice de generalização 
 A qualidade das hipóteses induzidas pelos atuais sistemas de aprendizado de máquina supervisionado depende da quantidade dos exemplos no conjunto de treinamento . Por outro_lado  , muitos dos sistemas de aprendizado de máquina conhecidos não estão preparados para trabalhar com uma grande quantidade de exemplos . Grandes conjuntos de dados são típicos em mineração de dados . Uma maneira para resolver este problema consiste em construir ensembles de classificadores . Um ensemble é um conjunto de classificadores cujas decisões são combinadas de alguma maneira para classificar um novo caso . Apesar de melhorar o poder de predição dos algoritmos de aprendizado , ensembles podem ser compostos por muitos classificadores , o que pode ser indesejável . Ainda , apesar de ensembles classificarem novos exemplos melhor que cada classificador individual , eles se comportam como caixas pretas , no sentido de não oferecer ao usuário alguma explicação relacionada à classificação por eles fornecida . Assim , neste trabalho propomos uma abordagem que utiliza algoritmos de aprendizado simbólico para construir ensembles de classificadores simbólicos que explicam suas decisões de classificação e são tão ou mais precisos que o mais preciso dos seus classificadores individuais . Além_disso  , considerando que algoritmos de aprendizado simbólico utilizam métodos de busca local para induzir classificadores quanto que algoritmos genéticos utilizam métodos de busca global , propomos uma segunda abordagem para aprender conceitos simbólicos de grandes bases de dados utilizando algoritmos genéticos para evoluir classificadores simbólicos em um u´ nico classificador simbólico , de maneira que o classificador evoluído é mais preciso que os classificadores iniciais . Ambas propostas foram implementadas em dois sistemas computacionais . Diversos experimentos usando diferentes conjuntos de dados foram conduzidos para avaliar ambas as propostas . Ainda que os resultados experimenta das duas soluções propostas são promissores , os melhores_resultados  foram obtidos utilizando a abordagem relacionada a algoritmos 
 Muitos dos sistemas de computação existentes atualmente são concorrentes , ou seja , neles constam diversas entidades que , ao mesmo tempo , operam sobre um conjunto de recursos compartilhados . Nesse contexto , devemos controlar a concorrência das diversas operações realizadas , ou então a interferência entre elas poderia causar inconsistências nos recursos compartilhados ou nas próprias operações realizadas . Nesse texto , vamos tratar especificamente de estruturas de dados concorrentes , ou seja , estruturas de dados cujas operações associadas -- consideramos inserção , remoção e busca -- sejam passíveis de execução simultânea por diversas entidades . Tendo em vista o controle da concorrência , vamos adotar uma abordagem baseada no emprego de locks , uma primitiva de sincronização muito usual na literatura . Nossa discussão será apresentada em termos de certas estruturas de dados chamadas skip graphs , que têm propriedades interessantes para outros contextos , como o contexto de sistemas distribuídos 
 Hoje , diante das contínuas mudanças e do mercado competitivo , as empresas e organizações têm sempre a necessidade de adaptar suas práticas de negócios para atender às diferentes exigências de seus clientes e manter-se em vantagem com relação às suas concorrentes . Para ajudá-las a atingir esta meta , uma proposta promissora é o Desenvolvimento Baseado em Componentes ( DBC ) , cuja ideia básica é a de que um novo software possa ser construído rapidamente a partir de componentes pré-existentes . Entretanto , a montagem de sistemas corporativos mais confiáveis e tolerantes a falhas a partir da integração de componentes tem-se mostrado uma tarefa relativamente complexa . E a necessidade de garantir que tal integração não falhe tornou-se algo imprescindível , sobretudo porque as consequências de uma falha podem ser extremamente graves . Para que haja uma certa garantia de que o software seja tolerante a falhas , devem ser realizadas atividades de testes e verificação formal de programas . Isto porque ambas , em conjunto , procuram garantir ao desenvolvedor que o sistema resultante da integração é , de fato , confiável . Mas a viabilidade prática de execução destas atividades depende de ferramentas que auxiliem sua realização , uma vez que a execução de ambas constitui um alto custo para o desenvolvimento do software . Tendo em vista esta necessidade de facilitar a realização de testes e verificação nos sistemas baseados em componentes ( DBC ) , este trabalho de Mestrado se propõe a desenvolver um ambiente integrado para a verificação e teste de protocolos para a coordenação do comportamento excepcional de componentes 
 Este trabalho de doutorado tem como objetivo principal mostrar que a abstração de redes de sensores , por meio de arquiteturas orientadas a serviço , pode impor alguma degradação do desempenho , mas não inviabiliza a maioria das aplicações . A metodologia utilizada na condução deste trabalho consta de experimentos que realizam avaliações de desempenho que analisam o comportamento de diferentes mecanismos de exposição de redes de sensores , bem como avalia três dos principais serviços do modelo de interfaces do Sensor Web Enablement ( SWE ) . Os resultados obtidos_mostram  diferenças de desempenho significativas entre os métodos de acesso considerados . Esses resultados ainda mostram que a sobrecarga gerada não é significativa se comparada com a frequência de obtenção de observações da maioria das redes de sensores . Visando minimizar os problemas de desempenho impostos pelo framework SWE é proposta e implementada uma arquitetura de provisão de qualidade de serviço ( QoS ) , denominada Sensor Web Architecture ( SWARCH ) . Um estudo de caso da SWARCH demonstra que é possível reduzir os tempos de resposta no acesso aos serviços do SWE por meio de um Broker que monitora constantemente diferentes provedores de 
 A Modernização Dirigida a Arquitetura ( do inglês - Architecture-Driven Modernization ( ADM ) ) é uma iniciativa do Object Management Group ( OMG ) no sentido de padronizar os tradicionais processos de reengenharia de software utilizando metamodelos . O metamodelo mais importante da ADM é o Knowledge Discovery Metamodel ( KDM ) , cujo_objetivo  é representar todos artefatos de um determinado sistema , de forma independente de linguagem e plataforma . Um passo primordial durante processos de modernização de software é a aplicação de refatorações . No entanto , até o presente momento , há carência de abordagens que tratam das questões referentes a refatorações no contexto da ADM , desde a criação até a aplicação das mesmas . Além_disso  , atualmente , não existe uma forma sistemática e controlada de facilitar o reúso de refatorações que são específicas do KDM . Diante disso , são apresentados uma abordagem para criação e disponibilização de refatorações para o metamodelo KDM e um apoio ferramental que permite aplicá-las em diagramas de classe da UML . A abordagem possui dois passos : ( i ) o primeiro envolve passos que apoiam o engenheiro de modernização durante a criação de refatorações para o KDM ; ( ii ) o segundo resume-se na especificação das refatorações por meio da criação de instâncias do metamodelo Structured Refactoring Metamodel ( SRM ) e posterior disponibilização delas em um repositório . O apoio ferramental , denominado KDM-RE , é composto por três plug-ins do Eclipse : ( i ) o primeiro consiste em um conjunto de Wizards que apoia o engenheiro de software na aplicação das refatorações em diagramas de classe UML ; ( ii ) o segundo consiste em um módulo de propagação de mudanças , permitindo manter modelos internos do KDM sincronizados ; ( iii ) o terceiro fornece apoio à importação e reúso de refatorações disponíveis no repositório . Além_disso  , o terceiro módulo também contém uma linguagem específica de domínio , a qual é utilizada para auxiliar o engenheiro de software a instanciar o metamodelo SRM . Foi realizado um experimento , buscando reproduzir os cenários em que engenheiros de software realizam refatorações em instâncias do metamodelo KDM . Os resultados mostraram que a abordagem , bem como o apoio ferramental podem trazer benefícios para o engenheiro de software durante a atividade de aplicação de refatorações em sistemas , representados pelo metamodelo KDM 
 As cidades são sistemas complexos de interação social e de transporte . Suas estruturas podem ser usadas para modelar redes de mobilidade urbana i.e . redes complexas que representam a geometria de uma cidade permitindo a consecução de atividades analíticas para descoberta de padrões e para a tomada de decisão baseada em dados . A geometria da cidade carrega informações intrínsecas que auxiliam atividades relacionadas à análise de dados provenientes do cenário urbano . As informações inerentes a tais análises podem ser usadas para melhorar a qualidade de vida dos habitantes de uma região , ou para entender a dinâmica de centros urbanos . Diversos processos analíticos aplicados a tais cenários carecem de metodologias para analisar o padrão criminal e para identificar estruturas urbanas mal planejadas . Deste modo , este trabalho tem por objetivo prover meios para análise topológica de regiões criminais e para a identificação de inconsistências urbanas , as quais apontam para regiões que carecem de mobilidade e acesso para outras regiões de uma cidade . Neste sentido , foi desenvolvido um conjunto de procedimentos algébricos e algorítmicos capazes de revelar padrões e meios para compreensão e análise dos dados . Mais especificamente , foram desenvolvidos métodos de pré-processamento para transformar mapas eletrônicos georreferenciados em grafos que representam cidades , foi utilizado um conjunto métrico analítico e outro com base em processos epidêmicos para entender a dinâmica intrínseca à criminalidade de uma cidade , e por fim , foi desenvolvido um conjunto de formalismos e operações baseados em teoria dos conjuntos para identificar falhas no desenho das estruturas urbanas que impactam no acesso viário em centros urbanos . Os resultados deste trabalho versam sobre o desenvolvimento de novos métodos para preparar mapas na forma de redes de mobilidade urbana ; na análise de crimes baseada em sua disposição espacial ; no desenvolvimento de um modelo capaz de descrever a atividade criminal de uma cidade ; e , em um conceito baseado na análise de regiões críticas identificadas a partir do desenho urbano 
 Esta tese teve como objetivo estudar problemas cinéticos de clustering , ou seja , problemas de clustering nos quais os objetos se movimentam . O trabalho se concentrou no caso unidimensional , em que os objetos são pontos se movendo na reta real . Diversas variantes desse caso foram abordadas . Em termos do movimento , consideramos o caso em que cada ponto se move com uma velocidade constante num dado intervalo de tempo , o caso em que os pontos se movem arbitrariamente e temos apenas as suas posições em instantes discretos de tempo , o caso em que os pontos se movem com uma velocidade aleatória em que se conhece apenas o valor esperado da velocidade , e o caso em que , dada uma partição do intervalo de tempo , os pontos se movem com velocidades constantes em cada subintervalo . Em termos do tipo de clustering buscado , nos concentramos no caso em que o número de clusters é um dado do problema e consideramos diferentes medidas de qualidade para o clustering . Duas delas são tradicionais para problemas de clustering : a soma dos diâmetros dos clusters e o diâmetro máximo de um cluster . A terceira medida considerada leva em conta a característica cinética do problema , e permite , de uma maneira controlada , que o clustering mude com o tempo . Para cada uma das variantes do problema , são apresentados algoritmos , exatos ou de aproximação , alguns resultados de complexidade obtidos , e questões que ficaram em aberto 
 Este trabalho apresenta um ambiente de simulação de escoamentos com simetria radial e superfícies_livres  , baseado no sistema Freeflow . O sistema é formado por três_módulos  : um modelador de moldes , um simulador e um visualizador de escoamentos . O simulador implementa o método GENSMAC para a solução das equações de Navier-Stokes em coordenadas cilíndricas , utilizando diferenças_finitas  em uma malha diferenciada . São introduzidos os efeitos da tensão superficial e do ângulo de contato nas simulações com simetria radial 
 Alguns resultados de simulações utilizando este sistema e uma validação do código são apresentados , comparando simulações com soluções analíticas e experimentais , e estudando a convergência do método 
 O planejamento da produção em indústrias moveleiras de pequeno_porte  é comumente constituído por decisões referentes ao volume de produção e à política de estoque , com o objetivo de minimizar o desperdício de material , os atrasos e as horas-extras utilizadas ao longo do horizonte de planejamento . Administrar tais decisões de uma maneira tratável e eficiente é , em geral , um desafio , especialmente considerando a natureza incerta dos dados . Nessa tese , são desenvolvidos modelos de otimização para apoiar tais decisões no contexto do problema combinado de dimensionamento de lotes e corte de estoque sob incertezas que surge em indústrias moveleiras . Para lidar com as incertezas dos dados , são investigadas duas metodologias : programação estocástica e otimização robusta . Dessa maneira , são propostos modelos de programação estocástica de dois estágios com recurso , assim como modelos estocásticos robustos que incorporam aversão ao risco . A motivação em também desenvolver modelos baseados em otimização robusta é considerar casos práticos em que não há uma descrição probabilística explícita dos dados de entrada , assim como evitar trabalhar com numerosos cenários , o que pode tornar o modelo estocástico computacionalmente intratável . Os experimentos numéricos baseados em exemplares reais de uma empresa moveleira de pequeno_porte  mostram que as soluções obtidas pelos modelos de programação estocástica fornecem planos de produção robustos e que o ( a ) decisor ( a ) pode designar suas preferências em relação ao risco aos modelos , assim como controlar o tradeoff entre o custo total esperado e a robustez da solução . Em relação aos resultados dos modelos de otimização robusta , são obtidos alguns insights entre os chamados budgets de incerteza , as taxas de atendimento da demanda e os valores ótimos . Além_disso  , evidências numéricas indicam que budgets de incerteza menos conservadores resultam em níveis de serviço razoáveis com baixos custos globais , enquanto a abordagem de pior caso gera , relativamente , boas taxas de atendimento da demanda , mas com custos globais 
 MOREIRA , C. C. Software Para Prática De Regência Coral . 2013 . 60 f. Dissertação ( Mestrado ) - Instituto de Matemática e Estatística , Universidade de São Paulo , São Paulo , 2013 . Neste documento um software é proposto para apoiar o ensino da regência coral com base em um modelo de aula padrão observado na disciplina : Análise Musical para Performance : Interfaces entre Criação e Interpretação a Partir do Universo Coral do Departamento de Música da Escola de Comunicações e Artes da Universidade de São Paulo . Neste software é possível : ( i ) monitorar , ( ii ) mensurar e ( iii ) avaliar os padrões de regência ( gestos ) de regência realizados e as posições corporais de um estudante de regência coral . O principal objetivo é permitir , através da interação com o software , que o estudante possa aprimorar sua consciência corporal através do monitoramento de seu posicionamento espacial enquanto realiza os padrões de regência e se vê projetado tridimensionalmente no sistema . Para que este monitoramento seja suportado , três_módulos  foram criados , de modo que as devidas configurações possam ser realizadas , assim como a visualização seja feita , a saber : ( a ) módulo de configuração de padrões de regência , onde padrões de regência são criados ou modificados ; ( b ) módulo de criação de práticas , que podem ser : ( b.1 ) sintéticas , e neste caso utilizam os padrões de regência pré-definidos em ( a ) , dispondo-os em sequências ao longo do tempo as quais são associadas às mãos do regente , ou ( b.2 ) gravadas , onde através do registro de movimentos corporais por um período de tempo em frente ao sensor , as juntas corporais são capturadas ao longo do tempo , de modo que se possa comparar estes movimentos com os realizados em tempo real ; e ( c ) , o módulo de visualização , onde o usuário poderá ( c.1 ) assistir a reprise de práticas já realizadas , ou configuradas anteriormente ; ( c.2 ) comparar e visualizar práticas diversas entre si ; ou ainda , ( c.3 ) praticá-las de fato , onde receberá uma nota ao final do exercício . A idéia é que estudantes possam realizar práticas repetidas vezes , onde irão melhorando suas notas , e aprimorando seus gestos através da conscientização corporal . Outro ponto importante é que estudantes e regentes podem interagir entre si através da troca das configurações , já que estas são salvas em arquivos , possibilitando a criação de um repositório de regência coral , o que permite que as informações extrapolem os limites de sala de aula . Para implementar tal solução , algoritmos de visão_computacional  foram aplicados no contexto da regência coral , em conjunto com o sensor Kinect para auxiliar na captura das informações corporais . Por fim , busca-se iniciar um trabalho com o sentido de aprimorar a formação de regentes e possibilitar que o ensino desta arte seja fundamentado em dados quantitativos que poderão auxiliar o regente a tirar conclusões sobre os métodos e escolas de regência atualmente existentes 
 O som gerado por escoamentos , também conhecido como aeroacústica , tem se tornado cada vez mais importante em áreas industriais diversas desde aviação comercial até aparelhos eletrodomésticos , afetando diretamente os requisitos necessários para o desenvolvimento de novos produtos . Um caso particular é o ruído gerado por válvulas de compressores herméticos de refrigeração , sendo o compressor a principal fonte de ruído em refrigeradores domésticos . O presente_trabalho  tem por objetivo iniciar o desenvolvimento de uma ferramenta confiável de simulação capaz de auxiliar engenheiros na predição de problemas de aeroacústica , especialmente um que possa no futuro ser utilizado para estudar o ruído gerado pelo escoamento em válvulas de compressores herméticos . Para isso , foi desenvolvido um código para simulação_numérica  direta de aeroacústica . Utilizou-se processamento paralelo com decomposição de domínio para usar Simulação Numérica Direta em um tempo factível ; esquemas de discretização espaciais e temporais de alta ordem para minimizar ao máximo os fenômenos de dissipação e dispersão do escoamento e das ondas acústicas e uma série de tratamentos no domínio como filtragem e estiramento da malha como também condições de contorno características com o intuito de obter uma solução adequada para estudo de aeroacústica . Assim , são apresentadas todas as etapas desenvolvidas no equacionamento , implementação e verificação . A verificação foi realizada segundo um processo matemático formal ( Método das Soluções Manufaturadas ) com o qual obteve-se que a ordem de precisão dos cálculos era a mesma da ordem formal dos esquemas de discretização utilizados para todas as variáveis . Também obteve-se a mesma concordância para análise do divergente da velocidade , verificando o código para simulação_numérica  direta de aeroacústica . Posteriormente , foram realizadas simulações de escoamentos compressíveis cisalhantes e seus resultados comparados com dados apresentados em literatura . Também foram calculadas as taxas de amplificação de perturbações e comparadas com a Teoria de Estabilidade Linear . Novamente , foram obtidos resultados_satisfatórios  nessas etapas , mostrando que a implementação do código DNS está verificada 
 Em diversas_áreas  do conhecimento , um tempo considerável tem sido gasto na compreensão e tratamento de dados ruidosos . Trata-se de uma ocorrência comum quando nos referimos a coleta , a transmissão e ao armazenamento de informações . Esses dados ruidosos , quando utilizados na indução de classificadores por técnicas de Aprendizado de Maquina , aumentam a complexidade da hipótese obtida , bem como o aumento do seu tempo de indução , além de prejudicar sua acurácia preditiva . Trata-los na etapa de pré-processamento pode significar uma melhora da qualidade dos dados e um aumento na compreensão do problema estudado . Esta Tese investiga medidas de complexidade capazes de caracterizar a presença de ruídos em um conjunto de dados , desenvolve novos filtros que sejam mais eficientes em determinados nichos do problema de detecção e remoção de ruídos que as técnicas consideradas estado da arte e recomenda as mais apropriadas técnicas ou comitês de técnicas para um determinado conjunto de dados por meio de meta-aprendizado . As bases de dados utilizadas nos experimentos_realizados  neste trabalho são tanto artificiais quanto reais , coletadas de repositórios públicos e fornecidas por projetos de cooperação . A avaliação consiste tanto da adição de ruídos artificiais quanto da validação de um especialista . Experimentos realizados mostraram o potencial das propostas investigadas 
 Atualmente , informações de referência são disponibilizadas através de repositórios de documentos semanticamente ligados , criados de forma colaborativa e com acesso livre na Web . Entre os muitos problemas enfrentados pelos provedores de conteúdo desses repositórios , destaca-se a Wikification , isto é , a inclusão de links nos artigos desses repositórios . Esses links possibilitam a navegação pelos artigos e permitem ao usuário um aprofundamento semântico do conteúdo . A Wikification é uma tarefa complexa , uma vez que o crescimento contínuo de tais repositórios resulta em um esforço cada vez maior dos editores . Como consequência , eles têm seu foco desviado da criação de conteúdo , que deveria ser o seu principal objetivo . Isso tem motivado o desenvolvimento de ferramentas de Wikification automática que , tradicionalmente , abordam dois problemas distintos : ( a ) como identificar que palavras ( ou frases ) em um artigo deveriam ser selecionados como texto de âncora e ( b ) como determinar para que artigos o link , associado ao texto de âncora , deveria apontar . A maioria dos métodos na literatura que abordam esses problemas usam aprendizado de máquina . Eles tentam capturar , através de atributos estatísticos , características dos conceitos e seus links . Embora essas estratégias tratam o repositório como um grafo de conceitos , normalmente elas pouco exploram a estrutura topológica do grafo , uma vez que se limitam a descrevê-lo por meio de atributos estatísticos dos links , projetados por especialistas humanos . Embora tais métodos sejam eficazes , novos modelos poderiam tirar mais proveito da topologia se a descrevessem por meio de abordagens orientados a dados , tais como a fatoração matricial . De fato , essa abordagem tem sido aplicada com sucesso em outros domínios como recomendação de filmes . Neste trabalho , propomos um modelo de previsão para Wikification que combina a força dos previsores tradicionais baseados em atributos estatísticos , projetados por seres_humanos  , com um componente de previsão latente , que modela a topologia do grafo de conceitos usando fatoração matricial . Ao comparar nosso modelo com o estado-da-arte em Wikification , usando uma amostra de artigos Wikipédia , observamos um ganho de até 13 % em F1 . Além_disso  , fornecemos uma análise detalhada do desempenho do modelo enfatizando a importância do componente de previsão latente e dos atributos derivados dos links entre os conceitos . Também analisamos o impacto de conceitos ambíguos , o que permite concluir que nosso modelo se porta bem mesmo diante de ambiguidade , apesar de não tratar explicitamente este problema . Ainda realizamos um estudo sobre o impacto da seleção das amostras de treino conforme a qualidade dos seus conteúdos , uma informação disponível em alguns repositórios , tais como a Wikipédia . Nós observamos que o treino com documentos de alta qualidade melhora a precisão do método , minimizando o uso de links desnecessários 
 Produção multimídia é uma atividade complexa composta por múltiplas atividades de gerência e transformação de informação , as quais suportam um objetivo de criar conteúdo . Exemplos dessas atividades são estruturação , organização , modificação e versionamento de elementos de mídia , os quais dependem da manutenção de documentos auxiliares e metadados . Em produções profissionais , as quais podem contar com recursos humanos e materiais adequados , tal documentação é mantida pela equipe de produção , sendo instrumental para garantir a uma alta qualidade no produto_final  . Em configurações com menos recursos , como produções amadoras , ao menos padrões razoáveis de qualidade são desejados na maioria dos casos , contudo a dificuldade em gerenciar e transformar conteúdo pode inibir amadores a produzir conteúdo com qualidade aceitável . Esse problema tem sido atacado em várias frentes , por exemplo via métodos de anotação , métodos de navegação e técnicas de autoria , apenas para nomear algumas . Nesta tese , o objetivo principal é tirar proveito de anotações criadas pelo usuário com o intuito de apoiar autoria multimídia por amadores . De modo a subsidiar esse objetivo , as contribuições são construídas em torno uma abordagem de autoria baseada em documentos_multimídia  estruturados . Primeiramente , uma linguagem customizada para documentos_multimídia  baseados na Web é definida , baseada na linguagem SMIL ( Synchronized Multimedia Integration Language ) . Esta linguagem traz diversas contribuições , como a formalização de um modelo estendido para formatação temporal baseado em grafos , edição ao vivo de elementos de um documento e funcionalidades de reúso . Em segundo , um modelo para anotação de documentos e uma álgebra para transformação de documentos são definidos , ambos permitindo composição e extração de fragmentos de documentos_multimídia  com base em anotações . Em terceiro , as contribuições anteriores são integradas em uma ferramenta de autoria baseada na Web , a qual permite manipular um documento enquanto o mesmo está ativo . Tais manipulações envolvem diferentes técnicas de interação com o objetivo de enriquecer , editar , publicar e estender documentos_multimídia  interativos . As contribuições são instanciadas com sessões multimídia obtidas de ferramentas de colaboração síncrona , em cenários de aulas baseadas em vídeos , reuniões e pesquisa qualitativa baseada em vídeos . Tais instanciações demonstram a aplicabilidade e utilidade das 
 Neste trabalho propomos uma nova maneira de visualizar campos vetoriais , dados de considerável importância em vários ramos da ciência . Fizemos uma revisão bibliográfica sobre segmentação de campos vetoriais e desenvolvemos nosso próprio método . Neste método são extraídas informações do campo e , de distribuições de frequências dos dados coletados são formados vetores multidimensionais . Esses vetores são projetados em duas dimensões e os agrupamentos destes pontos são utilizados para formar a segmentação do campo original . Os profissionais que fazem_uso  de ferramentas de visualização científica possuem , em geral , informações relevantes sobre o domínio do campo vetorial , mas essa informação é raramente aproveitada nas técnicas de segmentação . A técnica desenvolvida permite que o usuário interaja com os resultados , de maneira intuitiva , corrigindo e explorando a segmentação usando seu próprio conhecimento . Como contribuições desta pesquisa podemos citar o mecanismo de interação com o usuário para o auxílio da segmentação e uma nova maneira para representar os dados colhidos de campos vetoriais em dimensão 
 Redes_neurais  biológicas contêm bilhões de células ( neurônios ) agrupadas em regiões espacial e funcionalmente distintas . Elas também apresentam comportamentos complexos , tais como dinâmicas periódicas e caóticas . Na área da Inteligência_Artificial  , pesquisas mostram que Redes_Neurais  Caóticas , isto é , modelos de Redes_Neurais  Artificiais que operam com dinâmicas complexas , são mais eficientes do que modelos tradicionais no que diz_respeito  a evitar memórias espúrias . Inspirado pelo fato de que o córtex cerebral contém agrupamentos de células e motivado pela eficiência no uso de dinâmicas complexas , este projeto de pesquisa investiga o comportamento dinâmico de um modelo de Rede_Neural  Artificial Recorrente , como o de Hopfield , porém com a topologia sináptica reorganizada a ponto de originar agrupamentos de neurônios , tal como acontece em uma Rede Complexa quando esta apresenta uma estrutura de comunidades . O modelo de treinamento tradicional de Hopfield também é alterado para uma regra de aprendizado que posta os padrões em ciclos , gerando uma matriz de pesos assimétrica . Resultados indicam que o modelo proposto oscila entre comportamentos periódicos e caóticos , dependendo do grau de fragmentação das sinapses . Com baixo grau de fragmentação , a rede opera com dinâmica periódica , como consequência da regra de treinamento utilizada . Dinâmicas caóticas parecem surgir quando existe um alto grau de fragmentação . Mostra-se , também , que é possível_obter  caoticidade em uma topologia adequadamente modular , ou seja , como uma estrutura de comunidades válida . Desta forma , este projeto de pesquisa provê uma metodologia alternativa para se construir um modelo de Rede_Neural  Artificial que realiza tarefas de reconhecimento de padrões , explorando dinâmicas complexas por meio de uma estrutura de conexões que se mostra mais similar à topologia existente no 
 
 Gamificação é um termo que se refere ao uso de elementos do projeto de jogos em contextos que não são jogos . Nestes contextos , o objetivo primário da gamificação não é lúdico , mas sim o de motivar os usuários a realizarem tarefas ou alterarem comportamentos . Também é objetivo da gamificação , cativar usuários e influenciá-los a persistirem na utilização do sistema gamificado . Nos últimos_anos  , testemunhamos um crescente_interesse  em gamificação e sua aplicação em ambientes de aprendizagem , especialmente online . No contexto da aprendizagem , motivar os estudantes a dar seguimento as tarefas pedagógicas é um papel_importante  dos professores e dos ambientes educacionais inteligentes . Por essa razão , as tecnologias persuasivas como a gamificação têm sido usadas também em ambientes de aprendizagem colaborativa para aumentar o engajamento dos estudantes e para reduzir o sentimento de obrigação na execução de tarefas pedagógicas . Contudo , quando mal utilizada , a gamificação pode se tornar uma distração capaz de interferir no processo de ensino-aprendizagem . Entretanto , a maioria dos estudos na área continuam focados nos potenciais benefícios da gamificação e menos em investigar soluções sistematizadas para se atingir os benefícios . Nossa contribuição para a solução do problema é baseada no uso de perfis de persuasão que levam em consideração o papel de jogador do estudante . Nós conduzimos mapeamentos sistemáticos da literatura para obter informação sobre gamificação em educação e como são formados grupos de estudantes em ambientes de aprendizagem colaborativa . Como resultado nós criamos dois arcabouços conceituais . Um arcabouço para ajudar a compreender e classificar a formação de grupos no contexto da aprendizagem colaborativa com suporte computacional , e outro para apoiar a definição de papéis de jogadores em ambientes colaborativos . Em um estudo preliminar ( N=481 ) , adaptamos e validamos para o português brasileiro uma escala para medir a susceptibilidade à persuasão dos usuários . Em outro estudo ( N=149 ) desenvolvemos um modelo teórico para mapear estratégias persuasivas e diferentes papéis de jogadores para apoiar a elaboração de perfis de persuasão . Para verificar a viabilidade de nosso modelo , em outro estudo ( N=18 ) elaboramos protótipos de interfaces do usuário . Analisamos a capacidade de influenciar das interfaces comparando papéis de jogadores e susceptibilidade a princípios de influência . Os resultados mostram que os estudantes menos motivados eram mais susceptíveis a aceitar as sugestões do protótipo , enquanto usuários com índices de motivação acima da média ( dentre estudantes observados ) , tendiam a reagir negativamente às tentativas de influenciá-los , apresentando índices menores de concordância para com as solicitações do protótipo gamificado . Observamos ainda nos três estudos conduzidos ( N=648 ) , comparado aos outros princípios de influência , o número de indivíduos suscetíveis ao princípio de autoridade eram os menores . Poucas iniciativas de pesquisa vêm investigando como desenvolver sistemas de gamificados que se adaptam aos papéis de jogadores . Parte desta deficiência pode ser explicada devido à complexidade no projeto e desenvolvimento destes sistemas . Entretanto como evidenciado , além da ineficácia dos modelos de gamificação baseados em uma solução para todos , o maior risco observado está no uso de modelos contraproducentes , uma vez que a estratégia apropriada para motivar um indivíduo , pode acabar desmotivando outros ( backfire effect ) 
 A área de robótica_móvel  se encontra numa fase de grande expansão , mas um dos obstáculos a ser vencido é o desenvolvimento de sistemas computacionais embarcados que combinem baixo consumo de energia com alta capacidade de processamento . A computação reconfigurável tem o potencial para atender esta demanda . Este trabalho visa avaliar as dificuldades no aproveitamento desta tecnologia através da implementação em hardware de um sistema de desvio de obstáculos para robôs_móveis  usando uma única câmera de baixo custo como sensor . Normalmente os algorítmos de fluxo óptico usados neste projeto são implementados inteiramente em software e sofrem várias restrições para poderem operar nos computadores embarcados nos robôs . O projeto descrito neste trabalho não tem estas restrições mas exige um esforço maior de 
 A visãao é o sentido humano mais complexo e importante para os processos de cognição e de interação de um indivíduo com o mundo . Neurofiisiologistas buscam identificar e compreender como funcionam os mecanismos celulares envolvidos neste processo . O sistema visual recebe os sinais de imagens captadas pelos olhos e , por meio de transformações e processamento diversos , integra esses sinais em representações de objetos internos perceptuais . O projeto Olho Virtual consegue reconstruir , em três dimensões , modelos de olhos utilizando córneas reais ou simuladas em computador , reproduzindo suas propriedades ópticas captando imagens de maneira satisfatória . Este trabalho introduz , no projeto Olho Virtual , um modelo computacional de retina baseado no modelo biológico , capaz de reproduzir as propriedades das células cones e bastonetes em suas distribuições radiais e também em suas funcionalidades em particular . Além dessa , é apresentado uma modelagem para reprodução dos campos receptivos das células ganglionares presentes na retina , gerando sinais de saída nos sistemas parvo e magno . Por fim são feitas simulações de experimentos psicofísicos com propósito de verificar a validade da modelagem 
 Os Sistemas de Gerenciamento de Bases de Dados devem atualmente ser capazes de gerenciar dados complexos , como dados multimídia , sequências genéticas , séries_temporais  , além dos dados tradicionais . Em consultas em grandes coleções de dados complexos , a similaridade entre os dados é o fator mais importante , e pode ser adequadamente expressada quando esses dados são representados em espaços_métricos  . Independentemente do domínio de um tipo de dados , existem aplicações que devem acompanhar a evolução_temporal  dos elementos de dados . Porém , os Métodos de Acesso Métrico existentes consideram que os dados são imutáveis com o decorrer do tempo . Visando o tratamento do tempo e dinamicidade em dados representados em espaços_métricos  , o trabalho apresentado nesta tese foi desenvolvido em duas frentes principais de atividades . A primeira frente tratou da inclusão das operações de remoção e atualização em métodos de acesso métrico , e visa atender às necessidades de domínios de aplicação em que dados em espaços_métricos  sofram atualização frequente , independentemente de necessitarem de tratamento temporal . Desta frente de atividades também resultou um novo método de otimização de àrvores métricas , baseado no algoritmo de remoção desenvolvido . A segunda frente de atividades aborda a inclusão do conceito de evolução_temporal  em dados representados em espaços_métricos  . Para isso foi proposto o Espaço Métrico-temporal , um modelo de representação de dados que permite a comparação de elementos métricos associado a informações temporais . O modelo conta com um método para identificar as contribuições relativas das componentes métrica e temporal no cálculo da similaridade . Também foram apresentadas estratégias para análise de trajetórias de dados métricos com o decorrer do tempo , através da imersão de espaços métrico-temporais em espaços dimensionais . Por fim , foi apresentado um novo método de balanceamento de múltiplos descritores para representação de imagens , fruto de modificações no método proposto para identificar as contribuições das componentes que podem formar um espaço 
 MÉTODOS de geração de casos de teste visam a gerar um conjunto de casos de teste com uma boa relação custo/benefício . Critérios de cobertura de teste definem requisitos de teste , os quais um conjunto de teste adequado deve cobrir . Métodos e critérios visam a selecionar casos de teste baseados em especificações , que podem ser descritas por meio de modelos , tais como Máquinas de Estados_Finitos  ( MEF ) . Existem diversos métodos de geração e critérios de cobertura , diferindo entre si em função das propriedades exigidas da MEF , do custo dos testes gerados e da eficácia na revelação de defeitos . Apesar de pesquisas intensas na definição desses métodos e critérios , são poucas as ferramentas de apoio disponíveis assim como são poucos os relatos de aplicação em termos de custo e eficácia para a definição de estratégias de teste efetivas . Dessa forma , é necessário obter dados reais das vantagens e desvantagens dos métodos e critérios para subsidiar a tomada de decisão no processo de desenvolvimento de software no que tange às atividades de teste e validação . Este trabalho apresenta resultados de experimentos para avaliar o custo e a eficácia de aplicação dos métodos e critérios mais relevantes para subsidiar a definição de estratégias de teste em diversos contextos , como por exemplo , no desenvolvimento de protocolos e de sistemas reativos . Utiliza-se um protótipo desenvolvido a partir de uma reengenharia da ferramenta Plavis/FSM para apoiar os 
 A interferência espectral gera patologias nos sistemas de comunicação sem fio ( wireless ) , como por exemplo , quedas na comunicação e degradação na vazão . O espectro de RF ( rádio frequência ) é fiscalizado e controlado por órgãos governamentais , no entanto as redes sem fio padrão IEEE 802.11 , conhecidas por WLANs ( Wireless Local Area Networks ) , trabalham em faixas espectrais não licenciadas , conhecidas por ISM . Estas redes estão cada vez mais presentes nos ambientes comerciais e residenciais , contribuindo para questões de ubiquidade e acesso à Internet . Com este aumento expressivo , a cobertura espectral está cada vez mais densa . A densidade elevada de sinais aponta para a saturação do espectro ISM , causando interferências mútuas das redes IEEE 802.11 . O objetivo deste trabalho é analisar a cobertura do espectro , por redes WLANs , e avaliar quedas de vazão ocasionadas por interferências espectrais , variando no espaço e no tempo . Dois cenários foram mapeados para analisar as degradações , um com baixa a média e outro com alta densidade e complexidade . O objetivo da criação desses cenários foi comparar as degradações causadas pelas interferências em ambientes diferentes , na ocupação , na utilização e na propagação de sinais de redes WLANs . Através dos resultados obtidos , um ciclo de vida de gerenciamento do espectro de redes padrão 802.11 foi proposto . Este ciclo contribui para avaliar e classificar o estado de uma rede em densa , não densa , complexa e não complexa , visto que alterações de ocupação do espectro no espaço e no tempo são plausíveis de ocorrerem . Conclui-se que os impactos da sobreposição total do canal , por fontes 802.11 , não são suficientes para a substancial degradação da vazão em ambientes de baixa a média complexidade . Em ambientes com alta densidade e complexidade as degradações são mais evidentes , principalmente quando ocorrem perturbações vindas de duas fontes 
 O teste de software é reconhecido como uma importante atividade na garantia da qualidade de sistemas de software . Com o objetivo de dar_apoio  a essa atividade , uma diversidade de ferramentas de teste têm sido desenvolvida . Entretanto , grande parte dessas ferramentas é construída de forma isolada , possuindo arquiteturas e estruturas próprias , o que tem impactado negativamente a capacidade de integração e o reúso dessas ferramentas . Nesse contexto , esforços têm sido dedicados à disponibilização de ferramentas de teste orientadas a serviço , ou seja , ferramentas que são baseadas na SOA ( Service Oriented Architecture ) . Em uma outra perspectiva , arquiteturas de referência têm desempenhado um importante papel no desenvolvimento de sistemas de software , uma vez que contém informações sobre como desenvolver sistemas para um determinado domínio de aplicação , buscando contribuir para o sucesso de sistemas desse domínio . Assim , o principal objetivo deste trabalho é o estabelecimento de uma arquitetura de referência orientada a serviço , denominada RefTEST-SOA ( Reference Architecture for Software Testing Tools based on SOA ) , que agrega o conhecimento e a experiência de como organizar ferramentas de teste orientadas a serviço , visando também à integração , à escalabilidade e o reúso providos pela SOA . Para o estabelecimento dessa arquitetura , foi utilizado o ProSA-RA , um processo que sistematiza o projeto , representação e avaliação de arquiteturas de referência . Resultados alcançados no estudo de caso conduzido evidenciam que a RefTEST-SOA é uma arquitetura viável e reusável para o desenvolvimento de ferramentas de teste orientadas a 
 Este projeto de mestrado aborda a utilização de recursos daWeb Semântica na seleção de informações sobre Web_Services  no registro UDDI ( Universal Description , Discovery , and Integration ) . Esse registro possui a limitação de apenas armazenar informações funcionais de Web_Services  . As informações não funcionais que incluem as informações de qualidade de serviço ( QoS - Quality of Service ) não são contempladas e dessa forma dificulta a escolha do melhor serviço pelos clientes . Neste projeto , a representação da base de conhecimento com informações sobre os provedores , clientes , acordos , serviços e a qualidade dos serviços prestados foi feita por meio de uma ontologia . Essa ontologia é utilizada pelo módulo UDOnt-Q ( Universal Discovery with Ontology and QoS ) que foi projetado para servir de plataforma para algoritmos de busca e composição de serviços com qualidade . Embora a utilização de semântica possa ser empregada para a composição e automatização de serviços , o foco deste trabalho é a garantia de qualidade de serviço em Web_Services  . Os algoritmos desenvolvidos empregam recursos da Web Semântica para classificar e selecionar os Web_Services  adequados de acordo com as informações de qualidade que estão armazenados na ontologia . O módulo e os algoritmos foram submetidos a avaliações de desempenho que revelaram problemas de desempenho com relação a abordagem adotada durante o processo de inferência da ontologia . Tal processo é utilizado para a classificação das informações dos elementos presentes na ontologia . Contudo , uma vez que as informações foram inferidas , o processo de busca e seleção de serviços comprovou a viabilidade de utilização do módulo e de um dos seus algoritmos de 
 Estratégias para automatização de atividades de teste de software são bem aceitas tanto pela indústria quanto pela academia . Um elemento essencial para automatizações de teste são oráculos de teste . Oráculos , que podem ser mecanismos , funções , execuções paralelas , etc , são fundamentais por determinarem se as saídas de uma aplicação em teste estão corretas . A automatização de mecanismos de oráculos é um ponto crítico quando as saídas dos sistemas se manifestam por meio de formatos não triviais como , por exemplo , uma Interface Gráfica com o Usuário ( GUI - do ingês Graphical User Interface ) . Para esses casos , estratégias tradicionais de teste costumam ser custosas e exigir esforços consideráveis dos testadores . Este trabalho de mestrado_propõe  um método alternativo para a automatização de oráculos de teste para sistemas com GUIs . Para tanto , exploram-se conceitos de Recuperação de Imagens Baseada em Conteúdo para a composição de um método de automatização chamado de oráculos gráficos ( Gr-O - do inglês Graphical Oracle ) . Como contribuição , desenvolveram-se extratores de características visuais de GUIs . A condução e análise de estudos empíricos revelaram que o uso do Gr-O pode reduzir os custos para definições de oráculos de teste para sistemas com GUIs . Deste modo , o método proposto pode ser alternativo ou complementar às técnicas de teste tradicionais identificadas na 
 O crescente número de pessoas que utilizam a Web e sua vasta quantidade de conteúdo têm motivado especialistas em computação a investigar e acompanhar a experimentação desses usuários . Podese observar que um grande número de usuários que adere ao uso da Internet é o das pessoas com meia-idade , de 40 a 59 anos , e os idosos com idade superior a 60 anos . Trata-se de um público interessado e potencialmente pró-ativo para usufruir das possibilidades que a Web proporciona a cada dia . No entanto , barreiras que dificultam o acesso à informação são naturalmente encontradas com o avançar da idade . Dificuldades como a perda parcial ou total da visão , a redução de precisão dos movimentos , e a diminuição de memorização dos passos a serem realizados são alguns exemplos dessas barreiras . Para superar as dificuldades apresentadas pelos usuários , estudos têm sido realizados visando minimizá-las , de maneira geral . Seus resultados encontram-se sintetizados em normas / orientações a serem seguidas visando proporcionar melhor usabilidade e acessibilidade para toda diversidade de usuários . Nesse contexto , na presente pesquisa , investiga-se quais dessas normas junto com os padrões para a criação de websites que disponibilizam conteúdos proporcionam melhor adequação para atender à demanda das pessoas com idade mais avançada . Assim , foram estudados diferentes_tipos  de menus de navegação que disponibilizam as subnavegações conforme a necessidade do usuário ( no inglês conhecido como menus drop-down ) com diferentes propriedades e avaliado qual menu e suas propriedades apresentavam melhores_resultados  , entre eles , foi verificado o tempo de uso do menu e o número de erros cometidos para uma determinada tarefa . Conclui-se que , menus com melhores contraste e um tempo mediano de resposta ao realizar a interação , apresentaram melhores_resultados 
 Neste trabalho , apresentamos um método de simulação Monte_Carlo  para o cálculo do hedging dinâmico de opções do tipo europeia em mercados multidimensionais do tipo Browniano e livres de arbitragem . Baseado em aproximações martingales de variação limitada para as decomposições de Galtchouk-Kunita-Watanabe , propomos uma metodologia factível e construtiva que nos permite calcular estratégias de hedging puras com respeito a qualquer opção quadrado integrável em mercados completos e incompletos . Uma vantagem da abordagem apresentada aqui é a flexibilidade de aplicação do método para os critérios quadráticos de minimização do risco local e de variância média de forma geral , sem a necessidade de se considerar hipóteses de suavidade para a função payoff . Em particular , a metodologia pode ser aplicada para calcular estratégias de hedging quadráticas multidimensionais para opções que dependem de toda a trajetória dos ativos subjacentes em modelos de volatilidade estocástica e com funções payoff descontínuas . Ilustramos nossa metodologia , fornecendo exemplos numéricos dos cálculos das estratégias de hedging para opções vanilla e opções exóticas que dependem de toda a trajetória dos ativos subjacentes escritas sobre modelos de volatilidade local e modelos de volatilidade estocástica . Ressaltamos que as simulações são baseadas em aproximações para os processos de preços descontados e , para estas aproximações , utilizamos o método_numérico  de Euler-Maruyama aplicado em uma discretização aleatória simples . Além_disso  , fornecemos alguns resultados teóricos acerca da convergência desta aproximação para modelos simples em que podemos considerar a condição de Lipschitz e para o modelo de volatilidade estocástica de Heston 
 O número de estrangeiros interessados em aprender o português tem aumentado na última_década  , em consequência do crescimento da economia brasileira e do aumento da presença de multinacionais no Brasil . Esse fato se mostra pelo aumento do número de inscritos no exame de proficiência de português CELPE-Bras e de estudantes estrangeiros que ingressam nas universidades brasileiras . A maioria destes estudantes são de língua espanhola e precisam escrever seus textos acadêmicos em português . A proximidade das línguas portuguesa e espanhola apresenta-se tanto como um elemento positivo quanto como um obstáculo , pois oculta as diferenças e impede o domínio da língua_portuguesa  , mantendo , na fala e na escrita em português , interferências do espanhol . O maior número destas interferências acontece no nível lexical . Uma das alternativas para tratar os problemas em textos de aprendizes de uma língua é o emprego de ferramentas computacionais de pós-processamento e de suporte ao processo de escrita . No entanto , o número de recursos e ferramentas disponíveis para auxiliar a escrita de português como língua estrangeira é muito reduzido , diferentemente do cenário para a língua inglesa . Esta pesquisa propôs a criação de recursos e ferramentas de suporte à escrita no nível lexical como primeiro passo para a melhoria da qualidade linguística dos textos em português produzidos pelos nativos do espanhol . A Linguística de Córpus foi utilizada como metodologia para viabilizar a análise de erros de aprendizes . As ferramentas de auxílio utilizam léxicos bilíngues compilados por meio de técnicas de tradução , baseadas em alinhamento de córpus paralelos . Dado o número insuficiente de erros previamente anotados para suportar a detecção automática de erros , esta pesquisa propôs métodos baseados em modelo língua e na geração artificial de erros . A geração de erros artificiais se apresentou como um método eficiente para predizer erros lexicais dos aprendizes . As contribuições obtidas com a metodologia baseada em tradução automática para gerar auxílios à escrita entre línguas próximas , considerando a análise de erros lexicais extraídos de córpus de aprendizes , foco desta pesquisa , são : ( i ) do ponto de vista teórico , o levantamento e quantificação dos principais problemas causados pelas marcas do espanhol , deixadas nos textos acadêmicos em português escritos por nativos do espanhol ; ( ii ) do ponto de vista de geração_automática  de recursos linguísticos , léxicos bilíngues de cognatos e falsos cognatos ; léxico bilíngue de marcadores discursivos ; léxico de expressões formulaicas que aparecem nos textos científicos e léxico bilíngue de verbos relacionados com pesquisa científica em português e , ( iii ) do ponto de vista da criação de subsídios para a área de auxílio à escrita científica , o projeto e avaliação de auxílios para suportar a escrita científica em português por nativos do espanhol 
 O reconhecimento de padrões de textura em imagens tem sido uma importante ferramenta na área de visão_computacional  . Isso porque o atributo textura pode revelar características intrínsecas , tornando possível a classificação de um conjunto de imagens semelhantes . Embora a textura seja estudada há mais de meio século , ainda não existe um consenso sobre sua definição e nem mesmo um método de extração de características de textura que seja eficiente para todos os tipos de imagens . Além_disso  , os métodos da literatura analisam os padrões de textura de maneira global , considerando que uma imagem apresente um conjunto de micropadrões que formam um único padrão global ou homogêneo de textura na imagem . No entanto , alguns tipos de imagens apresentam heterogeneidade em sua composição , ou seja , o conjunto de micropadrões na imagem é responsável por formar mais de um padrão de textura dentro da mesma imagem . Esse tipo de imagens levou ao propósito de investigação deste trabalho . Independentemente do método de extração de característica utilizado , considerar a heterogeneidade do padrão de textura em uma imagem leva a uma melhor representação de suas características . Para melhorar a análise de padrões heterogêneos de textura , três abordagens são propostas : ( i ) lazy-patch , ( ii ) combinação de modelos e ( iii ) modelagem da textura por meio de autômatos celulares inspirados em corrosão alveolar . Os resultados ao aplicar essas abordagens em diferentes conjuntos de imagens de biologia e nanotecnologia , mostraram que a análise de padrões heterogêneos resulta em melhor representatividade de imagens que possuem padrões heterogêneos de textura em sua composição 
 Métodos tradicionais de aprendizado_supervisionado  , chamados de aprendizado monorrótulo , consideram que cada exemplo do conjunto de dados rotulados está associado a um único rótulo . No entanto , existe uma crescente quantidade de aplicações que lidam com exemplos que estão associados a múltiplos rótulos . Essas aplicações requerem métodos de aprendizado multirrótulo . Esse cenário de aprendizado introduz novos desafios que demandam abordagens diferentes daquelas tradicionalmente utilizadas no aprendizado monorrótulo . O custo associado ao processo de rotulação de exemplos , um problema presente em aprendizado monorrótulo , é ainda mais acentuado no contexto multirrótulo . O desenvolvimento de métodos para reduzir esse custo representa um desafio de pesquisa nessa área . Além_disso  , novos métodos de aprendizado também devem ser desenvolvidos para , entre outros objetivos , considerar a dependência de rótulos : uma nova característica presente no aprendizado multirrótulo . Há um consenso na comunidade de que métodos de aprendizado multirrótulo têm a capacidade de usufruir de melhor eficácia preditiva quando considerada a dependência de rótulos . Os principais objetivos deste trabalho estão relacionados a esses desafios : reduzir o custo do processo de rotulação de exemplos ; e desenvolver métodos de aprendizado que explorem a dependência de rótulos . No primeiro caso , entre outras contribuições , um novo método de aprendizado ativo , chamado score dev , é proposto para reduzir os custos associados ao processo de rotulação multirrótulo . Resultados experimentais indicam que o método score dev é superior a outros métodos em vários domínios . No segundo caso , um método para identificar dependência de rótulos , chamado UBC , é proposto , bem como o BR+ , um método para explorar essa característica . O método BR+ apresenta resultados superiores a métodos considerados estado da 
 Uma abordagem de teste_estrutural  de integração par-a-par para programas_OO  e OA escritos em Java e AspectJ é apresentada . A finalidade dessa abordagem é descobrir defeitos que possam existir na interface entre os pares de unidades que se relacionam em um programa . Para programas_OO  este tipo de teste envolve testar a interação entre os pares de métodos . Já para programas OA , o teste_estrutural  de integração par-a-par envolve testar a interação entre os seguintes pares de unidades : método-método , método-adendo , adendo-método e adendo-adendo . Para efetuar o teste_estrutural  de integração par-a-par deve-se considerar todo o fluxo de execução ( fluxo de controle e de dados ) que ocorre entre a unidade chamadora e a unidade chamada . Para isso é definido o grafo Def-Uso Par-a-Par ( PWDU ) que é uma abstração formada pela integração dos grafos Def-Uso Orientado a Aspectos ( AODU ) da unidade chamadora e da unidade chamada . Além_disso  , são propostos três critérios para derivar requisitos de teste para pares de unidades . Dentre eles , dois critérios são baseados em fluxo de controle : todos-nós-integrados e todas-arestas-integradas ; e um critério é baseado em fluxo de dados : todos-usos-integrados . Uma ferramenta que apóia o teste_estrutural  de unidade de programas_OO  e OA escritos em Java e AspectJ , chamada JaBUTi/AJ , foi estendida para dar_apoio  à abordagem de teste de integração proposta . Exemplos de usos são discutidos para explicar a aplicação da 
 Podemos encontrar em métodos ágeis como no Extreme Programming [ Beck , 1999 , Beck e Andres , 2006 ] , no Scrum [ Schwaber , 2008 ] , no Crystal Clear [ Cockburn , 2005 ] , e no Lean Software Development [ Poppendieck e Poppendieck , 2007 ] referências relacionadas à manipulação e disponibilização de métricas e outras informações no ambiente de desenvolvimento . Neste trabalho , estas atividades são consideradas como tarefas de acompanhamento ágil . Observamos em métodos ágeis a importância de se realizar ações ( práticas ) baseadas em alguns princípios como guidelines [ Poppendieck e Poppendieck , 2007 ] . Por isto , realizamos uma análise bibliográca na literatura disponível para compreender princípios ágeis que possam afetar na execução deste tipo de tarefa , além de escrever sobre métricas no contexto de métodos ágeis e engenharia de software . Apesar da bibliograa , não encontramos pesquisas experimentais com o objetivo de levantar e ( ou ) compreender aspectos_relacionados  ao sucesso na aplicação deste tipo de tarefa em ambientes de desenvolvimento . Para isto , realizamos neste trabalho uma pesquisa experimental com este objetivo , utilizando uma abordagem de métodos mistos sequenciais de pesquisa [ Creswell , 2009 ] . Escolhemos aplicar esta pesquisa em um conjunto de quinze equipes de desenvolvimento ágil , reunidas em realizações da disciplina Laboratório de Programação Extrema do IME-USP nos anos de 2010 e 2011 . Esta pesquisa foi realizada em quatro fases sequenciais . Na primeira fase , realizamos sugestões para as equipes de desenvolvimento vinculadas ao acompanhamento ágil a m de levantar aspectos valiosos em sua aplicação utilizando uma abordagem baseada em pesquisa-ação [ Thiollent , 2004 ] . Baseado nestes resultados , agrupamos alguns destes aspectos como heurísticas para o acompanhamento ágil , modelo similar ao de Hartmann e Dymond [ 2006 ] . Na segunda fase , aplicamos um questionário para vericar a validade das heurísticas levantadas . Na terceira fase , realizamos entrevistas semi-estruturadas com alguns integrantes destas equipes para compreender o por quê da validade das heurísticas levantadas , sendo analisadas com técnicas de teoria fundamentada em dados ( grounded theory ) [ Strauss e Corbin , 2008 ] . Na quarta fase , reaplicamos o questionário da fase 2 em outro ambiente para triangulação da validade das heurísticas . Como resultado nal da pesquisa , estabelecemos um conjunto de heurísticas para o acompanhamento ágil , além de avaliações quantitativas de seus aspectos em dois ambientes , juntamente a diversas considerações qualitativas sobre sua utilização . Realizamos um mapeamento tanto das heurísticas como de seus conceitos relacionados à literatura disponível , identicando aspectos já existentes porém expandidos pela realização da pesquisa , e aspectos ainda não discutidos que podem ser considerados como novos na área 
 Com o progresso da computação e popularização da Internet , a sociedade entrou na era da informação . Esta nova fase é marcada pela forma como produzimos e lidamos com a informação . Diariamente , são produzidos e armazenados milhares de Gigabytes de dados cujo valor é reduzido se a informação ali contida não puder ser transformada em conhecimento . Concomitante a este fato , o padrão da computação está caminhando para a miniaturização e acessibilidade com os dispositivos_móveis  . Estes novos equipamentos estão mudando o padrão de comportamento dos usuários que passam de leitores passivos a geradores de conteúdo . Neste contexto , este projeto de mestrado_propõe  a técnica de visualização de dados NMap e a ferramenta de visualização de dados web aplicável a dispositivo móvel SPCloud . A técnica NMap utiliza todo o espaço_visual  disponível para transmitir informações de grupos preservando a metáfora distância-similaridade . Teste comparativos com as principais técnicas do estado da arte mostraram que a técnica NMap tem melhores_resultados  em preservação de vizinhança com um tempo de processamento significativamente melhor . Este fato coloca a NMap como uma das principais técnicas de ocupação do espaço_visual  . A ferramenta SPCloud utiliza a NMap para visualizar notícias disponíveis na web . A ferramenta foi desenvolvida levando em consideração as características inerentes aos dispositivos moveis o que possibilita utiliza-la nestes equipamentos . Teste informais com usuários demonstraram que a ferramenta tem um bom_desempenho  para sumarizar grandes quantidades de notícias em um pequeno espaço_visual  
 Existe uma crescente busca por softwares e arquiteturas alternativas . Essa busca acontece pois houveram avanços na tecnologia do hardware , e estes avanços devem ser complementados por inovações nas metodologias de projetos , testes e verificação para que haja um uso eficaz da tecnologia . Os software e arquiteturas alternativas , geralmente são modelos que exploram o paralelismo das aplicações , ao contrário do modelo de Von Neumann . Dentre as arquiteturas alternativas de alto_desempenho  , tem-se a arquitetura a fluxo de dados . Nesse tipo de arquitetura , o processo de execução de programas é determinado pela disponibilidade dos dados , logo o paralelismo está embutido na própria natureza do sistema . O modelo a fluxo de dados possui a vantagem de expressar o paralelismo de maneira intrínseca , eliminando a necessidade do programador explicitar em seu código os trechos onde deve haver paralelismo . As arquiteturas a fluxo de dados voltaram a ser uma área de pesquisa devido aos avanços do hardware , em particular , os avanços da Computação Reconfigurável e dos Field Programmable Gate Arrays ( FPGAs ) .Nesta tese é descrita uma ferramenta de conversão de código que visa a geração de aplicações utilizando uma arquitetura a fluxo de dados estática . Também é descrito o projeto ChipCflow , cuja ferramenta de conversão de código , descrita nesta tese , é parte integrante . A especificação do algoritmo a ser convertido é feita em linguagem C e convertida para uma linguagem de descrição de hardware , respeitando o modelo proposto pelo ChipCflow . Os resultados alcançados visam a prova de conceito da conversão de código de uma linguagem de alto nível para uma arquitetura a fluxo de dados a ser configurada em FPGA 
 Esta tese de doutorado tem como objetivo principal , a proposta de um sistema onde é possível avaliar abordagens diferentes para composição automática de Web_services  , baseando-se em parâmetros de QoS que dependem da execução para serem medidos . O objetivo foi atingido por meio da implementação de um sistema denominado AWSCS ( Automatic Web Service Composition System ) . O AWSCS é um sistema onde é possível implementar abordagens diferentes para composição automática de Web_services  e também executar os fluxos resultantes dessas abordagens . Com o objetivo de demonstrar os resultados desta tese de doutorado , foram elaborados cenários , onde fluxos empíricos foram construídos para fazer a demonstração do funcionamento do AWSCS , uma vez que algoritmos para composição automática não foram encontrados para download na literatura . Com os resultados foi possível fazer um estudo do comportamento da execução de fluxos de Web_services  compostos , quando fluxos com mesma funcionalidade , mas estratégias de solução do problema diferentes foram comparados . Além_disso  , foi possível observar a influência das cargas aplicadas no sistema em execução , uma vez que o tipo de carga submetida ao sistema é um fator importante para se definir qual abordagem para composição de Web_services  pode_levar  ao melhor desempenho , de acordo com o ambiente e tipos de cargas que ele vai receber , quando em produção 
 Sistemas embarcados_críticos  ( SEC ) correspondem aos sistemas embarcados para aplicações críticas , que são aplicações nas quais falhas podem por vidas em perigo . A crescente imersão deste tipo de sistema em nossas atividades cotidianas aumentou sua demanda , e consequentemente , a necessidade por novas técnicas de desenvolvimento que possam aumentar a produtividade dos desenvolvedores . Linha de produtos ( LP ) é uma técnica de reúso , na qual famílias de produtos similares são desenvolvidos em conjunto , e portanto , objetiva-se o desenvolvimento em larga_escala  . Com a utilização de LP , pode-se reusar requisitos e arquitetura de forma sistemática . Entretanto , o reúso no domínio de sistemas embarcados_críticos  ainda é visto de forma cética . Por outro_lado  , o domínio de SEC tem obtido avanços de desenvolvimento por meio de técnicas orientadas a modelo . A Engenharia orientada a modelos ( MDE ) concentra-se na importância de modelos no ciclo de vida do sistema , tornando-os parte do produto_final  . Assim , mantêm-se os modelos simples , e a maioria da complexidade do software passa a pertencer às transformações , que podem ser obtidas automaticamente por motores de transformação . Isso ajuda a aumentar a qualidade do produto_final  , bem como facilitar a futura evolução do software , que poderá ser feita mudando-se os modelos de alto nível e obtendo-se , de forma automática , o novo produto . MDE é particularmente útil no domínio de SEC , pois a partir de modelos corretos do problema , a chance de introdução de erros durante a codificação é menor se geradores de aplicações forem utilizados em vez de codificação manual . Além_disso  , o aumento no nível de abstração proporcionado pelo MDE permite que técnicas de validação e verificação sejam usadas desde os estágios iniciais de desenvolvimento , tais como co-design e co-simulação . Portanto , este trabalho visa defender a tese de que é possível oferecer os benefícios do desenvolvimento em larga_escala  para o domínio de SEC e ainda continuar se beneficiando de técnicas de MDE . Para corroborar esta tese , define-se uma abordagem de desenvolvimento de linha de produtos de sistema com uso de técnicas de engenharia orientada a modelos para o desenvolvimento de famílias de sistemas embarcados_críticos  . Além_disso  , apresenta-se um estudo de caso de marca-passo elaborado segundo as indicações da abordagem proposta que indicam a viabilidade de sua utilização 
 Os layouts gerados por técnicas de projeção_multidimensional  podem ser a base para diferentes metáforas de visualização que são aplicáveis a diversos tipos de dados . Existe muito interesse em investigar metáforas alternativas à comumente usada , nuvem de pontos usada para exibir layouts gerados por projeções_multidimensionais  . Neste trabalho , foi estudado este problema , com foco no domínio da visualização de músicas . Existem muitas dimensões envolvidas na percepção e manipulação de músicas e portanto é difícil encontrar um modelo computacional intuitivo para representá-las . Nosso objetivo neste trabalho foi investigar as representações_visuais  capazes de transmitir a estrutura de uma música , assim como exibir uma coleção de músicas de modo a ressaltar as similaridades . A solução proposta consiste em uma representação icônica de músicas individuais , que é associada ao posicionamento espacial dos grupos ou coleções de músicas gerado por uma técnica de projeção_multidimensional  que reflete suas similaridades estruturais . Tanto a projeção quanto o ícone requerem um vetor de características para representar a música . As características são extraídas a partir de arquivos MIDI , já que a própria natureza das descrições MIDI permite a identificação das estruturas musicais relevantes . Estas características proporcionam a entrada tanto para a comparação de dissimilaridades quanto para a construção do ícone da música . Os posicionamentos espaciais são obtidos usando a técnica de projeção_multidimensional  Least Square Projection ( LSP ) , e as similaridades são calculadas usando a distância Dynamic Time Warping ( DTW ) . O ícone fornece um resumo visual das repetições de acordes em uma música em particular . Nessa dissertação são descritos os processos de geração destas representações_visuais  , além de descrever um sistema que implementa esses recursos e ilustrar como eles podem apoiar algumas tarefas exploratórias das coleções de músicas , identificando possíveis cenários de 
 Grades computacionais oportunistas utilizam recursos ociosos de máquinas compartilhadas para executar aplicações que necessitam de um alto poder_computacional  e/ou trabalham com grandes quantidades de dados . Mas a execução de aplicações paralelas computacionalmente intensivas em ambientes dinâmicos e heterogêneos , como grades_computacionais  oportunistas , é uma tarefa_difícil  . Máquinas podem falhar , ficar inacessíveis ou passar de ociosas para ocupadas inesperadamente , comprometendo a execução de aplicações . Um mecanismo de tolerância a falhas que dê suporte a arquiteturas heterogêneas é um importante requisito para estes sistemas . Neste trabalho , analisamos , implementamos e avaliamos um mecanismo de tolerância a falhas baseado em checkpointing para aplicações paralelas em grades_computacionais  oportunistas . Este mecanismo permite o monitoramento de execuções e a migração de aplicações entre nós heterogêneos da grade . Mas além da execução , é preciso gerenciar e armazenar os dados gerados e utilizados por estas aplicações . Desejamos uma infra-estrutura de armazenamento de dados de baixo custo e que utilize o espaço livre em disco de máquinas compartilhadas da grade . Devemos utilizar somente os ciclos ociosos destas máquinas para armazenar e recuperar dados , de modo que um sistema de armazenamento distribuído que as utilize deve ser redundante e tolerante a falhas . Para resolver o problema do armazenamento de dados em grades oportunistas , projetamos , implementamos e avaliamos o middleware OppStore . Este middleware provê armazenamento distribuído e confiável de dados , que podem ser acessados de qualquer máquina da grade . As máquinas são organizadas em aglomerados , que são conectados por uma rede peer-to-peer auto-organizável e tolerante a falhas . Dados são codificados em fragmentos redundantes antes de serem armazenados , de modo que arquivos podem ser reconstruídos utilizando apenas um subconjunto destes fragmentos . Finalmente , para lidar com a heterogeneidade dos recursos , desenvolvemos uma extensão ao protocolo de roteamento em redes peer-to-peer Pastry . Esta extensão adiciona balanceamento de carga e suporte à heterogeneidade de máquinas ao protocolo Pastry 
 O planejamento de rotações de culturas é um tema de interesse em ascensão por permitir uma redução significativa no uso de adubos industriais , agrotóxicos e outros produtos químicos no cultivo , permitindo a auto-sustentação e qualidade das terras cultivadas . Este trabalho centraliza em utilizar rotações para atender uma demanda periódica prédeterminada , respeitando as restrições relativas a aspectos ecológicos que auxiliam na estabilidade geral do solo para definir uma rotação de culturas factível . Modelos matemáticos que consideram um tamanho mínimo de lote a ser usado por uma rotação e métodos heurísticos , baseados em geração de colunas , são apresentados . Uma análise detalhada do comportamento dos métodos perante variações em diferentes parâmetros e critérios é realizada . A primeira heurística , denominada Algoritmo GC-BC , obteve resultados de melhor qualidade e de forma mais rápida que a segunda heurística , denominada Heurística Lote Fixo . Entretanto , combinando ambas heurísticas foi possível_obter  os resultados mais satisfatórios , ou seja , soluções que respeitam a condição de lote mínimo em um tempo computacional aceitável para um planejamento anual , cujos valores são próximos a um limitante superior . A ideia subjacente de gerar colunas adicionais para um problema mestre restrito produz soluções de qualidade , o que pode vir a ser aplicado em outras áreas de pesquisa que necessitam da geração de colunas para uma resolução em tempo computacional 
 Um dos desafios da Inteligência_Artificial  aplicada em jogos é o aprendizado de comportamento , em que o objetivo é utilizar estatísticas obtidas da interação entre jogador e jogo de modo a reconhecer características particulares de um jogador ou monitorar a evolução de seu comportamento no decorrer do tempo . A maior_parte  dos trabalhos na área emprega modelos previamente aprendidos , por meio da utilização de algoritmos de Aprendizado de Máquina . Entretanto , são poucos os trabalhos que consideram que o comportamento de um jogador pode evoluir no tempo e que , portanto , reconhecer quando essas mudanças ocorrem é o primeiro passo para produzir jogos que se adaptam automaticamente às capacidades do jogador . Para detectar variações comportamentais em um jogador , são necessários algoritmos que processem dados de modo incremental . Esse pré-requisito motiva o estudo de algoritmos para detecção de mudanças da área de Mineração em Fluxos Contínuos de Dados . Entretanto , algumas das características dos algoritmos disponíveis na literatura inviabilizam sua aplicação direta ao problema de detecção de mudança em jogos . Visando contornar essas dificuldades , esta tese propõe duas novas abordagens para detecção de mudanças de comportamento . A primeira abordagem é baseada em um algoritmo incremental de agrupamento e detecção de novidades que é independente do número e formato dos grupos presentes nos dados e que utiliza um mecanismo de janela deslizante para detecção de mudanças de comportamento . A segunda abordagem , por outro_lado  , é baseada na comparação de janelas de tempo consecutivas utilizando espectrogramas gerados a partir dos dados contidos em cada janela . Os resultados experimentais utilizando simulações e dados de jogos comerciais indicam a aplicabilidade dos algoritmos propostos na tarefa de detecção de mudanças de comportamento de um jogador , assim como mostram sua vantagem em relação a outros algoritmos para detecção de mudança disponíveis na 
 Considerando a crescente demanda por desempenho em sistemas computacionais , a implementação de algoritmos diretamente em hardware com o uso de FPGAs ( Field-programmable Gate Arrays ) é uma alternativa que tem apresentado bons_resultados  . Porém , os desafios de programação envolvidos no uso de FPGAs , de tal forma a explorar eficientemente seus recursos , limita o número de desenvolvedores em função da predominância do paradigma de programação tradicionalmente sequencial , imposto pelas linguagens imperativas . Assim , este trabalho busca desenvolver mecanismos que facilitem o desenvolvimento com FPGAs , otimizando o uso de memória e explorando o paralelismo das operações . Este documento apresenta a tese de doutorado de título LALP+ : um framework para o desenvolvimento de aceleradores de hardware em FPGAs . Dado que a latência para leitura e escrita de dados têm sido um gargalo para algumas aplicações de alto_desempenho  , este trabalho trata do desenvolvimento de técnicas para geração de arquiteturas de hardware , considerando aspectos relativos ao mapeamento , gerenciamento e acesso à memória em arquiteturas reconfiguráveis . Para isto , o projeto desenvolvido utiliza como base a linguagem LALP , cujo foco é o tratamento de loops com a técnica de loop pipelining . As técnicas descritas nesta tese são empregadas no desenvolvimento do framework LALP+ , o qual estende LALP com a implementação de novas características e funcionalidades , de forma a contribuir para o aumento do seu nível de abstração . As arquiteturas criadas utilizando LALP+ foram comparadas às geradas por ferramentas comerciais e acadêmicas , tendo apresentado , em média , um melhor desempenho , com redução do tempo de execução de 10 ; 01 , no melhor caso . Espera-se , por meio das contribuições aqui apresentadas , facilitar a implementação de produtos e projetos relacionados a aplicações de computação de alto_desempenho  que envolvam o uso de arquiteturas reconfiguráveis , promovendo uma maior absorção desta tecnologia 
 A World_Wide  Web é um meio de comunicação em constante crescimento , agregando diversos componentes e serviços em um ritmo acelerado . Os novos tipos de sites , tais como , o comércio_eletrônico  ( e-commerce ) , notí ? cia/informação ( Web-publishing ) , vídeo sob_demanda  exigem ainda mais recursos do servidor . Nesse contexto , visando adequar a avaliação de desempenho aos novos ambientes da Web , o presente_trabalho  apresenta um estudo caracterizando diversos traces de servidores_Web  Apache , permitindo coletar dados importantes que definem a forma como os usuários e servidores interagem . Com esses dados , quatro tipos de categorias de sites foram analisados : Padrão ( composto da média de todos os traces analisados ) , Acadêmico , Notícia/Informação e Tradicional . Nessa análise avaliam-se quatro aspectos : o intervalo de chegada , o código de resposta , o tipo objeto e o tamanho do objeto e ao final , modelos matemáticos são propostos para representar essas características . Além_disso  , este trabalho também desenvolveu um gerador de cargas de trabalho sintéticas , o W4Gen ( World_Wide  Web Workload Generator ) . Com uma interface_gráfica  amigável , ele permite aos seus usuários gerar novas cargas com base nos modelos matemáticos . Além_disso  , ele também permite modificar as características essenciais para simular novos tipos de cargas . Para validar os resultados deste trabalho , utilizou-se o modelo de servidor Web com diferenciação de serviços ( SWDS ) , verificando o desempenho em situações de 
 A Determinação da Estrutura tridimensional de Proteínas ( DEP ) a partir da sua seqüência de aminoácidos é importante para a engenharia de proteínas e o desenvolvimento de novos fármacos . Uma alternativa para este problema tem sido a aplicação de técnicas de computação evolutiva . As abordagens utilizando Algoritmos_Evolutivos  ( AEs ) tem obtido resultados relevantes , porém estão restritas a pequenas proteínas , com dezenas de aminoácidos e a algumas classes de proteínas . Este trabalho propõe a investigação de uma abordagem utilizando AEs para a predição da estrutura terciária de proteínas independentemente do seu tamanho e classe . Os resultados obtidos demonstram que apesar das dificuldades encontradas a abordagem investigada constitue-se em uma alternativa em relação aos métodos clássicos de determinação da estrutura terciária das proteínas 
 Funcionalidades de software expostas como serviços_Web  são cada vez mais comuns e suas formas de composição e coordenação são cada vez mais imprescindíveis . Orquestração e coreografia , tradicionais abordagens de composição de serviços_Web  , são providas por ferramentas voltadas ao gerenciamento de processos de negócio com diferentes enfoques . Apesar do sucesso dessas abordagens , existem ainda desafios a serem superados , tais como a dificuldade de manutenção em fluxos de controle já existentes , o custo de comunicação associado às interações com os serviços_Web  , o conhecimento do processo de negócio por parte dos serviços e ainda a compatibilidade dos mesmos em uma composição . Como alternativa às abordagens tradicionais , esta dissertação_propõe  o uso da abordagem WED-flow para composição de serviços_Web  , de modo que a execução de processos de negócio seja orientada pelas alterações do estado dos dados . Na abordagem proposta , o fluxo de controle não é um requisito , mas sim uma consequência da execução dos serviços_Web  , o que proporciona maior flexibilidade para o desenvolvimento e a manutenção das aplicações . Mais concretamente , a primeira contribuição deste trabalho é a proposição e a avaliação de cenários possíveis de orquestração e coreografia de acordo com critérios pré-definidos . A segunda contribuição é a implementação da abordagem WED-flow para a composição de serviços_Web  , bem como sua validação prática e sua avaliação em relação aos cenários de coreografia e orquestração 
 Ferramentas de análise visual desempenham um papel_importante  no cenário de soluções para grandes volumes de dados ( big data ) , combinando análise de dados e técnicas interativas de visualização de forma eficaz para apoiar a exploração incremental de coleções de dados em diversos_domínios  . Um desafio importante em análise visual é a exploração de conjuntos de dados multidimensionais , que consistem em muitas observações , sendo cada uma descrita por um grande número de dimensões , ou atributos . Encontrar e compreender os padrões presentes em tais espaços , tais como tendências , correlações , grupos de observações relacionadas e valores extremos , é difícil . Técnicas de redução de dimensionalidade ou projeções são utilizadas para construir , a partir de conjuntos de dados multidimensionais , representações de duas ou três dimensões que podem então ser utilizadas com substitutas do espaço original para sua interpretação visual , apoiando de forma eficiente as tarefas de análise de dados acima mencionadas . Projeções apresentam vantagens importantes sobre outras técnicas de visualização para dados multidimensionais , tais como escalabilidade visual , resistência a ruídos e baixa complexidade computacional . No entanto , um grande obstáculo para o uso prático de projeções vem da sua difícil interpretação . Dois principais tipos de desafios de interpretação de projeções são estudados nesta tese . Em primeiro lugar , mesmo que as técnicas de projeção tenham como objetivo preservar , na representação final , a estrutura do conjunto de dados original , elas podem introduzir uma certa quantidade de erros que influenciam a interpretação dos seus resultados . No entanto , é difícil transmitir aos usuários onde tais erros ocorrem na projeção , quão severos eles são e que aspectos específicos da interpretação dos dados eles afetam . Em segundo lugar , interpretar os padrões visuais que aparecem em uma projeção , além da percepção de grupos de observações semelhantes , está longe de ser trivial . Em particular , é difícil explicar tais padrões em termos do significado das dimensões dos dados originais . O trabalho desenvolvido nesta tese concentra-se no projeto e desenvolvimento de novas técnicas visuais explicativas para lidar com os dois desafios de interpretação de projeções_multidimensionais  descritos acima . São propostos alguns métodos para quantificar , classificar e representar visualmente diversos tipos de erros de projeção , e é descrito como essas representações explícitas ajudam na interpretação dos padrões dos dados . Além_disso  , também são propostas técnicas visuais para explicar projeções em termos dos atributos dos dados multidimensionais , tanto de forma global quanto local . As propostas apresentadas foram concebidas para serem facilmente incorporadas e usadas com qualquer técnica de projeção e em qualquer contexto de aplicação . As contribuições são demonstradas pela apresentação de vários cenários de exploração , envolvendo vários_tipos  de conjuntos de dados multidimensionais , desde medições e simulações científicas até métricas de qualidade de software , estruturas de sistema de software e redes 
 Há três décadas , os sistemas de informação corporativos eram projetados para apoiar a execução de tarefas pontuais . Atualmente , esses sistemas também precisam gerenciar os fluxos de trabalho ( workflows ) e processos de negócio de uma organização . Em comunidades científicas de físicos , astrônomos , biólogos , geólogos , entre outras , seus sistemas de informações distinguem-se dos existentes em ambientes corporativos por : tarefas repetitivas ( como re-execução de um mesmo experimento ) , processamento de dados brutos em resultados adequados para publicação ; e controle de condução de experimentos em diferentes ambientes de hardware e software . As diferentes características dos dois ambientes corporativo e científico propiciam que ferramentas e formalismos existentes ou priorizem o controle de fluxo de tarefas , ou o controle de fluxo de dados . Entretanto , há situações em que é preciso atender simultaneamente ao controle de transferência de dados e ao controle de fluxo de tarefas . Este trabalho visa caracterizar e delimitar o controle e representação do fluxo de dados em processos de negócios e workflows científicos . Para isso , são comparadas as ferramentas CPN Tools e KEPLER , que estão fundamentadas em dois formalismos : redes de Petri coloridas e grafos de workflow orientados a atores , respectivamente . A comparação é feita por meio de implementações de casos práticos , usando os padrões de controle de dados como base de comparação entre as ferramentas 
 A computação tem sido uma ferramenta de desenvolvimento para outras ciências e os desafios encontrados no desenvolvimento de ferramentas computacionais ajudam a desenvolver a própria computação . Uma das áreas que tem se beneficiado sobremaneira da utilização de sistemas e equipamentos computacionais é a medicina . O diagnóstico baseado em imagens e o rápido surgimento de novas modalidades de exames têm ajudado a medicina a se tornar cada vez mais precisa e eficiente , tanto no diagnóstico como na evolução dos tratamentos . No entanto , as possibilidades até então exploradas nos PACS ( Picture Archiving and Communication Systems ) não têm utilizado eficientemente os recursos empregados na sua implementação . Dois fatores que tornam estes sistemas inadequados são a baixa acessibilidade dos mesmos e a grande subjetividade na associação de palavras-chave às imagens que são empregadas nas buscas subsequentes . Este trabalho propõe a disponibilização de acesso ao sistema PACS pela Internet de modo a aumentar a acessibilidade do mesmo , visto que , um médico que não se encontra no hospital pode consultar os exames de seu paciente , os exames podem ser utilizados em salas de aulas de faculdades associadas a hospitais universitários , pode-se formar juntas médicas virtuais para a análise de um exame , entre outras possibilidades . Este trabalho é suportado por técnicas de recuperação de imagens baseadas em seu conteúdo para a resolução do problema associado à subjetividade , ou seja , utilizam-se extratores de características e estruturas de indexação métricas baseadas nas assinaturas geradas pelos extratores para recuperar as imagens similares 
 Programas para simulação_numérica  de escoamento de fluido são inerentemente aplicações complexas que envolvem múltiplas disciplinas de ciências de computação , matemática e engenharia . Normalmente , além do simulador , faz parte do ambiente de simulação os modeladores ( pré-processamento ) e o visualizadores ( pós-processamento ) . O Freeflow3D é um desses ambientes , desenvolvido pelo laboratório LCAD ( Laboratório de Computação de Alto Desempenho ) do ICMC ( Instituto de Ciências_Matemáticas  e de Computação ) da Universidade de São Paulo-USP . O simulador é bastante ativo com vários pesquisadores desenvolvendo ao mesmo tempo . Contudo , o visualizador e o modelador são praticamente os mesmos desde a criação do programa anos atrás , principalmente porque a estrutura desses programas dificultou suas evoluções e melhorias ao longo do tempo . Este trabalho propõe uma análise do modelador e visualizador do Freeflow3D a fim de executar uma reengenharia no software , propondo uma nova estrutura que promova uma mudança em sua dinâmica de funcionamento , modernizando-o , deixando-o principalmente mais flexível em relação a futuras evoluções e manutenções como também adicionando novas_funcionalidades  que promovam uma melhora na interface com o 
 Apresentamos algumas aplicações de ferramentas estatísticas que são comumente utilizadas na melhoria da qualidade de processos industriais . Inicialmente , desenvolveu-se procedimentos para testar a competência de laboratórios que participam de programas de ensaios de proficiência . Em situações onde os laboratórios medem várias vezes no mesmo ponto , utilizou-se o modelo de erros de medição , proposto por Jaech [ 39 ] ( 1985 ) . Além_disso  , a inferência sobre os parâmetros de tendência aditiva foi generalizada para a classe de distribuições elípticas . A competência dos laboratórios é avaliada pelo teste da razão de verossimilhança generalizada , do qual , obtemos a distribuição exata para a estatística proposta . Em situações onde os laboratórios medem várias vezes em vários pontos e a variável em análise apresenta variações naturais , utilizou-se o modelo com erro nas variáveis . Diante disso , vamos estender o modelo estrutural definido em Barnett [ 13 ] ( 1969 ) para o modelo ultra-estrutural com réplicas . Neste caso , vamos avaliar não somente a tendência aditiva , mas também , a tendência multiplicativa , ou seja , avaliar a linearidade das medições . As estimativas dos parâmetros foram obtidas via procedimento do algorítmo EM , com isso , desenvolvemos os teste de Wald , razão de verossimilhança e escore para avaliar a competência dos laboratórios . Nos dois modelos propostos , generalizamos o erro normalizado ( En ) sugerido pelo Guia 43 [ 37 ] para testar a competência dos laboratórios participantes de programas de ensaio de proficiência . Apresentamos também , um procedimento para calcular índices de performance para processos univariados e multivariados . Nestes casos , consideramos que a distribuição dos dados segue uma distribuição Normal assimétrica . Além_disso  , apresentamos uma análise de simulação onde concluímos que a presença de assimetria nos dados pode causar interpretações erradas sobre o processo , quando a distribuição assumida para os dados é a 
 As decisões operacionais de produção em uma indústria de fiação são planejadas na prática determinando soluções dos sub-problemas de dimensionamento e sequenciamento de lotes e da mistura de fardos de algodão . As tarefas são : definir o tamanho , a sequência , o tempo e alocação de cada lote de produção e quais fardos de algodão devem ser utilizados na produção . Por si só , os sub-problemas representam grandes desafios no planejamento da produção . Entretanto , para melhor representar o ambiente produtivo e alcançar custos de produção mais baixos , indústrias de processo , como as de fiação , procuram integrar mais e mais seus sub-problemas de planejamento . O objetivo dessa tese é apresentar modelos matemáticos e métodos de solução para auxiliar a tomada de decisão no nível operacional do planejamento da produção . Três formulações_matemáticas  para o dimensionamento e sequenciamento de lotes em um sistema de dois estágios com produção sincronizada são propostas . Um novo método baseado em programação matemática e metaheurísticas e também desenvolvida para a solucão desse sub-problema . Além_disso  , a integração das decisões relativas a matéria-prima ( fardos de algodão ) ao dimensionamento e sequenciamento de lotes é analisada . As novas formulações propostas representam de forma mais realista o problema de dimensionamento e sequenciamento de lotes da indústria de fiação e de indústrias de processo com ambiente produtivo similares . O método de solução encontra boas soluções para o problema e supera outros méodos similares presentes em softwares comerciais . Além_disso  , o método é geral o suficiente para a solução de outros problemas de otimização . O problema integrado de dimensionamento e sequenciamento de lotes e mistura comprovou que restrições relativas à qualidade dos fios influenciam os custos e viabilidade do planejamento da produção . O planejamento integrado dessas óperações trata o sistema considerando restrições que se relacionam , definindo planos de produção mais 
 Em interações humano-robô ainda existem muitas limitações a serem superadas referentes à provisão de uma comunicação natural quanto aos sentidos humanos . A capacidade de interagir com os seres_humanos  de maneira natural em contextos sociais ( pelo uso da fala , gestos , expressões faciais , movimentos do corpo ) é um ponto fundamental para garantir a aceitação de robôs em uma sociedade de pessoas não especialistas em manipulação de engenhos robóticos . Outrossim , a maioria dos robôs existentes possui habilidades limitadas de percepção , cognição e comportamento em comparação com seres_humanos  . Nesse contexto , este projeto de pesquisa investigou o potencial da arquitetura robótica do humanoide NAO , no tocante à capacidade de realizar interações com seres_humanos  através de imitação de movimentos do corpo de uma pessoa e pelo controle do robô . Quanto a sensores , foi utilizado um sensor câmera não-intrusivo de profundidade incorporado no dispositivo Kinect . Quanto às técnicas , alguns conceitos matemáticos foram abordados para abstração das configurações espaciais de algumas junções/membros do corpo humano essas configurações foram capturadas por meio da utilização da biblioteca OpenNI . Os experimentos_realizados  versaram sobre a imitação e o controle do robô por meio da avaliação de vários usuários . Os resultados desses experimentos revelaram um desempenho satisfatório quanto ao sistema 
 As imagens de angiografia por ressonância_magnética  ( angio-RM ) ou por tomografia computadorizada ( angio-TC ) permitem uma análise minuciosa das redes vasculares . A segmentação de redes vasculares a partir de tais imagens é uma das tarefas iniciais no diagnóstico de doenças vasculares como estenoses ou aneurismas . Porém , a grande diversidade de arquiteturas dos vasos dificulta a validação dos algoritmos de segmentação . Assim , a construção de redes vasculares sintéticas realistas permitem validar novas metodologias de segmentação de vasos . Este trabalho descreve uma metodologia de geração de redes vasculares sintéticas em três dimensões utilizando sistemas de Lindenmayer ( L-systems ) estocásticos . Para atingir esse objetivo , foram implementados um analisador léxico , um analisador sintático e um gerador de L-systems para a criação de vasos sintéticos baseado em gramáticas . A parametrização destas gramáticas possibilita a simulação de características naturais de vasos reais como o ângulo de bifurcação , comprimento , diâmetro médio e possibilita a simulação de anomalias vasculares . As expressões resultantes são utilizadas para criar imagens angiográficas sintéticas que simulam a distribuição de intensidades dos vasos em imagens angio-RM e angio-TC reais . As redes vasculares sintéticas podem também ser delimitadas por superfícies 3D arbitrárias de forma similar à geometria de órgãos . A flexibilidade de parametrização e natureza estocástica desta metodologia faz com que ela se torne uma ferramenta ideal para a validação de algoritmos de segmentação de vasos em imagens angiográficas 
 Um ambiente de data warehousing oferece suporte ao processo de tomada de decisão . Ele consolida dados de fontes de informação distribuições , autônomas e heterogêneas em um único componente , o data warehouse , e realiza o processamento eficiente de consultas analíticas , denominadas OLAP ( on-line analytical processing ) . Um data warehouse convencional armazena apenas dados alfanuméricos . Por outro_lado  , um data warehouse de imagens armazena , além desses dados convencionais , características intrínsecas de imagens , permitindo a realização de consultas analíticas estendidas com predicados de similaridade entre imagens . Esses ambientes demandam , portanto , a criação de estratégias que possibilitem o processamento eficiente dessas consultas complexas e custosas . Apesar de haver na literatura trabalhos voltados a índices bitmap para ambientes de data warehousing e métodos de acesso métricos para melhorar o desempenho de consultas por similaridade entre imagens , no melhor do nosso conhecimento , não há uma técnica que investigue essas duas questões em um mesmo contexto . Esta dissertação visa preencher essa lacuna na literatura por meio das seguintes contribuições : ( i ) proposta do ImageDWindex , um mecanismo para a otimização de consultas analíticas estendidas com predicados de similaridade entre imagens ; e ( ii ) definição de diferentes estratégias de processamento de consultas sobre data warehouses de imagens usando o ImageDW-index . Para validar as soluções propostas , foram desenvolvidas duas outras contribuições secundárias , que são : ( iii ) o ImageDW-Gen , um gerador de dados com o objetivo de povoar o data warehouse de imagens ; e ( iv ) a proposta de quatro classes de consulta , as quais enfocam em diferentes custos de processamento dos predicados de similaridade entre imagens . Utilizando o ImageDW-Gen , foram realizados testes de desempenho para investigar as vantagens introduzidas pelas estratégias propostas , de acordo com as classes de consultas definidas . Comparado com o trabalho mais correlato existente na literatura , o uso do ImageDWindex proveu uma melhora no desempenho do processamento de consultas IOLAP que variou em média de 55,57 % até 82,16 % , considerando uma das estratégias propostas 
 Projetos de software_livre  baseados em comunidade são , geralmente , auto-organizados e dinâmicos , e recebem contribuições de voluntários espalhados por todo o globo . A sobrevivência dessas comunidades , o sucesso a longo prazo , e sua continuidade exigem a entrada constante de novatos . No entanto , os novatos enfrentam muitas barreiras ao tentarem fazer a sua primeira contribuição para um projeto de software_livre  , o que leva , em muitos_casos  , a desistências . Portanto , um grande desafio para projetos de software_livre  é proporcionar maneiras de apoiar os novatos durante a realização de sua primeira contribuição . Nesta tese , nosso objetivo foi identificar e compreender as barreiras que os novatos enfrentam e fornecer estratégias adequadas para reduzir essas barreiras . Para isso , realizamos diversos estudos , utilizando vários métodos de pesquisa . Para identificar as barreiras , foram utilizados dados coletados a partir de : entrevistas semiestruturadas com 36 desenvolvedores de 14 projetos diferentes ; 24 respostas a um questionário aberto realizado com desenvolvedores de software_livre  ; feedback de 9 estudantes depois de tentarem contribuir para projetos de software_livre  ; e 20 estudos obtidos por meio de uma revisão_sistemática  da literatura . Os dados foram analisados utilizando-se procedimentos da Grounded Theory : codificação aberta e axial . Posteriormente , a análise resultou em um modelo_conceitual  preliminar composto por 58 barreiras agrupadas em seis categorias : diferenças culturais , características dos novatos , problemas de recepção , orientação dos novatos , obstáculos técnicos , e problemas de documentação . Com base no modelo_conceitual  , desenvolvemos o FLOSScoach , um portal para apoiar os novatos no curso de sua primeira contribuição a projetos de software_livre  . Para avaliar o portal , realizamos um estudo com alunos de graduação , utilizando dados qualitativos de diários , questionários de autoeficácia ( self-efficacy ) , e o modelo de aceitação tecnológica ( TAM , do inglês Technology Acceptance Model ) . Ao aplicar o modelo em uma aplicação prática e experimentá-lo , pudemos avaliar e melhorar o modelo de barreiras , adequando-o de acordo com as melhorias identificadas durante a concepção da ferramenta , bem como sugestões recebidas dos participantes do estudo . Os resultados do estudo indicam que o portal FLOSScoach desempenhou um papel_importante  no direcionamento dos novatos e na redução das barreiras relacionadas com o processo de orientação e contribuição , enquanto não foi muito eficiente na redução de barreiras técnicas . Evidenciamos ainda que o portal é útil , fácil de usar , e aumentou a confiança dos novatos . As principais_contribuições  desta tese são : ( i ) identificação empírica e modelagem de barreiras enfrentadas pelos novatos em projetos de software_livre  ; e ( ii ) um portal de informação para apoiar os novatos em projetos de software_livre  
 Melhoria de processo de software ( SPI ) é uma prática de engenharia de software motivada pela necessidade de aumentar a qualidade e a produtividade no desenvolvimento de software . Um fato amplamente reconhecido é que a qualidade do produto de software pode ser , em grande parte , determinada pela qualidade do processo utilizado para desenvolvê-lo e mantê-lo . A avaliação do processo de software ajuda as organizações de software a amadurecerem seus processos , identificando problemas críticos para estabelecer prioridades de melhoria . Essa avaliação pode ser feita por meio da comparação do estado dos processos da organização em relação a um modelo de referência que estabeleça estágios de melhoria . Uma avaliação geralmente se baseia em um modelo de processo de software que fornece um roteiro para melhorias . Este trabalho tem como objetivo estabelecer uma abordagem que : ( i ) define um conjunto de modelos de abstração ( metamodelos ) de modelos de maturidade de processo de software para apoiar uma metodologia de avaliação/melhoria de processo de software com o objetivo de certificação ; e ( ii ) permite avaliar os processos de uma organização em comparação com um modelo de maturidade por meio de transformações desses metamodelos . A abordagem é instanciada por meio de um estudo de caso utilizando os modelos MPS.Br e CMMI para exemplificar sua aplicação . Como resultado , é apresentado um comparativo entre as limitações encontradas nas metodologias encontradas na literatura e como a abordagem sugere superá-las 
 O problema de balanceamento de linhas de produção e designação de trabalhadores é uma extensão do problema simples de balanceamento de linhas onde os tempos de execução de tarefas são dependentes dos trabalhadores . Este problema tem sua motivação prática oriunda de linhas de produção com trabalhadores com deficiência . Nesta tese de doutorado estudamos duas extensões para o problema : a primeira layouts de linhas com estações em paralelo , enquanto que a segunda possibilita o uso de múltiplas linhas . As extensões fora aplicadas tanto ao problema básico quanto para o problema de rotação de tarefas . Apresentamos formulações_matemáticas  e métodos exatos e heurísticos para todos os casos . Teste computacionais em instâncias da literatura e novas instâncias e uma análise detalhada dos resultados são apresentados 
 Aplicações industriais envolvendo escoamentos multifásicos são inúmeras , sendo que , o aprimoramento de alguns desses processos pode resultar em um grande salto tecnológico com significativo impacto econômico . O estudo numérico dessas aplicações é imprescindível , pois fornece informações precisas e mais detalhadas do que a realização de testes experimentais . Um grande desafio é o estudo numérico de escoamentos viscoelásticos multifásicos envolvendo altas taxa de elasticidade , devido às instabilidades causadas por altas tensões elásticas , grandes deformações , e até mudanças topológicas na interface . Assim , a investigação numérica desse tipo de problema exige uma formulação precisa e robusta . No presente_trabalho  , um novo resolvedor de escoamentos bifásicos envolvendo fluidos complexos é apresentado , com particular interesse em escoamentos com altas taxas de elasticidade . A formulação proposta é baseada no método Volume-of-fluid ( VOF ) para representação da interface e no algoritmo Continuum Surface Force ( CSF ) para o balanço de forças na interface . A curvatura e advecção da interface são calculados via métodos geométricos para garantir a precisão dos resultados . Métodos de estabilização são utilizados quando números críticos de Weissenberg ( Wi ) são encontrados , devido ao famoso problema do alto número de Weissenberg ( HWNP ) . O método da projeção , combinado com um método implícito para solução da equação da quantidade de movimento , são discretizados por um esquema de diferenças_finitas  em uma malha_deslocada  . Problemas de benchmarks foram resolvidos para acessar a precisão numérica da formulação em diferentes níveis de complexidade física , tal como representação e advecção da interface , influência das forças interfaciais , e características reológicas do fluido . A fim de demonstrar a capacidade do novo resolvedor , dois problemas bifásicos transientes , envolvendo fluidos viscoelásticos , foram resolvidos : o efeito de Weissenberg e o reômetro extensional ( CaBER ) . O efeito de Weissenberg ou rod-climbing effect consiste em um bastão que gira dentro de um recipiente com fluido viscoelástico e , devido às forças elásticas , o fluido escala o bastão . Os resultados foram comparados com dados teóricos , numéricos e experimentais , encontrados na literatura para pequenas velocidades angulares . Além_disso  , resultados obtidos com altas velocidades angulares ( alta elasticidade ) são apresentados com o modelo Oldroyd-B , em que escaladas muito elevadas foram observadas . Valores críticos da velocidade angular foram identificados , e para valores acima foi observada a ocorrência de instabilidades elásticas , originadas pela combinação de tensões elásticas , curvatura interfacial , e escoamentos secundários . Até onde sabemos , numericamente , essas instabilidades nunca foram capturadas antes . O CaBER consiste no comportamento e colapso de um filamento de fluido viscoelástico , formado entre duas placas paralelas devido às forças capilares . Esse experimento envolve consideráveis dificuldades , dentre as quais podemos destacar a grande influência das forças capilares e a diferença de escalas de comprimento no escoamento . Em grande parte dos resultados encontrados na literatura , o CaBER é resolvido por modelos simplificados em uma dimensão . Resultados obtidos foram comparados com tais resultados da literatura e com soluções teóricas , apresentando admirável precisão 
 Na análise de experimentos com misturas é comum ter , simultaneamente , sob investigação , muitas variáveis-respostas ( características fisicas , químicas e/ou econômicas , entre outras ) do produto_final  . A determinação dos valores de um conjunto de variáveis controláveis ( proporções de misturas e/ou variáveis de processo ) que otimizem uma função multi-resposta é de grande interesse . Neste trabalho mostramos uma estratégia de solução que consiste em escolher uma única característica como função objetivo e estabelecer faixas de tolerância para as demais . Assim , o método da função Lagrangeana aumentada pode ser aplicado . A otimização do problema resultante é proposta utilizando o método dos simplexos de Nelder e Mead 
 Os sistemas computacionais_distribuídos  aplicados à computação_paralela  permitem uma melhor relação custo/benefício para a computação_paralela  . Esses sistemas oferecem a potência computacional adequada às aplicações que não necessitam de uma máquina maciçamente paralela , porém necessitam de uma potência computacional maior que uma máquina seqüencial pode oferecer . PVM ( Parallel Virtual Machine ) é um exemplo de ambiente de passagem de mensagens , amplamente discutido na literatura , que permite a criação de máquinas_paralelas  virtuais em estações de trabalho , normalmente máquinas RISC , e com o sistema_operacional  UNIX . Dentro desse contexto , este trabalho descreve detalhadamente a implementação do PVM-W95 ( PVM para Windows95 ) , um ambiente de passagem de mensagens que permite a criação de uma máquina_paralela  virtual , utilizando computadores_pessoais  conectados por uma rede de comunicação e com o sistema_operacional  Windows95 , de modo análogo ao PVM . Foram realizados estudos preliminares visando a validação e a avaliação do desempenho do PVM-W95 . Os resultados obtidos demonstram que o ambiente tem comportamento estável e que as aplicações paralelas desenvolvidas apresentam um excelente speedup , considerando-se o hardware disponível 
 Desde o trabalho de Karmarkar em 1984 , intensas pesquisas têm sido feitas sobre métodos de pontos_interiores  . Nesta dissertação estudamos a bibliografia relacionada a métodos de pontos_interiores  para a programação linear com ênfase em alguns algoritmos básicos : métodos de escala-afim primal e dual e método ( path-following ) primal-dual . Por último mostramos como estender algoritmos primais-duais a uma classe de programação linear por partes 
 Esta dissertação apresenta um estudo sobre problemas de planejamento da produção . Na primeira parte são revisados dois problemas particulares de programação da produção : o problema de programar máquinas numa industria têxtil e o de uma fundição automattzada . Na parte final estuda-se os problemas gerais de dimensionamento de lotes apresentando formulações_matemáticas  encontradas na literatura g um novo modelo é apresentado para o qual , foi proposto um método de resolução baseado na técnica da relaxação Lagrangeana 
 Um estudo empírico visando avaliar a eficácia em revelar erros , a dificuldade de satisfação e o custo de aplicação do critério_Análise  de Mutantes é apresentado neste trabalho . Eficácia e custo também foram avaliados para os critérios Potenciais-Usos , comparando-se assim os resultados obtidos para esses critérios . A especificação e implementação de uma estratégia de minimização de conjuntos de casos de teste adequados ao critério_Análise  de Mutantes também é apresentada . Através dos resultados obtidos observou-se que os critérios Potenciais-Usos ( baseado em fluxo dados ) e o critério_Análise  de Mutantes ( baseado em erros ) são promissores e apresentam características complementares que merecem ser investigadas em um experimento de maior porte . A utilização de mutação restrita e de minimização de conjunto de casos de teste constituem mecanismos que viabilizam a aplicação desses critérios em ambientes de produção de software 
 Consultas por similaridade constituem um paradigma de busca que fornece suporte à diversas tarefas computacionais , tais como agrupamento , classificação e recuperação de informação . Neste contexto , medir a similaridade entre objetos requer comparar a distância entre eles , o que pode ser formalmente modelado pela teoria de espaços_métricos  . Recentemente , um grande esforço de pesquisa tem sido dedicado à inclusão de consultas por similaridade em Sistemas Gerenciadores de Bases de Dados ( SGBDs ) , com o objetivo de ( i ) permitir a combinação de comparações por similaridade com as comparações por identidade e ordem já existentes em SGBDs e ( ii ) obter escalabilidade para grandes bases de dados . Nesta tese , procuramos dar um próximo passo ao estendermos também o otimizador de consultas de um SGBD . Em particular , propomos a ampliação de dois módulos do otimizador : o módulo de Espaço de Distribuição de Dados e o módulo de Modelo de Custo . Ainda que o módulo de Espaço de Distribuição de Dados permita representar os dados armazenados , essas representações são insuficientes para modelar o comportamento das comparações em espaços_métricos  , sendo necessário estender este módulo para contemplar distribuições de distância . De forma semelhante , o módulo Modelo de Custo precisa ser ampliado para dar suporte à modelos de custo que utilizem estimativas sobre distribuições de distância . Toda a investigação aqui conduzida se concentra em cinco contribuições . Primeiro , foi criada uma nova sinopse para distribuições de distância , o Histograma Compactado de Distância ( CDH ) , de onde é possível inferir valores de seletividade e raios para consultas por similaridade . Uma comparação experimental permitiu mostrar os ganhos das estimativas da sinopse CDH com relação à diversos competidores . Também foi proposto um modelo de custo baseado na sinopse CDH , o modelo Stockpile , cujas estimativas se mostraram mais precisas na comparação com outros modelos . Os Histogramas-Omni são apresentados como a terceira contribuição desta tese . Estas estruturas de indexação , construídas a partir de restrições de particionamento de histogramas , permitem a execução otimizada de consultas que mesclam comparações por similaridade , identidade e ordem . A quarta contribuição de nossa investigação se refere ao modelo RVRM , que é capaz de indicar quanto é possível empregar as estimativas das sinopses de distância para otimizar consultas por similaridade em conjuntos de dados de alta dimensionalidade . O modelo RVRM se mostrou capaz de identificar intervalos de dimensões para os quais essas consultas podem ser executadas eficientes . Finalmente , a última contribuição desta tese propõe a integração das sinopses e modelos revisados em um sistema com sintaxe de alto nível que pode ser acoplado em um otimizador de consultas 
 A disponibilidade de conteúdo gerado por usuários em sites colaborativos de perguntas e respostas tem impulsionado o avanço de modelos de Question Answering ( QA ) baseados em reúso . Essa abordagem pode ser implementada por meio da tarefa de seleção de respostas ( Answer Selection , AS ) , que consiste em encontrar a melhor resposta para uma dada pergunta em um conjunto pré-selecionado de respostas candidatas . Nos últimos_anos  , abordagens baseadas em vetores distribucionais e em redes neurais profundas , em particular em redes neurais convolutivas ( CNNs ) , têm apresentado bons_resultados  na tarefa de AS . Contudo , a maioria dos modelos é avaliada sobre córpus de perguntas objetivas e bem formadas , contendo poucas palavras . Raramente estruturas textuais complexas são consideradas . Perguntas de consumidores , comuns em sites colaborativos , podem ser bastante complexas . Em geral , são representadas por múltiplas frases inter-relacionadas , que apresentam pouca objetividade , vocabulário leigo e , frequentemente , contêm informações em excesso . Essas características aumentam a dificuldade da tarefa de AS . Neste trabalho , propomos um modelo de seleção de respostas para perguntas de consumidores . São contribuições deste trabalho : ( i ) uma definição para o objeto de pesquisa perguntas de consumidores ; ( ii ) um novo dataset desse tipo de pergunta , chamado MilkQA ; e ( iii ) um modelo de seleção de respostas , chamado SlimRank . O MilkQA foi criado a partir de um arquivo de perguntas e respostas coletadas pelo serviço de atendimento de uma renomada instituição pública de pesquisa agropecuária ( Embrapa ) . Anotadores guiados pela definição de perguntas de consumidores proposta neste trabalho selecionaram 2,6 mil pares de perguntas e respostas contidas nesse arquivo . A análise dessas perguntas levou ao desenvolvimento do modelo SlimRank , que combina representação de textos na forma de grafos semânticos com arquiteturas de CNNs . O SlimRank foi avaliado no dataset MilkQA e comparado com baselines e dois modelos do estado da arte . Os resultados alcançados pelo SlimRank foram bastante superiores aos resultados dos baselines , e compatíveis com resultados de modelos do estado da arte ; porém , com uma significativa redução do tempo computacional . Acreditamos que a representação de textos na forma de grafos semânticos combinada com CNNs seja uma abordagem promissora para o tratamento dos desafios impostos pelas características singulares das perguntas de consumidores 
 Este trabalho apresenta uma arquitetura de hardware , baseada em FPGA ( Field-Programmable Gate Array ) e com multi-câmeras , para o problema de localização e mapeamento simultâneos - SLAM ( Simultaneous Localization And Mapping ) aplicada a sistemas robóticos embarcados . A arquitetura é composta por módulos de hardware altamente especializados para a localização do robô e para geração do mapa do ambiente de navegação em tempo real com features extraídas de imagens obtidas diretamente de câmeras CMOS a uma velocidade de 30 frames por segundo . O sistema é totalmente embarcado em FPGA e apresenta desempenho_superior  em , pelo menos , uma ordem de magnitude em relaçãoo às implementações em software processadas por computadores_pessoais  de última geração . Esse desempenho deve-se à exploração do paralelismo em hardware junto com o processamento em pipeline e às otimizações realizadas nos algoritmos . As principais_contribuições  deste trabalho são as arquiteturas para o filtro de Kalman estendido - EKF ( Extended Kalman Filter ) e para a detecção de features baseada no algoritmo SIFT ( Scale Invariant Feature Transform ) . A complexidade para a implementaçãoo deste trabalho pode ser considerada alta , uma vez que envolve uma grande quantidade de operações aritméticas e trigonométricas em ponto utuante e ponto fixo , um intenso processamento de imagens para extração de features e verificação de sua estabilidade e o desenvolvimento de um sistema de aquisição de imagens para quatro câmeras CMOS em tempo real . Adicionalmente , foram criadas interfaces de comunicação para o software e o hardware embarcados no FPGA e para o controle e leitura dos sensores do robô_móvel  . Além dos detalhes e resultados da implementação , neste trabalho são apresentados os conceitos básicos de mapeamento e o estado da arte dos algoritmos SLAM com visão monocular e 
 O advento da tecnologia da Internet , juntamente com a World_Wide  Web , popularizaram e permitiram uma grande expansão e demanda por aplicações web . Por executarem em um ambiete heterogêneo e complexo , as aplicações web apresentam várias características que as diferenciam dos sistemas tradicionais . As particularidades dessas aplicações tornam o fator qualidade essencial para o sucesso dessas aplicações . Alguns requisitos de qualidade , como usabilidade , confiabilidade , interoperabilidade e segurança devem , então , ser validados . Para assegurar a qualidade desejada , são necessárias executar as atividades de Verificação , Validação e Teste ( VV & T ) . Dentre elas , as mais utilizadas são as atividades de teste . Os critérios , estratégias e ferramentas de teste precisam ser identificados e avaliados para se estabelecer uma relação entre os custos e benefícios entre elas , a fim de guiar a escolha de cada uma durante os testes de aplicações web . A realização de estudos_experimentais  para realizar essa análise beneficia tanto a academia , nas atividades de ensino e pesquisa , como a indústria , nas atividades de seleção e aplicação de critérios , estratégias e ferramentas de teste de aplicações web . Para que os resultados obtidos estejam acessíveis , é proposto o desenvolvimento de um portal de conhecimento que disponibilize de maneira sistemática o conhecimento obtido sobre critérios , estratégias e ferramentas de teste de aplicações web . Esse portal tem como objetivo oferecer às organizações um ambiente compartilhado de conhecimento , a fim de proporcionar a existência de um ciclo de criação , troca , retenção e reuso do 
 Os processos de gestão de requisitos têm influência direta na concepção do produto_final  e estão diretamente relacionados com a satisfação do cliente , pois é neles que se define o que o cliente espera do software . Seus produtos servem de base para os processos executados posteriormente e , portanto , a probabilidade de ocorrer falhas é maior caso haja falhas durante a elaboração dos requisitos do software . No entanto , tem-se observado que esses processos são uma das maiores fontes de problemas encontrados no desenvolvimento de software . Com o intuito de sistematizar os processos de desenvolvimento de software , a fim de se evitar prejuízos para as organizações desenvolvedoras e insatisfação para os adquirentes dos produtos desenvolvidos , surgiram os modelos para a melhoria de processo de desenvolvimento de software , tais como o Capability Matutity Model Integration - Development ( CMMI-Dev ) . Esses modelos atuam como guias para a melhoria contínua dos processos de desenvolvimento das organizações . Entretanto , o nível de abstração dos modelos nem sempre é suficientemente específico para orientar colaboradores de organizações não familiarizados com o corpo de conhecimento da engenharia de software . Outro aspecto que dificulta a utilização de tais modelos é o financeiro , pois a implantação de tais melhorias apresenta alto custo , podendo ser inviável para organizações de pequeno e médio porte . Este trabalho apresenta um guia , denominado PROREQ , cujo_objetivo  é facilitar a implantação de melhorias nos processos de requisitos de pequenas organizações . É composto por um conjunto de boas práticas classificadas segundo a estrutura de organização das áreas de processo Desenvolvimento e Gerenciamento de requisitos do CMMI-Dev ; uma estratégia de implantação , baseada na estratégia da norma ISO/IEC 15504 e em um conjunto de práticas retiradas de trabalhos empíricos relacionados à melhoria de processos de software ; e um modelo de avaliação , baseado na norma ISO/IEC 15504 e no método de avaliação do modelo de melhoria de processo de software brasileiro ( MPS.BR ) . Ao final é descrito um estudo de caso que apresenta os resultados da aplicação do guia PROREQ em uma pequena organização desenvolvedora de 
 A função de Ehrhart L_P ( t ) de um politopo P é definida como sendo o número de pontos com coordenadas inteiras no politopo dilatado tP . A teoria de Ehrhart clássica lida principalmente com valores inteiros de t ; esta dissertação de mestrado foca em como a função de Ehrhart se comporta quando permitimos que o parâmetro t seja um número real arbitrário . São três os resultados principais desta dissertação a respeito deste comportamento . Alguns politopos racionais ( como o cubo unitário [ 0 , 1 ] ^d ) apenas ganham pontos inteiros quando o parâmetro de dilatação t é um inteiro , de tal forma que computar L_P ( t ) devolve a mesma contagem de pontos que L_P ( t ) . Eles são chamados de politopos semi-reflexivos . O primeiro resultado desta dissertação é uma caracterização destes politopos em termos de suas descrições como interseção de semi-espaços . O segundo resultado é relacionado ao teorema de Ehrhart . No contexto clássico , o teorema de Ehrhart afirma que L_P ( t ) será um quasi-polinômio sempre que P for um politopo racional . Sabe-se que este teorema generaliza para parâmetros reais de dilatação ; nesta dissertação é apresentada uma nova demonstração deste fato , baseada na caracterização mencionada acima . O terceiro resultado é sobre como a função real de Ehrhart se comporta com respeito à translação neste novo contexto . Sabe-se que a função de Ehrhart clássica é invariante sob translações por vetores com coordenadas inteiras . Por outro_lado  , a função real de Ehrhart está bem longe de ser invariante : não só existem infinitas funções L_ { P + w } ( t ) distintas , mas também , sob certas condições , esta coleção de funções identifica P unicamente 
 O presente_trabalho  consiste em uma extensão do sistema FreeFlow-2D para simular_escoamentos  de fluidos não newtonianos bidimensionais com superfí cies livres , onde o fluido é descrito pelos modelos de Cross ou o modelo `` power-law '' . O método_numérico  empregado é o método GENSMAC . As equações governantes são aproximadas pelo método de diferenças_finitas  em uma malha_deslocada  e partículas marcadoras são utilizadas para a visualização do escoamento e localização da superfície_livre  . Resultados_numéricos  são apresentados . Em particular , a presente implementação é validada comparando-se a solução_numérica  com uma solução 
 Esta dissertação apresenta um novo método de segmentação de movimento baseado na obtenção dos contornos e em filtros morfológicos . A nova_técnica  apresenta vantagens em relação ao número de falsos positivos e falsos negativos em situações específicas quando comparada às técnicas tradicionais 
 Desenvolvimento Guiado por Testes ( TDD ) e uma das praticas sugeridas na Programacao Extrema . A mecanica da pratica e simples : o programador escreve o teste antes de escrever o codigo . E , portanto , possivel inferir que a pratica de TDD e uma pratica de testes de software . Entretanto , muitos autores de livros conhecidos pela industria e academia afirmam que os efeitos da pratica vao alem . Segundo eles , TDD ajuda o desenvolvedor durante o processo de criacao do projeto classes , fazendo-os criar classes menos acopladas e mais coesas . Entretanto , grande parte dos trabalhos da literatura sao voltados a descobrir se a pratica faz diferenca na qualidade do codigo gerado , mas poucos sao os autores que discutem como a pratica realmente auxilia . Mesmo os proprios praticantes nao entendem ou conseguem expressar bem como a pratica os guia . Este trabalho tem por objetivo compreender melhor os efeitos de TDD e como sua pratica influencia o desenvolvedor durante o processo de projeto de sistemas orientados a objetos . Para entende-las , neste trabalho optamos por um estudo exploratorio essencialmente qualitativo , no qual participantes foram convidados a resolver exercicios pre-preparados utilizando TDD e , a partir dos dados colhidos nessa primeira parte , nos levantamos detalhes sobre como a pratica influenciou as decisoes de projeto de classes dos participantes por meio de entrevistas . Ao final , observamos que a pratica de TDD pode guiar o desenvolvedor durante o processo de criacao do projeto de classes por meio de constantes feedbacks sobre a qualidade do projeto . Esses feedbacks alertam desenvolvedores sobre possiveis problemas , como alto acoplamento ou baixa coesao . Os desenvolvedores , por sua vez , devem interpretar e melhorar o projeto de classes . Este trabalho catalogou e nomeou os padroes de feedback percebidos pelos participantes 
 A proposta deste projeto de doutorado envolve a pesquisa sobre computação autônoma , focando na elaboração de mecanismos de autoconfiguração e auto-otimização para arquiteturas virtualizadas que buscam garantir a provisão de qualidade de serviço . Esses mecanismos fazem_uso  de elementos autônomos que são auxiliados por uma ontologia . Para isso , instrumentos de Web Semântica são utilizados para que a ontologia represente uma base de conhecimento com as informações dos recursos_computacionais  . Tais informações são utilizadas por algoritmos de otimização que , baseados em regras pré-definidas pelo administrador , tomam a decisão por uma nova configuração do sistema que vise a otimizar o desempenho . A configuração e a otimização geralmente envolvem elementos de software que precisam ser gerenciados pelos profissionais em Tecnologia da Informação ( TI ) . Parte desse gerenciamento é composto de tarefas corriqueiras , por exemplo , monitorações , reconfigurações e verificações de desempenho . Tais tarefas demandam tempo e , portanto , geram custos e desgastes para os profissionais . Dessa forma , este projeto visa automatizar algumas dessas tarefas corriqueiras , facilitando o trabalho dos profissionais de TI e permitindo que eles foquem em tarefas mais críticas . Portanto , para alcançar esse objetivo foi realizado um estudo e a criação de mecanismos distribuídos baseados em Computação Autônoma e Web Semântica que permitem a configuração e otimização de recursos de forma automática . Os resultados individuais de cada mecanismo indicam que é possível alcançar um nível satisfatório de auto-configuração e auto-otimização para arquiteturas virtualizadas . O mecanismo de auto-configuração obteve melhores_resultados  com a abordagem de monitoração de recursos ao invés de utilizar previsões , já o mecanismo de auto-otimização provou que sua metodologia e algoritmo são aplicáveis na busca de uma configuração otimizada para atender ao SLA acordado 
 O presente_trabalho  apresenta a implementação de um core de rede Ethernet 10/100Mbps com interface para o barramento Avalon para utilização em conjunto com o processador Nios_II  da Altera . A tecnologia Ethernet foi implementada em computação reconfigurável e utilizou-se como base um módulo disponível na Internet denominado OpenCores MAC 10/100 . O projeto foi desenvolvido para ser aplicado em sistemas embarcados , mais especificamente para o uso em um robô_móvel  em desenvolvimento no Laboratório de Computação Reconfigurável do ICMC/USP . O core foi incorporado à biblioteca da ferramenta SoPC Builder da Altera , visando uma fácil integração do mesmo em outros projetos . Foram utilizadas as ferramentas Quartus II e ModelSim para o desenvolvimento e testes do sistema , além de dois kits Nios versão Stratix para a validação do projeto , sendo as placas interligadas ponto-a-ponto sem a utilizaçao de transceivers analógicos 
 Aplicações de captura e acesso ( C & A ) exploram o paradigma de computação ubíqua para dar_apoio  à captura automática de informação em experiências ao vivo e à correspondente geração de documentos passíveis de armazenamento , recuperação , visualização e extensão ao longo do tempo . Essas aplicações são potencialmente capazes de simplificar o processo de autoria de documentos_multimídia  visto que permitem a combinação e a sincronização automáticas de informação coletada por sistemas computacionais de uso pessoal e coletivo disseminados no ambiente . Aplicações de C & A também tornam possível a incorporação de sistemas de software que utilizem a capacidade computacional instalada para apoiar atividades humanas em andamento , promovendo novas formas de colaboração e de interação entre seus participantes e permitindo o registro dos acontecimentos , a geração de documentos que possibilitem sua reconstituição e o compartilhamento dos artefatos capturados . Este trabalho aborda de maneira conjunta diferentes aspectos envolvidos na captura automatizada de atividades humanas . Primeiramente , estende-se a visão tradicional com a sugestão da captura automatizada não apenas de mídias convencionais , mas do processo interativo do usuário como um todo , de modo a permitir a autoria e a personalização de conteúdo digital interativo e das anotações associadas . Ao mesmo tempo , são tratados aspectos colaborativos com o desenvolvimento e o uso de uma infra-estrutura distribuída de comunicação . Baseada em uma plataforma de computação peer-to-peer , os componentes e os serviços implementados suportam atividades de colaboração e compartilhamento de conteúdo entre os usuários . É feita experimentação das propostas com sua aplicação em cenários reais de ambientes de aprendizado , de computação móvel e de TV digital 
 Este projeto de mestrado tem como objetivo principal construir Web_services  de modo a permitir a avaliação de desempenho de uma arquitetura denominada WSARCH . A arquitetura WSARCH foi proposta de modo a prover uma infra-estrutura de Web_services  considerando aspectos de qualidade de serviço ( QoS ) . Este projeto contribui diretamente com o desenvolvimento desta arquitetura , além de auxiliar na sua validação e na realização de estudos de desempenho de suas funcionalidades . Trabalhos preliminares de pesquisa foram desenvolvidos de forma que , além de auxiliar no desenvolvimento da WSARCH , também contribuíram em pesquisas relacionadas com a área de Web_services  . Destacam-se estudos realizados com anexos em Web_services  ( WS-Attachments ) e estudos com operações de pesquisa e publicação em repositórios UDDI . Por fim , foram realizados estudos de avaliação de desempenho com diferentes_tipos  de aplicação implantados em provedores que compõem a arquitetura 
 Esta pesquisa aborda o tema da Elaboração Textual para um público alvo que tem letramento nos níveis básicos e rudimentar , de acordo com a classificação do Indicador Nacional de Alfabetismo Funcional ( INAF , 2009 ) . A Elaboração Textual é definida como um conjunto de técnicas que acrescentam material redundante em textos , sendo tradicionalmente usadas a adição de definições , sinônimos , antônimos , ou qualquer informação externa com o objetivo de auxiliar na compreensão do texto . O objetivo deste projeto de mestrado foi a proposta de dois métodos originais de elaboração textual : ( 1 ) via definição das entidades mencionadas que aparecem em um texto e ( 2 ) via definições de perguntas elaboradas direcionadas aos verbos das orações de um texto . Para a primeira tarefa , usou-se um sistema de reconhecimento de entidades mencionadas da literatura , o Rembrandt , e definições curtas da enciclopédia Wikipédia , sendo este método incorporado no sistema Web FACILITA EDUCATIVO , uma das ferramentas desenvolvidas no projeto PorSimples . O método foi avaliado de forma preliminar com um pequeno grupo de leitores com baixo_nível  de letramento e a avaliação foi positiva , indicando que este auxílio facilitou a leitura dos usuários da avaliação . O método de geração de perguntas elaboradas aos verbos de uma oração é uma tarefa nova que foi definida , estudada , implementada e avaliada neste mestrado . A avaliação não foi realizada junto ao público alvo e sim com especialistas em processamento de língua natural que avaliaram positivamente o método e indicaram quais erros influenciam negativamente na qualidade das perguntas geradas automaticamente . Existem boas indicações de que os métodos de elaboração desenvolvidos podem ser úteis na melhoria da compreensão da leitura para o público alvo em questão , as pessoas com baixo_nível  de 
 OGrid Anywhere é um middleware de grade computacional ponto-aponto ( P2P ) , capaz de agrupar em uma organização virtual ou federação qualquer equipamento dotado de recursos_computacionais  , inclusive receptores digitais . O objetivo deste projeto de mestrado apresentado nesta monografia é desenvolver e avaliar algoritmos de escalonamento que possibilitem uma distribuição adequada de processos nos elementos da grade computacional proposta pelo Grid Anywhere . Foram realizados_experimentos  utilizando o simulador GridSim , simulando um ambiente definido por esse middleware . Por meio dessa junção entre Grades Computacionais e TV Digital , pretende-se promover a inclusão digital permitindo que recursos_computacionais  sejam compartilhados de maneira a possibilitar que usuários com receptores limitados executem aplicações que demandem mais recursos que aqueles ofertados pelo 
 Este trabalho de pesquisa tem por objetivo o desenvolvimento de um coprojeto de hardware/software para o algoritmo de correlação de imagens visando atingir um ganho de desempenho com relação à implementação totalmente em software . O trabalho apresenta um comparativo entre um conjunto bastante amplo e significativo de configurações diferentes do soft-processor Nios_II  implementadas em FPGA , inclusive com a adição de novas instruções dedicadas . O desenvolvimento do co-projeto foi feito com base em uma modificação do método baseado em profiling adicionando-se um ciclo de desenvolvimento e de otimização de software . A comparação foi feita com relação ao tempo de execução para medir o speedup alcançado durante o desenvolvimento do co-projeto que atingiu um ganho de desempenho significativo . Também analisou-se a influência de estruturas de hardware básicas e dedicadas no tempo de execução final do algoritmo . A análise dos resultados sugere que o método se mostrou eficiente considerando o speedup atingido , porém o tempo total de execução ainda ficou acima do esperado , considerando-se a necessidade de execução e processamento de imagens em tempo real dos sistemas de navegação robótica . No entanto , destaca-se que as limitações de processamento em tempo real estão também ligadas as restrições de desempenho impostas pelo hardware adotado no projeto , baseado em uma FPGA de baixo custo e capacidade 
 O número de documentos textuais disponíveis em formato digital tem aumentado incessantemente . Técnicas de Mineração de Textos são cada vez mais utilizadas para organizar e extrair conhecimento de grandes coleções de documentos textuais . Para o uso dessas técnicas é necessário que os documentos textuais estejam representados em um formato apropriado . A maioria das pesquisas de Mineração de Textos utiliza a abordagem bag-of-words para representar os documentos da coleção . Essa representação usa cada palavra presente na coleção de documentos como possível atributo , ignorando a ordem das palavras , informa ções de pontuação ou estruturais , e é caracterizada pela alta dimensionalidade e por dados esparsos . Por outro_lado  , a maioria dos conceitos são compostos por mais de uma palavra , como Inteligência Articial , Rede_Neural  , e Mineração de Textos . As abordagens que geram atributos compostos por mais de uma palavra apresentam outros problemas além dos apresentados pela representação bag-of-words , como a geração de atributos com pouco signicado e uma dimensionalidade muito maior . Neste projeto de mestrado foi proposta uma abordagem para representar documentos textuais nomeada bag-of-related-words . A abordagem proposta gera atributos compostos por palavras relacionadas com o uso de regras de associação . Com as regras de associação , espera-se identicar relações entre palavras de um documento , além de reduzir a dimensionalidade , pois são consideradas apenas as palavras que ocorrem ou que coocorrem acima de uma determinada frequência para gerar as regras . Diferentes maneiras de mapear o documento em transações para possibilitar a geração de regras de associação são analisadas . Diversas medidas de interesse aplicadas às regras de associação para a extração de atributos mais signicativos e a redução do número de atributos também são analisadas . Para avaliar o quanto a representação bag-of-related-words pode auxiliar na organização e extração de conhecimento de coleções de documentos textuais , e na interpretabilidade dos resultados , foram realizados três grupos de experimentos : 1 ) classicação de documentos textuais para avaliar o quanto os atributos da representação bag-of-related-words são bons para distinguir as categorias dos documentos ; 2 ) agrupamento de documentos textuais para avaliar a qualidade dos grupos obtidos com a bag-of-related-words e consequentemente auxiliar na obtenção da estrutura de uma hierarquia de tópicos ; e 3 ) construção e avaliação de hierarquias de tópicos por especialistas de domínio . Todos os resultados e dimensionalidades foram comparados com a representação bag-of-words . Pelos resultados dos experimentos_realizados  , pode-se vericar que os atributos da representação bag-of-related-words possuem um poder preditivo tão bom quanto os da representação bag-of-words . A qualidade dos agrupamentos de documentos textuais utilizando a representação bag-of-related-words foi tão boa quanto utilizando a representação bag-of-words . Na avaliação de hierarquias de tópicos por especialistas de domínio , a utilização da representação bag-of-related-words apresentou melhores_resultados  em todos os quesitos 
 ( Contexto ) Segundo a Organização Mundial da Saúde , as demências são um problema de custo social elevado , cujo manejo é um desafio para as próximas décadas . Demências comuns incluem a Doença de Alzheimer ( DA ) , bastante conhecida . Outra síndrome menos conhecida , o Comprometimento Cognitivo Leve ( CCL ) , é relevante por ser o estágio inicial clinicamente definido da DA . Embora o CCL não seja tão conhecido do público , pessoas com um tipo especial dessa síndrome , o CCL amnéstico , evoluem para a DA a uma taxa bastante maior que a da população em geral . O diagnóstico das demências e síndromes relacionadas é feito com base na análise de aspectos linguísticos e cognitivos do paciente . Testes clássicos incluem testes de fluência , nomeação , e repetição . Entretanto , pesquisas recentes têm reconhecido cada vez mais a importância da análise da produção discursiva , especialmente de narrativas , como uma alternativa mais adequada , principalmente para a detecção do CCL . ( Lacuna ) Enquanto uma análise qualitativa do discurso pode revelar o tipo da doença apresentada pelo paciente , uma análise quantitativa é capaz de revelar a intensidade do dano cerebral existente . A grande dificuldade de análises quantitativas de discurso é sua exigência de esforços : o processo de análise rigorosa e detalhada da produção oral é bastante laborioso , o que dificulta sua adoção em larga_escala  . Nesse cenário , análises computadorizadas despontam como uma solução de interesse . Ferramentas de análise automática de discurso com vistas ao diagnóstico de demências de linguagem já existem para o inglês , mas nenhum trabalho nesse sentido foi feito para o português até o presente momento . ( Objetivo ) Este projeto visa criar um ambiente unificado , intitulado Coh-Metrix-Dementia , que se valerá de recursos e ferramentas de Processamento de Línguas_Naturais  ( PLN ) e de Aprendizado de Máquina para possibilitar a análise e o reconhecimento automatizados de demências , com foco inicial na DA e no CCL . ( Hipótese ) Tendo como base o ambiente Coh-Metrix adaptado para o português do Brasil , denominado Coh-Metrix-Port , e incluindo a adaptação para o português e inserção de vinte e cinco novas métricas para calcular a complexidade sintática , a densidade de ideias , e a coerência textual , via semântica latente , é possível classificar narrativas de sujeitos normais , com DA , e com CCL , em uma abordagem de aprendizado de máquina , com precisão comparável a dos testes clássicos . ( Conclusão ) Nos resultados experimentais , foi possível separar os pacientes entre controles , CCL , e DA com medida_F  de 81,7 % , e separar controles e CCL com medida_F  de 90 % . Os resultados indicam que o uso das métricas da ferramenta Coh-Metrix-Dementia é bastante promissor como recurso na detecção precoce de declínio nas habilidades de linguagem 
 Propõe-se um método de construção arquitetural de espaços virtuais , baseado na sobreposição de diversos modos de pensar o espaço e no uso de estruturas da mnemotécnica . Para essa tarefa , faz-se , inicialmente , uma análise crítica do estado da arte em dois campos disciplinares recorrentemente envolvidos na construção de tais ambientes : Interface Usuário-Computador e Realidade Virtual . Em seguida apresenta-se a maneira de se sobrepor diversos modos de pensar o espaço em Arquitetura , a qual constitui o terceiro campo disciplinar desse estudo , explicitado através da introdução e da análise de diferentes exemplos de concepções arquitetônicas pertencentes ao âmbito virtual . Definido como pertencente à mente , o espaço virtual é concebido através de métodos arquitetônicos , acrescidos do uso da Mnemônica , que é a arte e a técnica de se desenvolver e fortalecer a memória . O processo de memorização torna-se central no processo de construção do contexto virtual . Introduzido através de exemplos na história e de recentes aplicações artísticas em realidade virtual , a tese propõe a inclusão da mnemotécnica para a estruturação espacial arquitetônica no 
 A detecção de objetos , pertencentes a uma determinada classe , em vídeos é de uma atividade amplamente estudada devido às aplicações potenciais que ela implica . Por exemplo , para vídeos obtidos por uma câmera estacionária , temos aplicações como segurança ou vigilância do tráfego , e por uma câmera dinâmica , para assistência ao condutor , entre outros . Na literatura , há diferentes métodos para tratar indistintamente cada um dos casos mencionados , e que consideram só imagens obtidas por um único tipo de câmera para treinar os detectores . Isto pode_levar  a uma baixa performance quando se aplica a técnica em vídeos de diferentes_tipos  de câmeras . O estado da arte na detecção de objetos de apenas uma classe , mostra uma tendência pelo uso de histogramas , treinamento supervisionado e , basicamente , seguem a seguinte estrutura : construção do modelo da classe de objeto , detecção de candidatos em uma imagem/quadro , e aplicação de uma medida sobre esses candidatos . Outra desvantagem observada é o uso de diferentes modelos para cada linha de visada de um objeto , gerando muitos modelos e , em alguns_casos  , um classificador para cada linha de visada . Nesta dissertação , abordamos o problema de detecção de objetos , usando um modelo da classe do objeto criada com um conjunto de dados de imagens estáticas e posteriormente usamos o modelo para detectar objetos na seqüência de imagens ( vídeos ) que foram coletadas a partir de câmeras estacionárias e dinâmicas , ou seja , num cenário totalmente diferente do usado para o treinamento . A criação do modelo é feita em uma fase de aprendizagem off-line , utilizando o conjunto de imagens PASCAL 2007 . O modelo baseia-se em uma mistura de modelos baseados em partes deformáveis ( MDPM ) , originalmente proposto por Felzenszwalb et_al  . ( 2010b ) no âmbito da detecção de objetos em imagens . Não limitamos o modelo para uma determinada linha de visada . Foi elaborado um conjunto de experimentos que exploram o melhor número de componentes da mistura e o número de partes do modelo . Além_disso  , foi realizado um estudo comparativo de MDPMs simétricas e assimétricas . Testamos esse método para detectar objetos como pessoas e carros em vídeos obtidos por câmera estacionária e dinâmica . Nossos resultados não mostram apenas o bom_desempenho  da MDPM e melhores_resultados  que o estado da arte na detecção de objetos em vídeos obtidos por câmeras estacionárias ou dinâmicas , mas também mostram o melhor número de componentes da mistura e as partes para o modelo criado . Finalmente , os resultados mostram algumas diferenças entre as MDPMs simétricas e assimétricas na detecção de objetos em diferentes vídeos 
 Dois problemas combinatórios são estudados : ( i ) determinar a quantidade de cópias de um hipergrafo fixo em um hipergrafo uniforme pseudoaleatório , e ( ii ) estimar números de Ramsey de ordem dois e três para grafos com largura de banda pequena e grau máximo limitado . Apresentamos um lema de contagem para estimar a quantidade de cópias de um hipergrafo k-uniforme linear livre de conectores ( conector é uma generalização de triângulo , para hipergrafos ) que estão presentes em um hipergrafo esparso pseudoaleatório G. Considere um hipergrafo k-uniforme linear H que é livre de conectores e um hipergrafo k-uniforme G com n vértices . Seja d_H=\max\ { \delta ( J ) : J\subset H\ } e D_H=\min\ { k d_H , \Delta ( H ) \ } . Estabelecemos que , se os vértices de G não possuem grau grande , famílias pequenas de conjuntos de k-1 elementos de V ( G ) não possuem vizinhança comum grande , e a maioria dos pares de conjuntos em { V ( G ) \choose k-1 } possuem a quantidade `` correta '' de vizinhos , então a quantidade de imersões de H em G é ( 1+o ( 1 ) ) n^ { |V ( H ) | } p^ { |E ( H ) | } , desde que p\gg n^ { 1/D_H } e |E ( G ) |= { n\choose k } p. Isso generaliza um resultado de Kohayakawa , Rödl e Sissokho [ Embedding graphs with bounded degree in sparse pseudo\-random graphs , Israel J . Math . 139 ( 2004 ) , 93 -- 137 ] , que provaram que , para p dado como acima , esse lema de imersão vale para grafos , onde H é um grafo livre de triângulos . Determinamos assintoticamente os números de Ramsey de ordem dois e três para grafos bipartidos com largura de banda pequena e grau máximo limitado . Mais especificamente , determinamos assintoticamente o número de Ramsey de ordem dois para grafos bipartidos com largura de banda pequena e grau máximo limitado , e o número de Ramsey de ordem três para tais grafos , com a suposição adicional de que ambas as classes do grafo bipartido têm aproximadamente o mesmo tamanho 
 
 Segundo relatório disponibilizado pela World Health Organization ( WHO ) ( WHO , 2015 ) , 1,3 milhões de pessoas morrem todos os anos no mundo devido à acidentes de trânsito . Veículos inteligentes se mostram como uma proeminente solução para reduzir esse drástico número . Por isso , diversos grupos de pesquisa no mundo têm concentrado esforços para o desenvolvimento de pesquisa que viabilize o desenvolvimento desse tipo de tecnologia . Diversos são os requisitos necessários para que um veículo possa circular de forma completamente autônoma . Localização , mapeamento , reconhecimento de semáforos e placas de trânsito são apenas alguns dentre tantos . Para que um veículo trafegue nas vias de forma segura , ele precisa saber onde estão os agentes que coabitam o mesmo espaço . Depois que esses agentes são detectados é necessário predizer suas movimentações de forma a reduzir os riscos de colisão . Neste projeto propôs-se a construção de um sistema que visa detectar agentes ( obstáculos ) e realizar o rastreamento deles para estimar suas velocidades e localizações enquanto estiverem no campo de visão do veículo autônomo , assim possibilitando realizar o cálculo da chance de colisão de cada um desses obstáculos com o veículo autônomo . O sistema utiliza unicamente a informação provida por uma câmera estereoscópica . Os pontos da cena são agrupados utilizando a informação da 24-vizinhança , disparidade e um valor que corresponde a chance de fazerem parte de um obstáculo . Após o agrupamento , cada grupo é dado como um possível obstáculo , após checar a consistência desses obstáculos por dois frames consecutivos , o grupo , agora considerado um obstáculo passa a ser rastreado utilizando filtro de Kalman ( WELCH ; BISHOP , 1995 ) e para checar a correspondência de obstáculos ao longo de toda a sequência é utilizado o algoritmo de Munkres ( MUNKRES , 1957 ) . A detecção e o rastreamento foram avaliados quantitativamente e qualitativamente utilizando dados coletados no Campus II da USP de São Carlos , bem como o conjunto de dados KITTI ( GEIGER ; LENZ ; URTASUN , 2012 ) . Os resultados demonstram a eficiência do algoritmo tanto na detecção dos obstáculos como no rastreamento dos mesmos 
 Ao longo dos anos , o rápido avanço nas tecnologias computacionais e de comunicação vem alterando de forma significativa o modo com que a sociedade se comunica e conduz seus negócios . De forma análoga , mudanças vêm ocorrendo na maneira com que os recursos educacionais são projetados , desenvolvidos e disponibilizados aos aprendizes . Seguindo essa tendência , o desenvolvimento e a adoção de Recursos Educacionais Abertos ( REAs ) vêm ganhando cada vez mais adeptos em todo o mundo , como uma forma de ampliar o acesso ao conhecimento e melhorar a educação . De fato , a distribuição livre e aberta de recursos educacionais contribui para a disseminação de conhecimento e facilita o acesso à informação , além de promover a democratização do acesso à educação , beneficiando a sociedade como um todo . Embora REAs possam trazer benefícios e impacto sobre a educação , ainda existem muitos desafios para sua ampla produção e adoção . Um dos desafios enfrentados pelos desenvolvedores ( incluindo educadores e praticantes ) de REAs é produzir materiais de aprendizagem de qualidade , capazes de serem reusados e adaptados a diferentes contextos e situações de aprendizagem . Evidencia-se também a necessidade de mecanismos que propiciem o aumento da produtividade do processo de desenvolvimento e da qualidade dos REAs elaborados . Este trabalho tem como objetivo investigar o desenvolvimento de REAs e estabelecer abordagens flexíveis para apoiar efetivamente o projeto e a criação desses recursos . Nesse contexto , um método ágil para o desenvolvimento e disponibilização de REAs , AM-OER , foi estabelecido . O método é fundamentado em práticas da Engenharia de Software e práticas de projeto de aprendizagem ( Learning Design ) , incorporadas no desenvolvimento de REAs no intuito de melhorar a sua qualidade e facilitar o reúso e adaptação . O objetivo final do método é apoiar o desenvolvimento de REAs de qualidade , capazes de motivar e guiar os aprendizes no processo de construção de conhecimento . Avaliações empíricas preliminares foram conduzidas para validar o AM-OER por meio de sua aplicação no projeto e criação de cursos nos domínios de desenvolvimento de software_livre  e teste de software . Os resultados obtidos até o momento demonstram que o método é viável e eficaz no projeto e criação de REAs 
 Ao longo da última_década  , questões energéticas atraíram forte atenção da sociedade , chegando às infraestruturas de TI para processamento de dados . Agora , essas infraestruturas devem se ajustar a essa responsabilidade , adequando plataformas existentes para alcançar desempenho aceitável enquanto promovem a redução no consumo de energia . Considerado um padrão para o processamento de Big Data , o Apache Hadoop tem evoluído significativamente ao longo dos últimos_anos  , com mais de 60 versões lançadas . Implementando o paradigma de programação MapReduce juntamente com o HDFS , seu sistema de arquivos distribuídos , o Hadoop tornou-se um middleware tolerante a falhas e confiável para a computação_paralela  e distribuída para grandes_conjuntos  de dados . No entanto , o Hadoop pode perder desempenho com determinadas cargas de trabalho , resultando em elevado consumo de energia . Cada vez mais , usuários exigem que a sustentabilidade e o consumo de energia controlado sejam parte intrínseca de soluções de computação de alto_desempenho  . Nesta tese , apresentamos o HDFSH , um sistema de armazenamento híbrido para o HDFS , que usa uma combinação de discos rígidos e discos de estado sólido para alcançar maior desempenho , promovendo economia de energia em aplicações usando Hadoop . O HDFSH traz ao middleware o melhor dos HDs ( custo acessível por GB e grande capacidade de armazenamento ) e SSDs ( alto_desempenho  e baixo consumo de energia ) de forma configurável , usando zonas de armazenamento dedicadas para cada dispositivo de armazenamento . Implementamos nosso mecanismo como uma política de alocação de blocos para o HDFS e o avaliamos em seis versões recentes do Hadoop com diferentes arquiteturas de software . Os resultados indicam que nossa abordagem aumenta o desempenho geral das aplicações , enquanto diminui o consumo de energia na maioria das configurações híbridas avaliadas . Os resultados também mostram que , em muitos_casos  , armazenar apenas uma parte dos dados em SSDs resulta em economia significativa de energia e aumento na velocidade de 
